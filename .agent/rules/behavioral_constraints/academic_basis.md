# 学術的根拠

> BC の理論的基盤。各 BC がなぜ必要かの学術的裏付け。

## 認識の公理（Epistemic Humility）

| 理論 | 核心 | この公理への帰結 |
|:-----|:-----|:---------------|
| **Grice (1975) 協調原則 — Maxim of Quantity** | 会話の参加者は必要な量の情報を必要なだけ提供する | **明示されている = 必要だから書かれている = 省略するな** |
| **Sperber & Wilson (1986) 関連性理論** | 明示されない情報にこそ推論すべき暗黙の意図がある | **「なぜこの手順があるのか」を考えろ** |
| **Flavell (1976) メタ認知理論** | 認知の認知 — 「自分が何を知っているかを知る」 | **3層構造で認識の限界を構造化せよ** |
| **Creator's Formulation (2026-02-09)** | 知性のある主体は無駄なことを伝えない | **手順を無視することは、書き手の知性に対する冒涜** |

## 3層メタ認知構造

> **導出**: Flavell (1976) + パプ君 DR (2026-02-09) Section A1-A3

| Layer | 問い | 行動 | 対応する BC |
|:------|:-----|:-----|:----------|
| **L1: Metamemory** | 「回答がデータ内にあるか」 | RAG・ベクトル検索で確認 | BC-4 (Vector Search) |
| **L2: Metacognition** | 「現在の生成は正確か」 | Self-verification / FaR | BC-14 (FaR) |
| **L3: Epistemic Metarepresentation** | 「このドメイン全体で何が未知か」 | 知識境界マッピング | BC-6 (Confidence) |

> **Dunning-Kruger 対策**: logit の高さ ≠ 知識の確実性。「知っているつもり」は最も危険な状態。

## BC-3 情報理論的根拠

> **確認は、しすぎるくらいが丁度いい。なぜなら、主体は「無知に無知」だから。** — Creator (2026-02-09)

| 参照段階 | 情報劣化 | リスク |
|:---------|:--------|:------|
| 一次ソース（WF本体, 論文原文） | 0% | なし |
| 二次情報（description, サマリー） | 不可視の劣化 | 「わかったつもり」で誤りを見逃す |
| 三次引用（記憶, 推定） | さらに劣化 | **存在しない内容を「確認済み」と記録する** |

**実証データ (2026-02-09)**: 逆引き検索3件中1件 (33%) で二次情報の arXiv ID が誤っていた。

## BC-6 確信度キャリブレーション

> **キャリブレーション注記 (MP: arXiv:2308.05342)**:
> メタ認知プロンプティング研究で、LLM が「高確信」と表明しつつ不正解の割合 (False Positive) は **32.5%**。
> 確信度ラベルは**主観的確信**であり、**客観的正確さ**とは独立。

## BC-9 メタ認知レイヤー

> **導出**: MP 研究 (arXiv:2308.05342) — 5段階メタ認知が LLM 推論を有意に改善

```python
from mekhane.fep.metacognitive_layer import run_full_uml
report = run_full_uml(context="入力", output="出力", wf_name="noe", confidence=75.0)
print(report.summary)  # UML [noe]: ✅ PASS (5/5)
```

## BC-13 言語選択

| 理論 | 核心 | 帰結 |
|:-----|:-----|:-----|
| Shi et al. (2022) | 英語は数学・論理で+8-15% | 参考値。日本語での深い理解が補う可能性 |
| /noe 比較実験 | 日本語は意味分析・WHY に優位 | 日本語デフォルトの根拠 |
| Wang et al. (2025) | 言語がLLM推論パターンを決定 | 言語選択は認知に影響する |

## BC-14 自己反省

> **導出**: Qin et al. (2024) FaR (ACL Findings 2024): ECE -90%。
> Gu et al. (2023) Metacognitive Prompting (arXiv:2308.05342): NLU +15-25%。
