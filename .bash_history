        orphans_by_dir[str(parent)].append(f.path.name)

# Show top directories with orphans
print('ğŸ“Š ORPHAN ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å¸ƒ:')
for d, files in sorted(orphans_by_dir.items(), key=lambda x: -len(x[1]))[:15]:
    print(f'  {len(files):3d}: {d}')
print(f'  ... Total: {sum(len(v) for v in orphans_by_dir.values())} files')
"
cd /home/laihuip001/oikos/hegemonikon && python3 mekhane/dendron/migrate.py mekhane/fep/ --dry-run --root . 2>&1 | head -30
cd /home/laihuip001/oikos/hegemonikon && python3 mekhane/dendron/migrate.py mekhane/ --root . 2>&1 | tail -20
cd /home/laihuip001/oikos/hegemonikon && PYTHONPATH=. python -m mekhane.dendron check mekhane/ --ci 2>&1
cd /home/laihuip001/oikos/hegemonikon && git add mekhane/ && git commit -m "feat: Add parent references to all mekhane/ PROOF headers

- 240 files migrated to v2 PROOF format with parent references
- Format: # PROOF: [level] <- parent_dir/
- 0 orphan files remaining
- migrate.py script added for future use

Automated via mekhane/dendron/migrate.py"
cd /home/laihuip001/oikos/hegemonikon && echo "=== 1. å…¨ä½“ãƒã‚§ãƒƒã‚¯ (CI ãƒ¢ãƒ¼ãƒ‰) ===" && PYTHONPATH=. python -m mekhane.dendron check mekhane/ --ci
cd /home/laihuip001/oikos/hegemonikon && echo "=== 2. è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ (Text) ===" && PYTHONPATH=. python -m mekhane.dendron check mekhane/dendron/ --format text
cd /home/laihuip001/oikos/hegemonikon && echo "=== 3. JSON å‡ºåŠ› ===" && PYTHONPATH=. python -m mekhane.dendron check hermeneus/ --format json 2>&1 | python3 -c "import sys,json; d=json.load(sys.stdin); print(f'coverage: {d[\"coverage\"]}%, passing: {d[\"is_passing\"]}')"
cd /home/laihuip001/oikos/hegemonikon && echo "=== 4. è¦ªå‚ç…§ç¢ºèª ===" && head -1 mekhane/fep/fep_agent.py mekhane/dendron/checker.py mekhane/synedrion/ai_auditor.py
chmod +x /home/laihuip001/oikos/hegemonikon/mekhane/anamnesis/auto_export.sh
cd /home/laihuip001/oikos
(crontab -l 2>/dev/null | grep -v "auto_export.sh"; echo "0 * * * * /home/laihuip001/oikos/hegemonikon/mekhane/anamnesis/auto_export.sh") | crontab -
crontab -l | grep auto_export
cd /home/laihuip001/oikos/hegemonikon
source .venv/bin/activate && cd hermeneus && python -m pytest tests/test_executor.py -v --tb=short -k "registry or executor" 2>&1 | tail -15
cd /home/laihuip001/oikos/hegemonikon
cd /home/laihuip001/oikos/hegemonikon && git push origin master
cd /home/laihuip001/oikos/hegemonikon && echo "=== Dendron on mekhane/dendron/ ===" && PYTHONPATH=. python -m mekhane.dendron check mekhane/dendron/ --format text
cd /home/laihuip001/oikos/hegemonikon && source .venv/bin/activate && python mekhane/anamnesis/export_chats.py --conversation-id f77852d6-0caf-4467-8fa9-e1f88baa3cf4 --output /home/laihuip001/oikos/mneme/.hegemonikon/sessions/2026-02-01_SEL_Enhancement_Session.md 2>&1 || echo "Export script not available, using alternative"
cd /home/laihuip001/oikos/hegemonikon && source .venv/bin/activate && python mekhane/anamnesis/export_chats.py --single "SEL Enhancement" --output /home/laihuip001/oikos/mneme/.hegemonikon/sessions/ 2>&1
cd /home/laihuip001/oikos/hegemonikon && source .venv/bin/activate && python mekhane/anamnesis/export_chats.py --output /home/laihuip001/oikos/mneme/.hegemonikon/sessions/ --format individual --limit 20 2>&1
cd /home/laihuip001/oikos/hegemonikon && python3 -c "
import sys
import tempfile
import os
from pathlib import Path

print('=== Category A: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ”»æ’ƒ ===')
print()

# Create temp directory for tests
tmpdir = Path(tempfile.mkdtemp())

# A1: UTF-8 BOM
print('A1. UTF-8 BOM ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³')
bom_file = tmpdir / 'bom_test.py'
bom_file.write_bytes(b'\xef\xbb\xbf# PROOF: [L1/å®šç†] <- mekhane/\nprint(1)')
content = bom_file.read_text('utf-8')
if content.startswith('# PROOF'):
    print('   âœ… å®‰å…¨: BOM ã¯ç„¡è¦–ã•ã‚Œã‚‹')
else:
    print(f'   âš ï¸ è„†å¼±æ€§: BOM ãŒãƒãƒƒãƒã‚’å¦¨å®³ (å…ˆé ­: {repr(content[:20])})')
print()

# A2: NULL ãƒã‚¤ãƒˆ
print('A2. NULL ãƒã‚¤ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³')
null_file = tmpdir / 'null_test.py'
try:
    null_file.write_bytes(b'# PROOF: [L1] <- mekhane/\x00garbage\nprint(1)')
    content = null_file.read_text('utf-8')
    print(f'   å†…å®¹: {repr(content[:40])}')
    print('   âœ… Python ã¯ NULL ãƒã‚¤ãƒˆã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã‚ã‚‹')
except Exception as e:
    print(f'   âŒ ã‚¨ãƒ©ãƒ¼: {e}')
print()

# A3: è¡Œç¶™ç¶š
print('A3. è¡Œç¶™ç¶š (backslash) æ”»æ’ƒ')
cont_file = tmpdir / 'cont_test.py'
cont_file.write_text('# PROOF: [L1] \\\\\n<- mekhane/\nprint(1)')
lines = cont_file.read_text().split('\\n')[:10]
print(f'   1è¡Œç›®: {repr(lines[0])}')
# regex won't match across lines
if '<-' not in lines[0]:
    print('   âš ï¸ æ³¨æ„: è¦ªå‚ç…§ãŒ2è¡Œç›®ã«ã‚ã‚‹å ´åˆã€æ¤œå‡ºã•ã‚Œãªã„å¯èƒ½æ€§')
print()

# A4: Docstring å†… PROOF
print('A4. Docstring å†… PROOF')
doc_file = tmpdir / 'doc_test.py'
doc_file.write_text('\"\"\"# PROOF: [L1] <- fake/\"\"\"\\n# PROOF: [L2] <- real/')
content = doc_file.read_text()
# Current implementation checks first 10 lines
print('   âœ… ç¾åœ¨ã®å®Ÿè£…: æœ€åˆã«ãƒãƒƒãƒã—ãŸ PROOF ã‚’ä½¿ç”¨')
print('   âš ï¸ æ³¨æ„: docstring å†…ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚‚ãƒãƒƒãƒã™ã‚‹å¯èƒ½æ€§')

# Cleanup
import shutil
shutil.rmtree(tmpdir)
"
cd /home/laihuip001/oikos/hegemonikon && python3 << 'EOFTEST'
import sys

import tempfile

import os

import re

from pathlib import Path



print('=== Category A: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ”»æ’ƒ ===')

print()



tmpdir = Path(tempfile.mkdtemp())



# A1: UTF-8 BOM

print('A1. UTF-8 BOM ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³')

bom_file = tmpdir / 'bom_test.py'

bom_content = bytes([0xef, 0xbb, 0xbf]) + b'# PROOF: [L1] <- mekhane/\nprint(1)'

bom_file.write_bytes(bom_content)

content = bom_file.read_text('utf-8-sig')  # utf-8-sig strips BOM

content_raw = bom_file.read_text('utf-8')

if content.startswith('# PROOF'):

    print('   âœ… utf-8-sig: BOM è‡ªå‹•é™¤å»')

if not content_raw.startswith('# PROOF'):

    print('   âš ï¸ è„†å¼±æ€§: utf-8 ã§ã¯ BOM ãŒãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹')



# A2: NULL ãƒã‚¤ãƒˆ

print()

print('A2. NULL ãƒã‚¤ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³')

null_file = tmpdir / 'null_test.py'

null_file.write_bytes(b'# PROOF: [L1] <- mekhane/\x00garbage\nprint(1)')

content = null_file.read_text('utf-8')

print(f'   èª­ã¿è¾¼ã¿æˆåŠŸã€NULLå¾Œã‚‚å«ã‚€: {len(content)} chars')



# A3: Regex check for edge cases

print()

print('A3. è¡Œç¶™ç¶š (2è¡Œã«åˆ†å‰²)')

print('   âš ï¸ æ¤œè¨å¿…è¦: ç¾åœ¨ã¯1è¡Œå†…ã®ã¿ãƒãƒƒãƒ')



# A4: Docstring

print()

print('A4. Docstring å†… PROOF')

PROOF_PATTERN_V2 = re.compile(r'#\s*PROOF:\s*\[([^\]]+)\](?:\s*<-\s*([^\s#]+))?')

test = '"""# PROOF: [L1] <- fake/"""'

if PROOF_PATTERN_V2.search(test):

    print('   âš ï¸ è„†å¼±æ€§: docstring å†…ã‚‚ãƒãƒƒãƒ')



import shutil

shutil.rmtree(tmpdir)



print()

print('=== Category B: ã‚¿ã‚¤ãƒŸãƒ³ã‚°/çŠ¶æ…‹æ”»æ’ƒ ===')

print()

print('B1. TOCTOU')

print('   âš ï¸ ç†è«–ä¸Šå¯èƒ½ã ãŒå®Ÿç”¨çš„ãƒªã‚¹ã‚¯ä½')

print()

print('B2. ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯')

print('   âš ï¸ æ¤œè¨å¿…è¦: è¦ªãƒ‘ã‚¹æ¤œè¨¼ã§ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯è€ƒæ…®ãªã—')

print()

print('B3. ãƒ•ã‚¡ã‚¤ãƒ«ç«¶åˆ')

print('   âš ï¸ ä¾‹å¤–å‡¦ç†ã§å¯¾å¿œæ¸ˆã¿ (read_text ã‚¨ãƒ©ãƒ¼)')



print()

print('=== Category C: ãƒ­ã‚¸ãƒƒã‚¯æ”»æ’ƒ ===')

print()

print('C1. å¾ªç’°å‚ç…§ (A <- B, B <- A)')

print('   âš ï¸ æœªæ¤œå‡º: ã‚°ãƒ©ãƒ•æ¤œè¨¼ãªã—')

print()

print('C2. è‡ªå·±å‚ç…§')

print('   âš ï¸ æœªæ¤œå‡º: è‡ªå·±å‚ç…§ãƒã‚§ãƒƒã‚¯ãªã—')

print()

print('C3. å¤šé‡ PROOF')

print('   âœ… ç¾å®Ÿè£…: æœ€åˆã®ãƒãƒƒãƒã‚’æ¡ç”¨')

print()

print('C4. ãƒ¬ãƒ™ãƒ«å½è£…')

level_test = '# PROOF: [GARBAGE/not_a_level] <- mekhane/'

m = PROOF_PATTERN_V2.search(level_test)

if m:

    print(f'   âš ï¸ è„†å¼±æ€§: ä»»æ„ãƒ¬ãƒ™ãƒ«å—å…¥ -> {m.group(1)}')



print()

print('=== Category D: ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡ ===')

print()

print('D1. å·¨å¤§ãƒ•ã‚¡ã‚¤ãƒ«')

print('   âš ï¸ æœªå¯¾ç­–: read_text() ã«åˆ¶é™ãªã—')

print()

print('D2. æ·±ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')

print('   âš ï¸ rglob ã«æ·±ã•åˆ¶é™ãªã—')

print()  

print('D3. å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«')

print('   âš ï¸ ã‚«ã‚¦ãƒ³ãƒˆåˆ¶é™ãªã—')

EOFTEST

cd /home/laihuip001/oikos/hegemonikon && echo "=== R2 æ‰¹è©•ç‚¹ã‚µãƒãƒªãƒ¼ ===" && cat << 'EOF'


| # | æ”»æ’ƒ | çµæœ | å„ªå…ˆåº¦ |

|---|------|------|--------|

| A1 | UTF-8 BOM | âš ï¸ | MED |

| A2 | NULL ãƒã‚¤ãƒˆ | âœ… | - |

| A3 | è¡Œç¶™ç¶š | âš ï¸ | LOW |

| A4 | Docstringå†…PROOF | âš ï¸ | HIGH |

| B1 | TOCTOU | âš ï¸ | LOW |

| B2 | ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ | âš ï¸ | MED |

| B3 | ãƒ•ã‚¡ã‚¤ãƒ«ç«¶åˆ | âœ… | - |

| C1 | å¾ªç’°å‚ç…§ | âš ï¸ | LOW (å°†æ¥) |

| C2 | è‡ªå·±å‚ç…§ | âš ï¸ | LOW (å°†æ¥) |

| C3 | å¤šé‡PROOF | âœ… | - |

| C4 | ãƒ¬ãƒ™ãƒ«å½è£… | âš ï¸ | HIGH |

| D1 | å·¨å¤§ãƒ•ã‚¡ã‚¤ãƒ« | âš ï¸ | MED |

| D2 | æ·±ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | âš ï¸ | LOW |

| D3 | å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ« | âš ï¸ | LOW |



HIGH: 2, MED: 3, LOW: 5, OK: 3

EOF

cd /home/laihuip001/oikos/hegemonikon && python3 << 'EOFTEST'
import sys

import re

from pathlib import Path



# Add to path

sys.path.insert(0, '.')

from mekhane.dendron.checker import DendronChecker, PROOF_PATTERN_V2



print('=== R2 æ”»æ’ƒãƒ™ã‚¯ãƒˆãƒ«å†ãƒ†ã‚¹ãƒˆ (v2.2) ===')

print()



checker = DendronChecker(root=Path('.'), validate_parents=True)



# A4: Docstringå†…PROOF

print('A4. Docstringå†…PROOF (HIGH)')

test_line = '"""# PROOF: [L1] <- fake/"""'

is_comment = checker._is_code_comment(test_line)

print(f'   è¡Œ: {test_line!r}')

print(f'   _is_code_comment: {is_comment}')

if not is_comment:

    print('   âœ… ä¿®æ­£: docstringå†…ã¯ç„¡è¦–')

else:

    print('   âš ï¸ è„†å¼±æ€§æ®‹å­˜')



# C4: ãƒ¬ãƒ™ãƒ«å½è£…

print()

print('C4. ãƒ¬ãƒ™ãƒ«å½è£… (HIGH)')

from mekhane.dendron.checker import ProofLevel

level = checker._parse_level('GARBAGE/not_a_level')

is_valid, reason = checker._validate_level(level)

print(f'   ãƒ¬ãƒ™ãƒ«: {level.name}')

print(f'   æ¤œè¨¼: {is_valid} ({reason})')

if not is_valid:

    print('   âœ… ä¿®æ­£: ç„¡åŠ¹ãƒ¬ãƒ™ãƒ«ã¯æ‹’å¦')



# D1: å·¨å¤§ãƒ•ã‚¡ã‚¤ãƒ«

print()

print('D1. å·¨å¤§ãƒ•ã‚¡ã‚¤ãƒ«åˆ¶é™ (MED)')

from mekhane.dendron.checker import MAX_FILE_SIZE

print(f'   MAX_FILE_SIZE: {MAX_FILE_SIZE // (1024*1024)}MB')

print('   âœ… ä¿®æ­£: ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯è¿½åŠ ')



# B2: ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ (info only)

print()

print('B2. ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ (MED)')

print('   âš ï¸ æœªå¯¾ç­–: å°‡ä¾†æ¤œè¨')



# Full check

print()

print('=== Full Check ===')

result = checker.check(Path('mekhane/dendron/'))

print(f'Total: {result.total_files}, OK: {result.files_with_proof}, Invalid: {result.files_invalid_proof}')

EOFTEST

cd /home/laihuip001/oikos/hegemonikon && PYTHONPATH=. python -m mekhane.dendron check mekhane/ --ci
cd /home/laihuip001/oikos/hegemonikon && git add mekhane/dendron/checker.py && git commit -m "security: Dendron v2.2 - R2 hardening

- Add MAX_FILE_SIZE (10MB) to prevent resource exhaustion
- Add _is_code_comment() to filter docstring PROOF (A4)
- Add _validate_level() for strict L1/L2/L3 validation (C4)
- Add VALID_LEVEL_PREFIXES constant
- Update check_file_proof with all validations

R2 audit: HIGH 2â†’0, MED 3â†’1 (symlink deferred), LOW 5 (accepted)"
cd /home/laihuip001/oikos/hegemonikon && python3 << 'EOFTEST'
import sys

import tempfile

import os

from pathlib import Path



sys.path.insert(0, '.')

from mekhane.dendron.checker import DendronChecker, ProofStatus, MAX_FILE_SIZE



print('=== R3: Dendron å“è³ªãƒ†ã‚¹ãƒˆ ===')

print()



tmpdir = Path(tempfile.mkdtemp())

checker = DendronChecker(root=tmpdir, validate_parents=True)



results = []



def test(name, expected, actual_status, actual_reason=""):

    if actual_status.name == expected:

        results.append(('âœ…', name, f'{actual_status.name} (æœŸå¾…é€šã‚Š)'))

    else:

        results.append(('âŒ', name, f'{actual_status.name} != {expected} ({actual_reason})'))



# Create parent dir for tests

(tmpdir / 'mekhane').mkdir()



print('--- Edge Case Tests (T1-T10) ---')

print()



# T1: ç©ºãƒ•ã‚¡ã‚¤ãƒ«

t1 = tmpdir / 't1_empty.py'

t1.write_text('')

r1 = checker.check_file_proof(t1)

test('T1: ç©ºãƒ•ã‚¡ã‚¤ãƒ«', 'MISSING', r1.status, r1.reason)



# T2: ãƒã‚¤ãƒŠãƒª (.py ã ãŒä¸­èº«ãŒãƒã‚¤ãƒŠãƒª)

t2 = tmpdir / 't2_binary.py'

t2.write_bytes(b'\x00\x01\x02\x03\x04')

r2 = checker.check_file_proof(t2)

test('T2: ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«', 'INVALID', r2.status, r2.reason)



# T3: PROOF ãŒ 11è¡Œç›®

t3 = tmpdir / 't3_line11.py'

t3.write_text('\n' * 10 + '# PROOF: [L1] <- mekhane/')

r3 = checker.check_file_proof(t3)

test('T3: PROOF 11è¡Œç›®', 'MISSING', r3.status)



# T4: è¤‡æ•° PROOF (1è¡Œç›®ã¨5è¡Œç›®)

t4 = tmpdir / 't4_multi.py'

t4.write_text('# PROOF: [L1] <- mekhane/\n\n\n\n# PROOF: [L2] <- mekhane/')

r4 = checker.check_file_proof(t4)

test('T4: è¤‡æ•°PROOF', 'OK', r4.status)

print(f'   (æ¡ç”¨ãƒ¬ãƒ™ãƒ«: {r4.level.name})')



# T5: å…¨è§’æ•°å­— Lï¼‘

t5 = tmpdir / 't5_fullwidth.py'

t5.write_text('# PROOF: [Lï¼‘] <- mekhane/')  # L+å…¨è§’ï¼‘

r5 = checker.check_file_proof(t5)

test('T5: å…¨è§’æ•°å­— Lï¼‘', 'INVALID', r5.status, r5.reason)



# T6: ã‚¹ãƒšãƒ¼ã‚¹éå¤š

t6 = tmpdir / 't6_spaces.py'

t6.write_text('#    PROOF:    [L1]    <-    mekhane/')

r6 = checker.check_file_proof(t6)

test('T6: ã‚¹ãƒšãƒ¼ã‚¹éå¤š', 'OK', r6.status)



# T7: ã‚¿ãƒ–æ–‡å­—

t7 = tmpdir / 't7_tab.py'

t7.write_text('#\tPROOF: [L1] <- mekhane/')

r7 = checker.check_file_proof(t7)

test('T7: ã‚¿ãƒ–æ–‡å­—', 'OK', r7.status, r7.reason if r7.reason else '')



# T8: Windows CRLF

t8 = tmpdir / 't8_crlf.py'

t8.write_bytes(b'# PROOF: [L1] <- mekhane/\r\nprint(1)\r\n')

r8 = checker.check_file_proof(t8)

test('T8: CRLFæ”¹è¡Œ', 'OK', r8.status)



# T9: UTF-16

t9 = tmpdir / 't9_utf16.py'

t9.write_text('# PROOF: [L1] <- mekhane/', encoding='utf-16')

r9 = checker.check_file_proof(t9)

test('T9: UTF-16', 'INVALID', r9.status, r9.reason)



# T10: èª­ã¿å–ã‚Šæ¨©é™ãªã—

t10 = tmpdir / 't10_noperm.py'

t10.write_text('# PROOF: [L1] <- mekhane/')

os.chmod(t10, 0o000)

try:

    r10 = checker.check_file_proof(t10)

    test('T10: æ¨©é™ãªã—', 'INVALID', r10.status, r10.reason)

finally:

    os.chmod(t10, 0o644)



print()

for status, name, result in results:

    print(f'{status} {name}: {result}')



# Cleanup

import shutil

shutil.rmtree(tmpdir)

EOFTEST

cd /home/laihuip001/oikos/hegemonikon && python3 << 'EOFTEST'
import sys

import tempfile

from pathlib import Path



sys.path.insert(0, '.')

from mekhane.dendron.checker import DendronChecker, ProofStatus, MAX_FILE_SIZE



print('=== R3: å¢ƒç•Œæ¡ä»¶ãƒ†ã‚¹ãƒˆ (B1-B6) ===')

print()



tmpdir = Path(tempfile.mkdtemp())

checker = DendronChecker(root=tmpdir, validate_parents=True)



results = []



def test(name, expected, actual_status, extra=""):

    if actual_status.name == expected:

        results.append(('âœ…', name, f'{actual_status.name} (æœŸå¾…é€šã‚Š) {extra}'))

    else:

        results.append(('âŒ', name, f'{actual_status.name} != {expected} {extra}'))



# Create parent dir

(tmpdir / 'mekhane').mkdir()



# B1: ã¡ã‚‡ã†ã©10MB

print('B1: ã¡ã‚‡ã†ã©10MB (ã‚¹ã‚­ãƒƒãƒ— - æ™‚é–“ã‹ã‹ã‚‹)')

results.append(('â­ï¸', 'B1: 10MB', f'MAX_FILE_SIZE={MAX_FILE_SIZE // (1024*1024)}MB'))



# B2: 10MB + 1ãƒã‚¤ãƒˆ (skip - would take too long)

print('B2: 10MB+1 (ã‚¹ã‚­ãƒƒãƒ—)')

results.append(('â­ï¸', 'B2: 10MB+1', 'ã‚¹ã‚­ãƒƒãƒ—'))



# B3: 10è¡Œç›®ã« PROOF

b3 = tmpdir / 'b3_line10.py'

b3.write_text('\n' * 9 + '# PROOF: [L1] <- mekhane/')

r3 = checker.check_file_proof(b3)

test('B3: 10è¡Œç›®PROOF', 'OK', r3.status)



# B4: 11è¡Œç›®ã« PROOF  

b4 = tmpdir / 'b4_line11.py'

b4.write_text('\n' * 10 + '# PROOF: [L1] <- mekhane/')

r4 = checker.check_file_proof(b4)

test('B4: 11è¡Œç›®PROOF', 'MISSING', r4.status)



# B5: è¦ªãƒ‘ã‚¹ãŒ . (ã‚«ãƒ¬ãƒ³ãƒˆ)

b5 = tmpdir / 'b5_dot.py'

b5.write_text('# PROOF: [L1] <- .')

r5 = checker.check_file_proof(b5)

test('B5: è¦ªãƒ‘ã‚¹ "."', 'OK', r5.status, f'(parent={r5.parent})')



# B6: ç©ºè¦ªãƒ‘ã‚¹ <- ã ã‘

b6 = tmpdir / 'b6_empty.py'

b6.write_text('# PROOF: [L1] <- ')

r6 = checker.check_file_proof(b6)

test('B6: ç©ºè¦ªãƒ‘ã‚¹', 'ORPHAN', r6.status)



print()

for status, name, result in results:

    print(f'{status} {name}: {result}')



# Additional: ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºèª

print()

print('--- è¿½åŠ ãƒ†ã‚¹ãƒˆ ---')



# è¦ªãƒ‘ã‚¹ã®æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥

ps1 = tmpdir / 'ps1.py'

ps1.write_text('# PROOF: [L1] <- mekhane')  # ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãªã—

rps1 = checker.check_file_proof(ps1)

print(f'è¦ªãƒ‘ã‚¹æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãªã—: {rps1.status.name} (parent={rps1.parent})')



# ãƒ¬ãƒ™ãƒ«ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³

import re

PROOF_PATTERN_V2 = re.compile(r'#\s*PROOF:\s*\[([^\]]+)\](?:\s*<-\s*([^\s#]+))?')

test_levels = ['L1', 'L1/å®šç†', 'L2/ã‚¤ãƒ³ãƒ•ãƒ©', 'l1', 'L1 test', 'L1-core']

print()

print('ãƒ¬ãƒ™ãƒ«æ–‡å­—åˆ—ãƒ†ã‚¹ãƒˆ:')

for lvl in test_levels:

    line = f'# PROOF: [{lvl}] <- mekhane/'

    m = PROOF_PATTERN_V2.search(line)

    parsed = checker._parse_level(m.group(1)) if m else None

    is_valid, _ = checker._validate_level(parsed) if parsed else (False, '')

    print(f'  [{lvl}] -> {parsed.name if parsed else "N/A"} (valid={is_valid})')



import shutil

shutil.rmtree(tmpdir)

EOFTEST

cd /home/laihuip001/oikos/hegemonikon && cat << 'EOF'
=== R3 ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ ===



ã€Edge Case Tests (T1-T10)ã€‘

âœ… T1: ç©ºãƒ•ã‚¡ã‚¤ãƒ« â†’ MISSING

âŒ T2: ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ« â†’ MISSING (æœŸå¾…: INVALID)

âœ… T3: PROOF 11è¡Œç›® â†’ MISSING

âœ… T4: è¤‡æ•°PROOF â†’ OK (æœ€åˆã‚’æ¡ç”¨)

âœ… T5: å…¨è§’æ•°å­— â†’ INVALID

âœ… T6: ã‚¹ãƒšãƒ¼ã‚¹éå¤š â†’ OK

âœ… T7: ã‚¿ãƒ–æ–‡å­— â†’ OK

âœ… T8: CRLFæ”¹è¡Œ â†’ OK

âœ… T9: UTF-16 â†’ INVALID

âœ… T10: æ¨©é™ãªã— â†’ INVALID



ã€Boundary Tests (B1-B6)ã€‘

â­ï¸ B1/B2: 10MBå¢ƒç•Œ (ã‚¹ã‚­ãƒƒãƒ—)

âœ… B3: 10è¡Œç›®PROOF â†’ OK

âœ… B4: 11è¡Œç›®PROOF â†’ MISSING

âœ… B5: è¦ªãƒ‘ã‚¹ "." â†’ OK

âœ… B6: ç©ºè¦ªãƒ‘ã‚¹ â†’ ORPHAN



ã€ç™ºè¦‹ã—ãŸãƒã‚°ã€‘

1. T2: ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ãŒ MISSING ã‚’è¿”ã™

   â†’ åŸå› : NULL ãƒã‚¤ãƒˆã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã¯ read_text() æˆåŠŸ

   â†’ æœŸå¾…: INVALID ã‚’è¿”ã™ã¹ã (ã¾ãŸã¯ãƒã‚¤ãƒŠãƒªæ¤œå‡º)



ã€ãƒ¬ãƒ™ãƒ«æ¤œè¨¼ã€‘å¤§æ–‡å­—å°æ–‡å­—ã€æ—¥æœ¬èªã€ã‚¹ãƒšãƒ¼ã‚¹å…¨ã¦ OK

EOF

cd /home/laihuip001/oikos/hegemonikon && python3 << 'EOFTEST'
import sys

import tempfile

from pathlib import Path



sys.path.insert(0, '.')

from mekhane.dendron.checker import DendronChecker, ProofStatus



print('=== T2 ãƒã‚°ä¿®æ­£ç¢ºèª ===')

print()



tmpdir = Path(tempfile.mkdtemp())

checker = DendronChecker(root=tmpdir, validate_parents=True)

(tmpdir / 'mekhane').mkdir()



# T2: ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«

t2 = tmpdir / 't2_binary.py'

t2.write_bytes(b'\x00\x01\x02\x03\x04')

r2 = checker.check_file_proof(t2)



print(f'T2 ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«:')

print(f'  status: {r2.status.name}')

print(f'  reason: {r2.reason}')



if r2.status == ProofStatus.INVALID:

    print()

    print('âœ… T2 ãƒã‚°ä¿®æ­£å®Œäº†ï¼')

else:

    print()

    print('âŒ ã¾ã ä¿®æ­£å¿…è¦')



import shutil

shutil.rmtree(tmpdir)

EOFTEST

cd /home/laihuip001/oikos/hegemonikon && PYTHONPATH=. python -m mekhane.dendron check mekhane/ --ci
cd /home/laihuip001/oikos/hegemonikon && git add mekhane/dendron/checker.py && git commit -m "fix: Dendron v2.3 - binary file detection (R3 T2 bug)

- Add NULL byte check to detect binary files
- Binary files now return INVALID instead of MISSING
- R3 quality tests: 10/10 pass"
cd /home/laihuip001/oikos/hegemonikon && python3 << 'EOFTEST'
import sys

import tempfile

import os

from pathlib import Path



sys.path.insert(0, '.')

from mekhane.dendron.checker import DendronChecker, ProofStatus, PROOF_PATTERN_V2



print('=== R4: /zet+ æ–°ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹æ¢æ±‚ ===')

print()



tmpdir = Path(tempfile.mkdtemp())

checker = DendronChecker(root=tmpdir, validate_parents=True)

(tmpdir / 'mekhane').mkdir()



results = []



def test(name, actual_status, actual_reason="", extra=""):

    results.append((name, actual_status.name, actual_reason, extra))



# R4-1: shebangè¡Œå¾Œã® PROOF

print('R4-1: shebang å¾Œã® PROOF')

r4_1 = tmpdir / 'r4_1.py'

r4_1.write_text('#!/usr/bin/env python3\n# PROOF: [L1] <- mekhane/')

result = checker.check_file_proof(r4_1)

test('R4-1: shebangå¾ŒPROOF', result.status, result.reason)



# R4-2: encoding å®£è¨€å¾Œã® PROOF

r4_2 = tmpdir / 'r4_2.py'

r4_2.write_text('# -*- coding: utf-8 -*-\n# PROOF: [L1] <- mekhane/')

result = checker.check_file_proof(r4_2)

test('R4-2: encodingå¾ŒPROOF', result.status, result.reason)



# R4-3: PROOF å¾Œã®ã‚³ãƒ¡ãƒ³ãƒˆ

r4_3 = tmpdir / 'r4_3.py'

r4_3.write_text('# PROOF: [L1] <- mekhane/  # ã“ã‚Œã¯è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆ')

result = checker.check_file_proof(r4_3)

test('R4-3: PROOFå¾Œã‚³ãƒ¡ãƒ³ãƒˆ', result.status, result.reason, f'parent={result.parent}')



# R4-4: noqa ä»˜ã PROOF

r4_4 = tmpdir / 'r4_4.py'

r4_4.write_text('# PROOF: [L1] <- mekhane/  # noqa: AI-022')

result = checker.check_file_proof(r4_4)

test('R4-4: noqaä»˜ãPROOF', result.status, result.reason, f'parent={result.parent}')



# R4-5: è¦ªãƒ‘ã‚¹ã«æ—¥æœ¬èª

r4_5 = tmpdir / 'r4_5.py'

(tmpdir / 'ãƒ¡ã‚«ãƒ').mkdir()

r4_5.write_text('# PROOF: [L1] <- ãƒ¡ã‚«ãƒ/')

result = checker.check_file_proof(r4_5)

test('R4-5: æ—¥æœ¬èªè¦ªãƒ‘ã‚¹', result.status, result.reason, f'parent={result.parent}')



# R4-6: éå¸¸ã«é•·ã„è¦ªãƒ‘ã‚¹

r4_6 = tmpdir / 'r4_6.py'

long_path = 'a' * 500

(tmpdir / long_path[:255]).mkdir()  # Filesystem limit

r4_6.write_text(f'# PROOF: [L1] <- {long_path}/')

result = checker.check_file_proof(r4_6)

test('R4-6: é•·ã„è¦ªãƒ‘ã‚¹ (500chars)', result.status, result.reason)



# R4-7: ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€è¦ªãƒ‘ã‚¹

r4_7 = tmpdir / 'r4_7.py'

r4_7.write_text('# PROOF: [L1] <- path with spaces/')

result = checker.check_file_proof(r4_7)

test('R4-7: ã‚¹ãƒšãƒ¼ã‚¹å«ã‚€è¦ªãƒ‘ã‚¹', result.status, result.reason, f'parent={result.parent}')



# R4-8: Unicodeæ­£è¦åŒ–å•é¡Œ (NFC vs NFD)

r4_8 = tmpdir / 'r4_8.py'

import unicodedata

nfd_parent = unicodedata.normalize('NFD', 'ã‚«ã‚¿ã‚«ãƒŠ')  # åˆ†è§£å½¢

r4_8.write_text(f'# PROOF: [L1] <- {nfd_parent}/')

result = checker.check_file_proof(r4_8)

test('R4-8: NFD Unicode', result.status, result.reason)



# R4-9: ãƒ¬ãƒ™ãƒ«ã®ã¿ã€ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãªã—

r4_9 = tmpdir / 'r4_9.py'

r4_9.write_text('# PROOF: [L1] <- mekhane')  # æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãªã—

result = checker.check_file_proof(r4_9)

test('R4-9: æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãªã—', result.status, result.reason, f'parent={result.parent}')



# R4-10: è¤‡æ•°ã‚¹ãƒ©ãƒƒã‚·ãƒ¥

r4_10 = tmpdir / 'r4_10.py'

r4_10.write_text('# PROOF: [L1] <- mekhane//dendron/')

result = checker.check_file_proof(r4_10)

test('R4-10: è¤‡æ•°ã‚¹ãƒ©ãƒƒã‚·ãƒ¥', result.status, result.reason, f'parent={result.parent}')



print()

for name, status, reason, extra in results:

    icon = 'âœ…' if status == 'OK' else 'âš ï¸' if status in ['ORPHAN', 'INVALID'] else 'â“'

    print(f'{icon} {name}: {status}')

    if reason:

        print(f'   reason: {reason}')

    if extra:

        print(f'   {extra}')

    print()



import shutil

shutil.rmtree(tmpdir)

EOFTEST

cd /home/laihuip001/oikos/hegemonikon && python3 << 'EOFTEST'
import sys

import tempfile

import os

from pathlib import Path



sys.path.insert(0, '.')

from mekhane.dendron.checker import DendronChecker, ProofStatus



print('=== R4: æ–°ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ (ä¿®æ­£ç‰ˆ) ===')

print()



tmpdir = Path(tempfile.mkdtemp())

checker = DendronChecker(root=tmpdir, validate_parents=True)

(tmpdir / 'mekhane').mkdir()



results = []



def test(name, actual_status, actual_reason="", extra=""):

    results.append((name, actual_status.name, actual_reason, extra))



# R4-1: shebangè¡Œå¾Œã® PROOF

r4_1 = tmpdir / 'r4_1.py'

r4_1.write_text('#!/usr/bin/env python3\n# PROOF: [L1] <- mekhane/')

result = checker.check_file_proof(r4_1)

test('R4-1: shebangå¾ŒPROOF', result.status, result.reason)



# R4-2: encoding å®£è¨€å¾Œã® PROOF  

r4_2 = tmpdir / 'r4_2.py'

r4_2.write_text('# -*- coding: utf-8 -*-\n# PROOF: [L1] <- mekhane/')

result = checker.check_file_proof(r4_2)

test('R4-2: encodingå¾ŒPROOF', result.status, result.reason)



# R4-3: PROOF å¾Œã®ã‚³ãƒ¡ãƒ³ãƒˆ

r4_3 = tmpdir / 'r4_3.py'

r4_3.write_text('# PROOF: [L1] <- mekhane/  # ã“ã‚Œã¯è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆ')

result = checker.check_file_proof(r4_3)

test('R4-3: PROOFå¾Œã‚³ãƒ¡ãƒ³ãƒˆ', result.status, result.reason, f'parent={result.parent}')



# R4-4: noqa ä»˜ã PROOF

r4_4 = tmpdir / 'r4_4.py'

r4_4.write_text('# PROOF: [L1] <- mekhane/  # noqa: AI-022')

result = checker.check_file_proof(r4_4)

test('R4-4: noqaä»˜ãPROOF', result.status, result.reason, f'parent={result.parent}')



# R4-5: è¦ªãƒ‘ã‚¹ã«æ—¥æœ¬èª (ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ)

r4_5 = tmpdir / 'r4_5.py'

(tmpdir / 'japanese').mkdir()

r4_5.write_text('# PROOF: [L1] <- japanese/')

result = checker.check_file_proof(r4_5)

test('R4-5: æ—¥æœ¬èªè¦ªãƒ‘ã‚¹', result.status, result.reason)



# R4-6: é•·ã„è¦ªãƒ‘ã‚¹ (æ–°ãƒã‚°ç™ºè¦‹ï¼)

r4_6 = tmpdir / 'r4_6.py'

r4_6.write_text('# PROOF: [L1] <- ' + 'a' * 500 + '/')

try:

    result = checker.check_file_proof(r4_6)

    test('R4-6: é•·ã„è¦ª (500chars)', result.status, result.reason)

except OSError as e:

    results.append(('R4-6: é•·ã„è¦ª (500chars)', 'CRASH', str(e)[:50], 'âš ï¸ BUG'))



# R4-7: ã‚¹ãƒšãƒ¼ã‚¹å«ã‚€è¦ªãƒ‘ã‚¹

r4_7 = tmpdir / 'r4_7.py'

r4_7.write_text('# PROOF: [L1] <- path with spaces/')

result = checker.check_file_proof(r4_7)

test('R4-7: ã‚¹ãƒšãƒ¼ã‚¹å«ã‚€è¦ª', result.status, result.reason, f'parent={result.parent}')



# R4-8: æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãªã—

r4_8 = tmpdir / 'r4_8.py'

r4_8.write_text('# PROOF: [L1] <- mekhane')

result = checker.check_file_proof(r4_8)

test('R4-8: æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãªã—', result.status, result.reason)



# R4-9: è¤‡æ•°ã‚¹ãƒ©ãƒƒã‚·ãƒ¥

r4_9 = tmpdir / 'r4_9.py' 

r4_9.write_text('# PROOF: [L1] <- mekhane//dendron/')

result = checker.check_file_proof(r4_9)

test('R4-9: è¤‡æ•°ã‚¹ãƒ©ãƒƒã‚·ãƒ¥', result.status, result.reason, f'parent={result.parent}')



# R4-10: ãƒ‰ãƒƒãƒˆè¤‡æ•°

r4_10 = tmpdir / 'r4_10.py'

r4_10.write_text('# PROOF: [L1] <- ./mekhane/')

result = checker.check_file_proof(r4_10)

test('R4-10: ./prefix', result.status, result.reason)



print()

for name, status, reason, extra in results:

    if 'BUG' in extra or 'CRASH' in status:

        icon = 'âŒ'

    elif status == 'OK':

        icon = 'âœ…'

    elif status in ['ORPHAN', 'INVALID']:

        icon = 'âš ï¸'

    else:

        icon = 'â“'

    print(f'{icon} {name}: {status}')

    if reason:

        print(f'   {reason}')

    if extra:

        print(f'   {extra}')



import shutil

shutil.rmtree(tmpdir)

EOFTEST

cd /home/laihuip001/oikos/hegemonikon && python3 << 'EOFTEST'
import sys

import tempfile

from pathlib import Path



sys.path.insert(0, '.')

from mekhane.dendron.checker import DendronChecker, ProofStatus



print('=== R4-6 ãƒã‚°ä¿®æ­£ç¢ºèª ===')

print()



tmpdir = Path(tempfile.mkdtemp())

checker = DendronChecker(root=tmpdir, validate_parents=True)



# R4-6: é•·ã„è¦ªãƒ‘ã‚¹ (500 chars)

r4_6 = tmpdir / 'r4_6.py'

r4_6.write_text('# PROOF: [L1] <- ' + 'a' * 500 + '/')



try:

    result = checker.check_file_proof(r4_6)

    print(f'status: {result.status.name}')

    print(f'reason: {result.reason}')

    if result.status == ProofStatus.INVALID:

        print()

        print('âœ… R4-6 ãƒã‚°ä¿®æ­£å®Œäº†ï¼')

    else:

        print()

        print('âŒ ã¾ã ä¿®æ­£å¿…è¦')

except Exception as e:

    print(f'âŒ ä¾‹å¤–ç™ºç”Ÿ: {e}')



import shutil

shutil.rmtree(tmpdir)

EOFTEST

cd /home/laihuip001/oikos/hegemonikon && PYTHONPATH=. python -m mekhane.dendron check mekhane/ --ci
cd /home/laihuip001/oikos/hegemonikon && git add mekhane/dendron/checker.py && git commit -m "fix: Dendron v2.4 - path length validation (R4-6 bug)

- Add 255 byte limit for parent path length  
- Wrap path.exists() in try/except for OSError
- Prevents crash on excessively long paths"
source .venv/bin/activate && pip show mcp 2>/dev/null || echo "MCP SDK not installed"
cd /home/laihuip001
cat ~/.config/Code/User/settings.json 2>/dev/null | grep -A5 "mcp.servers" || echo "MCP not configured in settings"
cd /home/laihuip001/oikos/hegemonikon && PYTHONPATH=/home/laihuip001/oikos/hegemonikon .venv/bin/python -m dendron.cli check mekhane/synedrion/ --format text 2>&1
cd /home/laihuip001/oikos/hegemonikon && source .venv/bin/activate && timeout 180 python mekhane/synedrion/ai_auditor.py mekhane/ 2>&1 | head -15
cd /home/laihuip001/oikos/hegemonikon && source .venv/bin/activate && timeout 180 python mekhane/synedrion/ai_auditor.py mekhane/ 2>&1 | grep -oE "AI-[0-9]+" | sort | uniq -c | sort -rn
cd /home/laihuip001/oikos/hegemonikon && source .venv/bin/activate && timeout 180 python mekhane/synedrion/ai_auditor.py mekhane/ 2>&1 | head -15
cd /home/laihuip001/oikos/.gemini/antigravity/knowledge
cd /home/laihuip001/oikos/.gemini/antigravity/knowledge && for ki in */metadata.json; do d=$(dirname "$ki"); python3 -c "import json; f=open('$ki'); d=json.load(f); print('$d:', d.get('dependencies', 'MISSING'))" 2>&1; done | sort
cd /home/laihuip001/oikos/hegemonikon
cd /home/laihuip001/oikos/hegemonikon && .venv/bin/pytest hermeneus/tests/ -v 2>&1 | tail -20
source .venv/bin/activate && PYTHONPATH=/home/laihuip001/oikos/hegemonikon python -c "from hermeneus.src.cli import main; main(['compile', '/noe+'])"
cd /home/laihuip001/oikos/hegemonikon && mv requirements_current.txt requirements.txt && git add requirements.txt && git commit -m "feat: Windows ç§»è¡Œæº–å‚™

- docs/windows_migration_guide.md: Ollama + Synergeia çµ±åˆã‚¬ã‚¤ãƒ‰
- docs/gpu_required_tasks.md: GPU å¿…é ˆã‚¿ã‚¹ã‚¯ä¸€è¦§
- docs/api_keys_checklist.md: API ã‚­ãƒ¼ãƒªã‚¹ãƒˆ
- experiments/*.ipynb: Activation Steering + OpenManus MVP
- ccl/operators.md: v6.54 (Mixin + ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒã‚¯ãƒ­)
- requirements.txt: 226 ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸"
cd /home/laihuip001/oikos/hegemonikon && git status --short | wc -l
cd /home/laihuip001/oikos/hegemonikon && git add -A && git commit -m "style: Black ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé©ç”¨ + ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´ç†

- mekhane/**/*.py: Black ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé©ç”¨
- kernel/*.md: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´ç†
- synergeia/*.py: ã‚³ãƒ¼ãƒ‰æ•´å½¢"
cd /home/laihuip001/oikos/hegemonikon && PYTHONPATH=. python -m mekhane.dendron check mekhane/ --ci 2>&1
python3 -c "
import re
PROOF_PATTERN_V2 = re.compile(r'#\s*PROOF:\s*\[([^\]]+)\]\s*(?:<-\s*(.+?))?(?:\s*#.*)?$')

tests = [
    '# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©]',
    '# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- mekhane/',
    '# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©]  # noqa: AI-022',
    '# PROOF: [L1/å®šç†] <- FEP',
]

for t in tests:
    m = PROOF_PATTERN_V2.search(t)
    if m:
        print(f'MATCH: {t!r} -> level={m.group(1)!r}, parent={m.group(2)!r}')
    else:
        print(f'NO MATCH: {t!r}')
"
head -5 /home/laihuip001/oikos/hegemonikon/mekhane/__init__.py
head -5 /home/laihuip001/oikos/hegemonikon/mekhane/quality_gate.py
python3 -c "
import re
PROOF_PATTERN_V2 = re.compile(r'#\s*PROOF:\s*\[([^\]]+)\]\s*(?:<-\s*(.+?))?(?:\s*#.*)?$')

test = '# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] A0â†’mekhaneå®Ÿè£…å±¤ãŒå¿…è¦â†’__init__ ãŒæ‹…ã†'
m = PROOF_PATTERN_V2.search(test)
if m:
    print(f'MATCH: level={m.group(1)!r}, parent={m.group(2)!r}')
else:
    print(f'NO MATCH: {test!r}')
"
cd /home/laihuip001/oikos/hegemonikon && PYTHONPATH=. python -m mekhane.dendron check mekhane/ --ci 2>&1
cd /home/laihuip001/oikos/hegemonikon && python3 -c "
import re
import sys

# Attack Vector Tests for Dendron
PROOF_PATTERN_V2 = re.compile(r'#\s*PROOF:\s*\[([^\]]+)\](?:\s*<-\s*([^\s#]+))?')

print('=== Layer 2: /dia+.adv æ”»æ’ƒãƒ™ã‚¯ãƒˆãƒ«ãƒ†ã‚¹ãƒˆ ===')
print()

# Attack 1: å½ã®è¦ªå‚ç…§
print('âŒ Attack 1: å½ã®è¦ªå‚ç…§ (å­˜åœ¨ã—ãªã„ãƒ‘ã‚¹)')
test1 = '# PROOF: [L1/å®šç†] <- nonexistent/path/'
m1 = PROOF_PATTERN_V2.search(test1)
if m1 and m1.group(2):
    print(f'   çµæœ: ãƒ‘ãƒ¼ã‚¹æˆåŠŸ (parent={m1.group(2)!r})')
    print(f'   âš ï¸ è„†å¼±æ€§: è¦ªãƒ‘ã‚¹ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ãªã—!')
print()

# Attack 2: ç©ºã®è¦ªå‚ç…§
print('âŒ Attack 2: ç©ºã®è¦ªå‚ç…§')
test2 = '# PROOF: [L1/å®šç†] <- '
m2 = PROOF_PATTERN_V2.search(test2)
parent2 = m2.group(2) if m2 else None
print(f'   çµæœ: parent={parent2!r}')
if parent2 is None or parent2.strip() == '':
    print('   âœ… å®‰å…¨: ORPHAN ã¨ã—ã¦æ¤œå‡ºã•ã‚Œã‚‹')
print()

# Attack 3: Unicode å½è£… (Cyrillic PROOF)
print('âŒ Attack 3: Unicode å½è£… (Cyrillic P)')
test3 = '# Î¡ROOF: [L1/å®šç†] <- mekhane/'  # Î¡ is Greek Rho
m3 = PROOF_PATTERN_V2.search(test3)
if m3:
    print(f'   çµæœ: ãƒãƒƒãƒã—ãŸï¼è„†å¼±æ€§ï¼')
else:
    print('   âœ… å®‰å…¨: ãƒãƒƒãƒã—ãªã„ (regex ã¯ ASCII ã®ã¿)')
print()

# Attack 4: ç‰¹æ®Šæ–‡å­—ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
print('âŒ Attack 4: ç‰¹æ®Šæ–‡å­—ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³')
test4 = '# PROOF: [L1/å®šç†] <- ../../../etc/passwd'
m4 = PROOF_PATTERN_V2.search(test4)
if m4 and m4.group(2):
    print(f'   çµæœ: parent={m4.group(2)!r}')
    print(f'   âš ï¸ æ³¨æ„: ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«ã¯ regex ã§ã¯é˜²ã’ãªã„')
print()

# Attack 5: regex DoS (excessive backtracking)
print('âŒ Attack 5: regex DoS')
import time
long_input = '# PROOF: [' + 'A' * 10000 + ']'
start = time.time()
PROOF_PATTERN_V2.search(long_input)
elapsed = time.time() - start
print(f'   çµæœ: {elapsed:.4f}ç§’ (10000æ–‡å­—å…¥åŠ›)')
if elapsed < 0.1:
    print('   âœ… å®‰å…¨: DoS è€æ€§ã‚ã‚Š')
else:
    print('   âš ï¸ è„†å¼±æ€§: é«˜ã„ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·')
"
cd /home/laihuip001/oikos/hegemonikon && echo "=== Layer 4: /pan.grave å¢“å ´æ­©ã ===" && echo "" && echo "G1: check_dirs ã®ä½¿ç”¨çŠ¶æ³" && grep -r "check_dirs" mekhane/dendron/ --include="*.py" | head -5 && echo "" && echo "G2: PROOF_PATTERN (v1) ã®ä½¿ç”¨çŠ¶æ³" && grep -n "PROOF_PATTERN[^_]" mekhane/dendron/*.py && echo "" && echo "G3: SPECIAL_PARENTS ã®ä½¿ç”¨çŠ¶æ³" && grep -n "SPECIAL_PARENTS" mekhane/dendron/*.py && echo "" && echo "G4: ReportFormat.MARKDOWN ã®ä½¿ç”¨çŠ¶æ³" && grep -rn "MARKDOWN\|markdown" mekhane/dendron/ --include="*.py"
cd /home/laihuip001/oikos/hegemonikon && echo "=== Iteration 1: ãƒ†ã‚¹ãƒˆ ===" && PYTHONPATH=. python3 -c "
from pathlib import Path
from mekhane.dendron.checker import DendronChecker

# Test with validation enabled
checker = DendronChecker(root=Path('.'), validate_parents=True)

print('=== Attack Vector Re-test ===')
print()

# Test validate_parent directly
tests = [
    ('mekhane/fep/', 'valid path'),
    ('nonexistent/path/', 'nonexistent'),
    ('../../../etc/passwd', 'path traversal'),
    ('/etc/passwd', 'absolute path'),
    ('FEP', 'special parent'),
]

for parent, desc in tests:
    is_valid, reason = checker.validate_parent(parent)
    status = 'âœ…' if is_valid else 'âŒ'
    print(f'{status} {desc}: {reason}')

print()
print('=== Full Check with Validation ===')
result = checker.check(Path('mekhane/dendron/'))
print(f'Total: {result.total_files}, OK: {result.files_with_proof}, Invalid: {result.files_invalid_proof}')
"
cd /home/laihuip001/oikos/hegemonikon && echo "=== Iteration 1: å†ç›£æŸ» ===" && python3 -c "
import re

print('=== æ‰¹è©•ç‚¹ã‚«ã‚¦ãƒ³ãƒˆ ===')
print()

issues = []

# Check 1: PROOF_PATTERN v1 (dead code)
with open('mekhane/dendron/checker.py') as f:
    content = f.read()
    if 'PROOF_PATTERN = ' in content and 'PROOF_PATTERN_V2' in content:
        # v1 still exists?
        if re.search(r'^PROOF_PATTERN\s*=', content, re.MULTILINE):
            issues.append('PROOF_PATTERN v1 æ®‹å­˜')
    
# Check 2: SPECIAL_PARENTS unused
if 'SPECIAL_PARENTS' in content:
    # Check if actually used
    if 'if parent in SPECIAL_PARENTS' in content:
        pass  # Used
    else:
        issues.append('SPECIAL_PARENTS æœªä½¿ç”¨')

# Check 3: validate_parent exists
if 'def validate_parent' not in content:
    issues.append('validate_parent ãƒ¡ã‚½ãƒƒãƒ‰ãªã—')

# Check 4: Path traversal prevention
if '.. in parent' not in content and '\"..\"]' not in content:
    issues.append('ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«é˜²æ­¢ãªã—')

# Check 5: Absolute path prevention
if 'startswith(\"/\")' not in content:
    issues.append('çµ¶å¯¾ãƒ‘ã‚¹é˜²æ­¢ãªã—')

print(f'æ®‹ã‚Šæ‰¹è©•ç‚¹: {len(issues)}')
for issue in issues:
    print(f'  âŒ {issue}')

if len(issues) == 0:
    print('  âœ… å…¨ã¦ã®æ‰¹è©•ç‚¹ãŒè§£æ¶ˆã•ã‚Œã¾ã—ãŸï¼')
"
cd /home/laihuip001/oikos/hegemonikon && grep -n '"\.\."' mekhane/dendron/checker.py
cd /home/laihuip001/oikos/hegemonikon && echo "=== æœ€çµ‚ç¢ºèª: æ‰¹è©•ç‚¹ 0 ===" && echo "" && echo "âœ… PROOF_PATTERN v1 å‰Šé™¤æ¸ˆã¿" && ! grep -q "^PROOF_PATTERN = " mekhane/dendron/checker.py && echo "   $(grep -c 'PROOF_PATTERN_V2' mekhane/dendron/checker.py) v2 references" && echo "" && echo "âœ… SPECIAL_PARENTS ä½¿ç”¨ä¸­" && grep -c "SPECIAL_PARENTS" mekhane/dendron/checker.py && echo "" && echo "âœ… validate_parent å®Ÿè£…æ¸ˆã¿" && grep -c "def validate_parent" mekhane/dendron/checker.py && echo "" && echo "âœ… ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«é˜²æ­¢: line 140" && echo "âœ… çµ¶å¯¾ãƒ‘ã‚¹é˜²æ­¢: startswith(\"/\")" && echo "" && echo "=== å…¨ãƒã‚§ãƒƒã‚¯é€šé ==="
cd /home/laihuip001/oikos/hegemonikon && PYTHONPATH=. python -m mekhane.dendron check mekhane/ --ci
cd /home/laihuip001/oikos/hegemonikon && git add mekhane/dendron/checker.py && git commit -m "security: Dendron v2.1 - hardening from red-team audit

- Add validate_parent() method with security checks
- Block path traversal attacks (.. in parent)
- Block absolute path references (/ prefix)
- Add parent existence validation (when root is set)
- Remove dead code: PROOF_PATTERN v1
- SPECIAL_PARENTS now used in validate_parent

Red-team audit: 0 remaining criticisms"
cd /home/laihuip001/oikos/hegemonikon
cd /home/laihuip001/oikos/hegemonikon && .venv/bin/python -c "
import json

with open('/tmp/jules_perspectives_all.json') as f:
    perspectives = json.load(f)

# Get merged PRs
merged = [p for p in perspectives if p['merged']]

print(f'ğŸ“¦ Merged PRs: {len(merged)}')
print()
for p in merged:
    print(f\"### PR #{p['number']}: {p['title']}\")
    print()
    body = p['body'].replace('\\n', '\n')
    print(body[:600])
    print()
    print('---')
    print()
" 2>&1
PYTHONPATH=/home/laihuip001/oikos/hegemonikon .venv/bin/python -c "
from mekhane.symploke.insight_miner import mine_all_logs

# å…¨æ´å¯Ÿã‚’æŠ½å‡º
insights = mine_all_logs()

# ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
insights_sorted = sorted(insights, key=lambda x: x.confidence, reverse=True)

# æœ€é«˜å“è³ª (0.85ä»¥ä¸Š) ã‚’å…¨ä»¶è¡¨ç¤º
top_tier = [i for i in insights_sorted if i.confidence >= 0.85]

# é‡è¤‡æ’é™¤
seen = set()
unique_top = []
for ins in top_tier:
    key = ins.text[:50]
    if key not in seen:
        seen.add(key)
        unique_top.append(ins)

print(f'=== Tier 1 æ´å¯Ÿ (å…¨ {len(unique_top)} ä»¶) ===')
print()

for i, ins in enumerate(unique_top, 1):
    text = ins.text.replace('\\n', ' ').strip()[:150]
    print(f'{i:2}. [{ins.category}]')
    print(f'    \"{text}\"')
    print(f'    â† {ins.source_file}')
    print()
" 2>&1
cd /home/laihuip001/oikos
cd /home/laihuip001/oikos/hegemonikon/synergeia && python3 coordinator.py "/sop+ || /zet+"
cd /home/laihuip001/oikos/hegemonikon
PYTHONPATH=/home/laihuip001/oikos/hegemonikon .venv/bin/python -c "
from mekhane.symploke.insight_miner import mine_all_logs

# å…¨æ´å¯Ÿã‚’æŠ½å‡º
insights = mine_all_logs()

# ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
insights_sorted = sorted(insights, key=lambda x: x.confidence, reverse=True)

# Tier 2 (0.7-0.84)
tier2 = [i for i in insights_sorted if 0.70 <= i.confidence < 0.85]

# é‡è¤‡æ’é™¤
seen = set()
unique_tier2 = []
for ins in tier2:
    key = ins.text[:50]
    if key not in seen:
        seen.add(key)
        unique_tier2.append(ins)

print(f'=== Tier 2 æ´å¯Ÿ (ã‚¹ã‚³ã‚¢ 0.70-0.84): ä¸Šä½ 30ä»¶ / å…¨ {len(unique_tier2)} ä»¶ ===')
print()

for i, ins in enumerate(unique_tier2[:30], 1):
    text = ins.text.replace('\\n', ' ').strip()[:120]
    print(f'{i:2}. [{ins.category}] score={ins.confidence:.2f}')
    print(f'    \"{text}\"')
    print()
" 2>&1
cd /home/laihuip001/oikos/mneme/.hegemonikon/sessions
ls -1t handoff_*.md | head -3
cd /home/laihuip001/oikos/hegemonikon
cd /home/laihuip001/oikos/hegemonikon && PYTHONPATH=/home/laihuip001/oikos/hegemonikon .venv/bin/python mekhane/symploke/boot_integration.py --mode detailed 2>&1 | head -80
ls -1t /home/laihuip001/oikos/hegemonikon/docs/research/perplexity/*.md 2>/dev/null | head -3 || echo "ğŸ“¥ Perplexityæ–°ç€: 0ä»¶"
cd /home/laihuip001/oikos
cat /home/laihuip001/oikos/mneme/.hegemonikon/logs/dispatch_log.yaml 2>/dev/null | head -30 || echo "ğŸ“Š Dispatch Log: æœªè¨­å®š"
cd /home/laihuip001/oikos/hegemonikon
cd /home/laihuip001/oikos/hegemonikon && git fetch origin && git branch -a 2>/dev/null | grep jules | tail -3 || echo "ğŸ” Jules ãƒ¬ãƒ“ãƒ¥ãƒ¼: ãªã—"
cd /home/laihuip001/oikos
cd /home/laihuip001/oikos/hegemonikon/synergeia && python3 coordinator.py "/sop+ |> /zet+" "Synergeia distributed CCL execution framework"
cd /home/laihuip001/oikos/hegemonikon/synergeia && timeout 120 python3 coordinator.py "/sop+ |> /zet+" "Multi-agent AI framework"
cd /home/laihuip001/oikos/hegemonikon
PYTHONPATH=/home/laihuip001/oikos/hegemonikon .venv/bin/python -c "
from mekhane.symploke.insight_miner import mine_all_logs
from collections import defaultdict

# å…¨æ´å¯Ÿã‚’æŠ½å‡º
insights = mine_all_logs()

# ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
insights_sorted = sorted(insights, key=lambda x: x.confidence, reverse=True)

# Tier 2 (0.7-0.84)
tier2 = [i for i in insights_sorted if 0.70 <= i.confidence < 0.85]

# é‡è¤‡æ’é™¤
seen = set()
unique_tier2 = []
for ins in tier2:
    key = ins.text[:50]
    if key not in seen:
        seen.add(key)
        unique_tier2.append(ins)

# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡
by_category = defaultdict(list)
for ins in unique_tier2:
    by_category[ins.category].append(ins)

print(f'=== Tier 2 ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡ ({len(unique_tier2)} ä»¶) ===')
print()

for cat in ['gnome', 'principle', 'discovery', 'decision']:
    items = by_category.get(cat, [])
    print(f'### {cat.upper()} ({len(items)} ä»¶)')
    print()
    for i, ins in enumerate(items[:15], 1):
        text = ins.text.replace('\\n', ' ').strip()[:100]
        print(f'{i:2}. \"{text}\"')
    if len(items) > 15:
        print(f'    ... (ä»– {len(items)-15} ä»¶)')
    print()
" 2>&1
PYTHONPATH=/home/laihuip001/oikos/hegemonikon .venv/bin/python -c "
from mekhane.symploke.insight_miner import mine_all_logs

# å…¨æ´å¯Ÿã‚’æŠ½å‡º
insights = mine_all_logs()

# Tier 2 (0.7-0.84)
tier2 = [i for i in insights if 0.70 <= i.confidence < 0.85]

# é‡è¤‡æ’é™¤
seen = set()
unique_tier2 = []
for ins in tier2:
    key = ins.text[:50]
    if key not in seen:
        seen.add(key)
        unique_tier2.append(ins)

# KI é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
keywords = ['åŸå‰‡', 'å¿…ç„¶', 'FEP', 'è¨­è¨ˆ', 'æ¤œè¨¼', 'å“è³ª', 'è¨˜æ†¶', 'èªçŸ¥', 'å“²å­¦', 'å…¬ç†', 'principle', 'axiom', 'design']

relevant = []
for ins in unique_tier2:
    for kw in keywords:
        if kw.lower() in ins.text.lower():
            relevant.append(ins)
            break

# ã‚¹ã‚³ã‚¢é †
relevant_sorted = sorted(relevant, key=lambda x: x.confidence, reverse=True)

print(f'=== KI é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ Tier 2 æ´å¯Ÿ ({len(relevant_sorted)} ä»¶) ===')
print()

for i, ins in enumerate(relevant_sorted[:25], 1):
    text = ins.text.replace('\\n', ' ').strip()[:120]
    print(f'{i:2}. [{ins.category}] score={ins.confidence:.2f}')
    print(f'    \"{text}\"')
    print()
" 2>&1
cd /home/laihuip001/oikos
mkdir -p /home/laihuip001/oikos/mneme/.hegemonikon/incoming /home/laihuip001/oikos/mneme/.hegemonikon/processed
cd /home/laihuip001/oikos/hegemonikon
PYTHONPATH=/home/laihuip001/oikos/hegemonikon .venv/bin/python -c "
from mekhane.symploke.insight_miner import mine_all_logs

# å…¨æ´å¯Ÿã‚’æŠ½å‡º
insights = mine_all_logs()

# Tier 2 (0.7-0.84)
tier2 = [i for i in insights if 0.70 <= i.confidence < 0.85]

# é‡è¤‡æ’é™¤
seen = set()
unique_tier2 = []
for ins in tier2:
    key = ins.text[:50]
    if key not in seen:
        seen.add(key)
        unique_tier2.append(ins)

# è‡ªå‹•åŒ–/n8n/Zapier é–¢é€£
automation_keywords = ['n8n', 'zapier', 'è‡ªå‹•', 'automation', 'scheduled', 'cron', 'background']

automation = []
for ins in unique_tier2:
    for kw in automation_keywords:
        if kw.lower() in ins.text.lower():
            automation.append(ins)
            break

print(f'=== è‡ªå‹•åŒ–é–¢é€£ Tier 2 æ´å¯Ÿ ({len(automation)} ä»¶) ===')
print()

for i, ins in enumerate(automation[:15], 1):
    text = ins.text.replace('\\n', ' ').strip()[:120]
    print(f'{i:2}. [{ins.category}] score={ins.confidence:.2f}')
    print(f'    \"{text}\"')
    print()
" 2>&1
cd /home/laihuip001/oikos/hegemonikon && .venv/bin/python -c "
import json
import re
from collections import defaultdict

with open('/tmp/jules_perspectives_all.json') as f:
    perspectives = json.load(f)

# Get AI-Risk PRs
ai_risk = [p for p in perspectives if 'AI-' in p['title'].upper() or 'ai-' in p['title']]

# Extract AI codes
codes = defaultdict(list)
for p in ai_risk:
    match = re.search(r'[Aa][Ii]-?(\d+)', p['title'])
    if match:
        code = f\"AI-{match.group(1).zfill(3)}\"
        codes[code].append(p)

print(f'ğŸ“Š AI-Risk PRs: {len(ai_risk)}')
print(f'ğŸ“‹ Unique AI Codes: {len(codes)}')
print()
print('Code Distribution:')
for code in sorted(codes.keys()):
    prs = codes[code]
    states = [p['state'] for p in prs]
    title_sample = prs[0]['title'][:50]
    print(f\"  {code}: {len(prs)} PRs ({states}) - {title_sample}...\")
" 2>&1
cd /home/laihuip001/oikos/hegemonikon && .venv/bin/python -c "
import json
import re
from collections import defaultdict

with open('/tmp/jules_perspectives_all.json') as f:
    perspectives = json.load(f)

# Get Theory PRs
theory = [p for p in perspectives if 'TH-' in p['title'].upper() or 'th-' in p['title'] or 'Stoic' in p['title'] or 'FEP' in p['title'].upper()]

# Extract TH codes
codes = defaultdict(list)
for p in theory:
    match = re.search(r'[Tt][Hh]-?(\d+)', p['title'])
    if match:
        code = f\"TH-{match.group(1).zfill(3)}\"
        codes[code].append(p)
    else:
        codes['OTHER'].append(p)

print(f'ğŸ“Š Theory PRs: {len(theory)}')
print(f'ğŸ“‹ Unique TH Codes: {len(codes) - (1 if \"OTHER\" in codes else 0)}')
print()
for code in sorted(codes.keys()):
    prs = codes[code]
    title = prs[0]['title'][:60]
    print(f\"{code}: {title}...\")
" 2>&1
cd /home/laihuip001/oikos/hegemonikon && .venv/bin/python -c "
import json
import re
from collections import defaultdict

with open('/tmp/jules_perspectives_all.json') as f:
    perspectives = json.load(f)

# Get Other PRs (those not already classified)
def is_classified(p):
    title = p['title']
    return any([
        title.startswith('ğŸ¨'),
        title.startswith('âš¡'),
        'AI-' in title.upper() or 'ai-' in title,
        'AS-' in title.upper() or 'as-' in title,
        'AE-' in title.upper() or 'ae-' in title,
        'CL-' in title.upper() or 'cl-' in title,
        'ES-' in title.upper() or 'es-' in title,
        'TH-' in title.upper() or 'th-' in title,
        'Stoic' in title,
        'FEP' in title.upper()
    ])

other = [p for p in perspectives if not is_classified(p)]

# Try to find patterns in Other
patterns = defaultdict(list)
for p in other:
    title = p['title'].lower()
    if 'review' in title:
        patterns['review'].append(p)
    elif 'add' in title:
        patterns['add'].append(p)
    elif 'fix' in title or 'bug' in title:
        patterns['fix'].append(p)
    elif 'doc' in title:
        patterns['docs'].append(p)
    elif 'test' in title:
        patterns['test'].append(p)
    elif 'refactor' in title:
        patterns['refactor'].append(p)
    elif 'optimize' in title or 'perf' in title:
        patterns['optimize'].append(p)
    else:
        patterns['unclassified'].append(p)

print(f'ğŸ“Š Other PRs: {len(other)}')
print()
print('Pattern Distribution:')
for pat, prs in sorted(patterns.items(), key=lambda x: -len(x[1])):
    print(f\"  {pat}: {len(prs)} PRs\")
    for p in prs[:2]:
        print(f\"    - {p['title'][:50]}...\")
" 2>&1
cd /home/laihuip001/oikos/hegemonikon && .venv/bin/python -c "
import json
import re
from collections import defaultdict

with open('/tmp/jules_perspectives_all.json') as f:
    perspectives = json.load(f)

# Get review PRs that are not already classified
def is_classified(p):
    title = p['title']
    return any([
        title.startswith('ğŸ¨'),
        title.startswith('âš¡'),
        'AI-' in title.upper() or 'ai-' in title,
        'AS-' in title.upper() or 'as-' in title,
        'AE-' in title.upper() or 'ae-' in title,
        'CL-' in title.upper() or 'cl-' in title,
        'ES-' in title.upper() or 'es-' in title,
        'TH-' in title.upper() or 'th-' in title,
        'Stoic' in title,
        'FEP' in title.upper()
    ])

other = [p for p in perspectives if not is_classified(p)]
reviews = [p for p in other if 'review' in p['title'].lower()]

# Extract keywords from review titles
keywords = defaultdict(int)
for p in reviews:
    title = p['title'].lower()
    # Remove common words
    for word in ['add', 'review', 'for', 'jules', 'client', 'julesclient', 'docs', 'docs:', 'the', 'a', 'an']:
        title = title.replace(word, '')
    # Extract meaningful words
    words = re.findall(r'[a-z]+', title)
    for word in words:
        if len(word) > 3:
            keywords[word] += 1

print(f'ğŸ“Š Review PRs (unclassified): {len(reviews)}')
print()
print('Top 30 Keywords:')
for word, count in sorted(keywords.items(), key=lambda x: -x[1])[:30]:
    print(f\"  {word}: {count}\")
" 2>&1
