# Walkthrough: Hermeneus LLM å®Ÿè¡Œã¨ Synteleia + LLM çµ±åˆ

**ã‚»ãƒƒã‚·ãƒ§ãƒ³**: 2026-02-01
**ä¸»é¡Œ**: Hermeneus ã® LLM å®Ÿè¡Œã‚’æœ‰åŠ¹åŒ–ã—ã€Synteleia + LLM ã®çµ±åˆåˆ†æã‚’å®Ÿç¾

---

## ğŸ¯ ç›®æ¨™

Hermeneus ã‚’ã€ŒCCL ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã€ã‹ã‚‰ã€Œå®Ÿç”¨çš„ãª LLM å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³ã€ã¸é€²åŒ–ã•ã›ã‚‹

---

## âœ… å®Œäº†ã—ãŸä½œæ¥­

### 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

- [hermeneus/.env](file:///home/laihuip001/oikos/hegemonikon/hermeneus/.env) ã« Google API Key ã‚’æ°¸ç¶šåŒ–
- [runtime.py](file:///home/laihuip001/oikos/hegemonikon/hermeneus/src/runtime.py) ã« dotenv è‡ªå‹•èª­ã¿è¾¼ã¿ã‚’è¿½åŠ 
- Gemini 3 Pro Preview ã§å‹•ä½œç¢ºèª

### 2. Synergeia + Hermeneus çµ±åˆ

- [coordinator.py](file:///home/laihuip001/oikos/hegemonikon/synergeia/coordinator.py) ã« `hermeneus_execute` ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
- [execute_hermeneus](file:///home/laihuip001/oikos/hegemonikon/synergeia/coordinator.py#289-386) ã§ LLM å®Ÿè¡Œã‚’æœ‰åŠ¹åŒ–
- PYTHONPATH è‡ªå‹•è¨­å®šã‚’è¿½åŠ 

### 3. Context Passing å•é¡Œä¿®æ­£

**å•é¡Œ**: Gemini ãŒã€Œã‚¨ã‚¢ãƒ—ã€å‡ºåŠ›ã‚’ã—ã¦ã„ãŸ

**åŸå› **: [_extract_prompt_from_lmql](file:///home/laihuip001/oikos/hegemonikon/hermeneus/src/runtime.py#365-380) ãŒ `{context}` ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’æŠ½å‡ºã—ãªã„

**ä¿®æ­£**: [_execute_fallback](file:///home/laihuip001/oikos/hegemonikon/hermeneus/src/runtime.py#177-229) ã§ context ã‚’å¸¸ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…ˆé ­ã«æ˜ç¤ºçš„ã«è¿½åŠ 

```diff:runtime.py
# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] LMQL å®Ÿè¡Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ 
"""
HermÄ“neus Runtime â€” LMQL ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å®Ÿè¡Œ

compile_ccl() ã®å‡ºåŠ›ã‚’å®Ÿéš›ã® LLM ã§å®Ÿè¡Œã—ã€
çµæœã‚’æ§‹é€ åŒ–ã—ã¦è¿”ã™ã€‚

Origin: 2026-01-31 CCL Execution Guarantee Architecture
"""

import os
import re
import json
import asyncio
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union
from enum import Enum


# =============================================================================
# Execution Result Types
# =============================================================================

class ExecutionStatus(Enum):
    """å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    SUCCESS = "success"
    PARTIAL = "partial"       # ä¸€éƒ¨æˆåŠŸ
    TIMEOUT = "timeout"
    ERROR = "error"
    RATE_LIMITED = "rate_limited"


@dataclass
class ExecutionResult:
    """CCL å®Ÿè¡Œçµæœ"""
    status: ExecutionStatus
    output: str                              # æœ€çµ‚å‡ºåŠ›
    iterations: int = 0                      # åå¾©å›æ•° (åæŸãƒ«ãƒ¼ãƒ—ã®å ´åˆ)
    confidence: float = 1.0                  # ç¢ºä¿¡åº¦ (0.0-1.0)
    intermediate_results: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None


@dataclass
class ExecutionConfig:
    """å®Ÿè¡Œè¨­å®š"""
    model: str = "openai/gpt-4o"
    timeout: int = 300                       # ç§’
    max_retries: int = 3
    max_iterations: int = 5                  # åæŸãƒ«ãƒ¼ãƒ—ã®æœ€å¤§åå¾©
    temperature: float = 0.7
    api_key: Optional[str] = None


# =============================================================================
# LMQL Executor
# =============================================================================

class LMQLExecutor:
    """LMQL ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿè¡Œå™¨
    
    LMQL ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ (ç›´æ¥ OpenAI API) ã‚’ä½¿ç”¨ã€‚
    """
    
    def __init__(self, config: Optional[ExecutionConfig] = None):
        self.config = config or ExecutionConfig()
        self._lmql_available = self._check_lmql()
    
    def _check_lmql(self) -> bool:
        """LMQL ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
        try:
            import lmql  # noqa: F401
            return True
        except ImportError:
            return False
    
    async def execute_async(
        self,
        lmql_code: str,
        context: str = "",
        variables: Optional[Dict[str, Any]] = None
    ) -> ExecutionResult:
        """LMQL ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’éåŒæœŸå®Ÿè¡Œ"""
        variables = variables or {}
        
        try:
            if self._lmql_available:
                return await self._execute_with_lmql(lmql_code, context, variables)
            else:
                return await self._execute_fallback(lmql_code, context, variables)
        except asyncio.TimeoutError:
            return ExecutionResult(
                status=ExecutionStatus.TIMEOUT,
                output="",
                error=f"Execution timed out after {self.config.timeout} seconds"
            )
        except Exception as e:
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error=str(e)
            )
    
    def execute(
        self,
        lmql_code: str,
        context: str = "",
        variables: Optional[Dict[str, Any]] = None
    ) -> ExecutionResult:
        """LMQL ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’åŒæœŸå®Ÿè¡Œ"""
        return asyncio.run(self.execute_async(lmql_code, context, variables))
    
    async def _execute_with_lmql(
        self,
        lmql_code: str,
        context: str,
        variables: Dict[str, Any]
    ) -> ExecutionResult:
        """LMQL ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œ"""
        import lmql
        
        # LMQL ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
        # (compile_ccl ã®å‡ºåŠ›ã¯æ–‡å­—åˆ—ãªã®ã§ã€exec ã§å®Ÿè¡Œ)
        local_vars = {"lmql": lmql, "context": context, **variables}
        
        try:
            exec(lmql_code, local_vars)
            
            # é–¢æ•°ã‚’æ¢ã—ã¦å®Ÿè¡Œ
            for name, obj in local_vars.items():
                if callable(obj) and name.startswith("ccl_"):
                    result = await obj(context)
                    return ExecutionResult(
                        status=ExecutionStatus.SUCCESS,
                        output=str(result),
                        metadata={"function": name}
                    )
            
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error="No executable CCL function found in LMQL code"
            )
        except Exception as e:
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error=f"LMQL execution error: {e}"
            )
    
    async def _execute_fallback(
        self,
        lmql_code: str,
        context: str,
        variables: Dict[str, Any]
    ) -> ExecutionResult:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: OpenAI API ç›´æ¥å‘¼ã³å‡ºã—"""
        try:
            from openai import AsyncOpenAI
        except ImportError:
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error="Neither LMQL nor OpenAI SDK is installed. Run: pip install lmql or pip install openai"
            )
        
        # LMQL ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŠ½å‡º
        prompt = self._extract_prompt_from_lmql(lmql_code)
        if not prompt:
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error="Could not extract prompt from LMQL code"
            )
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ¿å…¥
        full_prompt = prompt.replace("{context}", context)
        
        # OpenAI API å‘¼ã³å‡ºã—
        api_key = self.config.api_key or os.environ.get("OPENAI_API_KEY")
        if not api_key:
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error="OPENAI_API_KEY not set"
            )
        
        client = AsyncOpenAI(api_key=api_key)
        
        for attempt in range(self.config.max_retries):
            try:
                response = await asyncio.wait_for(
                    client.chat.completions.create(
                        model=self.config.model.replace("openai/", ""),
                        messages=[{"role": "user", "content": full_prompt}],
                        temperature=self.config.temperature,
                    ),
                    timeout=self.config.timeout
                )
                
                output = response.choices[0].message.content
                return ExecutionResult(
                    status=ExecutionStatus.SUCCESS,
                    output=output,
                    metadata={
                        "model": self.config.model,
                        "attempt": attempt + 1,
                        "fallback": True
                    }
                )
            except asyncio.TimeoutError:
                if attempt == self.config.max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
        
        return ExecutionResult(
            status=ExecutionStatus.ERROR,
            output="",
            error="Max retries exceeded"
        )
    
    def _extract_prompt_from_lmql(self, lmql_code: str) -> Optional[str]:
        """LMQL ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’æŠ½å‡º"""
        # æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã‚’æ¢ã™
        prompts = []
        
        # "..." ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        for match in re.finditer(r'"([^"]+)"', lmql_code):
            text = match.group(1)
            # ã‚³ãƒ¼ãƒ‰çš„ãªæ–‡å­—åˆ—ã¯é™¤å¤–
            if not text.startswith("@") and not "import" in text:
                prompts.append(text)
        
        if prompts:
            return "\n".join(prompts)
        return None


# =============================================================================
# Convergence Loop Executor
# =============================================================================

class ConvergenceExecutor:
    """åæŸãƒ«ãƒ¼ãƒ—å®Ÿè¡Œå™¨
    
    CCL ã® >> (åæŸ) æ¼”ç®—å­ã‚’å®Ÿè¡Œã—ã€
    æ¡ä»¶ã‚’æº€ãŸã™ã¾ã§åå¾©ã™ã‚‹ã€‚
    """
    
    def __init__(self, executor: LMQLExecutor):
        self.executor = executor
    
    async def execute_convergence(
        self,
        lmql_code: str,
        context: str,
        condition_var: str = "V",
        condition_op: str = "<",
        condition_value: float = 0.3,
        max_iterations: int = 5
    ) -> ExecutionResult:
        """åæŸãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ"""
        results = []
        current_value = 1.0  # åˆæœŸä¸ç¢ºå®Ÿæ€§
        
        for i in range(max_iterations):
            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
            result = await self.executor.execute_async(lmql_code, context)
            results.append(result.output)
            
            if result.status != ExecutionStatus.SUCCESS:
                return ExecutionResult(
                    status=result.status,
                    output=result.output,
                    iterations=i + 1,
                    intermediate_results=results,
                    error=result.error
                )
            
            # ä¸ç¢ºå®Ÿæ€§ã‚’è©•ä¾¡ (ç°¡æ˜“ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯)
            current_value = self._estimate_uncertainty(result.output)
            
            # åæŸæ¡ä»¶ãƒã‚§ãƒƒã‚¯
            if self._check_condition(current_value, condition_op, condition_value):
                return ExecutionResult(
                    status=ExecutionStatus.SUCCESS,
                    output=results[-1],
                    iterations=i + 1,
                    confidence=1.0 - current_value,
                    intermediate_results=results,
                    metadata={"converged": True, "final_uncertainty": current_value}
                )
            
            # æ¬¡ã®åå¾©ã®ãŸã‚ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ›´æ–°
            context = f"{context}\n\nå‰å›ã®åˆ†æ:\n{result.output}"
        
        # æœ€å¤§åå¾©ã«é”ã—ãŸ
        return ExecutionResult(
            status=ExecutionStatus.PARTIAL,
            output=results[-1] if results else "",
            iterations=max_iterations,
            confidence=1.0 - current_value,
            intermediate_results=results,
            metadata={"converged": False, "final_uncertainty": current_value}
        )
    
    def _estimate_uncertainty(self, output: str) -> float:
        """å‡ºåŠ›ã‹ã‚‰ä¸ç¢ºå®Ÿæ€§ã‚’æ¨å®š (ç°¡æ˜“ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯)"""
        uncertainty_indicators = [
            "ã‹ã‚‚ã—ã‚Œãªã„", "ãŠãã‚‰ã", "might", "maybe", "perhaps",
            "ä¸æ˜", "unclear", "uncertain", "possible",
            "?", "æ¨æ¸¬", "guess", "assume"
        ]
        
        certainty_indicators = [
            "ç¢ºå®Ÿ", "definitely", "certainly", "æ˜ç¢º",
            "çµè«–", "conclusion", "therefore", "ã—ãŸãŒã£ã¦"
        ]
        
        output_lower = output.lower()
        
        uncertainty_count = sum(1 for ind in uncertainty_indicators if ind in output_lower)
        certainty_count = sum(1 for ind in certainty_indicators if ind in output_lower)
        
        # ç°¡æ˜“ã‚¹ã‚³ã‚¢è¨ˆç®—
        if certainty_count > uncertainty_count:
            return max(0.1, 0.5 - (certainty_count * 0.1))
        elif uncertainty_count > 0:
            return min(0.9, 0.5 + (uncertainty_count * 0.1))
        else:
            return 0.4  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _check_condition(self, value: float, op: str, threshold: float) -> bool:
        """æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯"""
        if op == "<":
            return value < threshold
        elif op == "<=":
            return value <= threshold
        elif op == ">":
            return value > threshold
        elif op == ">=":
            return value >= threshold
        elif op == "=":
            return abs(value - threshold) < 0.01
        return False


# =============================================================================
# High-Level API
# =============================================================================

def execute_ccl(
    ccl: str,
    context: str = "",
    model: str = "openai/gpt-4o",
    macros: Optional[Dict[str, str]] = None,
    **kwargs
) -> ExecutionResult:
    """CCL å¼ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦å®Ÿè¡Œ
    
    Args:
        ccl: CCL å¼ (ä¾‹: "/noe+ >> V[] < 0.3")
        context: å…¥åŠ›ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹ LLM ãƒ¢ãƒ‡ãƒ«
        macros: ãƒã‚¯ãƒ­å®šç¾©
        **kwargs: ExecutionConfig ã«æ¸¡ã™è¿½åŠ è¨­å®š
        
    Returns:
        ExecutionResult
        
    Example:
        >>> result = execute_ccl("/noe+", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­è¨ˆã‚’åˆ†æ")
        >>> print(result.output)
    """
    # é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (å¾ªç’°å‚ç…§å›é¿)
    from . import compile_ccl
    from .parser import parse_ccl
    from .ast import ConvergenceLoop
    
    # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    lmql_code = compile_ccl(ccl, macros=macros, model=model)
    
    # è¨­å®š
    config = ExecutionConfig(model=model, **kwargs)
    executor = LMQLExecutor(config)
    
    # åæŸãƒ«ãƒ¼ãƒ—ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    try:
        ast = parse_ccl(ccl)
        if isinstance(ast, ConvergenceLoop):
            conv_executor = ConvergenceExecutor(executor)
            return asyncio.run(conv_executor.execute_convergence(
                lmql_code,
                context,
                condition_var=ast.condition.var,
                condition_op=ast.condition.op,
                condition_value=ast.condition.value,
                max_iterations=config.max_iterations
            ))
    except Exception:
        pass  # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯é€šå¸¸å®Ÿè¡Œ
    
    # é€šå¸¸å®Ÿè¡Œ
    return executor.execute(lmql_code, context)
===
# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] LMQL å®Ÿè¡Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ 
"""
HermÄ“neus Runtime â€” LMQL ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å®Ÿè¡Œ

compile_ccl() ã®å‡ºåŠ›ã‚’å®Ÿéš›ã® LLM ã§å®Ÿè¡Œã—ã€
çµæœã‚’æ§‹é€ åŒ–ã—ã¦è¿”ã™ã€‚

Origin: 2026-01-31 CCL Execution Guarantee Architecture
"""

import os
import re
import json
import asyncio
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union
from enum import Enum
from pathlib import Path

# .env ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
def _load_env():
    """hermeneus/.env ã‹ã‚‰ API ã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€"""
    env_paths = [
        Path(__file__).parent.parent / ".env",  # hermeneus/.env
        Path(__file__).parent.parent.parent / ".env",  # hegemonikon/.env
    ]
    for env_path in env_paths:
        if env_path.exists():
            with open(env_path) as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#") and "=" in line:
                        key, value = line.split("=", 1)
                        if key not in os.environ:  # æ—¢å­˜ã®ç’°å¢ƒå¤‰æ•°ã¯ä¸Šæ›¸ãã—ãªã„
                            os.environ[key] = value.strip()

_load_env()


# =============================================================================
# Execution Result Types
# =============================================================================

class ExecutionStatus(Enum):
    """å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    SUCCESS = "success"
    PARTIAL = "partial"       # ä¸€éƒ¨æˆåŠŸ
    TIMEOUT = "timeout"
    ERROR = "error"
    RATE_LIMITED = "rate_limited"


@dataclass
class ExecutionResult:
    """CCL å®Ÿè¡Œçµæœ"""
    status: ExecutionStatus
    output: str                              # æœ€çµ‚å‡ºåŠ›
    iterations: int = 0                      # åå¾©å›æ•° (åæŸãƒ«ãƒ¼ãƒ—ã®å ´åˆ)
    confidence: float = 1.0                  # ç¢ºä¿¡åº¦ (0.0-1.0)
    intermediate_results: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None


@dataclass
class ExecutionConfig:
    """å®Ÿè¡Œè¨­å®š"""
    model: str = "openai/gpt-4o"
    timeout: int = 300                       # ç§’
    max_retries: int = 3
    max_iterations: int = 5                  # åæŸãƒ«ãƒ¼ãƒ—ã®æœ€å¤§åå¾©
    temperature: float = 0.7
    api_key: Optional[str] = None


# =============================================================================
# LMQL Executor
# =============================================================================

class LMQLExecutor:
    """LMQL ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿè¡Œå™¨
    
    LMQL ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ (ç›´æ¥ OpenAI API) ã‚’ä½¿ç”¨ã€‚
    """
    
    def __init__(self, config: Optional[ExecutionConfig] = None):
        self.config = config or ExecutionConfig()
        self._lmql_available = self._check_lmql()
    
    def _check_lmql(self) -> bool:
        """LMQL ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
        try:
            import lmql  # noqa: F401
            return True
        except ImportError:
            return False
    
    async def execute_async(
        self,
        lmql_code: str,
        context: str = "",
        variables: Optional[Dict[str, Any]] = None
    ) -> ExecutionResult:
        """LMQL ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’éåŒæœŸå®Ÿè¡Œ"""
        variables = variables or {}
        
        try:
            if self._lmql_available:
                result = await self._execute_with_lmql(lmql_code, context, variables)
                # LMQL exec() ãŒå¤±æ•—ã—ãŸå ´åˆã¯ Fallback ã¸
                if result.status == ExecutionStatus.ERROR and "exec" in str(result.error).lower():
                    return await self._execute_fallback(lmql_code, context, variables)
                return result
            else:
                return await self._execute_fallback(lmql_code, context, variables)
        except asyncio.TimeoutError:
            return ExecutionResult(
                status=ExecutionStatus.TIMEOUT,
                output="",
                error=f"Execution timed out after {self.config.timeout} seconds"
            )
        except Exception as e:
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error=str(e)
            )
    
    def execute(
        self,
        lmql_code: str,
        context: str = "",
        variables: Optional[Dict[str, Any]] = None
    ) -> ExecutionResult:
        """LMQL ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’åŒæœŸå®Ÿè¡Œ"""
        return asyncio.run(self.execute_async(lmql_code, context, variables))
    
    async def _execute_with_lmql(
        self,
        lmql_code: str,
        context: str,
        variables: Dict[str, Any]
    ) -> ExecutionResult:
        """LMQL ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œ"""
        import lmql
        
        # LMQL ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
        # (compile_ccl ã®å‡ºåŠ›ã¯æ–‡å­—åˆ—ãªã®ã§ã€exec ã§å®Ÿè¡Œ)
        local_vars = {"lmql": lmql, "context": context, **variables}
        
        try:
            exec(lmql_code, local_vars)
            
            # é–¢æ•°ã‚’æ¢ã—ã¦å®Ÿè¡Œ
            for name, obj in local_vars.items():
                if callable(obj) and name.startswith("ccl_"):
                    result = await obj(context)
                    return ExecutionResult(
                        status=ExecutionStatus.SUCCESS,
                        output=str(result),
                        metadata={"function": name}
                    )
            
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error="No executable CCL function found in LMQL code"
            )
        except Exception as e:
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error=f"LMQL execution error: {e}"
            )
    
    async def _execute_fallback(
        self,
        lmql_code: str,
        context: str,
        variables: Dict[str, Any]
    ) -> ExecutionResult:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è¤‡æ•°ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œã® LLM å‘¼ã³å‡ºã—
        
        å„ªå…ˆé †ä½:
        1. Anthropic (Claude) - ANTHROPIC_API_KEY
        2. Google (Gemini) - GOOGLE_API_KEY  
        3. OpenAI - OPENAI_API_KEY
        """
        # LMQL ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŠ½å‡º
        prompt = self._extract_prompt_from_lmql(lmql_code)
        if not prompt:
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error="Could not extract prompt from LMQL code"
            )
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ˜ç¤ºçš„ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
        # (LMQL ã® {context} ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãŒæŠ½å‡ºã•ã‚Œãªã„å ´åˆãŒã‚ã‚‹ãŸã‚)
        if context and context.strip():
            full_prompt = f"""## ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{context}

## ã‚¿ã‚¹ã‚¯
{prompt}

ä¸Šè¨˜ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã€ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"""
        else:
            full_prompt = prompt
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ¤œå‡ºã¨å®Ÿè¡Œ
        anthropic_key = os.environ.get("ANTHROPIC_API_KEY")
        google_key = os.environ.get("GOOGLE_API_KEY")
        openai_key = self.config.api_key or os.environ.get("OPENAI_API_KEY")
        
        if anthropic_key:
            return await self._execute_anthropic(full_prompt, anthropic_key)
        elif google_key:
            return await self._execute_google(full_prompt, google_key)
        elif openai_key:
            return await self._execute_openai(full_prompt, openai_key)
        else:
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error="No API key found. Set ANTHROPIC_API_KEY, GOOGLE_API_KEY, or OPENAI_API_KEY"
            )
    
    async def _execute_anthropic(self, prompt: str, api_key: str) -> ExecutionResult:
        """Anthropic Claude API å‘¼ã³å‡ºã—"""
        try:
            from anthropic import AsyncAnthropic
        except ImportError:
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error="anthropic not installed. Run: pip install anthropic"
            )
        
        client = AsyncAnthropic(api_key=api_key)
        
        for attempt in range(self.config.max_retries):
            try:
                response = await asyncio.wait_for(
                    client.messages.create(
                        model="claude-sonnet-4-20250514",
                        max_tokens=4096,
                        messages=[{"role": "user", "content": prompt}],
                    ),
                    timeout=self.config.timeout
                )
                
                output = response.content[0].text
                return ExecutionResult(
                    status=ExecutionStatus.SUCCESS,
                    output=output,
                    metadata={
                        "provider": "anthropic",
                        "model": "claude-sonnet-4-20250514",
                        "attempt": attempt + 1,
                        "fallback": True
                    }
                )
            except asyncio.TimeoutError:
                if attempt == self.config.max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)
        
        return ExecutionResult(
            status=ExecutionStatus.ERROR,
            output="",
            error="Anthropic API call failed after retries"
        )
    
    async def _execute_google(self, prompt: str, api_key: str) -> ExecutionResult:
        """Google Gemini API å‘¼ã³å‡ºã—"""
        try:
            import google.generativeai as genai
        except ImportError:
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error="google-generativeai not installed. Run: pip install google-generativeai"
            )
        
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-3-pro-preview")
        
        for attempt in range(self.config.max_retries):
            try:
                response = await asyncio.wait_for(
                    asyncio.to_thread(model.generate_content, prompt),
                    timeout=self.config.timeout
                )
                
                output = response.text
                return ExecutionResult(
                    status=ExecutionStatus.SUCCESS,
                    output=output,
                    metadata={
                        "provider": "google",
                        "model": "gemini-3-pro-preview",
                        "attempt": attempt + 1,
                        "fallback": True
                    }
                )
            except asyncio.TimeoutError:
                if attempt == self.config.max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)
        
        return ExecutionResult(
            status=ExecutionStatus.ERROR,
            output="",
            error="Google API call failed after retries"
        )
    
    async def _execute_openai(self, prompt: str, api_key: str) -> ExecutionResult:
        """OpenAI API å‘¼ã³å‡ºã—"""
        try:
            from openai import AsyncOpenAI
        except ImportError:
            return ExecutionResult(
                status=ExecutionStatus.ERROR,
                output="",
                error="openai not installed. Run: pip install openai"
            )
        
        client = AsyncOpenAI(api_key=api_key)
        
        for attempt in range(self.config.max_retries):
            try:
                response = await asyncio.wait_for(
                    client.chat.completions.create(
                        model=self.config.model.replace("openai/", ""),
                        messages=[{"role": "user", "content": prompt}],
                        temperature=self.config.temperature,
                    ),
                    timeout=self.config.timeout
                )
                
                output = response.choices[0].message.content
                return ExecutionResult(
                    status=ExecutionStatus.SUCCESS,
                    output=output,
                    metadata={
                        "provider": "openai",
                        "model": self.config.model,
                        "attempt": attempt + 1,
                        "fallback": True
                    }
                )
            except asyncio.TimeoutError:
                if attempt == self.config.max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)
        
        return ExecutionResult(
            status=ExecutionStatus.ERROR,
            output="",
            error="Max retries exceeded"
        )
    
    def _extract_prompt_from_lmql(self, lmql_code: str) -> Optional[str]:
        """LMQL ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’æŠ½å‡º"""
        # æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã‚’æ¢ã™
        prompts = []
        
        # "..." ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        for match in re.finditer(r'"([^"]+)"', lmql_code):
            text = match.group(1)
            # ã‚³ãƒ¼ãƒ‰çš„ãªæ–‡å­—åˆ—ã¯é™¤å¤–
            if not text.startswith("@") and not "import" in text:
                prompts.append(text)
        
        if prompts:
            return "\n".join(prompts)
        return None


# =============================================================================
# Convergence Loop Executor
# =============================================================================

class ConvergenceExecutor:
    """åæŸãƒ«ãƒ¼ãƒ—å®Ÿè¡Œå™¨
    
    CCL ã® >> (åæŸ) æ¼”ç®—å­ã‚’å®Ÿè¡Œã—ã€
    æ¡ä»¶ã‚’æº€ãŸã™ã¾ã§åå¾©ã™ã‚‹ã€‚
    """
    
    def __init__(self, executor: LMQLExecutor):
        self.executor = executor
    
    async def execute_convergence(
        self,
        lmql_code: str,
        context: str,
        condition_var: str = "V",
        condition_op: str = "<",
        condition_value: float = 0.3,
        max_iterations: int = 5
    ) -> ExecutionResult:
        """åæŸãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ"""
        results = []
        current_value = 1.0  # åˆæœŸä¸ç¢ºå®Ÿæ€§
        
        for i in range(max_iterations):
            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
            result = await self.executor.execute_async(lmql_code, context)
            results.append(result.output)
            
            if result.status != ExecutionStatus.SUCCESS:
                return ExecutionResult(
                    status=result.status,
                    output=result.output,
                    iterations=i + 1,
                    intermediate_results=results,
                    error=result.error
                )
            
            # ä¸ç¢ºå®Ÿæ€§ã‚’è©•ä¾¡ (ç°¡æ˜“ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯)
            current_value = self._estimate_uncertainty(result.output)
            
            # åæŸæ¡ä»¶ãƒã‚§ãƒƒã‚¯
            if self._check_condition(current_value, condition_op, condition_value):
                return ExecutionResult(
                    status=ExecutionStatus.SUCCESS,
                    output=results[-1],
                    iterations=i + 1,
                    confidence=1.0 - current_value,
                    intermediate_results=results,
                    metadata={"converged": True, "final_uncertainty": current_value}
                )
            
            # æ¬¡ã®åå¾©ã®ãŸã‚ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ›´æ–°
            context = f"{context}\n\nå‰å›ã®åˆ†æ:\n{result.output}"
        
        # æœ€å¤§åå¾©ã«é”ã—ãŸ
        return ExecutionResult(
            status=ExecutionStatus.PARTIAL,
            output=results[-1] if results else "",
            iterations=max_iterations,
            confidence=1.0 - current_value,
            intermediate_results=results,
            metadata={"converged": False, "final_uncertainty": current_value}
        )
    
    def _estimate_uncertainty(self, output: str) -> float:
        """å‡ºåŠ›ã‹ã‚‰ä¸ç¢ºå®Ÿæ€§ã‚’æ¨å®š (ç°¡æ˜“ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯)"""
        uncertainty_indicators = [
            "ã‹ã‚‚ã—ã‚Œãªã„", "ãŠãã‚‰ã", "might", "maybe", "perhaps",
            "ä¸æ˜", "unclear", "uncertain", "possible",
            "?", "æ¨æ¸¬", "guess", "assume"
        ]
        
        certainty_indicators = [
            "ç¢ºå®Ÿ", "definitely", "certainly", "æ˜ç¢º",
            "çµè«–", "conclusion", "therefore", "ã—ãŸãŒã£ã¦"
        ]
        
        output_lower = output.lower()
        
        uncertainty_count = sum(1 for ind in uncertainty_indicators if ind in output_lower)
        certainty_count = sum(1 for ind in certainty_indicators if ind in output_lower)
        
        # ç°¡æ˜“ã‚¹ã‚³ã‚¢è¨ˆç®—
        if certainty_count > uncertainty_count:
            return max(0.1, 0.5 - (certainty_count * 0.1))
        elif uncertainty_count > 0:
            return min(0.9, 0.5 + (uncertainty_count * 0.1))
        else:
            return 0.4  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _check_condition(self, value: float, op: str, threshold: float) -> bool:
        """æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯"""
        if op == "<":
            return value < threshold
        elif op == "<=":
            return value <= threshold
        elif op == ">":
            return value > threshold
        elif op == ">=":
            return value >= threshold
        elif op == "=":
            return abs(value - threshold) < 0.01
        return False


# =============================================================================
# High-Level API
# =============================================================================

def execute_ccl(
    ccl: str,
    context: str = "",
    model: str = "openai/gpt-4o",
    macros: Optional[Dict[str, str]] = None,
    **kwargs
) -> ExecutionResult:
    """CCL å¼ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦å®Ÿè¡Œ
    
    Args:
        ccl: CCL å¼ (ä¾‹: "/noe+ >> V[] < 0.3")
        context: å…¥åŠ›ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹ LLM ãƒ¢ãƒ‡ãƒ«
        macros: ãƒã‚¯ãƒ­å®šç¾©
        **kwargs: ExecutionConfig ã«æ¸¡ã™è¿½åŠ è¨­å®š
        
    Returns:
        ExecutionResult
        
    Example:
        >>> result = execute_ccl("/noe+", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­è¨ˆã‚’åˆ†æ")
        >>> print(result.output)
    """
    # é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (å¾ªç’°å‚ç…§å›é¿)
    from . import compile_ccl
    from .parser import parse_ccl
    from .ast import ConvergenceLoop
    
    # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    lmql_code = compile_ccl(ccl, macros=macros, model=model)
    
    # è¨­å®š
    config = ExecutionConfig(model=model, **kwargs)
    executor = LMQLExecutor(config)
    
    # åæŸãƒ«ãƒ¼ãƒ—ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    try:
        ast = parse_ccl(ccl)
        if isinstance(ast, ConvergenceLoop):
            conv_executor = ConvergenceExecutor(executor)
            return asyncio.run(conv_executor.execute_convergence(
                lmql_code,
                context,
                condition_var=ast.condition.var,
                condition_op=ast.condition.op,
                condition_value=ast.condition.value,
                max_iterations=config.max_iterations
            ))
    except Exception:
        pass  # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯é€šå¸¸å®Ÿè¡Œ
    
    # é€šå¸¸å®Ÿè¡Œ
    return executor.execute(lmql_code, context)
```

### 4. Synteleia + LLM çµ±åˆåˆ†æ

**ç™ºè¦‹**: è£¸ã® API å‘¼ã³å‡ºã—ã¯ã€Œã‚¨ã‚¢ãƒ—ã€ã‚’ç”Ÿæˆã™ã‚‹

**è§£æ±º**: Synteleia ã§å®Ÿã‚³ãƒ¼ãƒ‰ã‚’ç›£æŸ» â†’ Issue è©³ç´°ã‚’ LLM ã«æ¸¡ã™ â†’ å…·ä½“çš„ãªåˆ†æãŒå¾—ã‚‰ã‚Œã‚‹

| ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ | å“è³ª | ã‚³ã‚¹ãƒˆ |
|:-----------|:-----|:-------|
| è£¸ã® API | âŒ ã‚¨ã‚¢ãƒ— | ç„¡é§„ |
| Synteleia å˜ä½“ | âœ… å…·ä½“çš„ | ç„¡æ–™ |
| **Synteleia + LLM** | âœ…âœ… æœ€å¼· | ~3å†† |

### 5. Synteleia Monitor å®Ÿè£…

[execute_hermeneus](file:///home/laihuip001/oikos/hegemonikon/synergeia/coordinator.py#289-386) ã« Synteleia ç›£æŸ»çµæœã‚’è‡ªå‹•è¿½åŠ ã™ã‚‹æ©Ÿèƒ½ã‚’å®Ÿè£…

```diff:coordinator.py
#!/usr/bin/env python3
"""
Synergeia Coordinator (ç°¡æ˜“ç‰ˆ)
==============================

CCLå¼ã‚’è§£æã—ã€é©åˆ‡ãªã‚¹ãƒ¬ãƒƒãƒ‰ã«åˆ†é…ã™ã‚‹ç°¡æ˜“Coordinatorã€‚

Usage:
    python coordinator.py "/noe+ || /sop+"
    python coordinator.py "/noe+ |> /dia+ |> /ene+"

Supported:
    - || (ä¸¦åˆ—): è¤‡æ•°ã‚¹ãƒ¬ãƒƒãƒ‰ã§åŒæ™‚å®Ÿè¡Œ
    - |> (ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³): é †æ¬¡å®Ÿè¡Œ
    - @thread[agent]{CCL}: ã‚¹ãƒ¬ãƒƒãƒ‰æŒ‡å®š
"""

import sys
import json
import re
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any

# Import Perplexity API
PERPLEXITY_SCRIPT = Path("/home/laihuip001/oikos/hegemonikon/mekhane/peira/scripts")
sys.path.insert(0, str(PERPLEXITY_SCRIPT))

from perplexity_api import search as perplexity_search

# Import Hermeneus CCL Compiler
try:
    from hermeneus.src import compile_ccl, expand_ccl, parse_ccl as hermeneus_parse
    from hermeneus.src.macros import get_all_macros
    HERMENEUS_AVAILABLE = True
    STANDARD_MACROS = get_all_macros()
except ImportError:
    HERMENEUS_AVAILABLE = False
    STANDARD_MACROS = {}
    print("[Warning] Hermeneus not available, falling back to manual execution")

# Import FEP Selector
try:
    from synergeia.fep_selector import select_thread_by_fep, ThreadRecommendation
    FEP_SELECTOR_AVAILABLE = True
except ImportError:
    FEP_SELECTOR_AVAILABLE = False

EXPERIMENTS_DIR = Path(__file__).parent / "experiments"


# =============================================================================
# Thread Definitions
# =============================================================================

THREAD_REGISTRY = {
    "hermeneus": {
        "name": "Hermeneus CCL Compiler",
        "supported_ccl": ["*"],  # ã™ã¹ã¦ã® CCL ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¯èƒ½
        "executor": "lmql",
        "priority": 0,  # æœ€å„ªå…ˆ
    },
    "antigravity": {
        "name": "Antigravity",
        "supported_ccl": ["/noe", "/dia", "/u", "/bou"],
        "executor": "manual",  # æ‰‹å‹•å®Ÿè¡Œ
        "priority": 10,
    },
    "perplexity": {
        "name": "Perplexity",
        "supported_ccl": ["/sop", "/zet"],
        "executor": "api",
        "priority": 5,
    },
    "claude": {
        "name": "Claude CLI",
        "supported_ccl": ["/s", "/mek"],
        "executor": "cli",
        "priority": 5,
    },
    "gemini": {
        "name": "Gemini CLI",
        "supported_ccl": ["/tek", "/sta"],
        "executor": "cli",
        "priority": 5,
    },
    "codex": {
        "name": "OpenAI Codex",
        "supported_ccl": ["/ene", "/pra"],  # å®Ÿè¡Œç³»
        "executor": "cli",
        "priority": 5,
    },
}


def select_thread(ccl: str, use_fep: bool = True) -> str:
    """
    CCLè¦ç´ ã‹ã‚‰æœ€é©ãªã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é¸æŠã€‚
    
    Args:
        ccl: CCL å¼
        use_fep: True ã®å ´åˆ FEP ãƒ™ãƒ¼ã‚¹ã®è¤‡é›‘åº¦åˆ†æã‚’ä½¿ç”¨
    
    Returns:
        ã‚¹ãƒ¬ãƒƒãƒ‰ ID
    """
    # FEP ãƒ™ãƒ¼ã‚¹é¸æŠ (è¤‡é›‘åº¦ã«åŸºã¥ã)
    if use_fep and FEP_SELECTOR_AVAILABLE:
        try:
            recommendation = select_thread_by_fep(ccl)
            print(f"[FEP] {ccl} -> {recommendation.thread} (complexity: {recommendation.complexity_score:.2f})")
            return recommendation.thread
        except Exception as e:
            print(f"[FEP] Error: {e}, falling back to rule-based selection")
    
    # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹é¸æŠ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
    base = re.match(r"(/\w+)", ccl)
    if not base:
        return "antigravity"
    
    base_ccl = base.group(1)
    
    for thread_id, config in THREAD_REGISTRY.items():
        if base_ccl in config["supported_ccl"]:
            return thread_id

    return "antigravity"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ


# =============================================================================
# CLI Paths
# =============================================================================

CLAUDE_CLI = "/home/laihuip001/oikos/.local/bin/claude"
GEMINI_CLI = "node /home/laihuip001/oikos/.npm/_npx/38c708f8d73fe4c9/node_modules/@google/gemini-cli/bundle/gemini.js"
CODEX_CLI = "/home/laihuip001/oikos/hegemonikon/synergeia/node_modules/.bin/codex"


# =============================================================================
# Executors
# =============================================================================

def execute_perplexity(ccl: str, context: str) -> Dict[str, Any]:
    """Perplexity API ã§CCLã‚’å®Ÿè¡Œã€‚"""
    # CCLã‚’æ¤œç´¢ã‚¯ã‚¨ãƒªã«å¤‰æ›
    query = f"{context} {ccl.replace('/', ' ').replace('+', ' deep').replace('-', ' brief')}"
    
    result = perplexity_search(query)
    
    if "error" in result:
        return {"status": "error", "error": result["error"], "ccl": ccl}
    
    return {
        "status": "success",
        "ccl": ccl,
        "thread": "perplexity",
        "answer": result["answer"],
        "citations": result.get("citations", []),
        "cost": result["cost"],
    }


def execute_claude(ccl: str, context: str) -> Dict[str, Any]:
    """Claude CLI ã§CCLã‚’å®Ÿè¡Œã€‚"""
    import subprocess
    
    prompt = f"{context}\n\nExecute CCL: {ccl}\n\nProvide a detailed response."
    
    try:
        result = subprocess.run(
            [CLAUDE_CLI, "-p", prompt],
            capture_output=True,
            text=True,
            timeout=600,  # æœ€å¤§10åˆ†
            cwd="/home/laihuip001/oikos/hegemonikon"
        )
        answer = result.stdout.strip()
        if result.returncode != 0:
            return {"status": "error", "error": result.stderr, "ccl": ccl}
    except subprocess.TimeoutExpired:
        return {"status": "error", "error": "Timeout (120s)", "ccl": ccl}
    except Exception as e:
        return {"status": "error", "error": str(e), "ccl": ccl}
    
    return {
        "status": "success",
        "ccl": ccl,
        "thread": "claude",
        "answer": answer,
    }


def execute_gemini(ccl: str, context: str) -> Dict[str, Any]:
    """Gemini CLI ã§CCLã‚’å®Ÿè¡Œã€‚"""
    import subprocess
    
    # ãƒ„ãƒ¼ãƒ«æ“ä½œã‚’é¿ã‘ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = f"""You are a Hegemonikon CCL interpreter.
Do NOT use any file system tools. Just analyze and respond in text.

{context}

Analyze and explain the CCL command: {ccl}

Provide a detailed conceptual response without executing any tools."""
    
    try:
        result = subprocess.run(
            [
                "node", 
                "/home/laihuip001/oikos/.npm/_npx/38c708f8d73fe4c9/node_modules/@google/gemini-cli/bundle/gemini.js", 
                "-p", prompt
            ],
            capture_output=True,
            text=True,
            timeout=600,  # æœ€å¤§10åˆ†
            cwd="/home/laihuip001/oikos/hegemonikon"
        )
        # Geminiã¯æœ€åˆã®2è¡ŒãŒãƒ­ã‚°ãªã®ã§é™¤å»
        lines = result.stdout.strip().split("\n")
        answer = "\n".join(lines[2:]) if len(lines) > 2 else result.stdout.strip()
        if result.returncode != 0:
            return {"status": "error", "error": result.stderr, "ccl": ccl}
    except subprocess.TimeoutExpired:
        return {"status": "error", "error": "Timeout (120s)", "ccl": ccl}
    except Exception as e:
        return {"status": "error", "error": str(e), "ccl": ccl}
    
    return {
        "status": "success",
        "ccl": ccl,
        "thread": "gemini",
        "answer": answer,
    }


def execute_codex(ccl: str, context: str) -> Dict[str, Any]:
    """OpenAI Codex CLI ã§CCLã‚’å®Ÿè¡Œã€‚"""
    import subprocess
    
    prompt = f"{context}\n\nExecute CCL: {ccl}\n\nProvide a detailed response."
    
    try:
        result = subprocess.run(
            [CODEX_CLI, "exec", prompt],
            capture_output=True,
            text=True,
            timeout=600,  # æœ€å¤§10åˆ†
            cwd="/home/laihuip001/oikos/hegemonikon"
        )
        # Codex ã¯æœ€åˆã¨æœ€å¾Œã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã®ã§æ•´ç†
        lines = result.stdout.strip().split("\n")
        # "Hello from Codex" ã®ã‚ˆã†ãªå®Ÿéš›ã®å‡ºåŠ›ã‚’æŠ½å‡º
        answer = "\n".join([l for l in lines if not l.startswith("OpenAI Codex") and not l.startswith("---") and not l.startswith("workdir:") and not l.startswith("model:") and not l.startswith("provider:") and not l.startswith("approval:") and not l.startswith("sandbox:") and not l.startswith("reasoning") and not l.startswith("session id:") and not l.startswith("user") and not l.startswith("mcp startup:") and not l.startswith("codex") and not l.startswith("tokens used")])
        if result.returncode != 0:
            return {"status": "error", "error": result.stderr, "ccl": ccl}
    except subprocess.TimeoutExpired:
        return {"status": "error", "error": "Timeout (600s)", "ccl": ccl}
    except Exception as e:
        return {"status": "error", "error": str(e), "ccl": ccl}
    
    return {
        "status": "success",
        "ccl": ccl,
        "thread": "codex",
        "answer": answer.strip(),
    }


def execute_manual(ccl: str, context: str) -> Dict[str, Any]:
    """æ‰‹å‹•å®Ÿè¡Œ (Antigravity)ã€‚"""
    return {
        "status": "manual",
        "ccl": ccl,
        "thread": "antigravity",
        "note": "AntigravityãŒæ‰‹å‹•ã§å®Ÿè¡Œã€‚çµæœã¯çµ±åˆæ™‚ã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
    }


def execute_hermeneus(ccl: str, context: str, compile_only: bool = False) -> Dict[str, Any]:
    """
    Hermeneus CCL Compiler ã§å®Ÿè¡Œã€‚
    
    Args:
        ccl: CCL å¼
        context: å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        compile_only: True ã®å ´åˆã¯ LMQL ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™ã ã‘ï¼ˆå®Ÿè¡Œã—ãªã„ï¼‰
    """
    if not HERMENEUS_AVAILABLE:
        return execute_manual(ccl, context)
    
    try:
        # Step 1: CCL ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« (æ¨™æº–ãƒã‚¯ãƒ­ã‚’ä½¿ç”¨)
        lmql_code = compile_ccl(ccl, macros=STANDARD_MACROS)
        
        if compile_only:
            return {
                "status": "compiled",
                "ccl": ccl,
                "thread": "hermeneus",
                "lmql": lmql_code,
                "macros_available": list(STANDARD_MACROS.keys()),
                "note": "LMQL code generated (not executed)",
            }
        
        # Step 2: å±•é–‹æƒ…å ±ã‚’å–å¾—
        expansion = expand_ccl(ccl, macros=STANDARD_MACROS)
        
        # Step 3: AST ã‚’å–å¾—
        ast = hermeneus_parse(expansion.expanded)
        
        return {
            "status": "success",
            "ccl": ccl,
            "thread": "hermeneus",
            "expansion": {
                "original": expansion.original,
                "expanded": expansion.expanded,
                "formal": expansion.formal,
            },
            "ast_type": type(ast).__name__,
            "lmql": lmql_code,
            "macros_used": len(STANDARD_MACROS),
            "note": "Compiled by Hermeneus with standard macros.",
        }
        
    except Exception as e:
        return {
            "status": "error",
            "ccl": ccl,
            "thread": "hermeneus",
            "error": str(e),
        }


def execute_ccl(ccl: str, context: str, use_hermeneus: bool = True) -> Dict[str, Any]:
    """
    CCLè¦ç´ ã‚’é©åˆ‡ãªã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œã€‚
    
    Args:
        ccl: CCL å¼
        context: å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        use_hermeneus: True ã®å ´åˆã¯ Hermeneus ã§æ§‹é€ åŒ–ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    """
    # Hermeneus ãƒ¢ãƒ¼ãƒ‰: æ§‹é€ åŒ– LMQL ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    if use_hermeneus and HERMENEUS_AVAILABLE:
        hermeneus_result = execute_hermeneus(ccl, context)
        if hermeneus_result["status"] != "error":
            return hermeneus_result
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        print(f"[Coordinator] Hermeneus failed, falling back to thread selection")
    
    thread = select_thread(ccl)
    
    print(f"[Coordinator] {ccl} -> {thread}")
    
    if thread == "perplexity":
        return execute_perplexity(ccl, context)
    elif thread == "claude":
        return execute_claude(ccl, context)
    elif thread == "gemini":
        return execute_gemini(ccl, context)
    elif thread == "codex":
        return execute_codex(ccl, context)
    else:
        return execute_manual(ccl, context)


# =============================================================================
# CCL Parser
# =============================================================================

def parse_ccl(ccl_expr: str) -> Dict[str, Any]:
    """
    CCLå¼ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦å®Ÿè¡Œè¨ˆç”»ã‚’ç”Ÿæˆã€‚
    
    Returns:
        {
            "type": "parallel" | "pipeline" | "single",
            "elements": [...]
        }
    """
    ccl_expr = ccl_expr.strip()
    
    # ä¸¦åˆ— (||)
    if "||" in ccl_expr:
        elements = [e.strip() for e in ccl_expr.split("||")]
        return {"type": "parallel", "elements": elements}
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (|>)
    if "|>" in ccl_expr:
        elements = [e.strip() for e in ccl_expr.split("|>")]
        return {"type": "pipeline", "elements": elements}
    
    # å˜ä¸€
    return {"type": "single", "elements": [ccl_expr]}


# =============================================================================
# Coordinator
# =============================================================================

def coordinate(ccl_expr: str, context: str = "") -> Dict[str, Any]:
    """
    CCLå¼ã‚’è§£æã—ã€åˆ†æ•£å®Ÿè¡Œã‚’èª¿æ•´ã€‚
    """
    plan = parse_ccl(ccl_expr)
    results = []
    
    print(f"\n{'='*60}")
    print(f"[Coordinator] CCL: {ccl_expr}")
    print(f"[Coordinator] Type: {plan['type']}")
    print(f"[Coordinator] Elements: {plan['elements']}")
    print(f"{'='*60}\n")
    
    if plan["type"] == "parallel":
        # ä¸¦åˆ—å®Ÿè¡Œ
        with ThreadPoolExecutor(max_workers=len(plan["elements"])) as executor:
            futures = {
                executor.submit(execute_ccl, elem, context): elem
                for elem in plan["elements"]
            }
            for future in as_completed(futures):
                results.append(future.result())
    
    elif plan["type"] == "pipeline":
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        prev_result = None
        for elem in plan["elements"]:
            ctx = context
            if prev_result and prev_result.get("answer"):
                ctx = f"{context} Previous: {prev_result['answer'][:200]}"
            result = execute_ccl(elem, ctx)
            results.append(result)
            prev_result = result
    
    else:
        # å˜ä¸€å®Ÿè¡Œ
        results.append(execute_ccl(plan["elements"][0], context))
    
    return {
        "ccl": ccl_expr,
        "plan": plan,
        "results": results,
        "timestamp": datetime.now().isoformat(),
    }


def save_result(result: Dict[str, Any]):
    """çµæœã‚’ä¿å­˜ã€‚"""
    EXPERIMENTS_DIR.mkdir(exist_ok=True)
    
    exp_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = EXPERIMENTS_DIR / f"coord_{exp_id}.json"
    log_file.write_text(json.dumps(result, indent=2, ensure_ascii=False))
    
    print(f"\n[Coordinator] Result saved: {log_file}")
    return log_file


def main():
    if len(sys.argv) < 2:
        print(__doc__)
        return
    
    ccl_expr = sys.argv[1]
    context = sys.argv[2] if len(sys.argv) > 2 else "Hegemonikon CCL execution"
    
    result = coordinate(ccl_expr, context)
    log_file = save_result(result)
    
    # ã‚µãƒãƒªå‡ºåŠ›
    print(f"\n{'='*60}")
    print("SUMMARY")
    print(f"{'='*60}")
    for r in result["results"]:
        status = r.get("status", "unknown")
        thread = r.get("thread", "unknown")
        ccl = r.get("ccl", "unknown")
        print(f"  [{status}] {ccl} -> {thread}")
        if status == "success" and "answer" in r:
            print(f"    Answer: {r['answer'][:100]}...")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
===
#!/usr/bin/env python3
"""
Synergeia Coordinator (ç°¡æ˜“ç‰ˆ)
==============================

CCLå¼ã‚’è§£æã—ã€é©åˆ‡ãªã‚¹ãƒ¬ãƒƒãƒ‰ã«åˆ†é…ã™ã‚‹ç°¡æ˜“Coordinatorã€‚

Usage:
    python coordinator.py "/noe+ || /sop+"
    python coordinator.py "/noe+ |> /dia+ |> /ene+"

Supported:
    - || (ä¸¦åˆ—): è¤‡æ•°ã‚¹ãƒ¬ãƒƒãƒ‰ã§åŒæ™‚å®Ÿè¡Œ
    - |> (ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³): é †æ¬¡å®Ÿè¡Œ
    - @thread[agent]{CCL}: ã‚¹ãƒ¬ãƒƒãƒ‰æŒ‡å®š
"""

import sys
import json
import re
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any

# Add hegemonikon to path for Hermeneus import
HEGEMONIKON_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(HEGEMONIKON_ROOT))

# Import Perplexity API
PERPLEXITY_SCRIPT = Path("/home/laihuip001/oikos/hegemonikon/mekhane/peira/scripts")
sys.path.insert(0, str(PERPLEXITY_SCRIPT))

from perplexity_api import search as perplexity_search

# Import Hermeneus CCL Compiler
try:
    from hermeneus.src import compile_ccl, expand_ccl
    from hermeneus.src import execute_ccl as hermeneus_execute
    from hermeneus.src import parse_ccl as hermeneus_parse
    from hermeneus.src.macros import get_all_macros
    HERMENEUS_AVAILABLE = True
    STANDARD_MACROS = get_all_macros()
except ImportError:
    HERMENEUS_AVAILABLE = False
    STANDARD_MACROS = {}
    hermeneus_execute = None
    print("[Warning] Hermeneus not available, falling back to manual execution")

# Import FEP Selector
try:
    from synergeia.fep_selector import select_thread_by_fep, ThreadRecommendation
    FEP_SELECTOR_AVAILABLE = True
except ImportError:
    FEP_SELECTOR_AVAILABLE = False

EXPERIMENTS_DIR = Path(__file__).parent / "experiments"


# =============================================================================
# Thread Definitions
# =============================================================================

THREAD_REGISTRY = {
    "hermeneus": {
        "name": "Hermeneus CCL Compiler",
        "supported_ccl": ["*"],  # ã™ã¹ã¦ã® CCL ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¯èƒ½
        "executor": "lmql",
        "priority": 0,  # æœ€å„ªå…ˆ
    },
    "antigravity": {
        "name": "Antigravity",
        "supported_ccl": ["/noe", "/dia", "/u", "/bou"],
        "executor": "manual",  # æ‰‹å‹•å®Ÿè¡Œ
        "priority": 10,
    },
    "perplexity": {
        "name": "Perplexity",
        "supported_ccl": ["/sop", "/zet"],
        "executor": "api",
        "priority": 5,
    },
    "claude": {
        "name": "Claude CLI",
        "supported_ccl": ["/s", "/mek"],
        "executor": "cli",
        "priority": 5,
    },
    "gemini": {
        "name": "Gemini CLI",
        "supported_ccl": ["/tek", "/sta"],
        "executor": "cli",
        "priority": 5,
    },
    "codex": {
        "name": "OpenAI Codex",
        "supported_ccl": ["/ene", "/pra"],  # å®Ÿè¡Œç³»
        "executor": "cli",
        "priority": 5,
    },
}


def select_thread(ccl: str, use_fep: bool = True) -> str:
    """
    CCLè¦ç´ ã‹ã‚‰æœ€é©ãªã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é¸æŠã€‚
    
    Args:
        ccl: CCL å¼
        use_fep: True ã®å ´åˆ FEP ãƒ™ãƒ¼ã‚¹ã®è¤‡é›‘åº¦åˆ†æã‚’ä½¿ç”¨
    
    Returns:
        ã‚¹ãƒ¬ãƒƒãƒ‰ ID
    """
    # FEP ãƒ™ãƒ¼ã‚¹é¸æŠ (è¤‡é›‘åº¦ã«åŸºã¥ã)
    if use_fep and FEP_SELECTOR_AVAILABLE:
        try:
            recommendation = select_thread_by_fep(ccl)
            print(f"[FEP] {ccl} -> {recommendation.thread} (complexity: {recommendation.complexity_score:.2f})")
            return recommendation.thread
        except Exception as e:
            print(f"[FEP] Error: {e}, falling back to rule-based selection")
    
    # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹é¸æŠ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
    base = re.match(r"(/\w+)", ccl)
    if not base:
        return "antigravity"
    
    base_ccl = base.group(1)
    
    for thread_id, config in THREAD_REGISTRY.items():
        if base_ccl in config["supported_ccl"]:
            return thread_id

    return "antigravity"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ


# =============================================================================
# CLI Paths
# =============================================================================

CLAUDE_CLI = "/home/laihuip001/oikos/.local/bin/claude"
GEMINI_CLI = "node /home/laihuip001/oikos/.npm/_npx/38c708f8d73fe4c9/node_modules/@google/gemini-cli/bundle/gemini.js"
CODEX_CLI = "/home/laihuip001/oikos/hegemonikon/synergeia/node_modules/.bin/codex"


# =============================================================================
# Executors
# =============================================================================

def execute_perplexity(ccl: str, context: str) -> Dict[str, Any]:
    """Perplexity API ã§CCLã‚’å®Ÿè¡Œã€‚"""
    # CCLã‚’æ¤œç´¢ã‚¯ã‚¨ãƒªã«å¤‰æ›
    query = f"{context} {ccl.replace('/', ' ').replace('+', ' deep').replace('-', ' brief')}"
    
    result = perplexity_search(query)
    
    if "error" in result:
        return {"status": "error", "error": result["error"], "ccl": ccl}
    
    return {
        "status": "success",
        "ccl": ccl,
        "thread": "perplexity",
        "answer": result["answer"],
        "citations": result.get("citations", []),
        "cost": result["cost"],
    }


def execute_claude(ccl: str, context: str) -> Dict[str, Any]:
    """Claude CLI ã§CCLã‚’å®Ÿè¡Œã€‚"""
    import subprocess
    
    prompt = f"{context}\n\nExecute CCL: {ccl}\n\nProvide a detailed response."
    
    try:
        result = subprocess.run(
            [CLAUDE_CLI, "-p", prompt],
            capture_output=True,
            text=True,
            timeout=600,  # æœ€å¤§10åˆ†
            cwd="/home/laihuip001/oikos/hegemonikon"
        )
        answer = result.stdout.strip()
        if result.returncode != 0:
            return {"status": "error", "error": result.stderr, "ccl": ccl}
    except subprocess.TimeoutExpired:
        return {"status": "error", "error": "Timeout (120s)", "ccl": ccl}
    except Exception as e:
        return {"status": "error", "error": str(e), "ccl": ccl}
    
    return {
        "status": "success",
        "ccl": ccl,
        "thread": "claude",
        "answer": answer,
    }


def execute_gemini(ccl: str, context: str) -> Dict[str, Any]:
    """Gemini CLI ã§CCLã‚’å®Ÿè¡Œã€‚"""
    import subprocess
    
    # ãƒ„ãƒ¼ãƒ«æ“ä½œã‚’é¿ã‘ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = f"""You are a Hegemonikon CCL interpreter.
Do NOT use any file system tools. Just analyze and respond in text.

{context}

Analyze and explain the CCL command: {ccl}

Provide a detailed conceptual response without executing any tools."""
    
    try:
        result = subprocess.run(
            [
                "node", 
                "/home/laihuip001/oikos/.npm/_npx/38c708f8d73fe4c9/node_modules/@google/gemini-cli/bundle/gemini.js", 
                "-p", prompt
            ],
            capture_output=True,
            text=True,
            timeout=600,  # æœ€å¤§10åˆ†
            cwd="/home/laihuip001/oikos/hegemonikon"
        )
        # Geminiã¯æœ€åˆã®2è¡ŒãŒãƒ­ã‚°ãªã®ã§é™¤å»
        lines = result.stdout.strip().split("\n")
        answer = "\n".join(lines[2:]) if len(lines) > 2 else result.stdout.strip()
        if result.returncode != 0:
            return {"status": "error", "error": result.stderr, "ccl": ccl}
    except subprocess.TimeoutExpired:
        return {"status": "error", "error": "Timeout (120s)", "ccl": ccl}
    except Exception as e:
        return {"status": "error", "error": str(e), "ccl": ccl}
    
    return {
        "status": "success",
        "ccl": ccl,
        "thread": "gemini",
        "answer": answer,
    }


def execute_codex(ccl: str, context: str) -> Dict[str, Any]:
    """OpenAI Codex CLI ã§CCLã‚’å®Ÿè¡Œã€‚"""
    import subprocess
    
    prompt = f"{context}\n\nExecute CCL: {ccl}\n\nProvide a detailed response."
    
    try:
        result = subprocess.run(
            [CODEX_CLI, "exec", prompt],
            capture_output=True,
            text=True,
            timeout=600,  # æœ€å¤§10åˆ†
            cwd="/home/laihuip001/oikos/hegemonikon"
        )
        # Codex ã¯æœ€åˆã¨æœ€å¾Œã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã®ã§æ•´ç†
        lines = result.stdout.strip().split("\n")
        # "Hello from Codex" ã®ã‚ˆã†ãªå®Ÿéš›ã®å‡ºåŠ›ã‚’æŠ½å‡º
        answer = "\n".join([l for l in lines if not l.startswith("OpenAI Codex") and not l.startswith("---") and not l.startswith("workdir:") and not l.startswith("model:") and not l.startswith("provider:") and not l.startswith("approval:") and not l.startswith("sandbox:") and not l.startswith("reasoning") and not l.startswith("session id:") and not l.startswith("user") and not l.startswith("mcp startup:") and not l.startswith("codex") and not l.startswith("tokens used")])
        if result.returncode != 0:
            return {"status": "error", "error": result.stderr, "ccl": ccl}
    except subprocess.TimeoutExpired:
        return {"status": "error", "error": "Timeout (600s)", "ccl": ccl}
    except Exception as e:
        return {"status": "error", "error": str(e), "ccl": ccl}
    
    return {
        "status": "success",
        "ccl": ccl,
        "thread": "codex",
        "answer": answer.strip(),
    }


def execute_manual(ccl: str, context: str) -> Dict[str, Any]:
    """æ‰‹å‹•å®Ÿè¡Œ (Antigravity)ã€‚"""
    return {
        "status": "manual",
        "ccl": ccl,
        "thread": "antigravity",
        "note": "AntigravityãŒæ‰‹å‹•ã§å®Ÿè¡Œã€‚çµæœã¯çµ±åˆæ™‚ã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
    }


def execute_hermeneus(ccl: str, context: str, compile_only: bool = False) -> Dict[str, Any]:
    """
    Hermeneus CCL Compiler ã§å®Ÿè¡Œã€‚
    
    Args:
        ccl: CCL å¼
        context: å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        compile_only: True ã®å ´åˆã¯ LMQL ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™ã ã‘ï¼ˆå®Ÿè¡Œã—ãªã„ï¼‰
    """
    if not HERMENEUS_AVAILABLE:
        return execute_manual(ccl, context)
    
    try:
        # Step 1: CCL ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« (æ¨™æº–ãƒã‚¯ãƒ­ã‚’ä½¿ç”¨)
        lmql_code = compile_ccl(ccl, macros=STANDARD_MACROS)
        
        if compile_only:
            return {
                "status": "compiled",
                "ccl": ccl,
                "thread": "hermeneus",
                "lmql": lmql_code,
                "macros_available": list(STANDARD_MACROS.keys()),
                "note": "LMQL code generated (not executed)",
            }
        
        # Step 2: å±•é–‹æƒ…å ±ã‚’å–å¾—
        expansion = expand_ccl(ccl, macros=STANDARD_MACROS)
        
        # Step 3: AST ã‚’å–å¾—
        ast = hermeneus_parse(expansion.expanded)
        
        # Step 4: Synteleia Monitor - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç›£æŸ»æƒ…å ±ã‚’è‡ªå‹•è¿½åŠ 
        enriched_context = context
        synteleia_report = None
        
        try:
            from synteleia import SynteleiaOrchestrator, AuditTarget, AuditTargetType
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã« Synteleia ç›£æŸ»ã‚’å®Ÿè¡Œ
            if context and len(context) > 100:
                orch = SynteleiaOrchestrator()
                target = AuditTarget(
                    content=context,
                    target_type=AuditTargetType.THOUGHT,
                    source="user_context"
                )
                audit_result = orch.audit(target)
                
                if audit_result.all_issues:
                    issue_lines = []
                    for issue in audit_result.all_issues:
                        issue_lines.append(f"- [{issue.severity.value.upper()}] {issue.code}: {issue.message}")
                        if issue.suggestion:
                            issue_lines.append(f"  - ææ¡ˆ: {issue.suggestion}")
                    
                    synteleia_report = f"""
## Synteleia ç›£æŸ»çµæœ (è‡ªå‹•è¿½åŠ )
Issues: {len(audit_result.all_issues)}
{chr(10).join(issue_lines)}
"""
                    enriched_context = f"{context}\n{synteleia_report}"
        except ImportError:
            pass  # Synteleia æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        except Exception:
            pass  # ç›£æŸ»ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦– (LLM å®Ÿè¡Œã‚’å„ªå…ˆ)
        
        # Step 5: LLM ã§å®Ÿè¡Œ (hermeneus_execute ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ)
        llm_output = None
        if hermeneus_execute is not None:
            exec_result = hermeneus_execute(ccl, enriched_context)
            if exec_result.status.value == "success":
                llm_output = exec_result.output
        
        return {
            "status": "success",
            "ccl": ccl,
            "thread": "hermeneus",
            "expansion": {
                "original": expansion.original,
                "expanded": expansion.expanded,
                "formal": expansion.formal,
            },
            "ast_type": type(ast).__name__,
            "lmql": lmql_code,
            "llm_output": llm_output,
            "macros_used": len(STANDARD_MACROS),
            "note": "Compiled and executed by Hermeneus.",
        }
        
    except Exception as e:
        return {
            "status": "error",
            "ccl": ccl,
            "thread": "hermeneus",
            "error": str(e),
        }


def execute_ccl(ccl: str, context: str, use_hermeneus: bool = True) -> Dict[str, Any]:
    """
    CCLè¦ç´ ã‚’é©åˆ‡ãªã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œã€‚
    
    Args:
        ccl: CCL å¼
        context: å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        use_hermeneus: True ã®å ´åˆã¯ Hermeneus ã§æ§‹é€ åŒ–ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    """
    # Hermeneus ãƒ¢ãƒ¼ãƒ‰: æ§‹é€ åŒ– LMQL ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    if use_hermeneus and HERMENEUS_AVAILABLE:
        hermeneus_result = execute_hermeneus(ccl, context)
        if hermeneus_result["status"] != "error":
            return hermeneus_result
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        print(f"[Coordinator] Hermeneus failed, falling back to thread selection")
    
    thread = select_thread(ccl)
    
    print(f"[Coordinator] {ccl} -> {thread}")
    
    if thread == "perplexity":
        return execute_perplexity(ccl, context)
    elif thread == "claude":
        return execute_claude(ccl, context)
    elif thread == "gemini":
        return execute_gemini(ccl, context)
    elif thread == "codex":
        return execute_codex(ccl, context)
    else:
        return execute_manual(ccl, context)


# =============================================================================
# CCL Parser
# =============================================================================

def parse_ccl(ccl_expr: str) -> Dict[str, Any]:
    """
    CCLå¼ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦å®Ÿè¡Œè¨ˆç”»ã‚’ç”Ÿæˆã€‚
    
    Returns:
        {
            "type": "parallel" | "pipeline" | "single",
            "elements": [...]
        }
    """
    ccl_expr = ccl_expr.strip()
    
    # ä¸¦åˆ— (||)
    if "||" in ccl_expr:
        elements = [e.strip() for e in ccl_expr.split("||")]
        return {"type": "parallel", "elements": elements}
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (|>)
    if "|>" in ccl_expr:
        elements = [e.strip() for e in ccl_expr.split("|>")]
        return {"type": "pipeline", "elements": elements}
    
    # å˜ä¸€
    return {"type": "single", "elements": [ccl_expr]}


# =============================================================================
# Coordinator
# =============================================================================

def coordinate(ccl_expr: str, context: str = "") -> Dict[str, Any]:
    """
    CCLå¼ã‚’è§£æã—ã€åˆ†æ•£å®Ÿè¡Œã‚’èª¿æ•´ã€‚
    """
    plan = parse_ccl(ccl_expr)
    results = []
    
    print(f"\n{'='*60}")
    print(f"[Coordinator] CCL: {ccl_expr}")
    print(f"[Coordinator] Type: {plan['type']}")
    print(f"[Coordinator] Elements: {plan['elements']}")
    print(f"{'='*60}\n")
    
    if plan["type"] == "parallel":
        # ä¸¦åˆ—å®Ÿè¡Œ
        with ThreadPoolExecutor(max_workers=len(plan["elements"])) as executor:
            futures = {
                executor.submit(execute_ccl, elem, context): elem
                for elem in plan["elements"]
            }
            for future in as_completed(futures):
                results.append(future.result())
    
    elif plan["type"] == "pipeline":
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        prev_result = None
        for elem in plan["elements"]:
            ctx = context
            if prev_result and prev_result.get("answer"):
                ctx = f"{context} Previous: {prev_result['answer'][:200]}"
            result = execute_ccl(elem, ctx)
            results.append(result)
            prev_result = result
    
    else:
        # å˜ä¸€å®Ÿè¡Œ
        results.append(execute_ccl(plan["elements"][0], context))
    
    return {
        "ccl": ccl_expr,
        "plan": plan,
        "results": results,
        "timestamp": datetime.now().isoformat(),
    }


def save_result(result: Dict[str, Any]):
    """çµæœã‚’ä¿å­˜ã€‚"""
    EXPERIMENTS_DIR.mkdir(exist_ok=True)
    
    exp_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = EXPERIMENTS_DIR / f"coord_{exp_id}.json"
    log_file.write_text(json.dumps(result, indent=2, ensure_ascii=False))
    
    print(f"\n[Coordinator] Result saved: {log_file}")
    return log_file


def main():
    if len(sys.argv) < 2:
        print(__doc__)
        return
    
    ccl_expr = sys.argv[1]
    context = sys.argv[2] if len(sys.argv) > 2 else "Hegemonikon CCL execution"
    
    result = coordinate(ccl_expr, context)
    log_file = save_result(result)
    
    # ã‚µãƒãƒªå‡ºåŠ›
    print(f"\n{'='*60}")
    print("SUMMARY")
    print(f"{'='*60}")
    for r in result["results"]:
        status = r.get("status", "unknown")
        thread = r.get("thread", "unknown")
        ccl = r.get("ccl", "unknown")
        print(f"  [{status}] {ccl} -> {thread}")
        if status == "success" and "answer" in r:
            print(f"    Answer: {r['answer'][:100]}...")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
```

---

## ğŸ“Š æ¤œè¨¼çµæœ

### Gemini å‡ºåŠ›æ¯”è¼ƒ

| æ¡ä»¶ | å‡ºåŠ›ä¾‹ |
|:-----|:-------|
| ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã— | ã€ŒSOTAãƒ¢ãƒ‡ãƒ«çµ±åˆãŒå¼·ã¿ã€(æ±ç”¨çš„) |
| Synteleia ç›£æŸ»ä»˜ã | ã€ŒHermeneus 12 Issues ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã€(å…·ä½“çš„) |
| Issue è©³ç´°ä»˜ã | ã€ŒS-030: 115è¡Œé–¢æ•°ã‚’åˆ†å‰²ã›ã‚ˆã€(å®Ÿè¡Œå¯èƒ½) |

### ãƒ†ã‚¹ãƒˆçµæœ

```
hermeneus/tests/test_optimizer.py::8 passed in 1.68s
```

---

## ğŸ’¡ å­¦ã‚“ã ã“ã¨

> **API ã®ã‚³ã‚¹ãƒˆã§ã¯ãªãã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ¿ƒåº¦ãŒä¾¡å€¤ã‚’æ±ºã‚ã‚‹**

- 3å††æ‰•ã£ã¦ã‚‚ã€Œã‚¨ã‚¢ãƒ—ã€ãªã‚‰ä¾¡å€¤ã¯ã‚¼ãƒ­
- 3å††æ‰•ã£ã¦ã€Œå®Ÿç”¨çš„ãª Issue è©³ç´°åˆ†æã€ãªã‚‰ä¾¡å€¤ã¯é«˜ã„
- **Synteleia + LLM ã®çµ„ã¿åˆã‚ã›ãŒæœ€ã‚‚ã‚³ã‚¹ãƒ‘è‰¯ã„**

---

## ğŸ“ˆ API ä½¿ç”¨é‡

| é …ç›® | å€¤ |
|:-----|:---|
| Input tokens | ~6,800 |
| Output tokens | ~11,000 |
| åˆè¨ˆã‚³ã‚¹ãƒˆ | ç´„ 3å†† ($0.02) |

---

*Generated by HegemonikÃ³n H4 Doxa*
