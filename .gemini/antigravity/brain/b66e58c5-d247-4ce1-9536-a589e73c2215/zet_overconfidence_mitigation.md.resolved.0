# 調査依頼書（深掘り版）

## 【出力形式】

構造化テーブル（4列: 手法名, 効果, 実装難易度, 情報源URL）を返す。
その後、"LLMプロンプト設計への具体的な適用方法" を3-4段落で記述。

---

## 【調査目的】

**LLM（特にClaude）の「過信 (Overconfidence)」を低減する実証済み手法**を調査し、Hegemonikónの S4 Epochē（判断停止）スキルに組み込むための設計根拠を得る。

---

## 【時間制約】

⚠️ **情報カットオフ**:
- **採用範囲**: 過去6ヶ月以内（2025年7月〜2026年1月）の情報を優先
- **歴史的参考**: 2024年以前は「参考」としてのみ言及可
- **日付明記**: 各ソースに「取得時点: YYYY年MM月」を付記すること

---

## 【質問】

1. **定義と測定**: LLMの「過信 (Overconfidence / Calibration Gap)」はどのように定義・測定されるか？2025年以降の最新の研究では、どの程度の calibration error が典型的か？

2. **プロンプト戦略**: 過信を低減するためのプロンプト設計手法は何か？（例: Chain-of-Verification, Self-Consistency, Explicit Uncertainty Markers）それぞれの効果とトレードオフは？

3. **自己キャリブレーション**: LLMに「自分の確信度を正確に見積もらせる」技術（Self-Calibration, Verbalized Confidence, Reflection Prompting）の現状と限界は？

4. **Claude 固有**: Claude (Anthropic) には過信低減のための固有の設計や推奨プラクティスがあるか？Constitutional AI との関係は？

5. **実装パターン**: エンタープライズ環境で実際に過信低減を実装している事例はあるか？どのような KPI で効果を測定しているか？

---

## 【オプション詳細】

### 評価視点

Consider these dimensions:
- [A] **技術的観点**: プロンプトエンジニアリングで実装可能か
- [B] **認知科学観点**: 人間の判断停止 (Epochē) との対応関係
- [C] **実用性観点**: 日常的なAIアシスタント用途での適用可能性

### 背景・制約

- **目的**: Hegemonikón S4 Epochē スキルの設計に反映
- **前提条件**: Claude を主要モデルとして使用、プロンプトベースの介入が中心
- **優先評価軸**: 実装可能性 > 効果の大きさ > 学術的新規性

---

## 【末尾指示】

⚠️ **必須**:
- 各出典に URL を付記
- 引用できない情報は「わかりません」と明記
- 複数ソースが矛盾する場合は「ソースA vs ソースB」と明示

---

*Generated by Hegemonikón O3 Zētēsis v2.1*
