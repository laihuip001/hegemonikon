# FEP Persistence & Auto-EpochÄ“ å®Ÿè£…è¨ˆç”» v2

> **Goal 1**: Aè¡Œåˆ— Dirichlet å­¦ç¿’ã‚’ `/bye` ã§ä¿å­˜ã€`/boot` ã§èª­è¾¼
> **Goal 2**: é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ™‚ã«è‡ªå‹• `/epo` æ¨å¥¨

---

## /noe ç™ºè¦‹: é‡å¤§ãªä¸å‚™

| å•é¡Œ | å½±éŸ¿ | è§£æ±ºç­– |
|:-----|:-----|:-------|
| FEP ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã”ã¨ã«æ–°è¦ç”Ÿæˆ | å­¦ç¿’ãŒè“„ç©ã•ã‚Œãªã„ | ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä»‹ã—ã¦çŠ¶æ…‹å…±æœ‰ |
| `update_A_dirichlet()` ã®å‘¼ã³å‡ºã—ãŒæœªå®šç¾© | Aè¡Œåˆ—ãŒæ›´æ–°ã•ã‚Œãªã„ | ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã§ä¸€é€£ã®å‡¦ç†ã‚’å®Ÿè¡Œ |

---

## STAGE 0: Blindspot + Scale

| ã‚«ãƒ†ã‚´ãƒª | ãƒªã‚¹ã‚¯ | è§£æ±ºç­– |
|:---------|:-------|:-------|
| Dependencies | **é«˜ â†’ è§£æ±º** | `run_fep_with_learning()` ã§çŠ¶æ…‹ç®¡ç† |
| Scope | ä¸­ | encoding.py + SKILL.md ã®ã¿å¤‰æ›´ |

ğŸ“ **Scale**: Meso

---

## STAGE 1: Strategy Selection

**Explore/Exploit**: Exploitï¼ˆç¢ºå®Ÿæ€§é‡è¦–ï¼‰

**é¸æŠ**: Plan B (Robust) â€” ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã§ä¸€é€£ã®å‡¦ç†ã‚’ã‚«ãƒ—ã‚»ãƒ«åŒ–

---

## STAGE 3: Blueprint

### [MODIFY] [encoding.py](file:///home/laihuip001/oikos/hegemonikon/mekhane/fep/encoding.py)

```python
def run_fep_with_learning(
    obs_tuple: Tuple[int, int, int],
    a_matrix_path: str = "/home/laihuip001/oikos/mneme/.hegemonikon/fep/learned_A.npy",
    learning_rate: float = 50.0,
) -> Dict:
    """FEP æ¨è«– + Dirichlet å­¦ç¿’ + æ°¸ç¶šåŒ–ã‚’ä¸€é€£ã§å®Ÿè¡Œã€‚
    
    å‡¦ç†ãƒ•ãƒ­ãƒ¼: load â†’ step â†’ update_A_dirichlet â†’ save
    
    Returns:
        agent.step() ã®çµæœ + should_epoche ãƒ•ãƒ©ã‚°
    """
    from mekhane.fep import HegemonikÃ³nFEPAgent
    import os
    
    agent = HegemonikÃ³nFEPAgent(use_defaults=True)
    
    # 1. å­¦ç¿’æ¸ˆã¿ A-Matrix ãŒã‚ã‚Œã°èª­è¾¼
    agent.load_learned_A(a_matrix_path)
    
    # 2. æ¨è«–å®Ÿè¡Œ
    flat_obs = obs_tuple[0] + 2 * obs_tuple[1] + obs_tuple[2]
    result = agent.step(observation=flat_obs)
    
    # 3. Dirichlet æ›´æ–°
    agent.update_A_dirichlet(observation=flat_obs, learning_rate=learning_rate)
    
    # 4. ä¿å­˜
    os.makedirs(os.path.dirname(a_matrix_path), exist_ok=True)
    agent.save_learned_A(a_matrix_path)
    
    # 5. Auto-EpochÄ“ ãƒ•ãƒ©ã‚°
    result["should_epoche"] = result.get("entropy", 0) >= 2.0
    
    return result


def should_trigger_epoche(agent_result: Dict, threshold: float = 2.0) -> bool:
    """é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ™‚ã« EpochÄ“ ã‚’æ¨å¥¨ã™ã‚‹ã‹åˆ¤å®šã€‚"""
    return agent_result.get("entropy", 0.0) >= threshold
```

---

### [MODIFY] [o1-noesis/SKILL.md](file:///home/laihuip001/oikos/.agent/skills/ousia/o1-noesis/SKILL.md)

FEP Cognitive Layer ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ä½¿ç”¨ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°:

```python
from mekhane.fep.encoding import (
    encode_noesis_output,
    run_fep_with_learning,
    generate_fep_feedback_markdown,
)

# PHASE 5 ã®çµæœã‹ã‚‰è¦³å¯Ÿå€¤ã‚’ç”Ÿæˆ
obs = encode_noesis_output(
    confidence_score=phase5_result["confidence_score"],
    uncertainty_zones=phase5_result["uncertainty_zones"],
)

# FEP æ¨è«– + å­¦ç¿’ + æ°¸ç¶šåŒ–
result = run_fep_with_learning(obs)

# å‡ºåŠ›ç”Ÿæˆ
feedback = generate_fep_feedback_markdown(result, f"conf={phase5_result['confidence_score']}")
print(feedback)

# Auto-EpochÄ“
if result["should_epoche"]:
    print("âš ï¸ é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ¤œå‡º â†’ /epo ã‚’æ¨å¥¨")
```

---

### [DELETE] bye.md, boot.md ã®å¤‰æ›´

`/bye`, `/boot` ã¸ã®å¤‰æ›´ã¯ **ä¸è¦** ã«ãªã£ãŸã€‚
`run_fep_with_learning()` ãŒæ¯å› load/save ã™ã‚‹ãŸã‚ã€‚

---

## Verification Plan

```bash
PYTHONPATH=. pytest tests/test_fep_agent.py -v -k "learning"
```

---

## å®Ÿè£…é †åº

1. `encoding.py` ã« `run_fep_with_learning()` + `should_trigger_epoche()` è¿½åŠ 
2. ãƒ†ã‚¹ãƒˆè¿½åŠ 
3. O1 NoÄ“sis SKILL.md ã® FEP Cognitive Layer ã‚’æ›´æ–°
4. O2 BoulÄ“sis SKILL.md ã‚‚åŒæ§˜ã«æ›´æ–°
