# 調査依頼書（深掘り版）

テーマ: LLMの自己認識限界とマルチエージェント環境でのモデル識別

---

## 0. あなた（調査者）への依頼（最重要）

私は **AIエージェントシステム (Hegemonikón)** において、複数のLLMモデル（Claude, Gemini等）を切り替えて運用する際の**モデル識別問題**を検討している。

現在発生している問題:
- モデルをGeminiに切り替えても、AIが「私はClaudeです」と名乗り続ける
- AIはシステムプロンプトに書かれた名前を信じるだけで、実際の基盤モデルを検出できない

仮説:
> **「LLMはパターン認識で自己を推論できるが、内省的に自分が何者か実感することはできない」**

以下について、**一般論で終わらせず**、2024-2026時点の最新研究・設計パターン・ベストプラクティスを**一次情報と査読論文**で裏付けてほしい:

1. LLMの自己認識（Self-Awareness）能力の現状と限界
2. マルチエージェント環境でのモデル識別のベストプラクティス
3. 「推論はできるが実感はできない」という仮説の妥当性

結論は「可能/不可能」の二択ではなく、**設計上の対処法**と**運用戦略**まで落とし込んで提示してほしい。

---

## 1. 調査対象の定義（用語の揺れに対応）

### 1-1. 概念の確認

まず以下を確定してください:

- **Self-Awareness in LLMs**: 「自己認識」の学術的定義（意識、メタ認知、自己参照能力の区別）
- **Introspection vs Inference**: 内省（内部状態の直接観察）と推論（外部情報からのパターン認識）の違い
- **Multi-Agent Systems**: LLMを複数組み合わせるシステムの標準的アーキテクチャ

### 1-2. 比較対象

- Claude (Anthropic)
- Gemini (Google DeepMind)
- GPT-4 / GPT-5 (OpenAI)
- Llama 3 (Meta)

---

## 2. 調査すべき論点（抜け漏れ禁止）

### A. LLMの自己認識能力（学術研究）

**A1. Self-Awareness Research (2024-2026)**
- LLMは自分の能力・限界を正確に認識できるか？
- Self-Calibration（自己校正）に関する研究
- "I don't know" を言える能力（知識境界の認識）

**A2. Introspection Capabilities**
- LLMは内部状態（活性化パターン、注意重み等）にアクセスできるか？
- Chain-of-Thought は真の「思考」か、それともパターンマッチか？

**A3. Identity Confusion**
- LLMが誤った自己同一性を主張する事例研究
- プロンプト注入による自己認識の操作可能性

### B. マルチエージェント環境でのモデル識別

**B1. Industry Best Practices**
- AutoGen, CrewAI, LangGraph 等でのエージェント識別手法
- 複数モデル併用時のメタデータ管理

**B2. Agent Identifier Patterns**
- 各エージェントの発言に署名を付与する設計パターン
- IDE/オーケストレーターからのモデル情報注入

**B3. 信頼性と検証**
- 自己申告 vs 外部注入 の信頼性比較
- モデル識別の自動検証手法

### C. 設計思想（哲学的考察）

**C1. 「推論 vs 実感」仮説の検証**
- この区別は学術的に妥当か？
- 意識研究（Consciousness Studies）との接点

**C2. FEP (自由エネルギー原理) との関連**
- 自己モデル（Self-Model）の限界
- 予測誤差最小化の観点からの解釈

**C3. 設計上の含意**
- 「AIの自己申告は信用しない」という設計原則の妥当性
- 「意志より環境」（外部化・環境化）アプローチ

---

## 3. 成果物（この構成で必ず提出）

1. **結論サマリー**（10行以内）
2. **比較表**（モデル別の自己認識能力評価）
3. **学術研究サーベイ**: 2024-2026の主要論文リスト
4. **ベストプラクティス**: マルチエージェント識別の推奨パターン
5. **設計提案**: 問題を解決するアーキテクチャ案
6. **仮説検証**: 「推論 vs 実感」の妥当性評価
7. **根拠リンク**（必須）:
   - arXiv / ACL / NeurIPS / ICML 等の査読論文
   - 公式ドキュメント（Anthropic, Google, OpenAI）
   - 信頼できる技術ブログ（AI Alignment Forum等）

---

## 4. 調査ルール（品質担保）

- **新情報優先**: 2024-2026の情報を優先
- **事実/推測分離**: 必ず明確に分離
- **根拠必須**: 「一般に〜と言われる」で終わらず、根拠を提示
- **決断可能**: 設計判断に使える形に落とし込む

---

## 5. 追加要件（任意だが望ましい）

- **反論**: 「AIは自己認識できる」という反対意見とその根拠
- **失敗パターン集**: モデル識別で発生しがちな問題
- **実装例**: Agent Identifier Protocol の参考実装

---

## 6. 与件（背景/制約/目的）

- **目的**: Agent Identifier Protocol v2.0 の設計判断に使う
- **前提条件**: 
  - Hegemonikónシステム（Claude/Gemini併用）
  - Antigravity IDE環境
  - FEP（自由エネルギー原理）ベースの認知アーキテクチャ
- **検討中の選択肢**:
  - Option A: IDE側からモデル情報を注入
  - Option B: AIに「自動検出不可」を認めさせる
- **優先する評価軸**: 信頼性 > 実装容易性 > 哲学的妥当性

---

*Generated by /ask workflow | 2026-01-24*
