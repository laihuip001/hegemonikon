# ğŸ–¥ï¸ Windows PC ç§»è¡Œã‚¬ã‚¤ãƒ‰

> **å¯¾è±¡ç’°å¢ƒ**: Windows + RTX 2070 Super (8GB VRAM)
> **ç›®çš„**: ãƒ­ãƒ¼ã‚«ãƒ« LLM + Synergeia åŸºç›¤æ§‹ç¯‰
> **ä½œæˆæ—¥**: 2026-02-01

---

## 1. ç’°å¢ƒæ¦‚è¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è‡ªå®… PC (Windows)                                  â”‚
â”‚  â”œâ”€â”€ GPU: RTX 2070 Super (8GB)                      â”‚
â”‚  â”œâ”€â”€ Ollama: ãƒ­ãƒ¼ã‚«ãƒ« LLM ã‚µãƒ¼ãƒãƒ¼                  â”‚
â”‚  â””â”€â”€ Synergeia: 5ã‚¹ãƒ¬ãƒƒãƒ‰ Coordinator               â”‚
â”‚       â”œâ”€â”€ T1: Gemini (API)                          â”‚
â”‚       â”œâ”€â”€ T2: Claude (API)                          â”‚
â”‚       â”œâ”€â”€ T3: Perplexity (API)                      â”‚
â”‚       â”œâ”€â”€ T4: OpenManus + Qwen 7B (ãƒ­ãƒ¼ã‚«ãƒ«) â†NEW   â”‚
â”‚       â””â”€â”€ T5: Codex (API)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

### Step 1: Ollama ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```powershell
# winget ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
winget install Ollama.Ollama

# ã¾ãŸã¯å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰
# https://ollama.com/download/windows
```

### Step 2: Qwen 2.5 7B ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```powershell
# ãƒ¢ãƒ‡ãƒ«å–å¾— (~4GB)
ollama pull qwen2.5:7b

# å‹•ä½œç¢ºèª
ollama run qwen2.5:7b "ã“ã‚“ã«ã¡ã¯"

# API ç¢ºèª (åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«)
curl http://localhost:11434/api/generate -d '{"model":"qwen2.5:7b","prompt":"Hello"}'
```

### Step 3: è‡ªå‹•èµ·å‹•è¨­å®š

```powershell
# ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã§èµ·å‹•æ™‚ã«å®Ÿè¡Œ
# ã‚³ãƒãƒ³ãƒ‰: ollama serve
```

---

## 3. Synergeia çµ±åˆ

### 3.1 æ–°ã‚¹ãƒ¬ãƒƒãƒ‰å®šç¾©

```python
# synergeia/thread_config.py

THREAD_LOCAL_LLM = {
    "name": "LocalLLM",
    "type": "ollama",
    "base_url": "http://localhost:11434/v1",
    "model": "qwen2.5:7b",
    "capabilities": ["reasoning", "coding", "japanese"],
    "cost": 0,  # ç„¡æ–™
    "latency": "medium",
    "availability": "24/7",
}
```

### 3.2 Coordinator æ‹¡å¼µ

```python
# synergeia/coordinator.py ã«è¿½åŠ 

class OllamaAdapter:
    """ãƒ­ãƒ¼ã‚«ãƒ« LLM ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼"""
    
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.model = "qwen2.5:7b"
    
    async def query(self, prompt: str) -> str:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/api/generate",
                json={"model": self.model, "prompt": prompt, "stream": False},
                timeout=120,
            )
            return response.json()["response"]
```

### 3.3 OpenManus è¨­å®š

```toml
# OpenManus/config/config.toml

[llm]
model = "qwen2.5:7b"
base_url = "http://localhost:11434/v1"
api_key = "ollama"
max_tokens = 4096
temperature = 0.0
```

---

## 4. GPU ãƒ¡ãƒ¢ãƒªç®¡ç†

### æ¨å¥¨è¨­å®š

```powershell
# ç’°å¢ƒå¤‰æ•° (çœãƒ¡ãƒ¢ãƒªãƒ¢ãƒ¼ãƒ‰)
set OLLAMA_NUM_PARALLEL=1
set OLLAMA_MAX_LOADED_MODELS=1
```

### VRAM ä½¿ç”¨é‡

| ãƒ¢ãƒ‡ãƒ« | é‡å­åŒ– | VRAM | ä½™è£• |
|:-------|:-------|:-----|:-----|
| Qwen 2.5 7B | Q4_K_M | ~5GB | 3GB |
| LLaMA 3 8B | Q4_K_M | ~5.5GB | 2.5GB |
| Phi-3 Mini | Q4_K_M | ~3GB | 5GB |

---

## 5. æ¤œè¨¼ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] Ollama èµ·å‹•ç¢ºèª (`ollama serve`)
- [ ] Qwen 2.5 7B å¿œç­”ç¢ºèª
- [ ] OpenManus å‹•ä½œç¢ºèª
- [ ] Synergeia Coordinator çµ±åˆãƒ†ã‚¹ãƒˆ
- [ ] 24æ™‚é–“é€£ç¶šç¨¼åƒãƒ†ã‚¹ãƒˆ

---

## 6. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

| ç—‡çŠ¶ | å¯¾å‡¦ |
|:-----|:-----|
| CUDA ã‚¨ãƒ©ãƒ¼ | NVIDIA ãƒ‰ãƒ©ã‚¤ãƒæ›´æ–° |
| OOM (ãƒ¡ãƒ¢ãƒªä¸è¶³) | Q4_K_S é‡å­åŒ–ã«å¤‰æ›´ |
| ãƒ¬ã‚¹ãƒãƒ³ã‚¹é…ã„ | `OLLAMA_NUM_GPU=999` ã§ GPU ãƒ•ãƒ«æ´»ç”¨ |

---

## 7. é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«

- `experiments/activation_steering_mvp.ipynb`
- `experiments/openmanus_mvp.ipynb`
- `docs/gpu_required_tasks.md`

---

*Ready for Windows migration! ğŸš€*
