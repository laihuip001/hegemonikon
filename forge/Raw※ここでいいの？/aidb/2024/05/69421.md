---
source_url: https://ai-data-base.com/archives/69421
captured_at: 2026-01-18T22:02:09.840175
title: "ファインチューニングがLLMの幻覚（ハルシネーション）に与える影響　Googleなどによる検証結果"
publish_date: 2024.05.22
tags: ["サーベイ37", "ペルソナ・シミュレーション36", "オープンソース25", "エンタメ・アート23", "SE9", "コーディング56", "画像認識20", "ハルシネーション", "音声8", "教育・キャリア9", "実証137", "画像生成9", "医療・ヘルスケア33", "ハルシネーション16", "ポジション8", "ロボット6", "実証", "ファインチューニング16", "製造・デザイン9", "ベンチマーク・リソース22", "分析54", "テクニカルレポート15", "安全性39", "マルチモーダル23", "金融・経済10", "セキュリティ16", "ファインチューニング", "LLM", "政治・社会29", "手法426", "LLM659", "RAG50", "エージェント128", "プロンプト技術156"]
conversion_method: browser_subagent_v1_parallel
batch_id: 4
is_premium: unknown
---

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F69421&text=%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%8CLLM%E3%81%AE%E5%B9%BB%E8%A6%9A%EF%BC%88%E3%83%8F%E3%83%AB%E3%82%B7%E3%83%8D%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%EF%BC%89%E3%81%AB%E4%B8%8E%E3%81%88%E3%82%8B%E5%BD%B1%E9%9F%BF%E3%80%80Google%E3%81%AA%E3%81%A9%E3%81%AB%E3%82%88%E3%82%8B%E6%A4%9C%E8%A8%BC%E7%B5%90%E6%9E%9C) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F69421&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F69421)

[実証](https://ai-data-base.com/archives/type-tag/empirical)

[LLM](https://ai-data-base.com/archives/tech-tag/llm)[ファインチューニング](https://ai-data-base.com/archives/tech-tag/finetuning)[ハルシネーション](https://ai-data-base.com/archives/tech-tag/hallucination)

LLMをファインチューニングしたとき、幻覚（ハルシネーション）誤った情報の生成にはどう影響するのかが調査されました。指示に従うタスクや人間のフィードバックを通じて、望ましい行動をとるように調整することを「微調整」または「ファインチューニング」と呼びます。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69421-1024x576.jpg)

**参照論文情報**

  * タイトル：Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?

  * 著者：Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig

  * 所属：Technion – Israel Institute of Technology, Google Research




**本記事の関連研究** ：

  * [マルチモーダルLLMにおけるハルシネーション（幻覚）の原因と対策](https://ai-data-base.com/archives/68720)

  * [LLMの内部状態を観察することで「出力がハルシネーションか否かを判別する」手法『LLMファクトスコープ』](https://ai-data-base.com/archives/61651)

  * [LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ](https://ai-data-base.com/archives/58767)

  * [ファインチューニングデータが十分に大きい場合、タスク性能向上に追加の事前学習は不要の可能性 Googleなどによるスケーリング則の実験から](https://ai-data-base.com/archives/64001)




## 背景

モデルは微調整中に新しい事実に遭遇することがあります。そして、新しい知識に触れることが、いわゆる「ハルシネーション」を引き起こす可能性があると考えられています。モデルが事実とは異なる誤った応答を生成する現象です。  
事前学習においては知識を効果的に利用する方法を学びますが、微調整を通じて新しい知識を取得するのは難しいのではないかと議論されていますが、明確な答えは出ていません。

そこで今回研究者らは、新しい知識がLLMの性能に与える影響を詳細に調査することにしました。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信



[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F69421&text=%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%8CLLM%E3%81%AE%E5%B9%BB%E8%A6%9A%EF%BC%88%E3%83%8F%E3%83%AB%E3%82%B7%E3%83%8D%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%EF%BC%89%E3%81%AB%E4%B8%8E%E3%81%88%E3%82%8B%E5%BD%B1%E9%9F%BF%E3%80%80Google%E3%81%AA%E3%81%A9%E3%81%AB%E3%82%88%E3%82%8B%E6%A4%9C%E8%A8%BC%E7%B5%90%E6%9E%9C) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F69421&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F69421)
