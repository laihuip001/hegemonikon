---
source_url: https://ai-data-base.com/archives/78145
captured_at: 2026-01-18T23:29:12.405823
title: "直感に頼るようなタスクだとLLMに「ステップバイステップで考えて」は逆効果"
publish_date: 2024.11.06
tags: ["画像認識 20", "サーベイ 37", "安全性 39", "政治・社会 29", "分析 54", "オープンソース 25", "プロンプト技術", "マルチモーダル 23", "SE 9", "テクニカルレポート 15", "手法 426", "エージェント 128", "金融・経済 10", "RAG 50", "ベンチマーク・リソース 22", "ファインチューニング 16", "LLM", "ロボット 6", "プロンプト技術 156", "ハルシネーション 16", "コーディング 56", "分析", "LLM 659", "ペルソナ・シミュレーション 36", "ポジション 8", "セキュリティ 16", "実証 137", "エンタメ・アート 23", "製造・デザイン 9", "教育・キャリア 9", "画像生成 9", "音声 8", "医療・ヘルスケア 33"]
conversion_method: fast_collect_v1
batch_id: 1
---

# 直感に頼るようなタスクだとLLMに「ステップバイステップで考えて」は逆効果

2024.11.072025.12.22

[深堀り解説](https://ai-data-base.com/archives/category/deep-dive)

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAQAAQMAAABF07nAAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAJZJREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAewEEHgABB9i6GAAAAABJRU5ErkJggg==) ![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78145_eye.jpeg)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F78145&text=%E7%9B%B4%E6%84%9F%E3%81%AB%E9%A0%BC%E3%82%8B%E3%82%88%E3%81%86%E3%81%AA%E3%82%BF%E3%82%B9%E3%82%AF%E3%81%A0%E3%81%A8LLM%E3%81%AB%E3%80%8C%E3%82%B9%E3%83%86%E3%83%83%E3%83%97%E3%83%90%E3%82%A4%E3%82%B9%E3%83%86%E3%83%83%E3%83%97%E3%81%A7%E8%80%83%E3%81%88%E3%81%A6%E3%80%8D%E3%81%AF%E9%80%86%E5%8A%B9%E6%9E%9C) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F78145&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F78145)

[分析](https://ai-data-base.com/archives/type-tag/analysis)

[LLM](https://ai-data-base.com/archives/tech-tag/llm)[プロンプト技術](https://ai-data-base.com/archives/tech-tag/prompt)

本記事では、LLMの性能向上テクニック「Chain of Thought（CoT）」が逆効果になるケースについての研究を紹介します。

これまでCoTは多くの課題でLLMの性能を向上させる手法として知られてきましたが、人間の認知研究から「考えすぎると失敗するケース」があることにヒントを得た研究者たちが、LLMでも同様の現象が起きるのではないかと考えて検証を行いました。

「ステップバイステップで考えて」があまり意味がないタスクもあるという話は以前にもありましたが、本件では逆効果まで確認されています。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78145-1024x576.png)

**本記事の関連研究**

  * [計画のステップが増えるほど、LLMは最初の目標を見失っていく傾向がある](https://ai-data-base.com/archives/77302)

  * [CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果](https://ai-data-base.com/archives/62364)

  * [CoT（思考の連鎖）は数学や論理で劇的に性能を向上させる一方、常識や知識のタスクでほとんど効果がない](https://ai-data-base.com/archives/75942)




## 背景

LLMの性能を向上させる手法として、「思考の連鎖」(Chain of Thought、以下CoT)というテクニックが広く使われています。モデルに「ステップバイステップで考えて」と指示したり、回答の過程で考え方を説明してもらったりと段階的な推論を行わせる方法です。

CoTを使うと、いくつかの課題でモデルの性能が向上することが分かっています。多くの場合は数学的な問題や論理的な推論を必要とする課題で効果を発揮します。また、最新のLLMでは標準的な機能としてCoTが組み込まれている場合もあります。

しかし、今回研究者たちは「CoTが性能を低下させるケースもあるのではないか」という疑問を持ちました。というのも、人間の場合、「考えすぎること」が逆効果になる場面があることが心理学の研究などで分かっているからです。

例えば、以下のような場合に人間は「考えすぎると」むしろパフォーマンスが下がることが知られています。

  * 暗黙のうちに学んだパターンを認識する時

  * 顔を記憶して認識する時

  * 例外を含むデータから規則性を見つける時




そこで研究チームは「人間が考えすぎると失敗するようなタスクで、LLMもCoTを使うと失敗するかどうか」という課題を立てました。

この課題を検証するため、心理学の研究で使われた様々なタスクを大規模に再現し、最新のLLMで実験を行いました。

実験アプローチや結果の詳細を以下にまとめます。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信



[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F78145&text=%E7%9B%B4%E6%84%9F%E3%81%AB%E9%A0%BC%E3%82%8B%E3%82%88%E3%81%86%E3%81%AA%E3%82%BF%E3%82%B9%E3%82%AF%E3%81%A0%E3%81%A8LLM%E3%81%AB%E3%80%8C%E3%82%B9%E3%83%86%E3%83%83%E3%83%97%E3%83%90%E3%82%A4%E3%82%B9%E3%83%86%E3%83%83%E3%83%97%E3%81%A7%E8%80%83%E3%81%88%E3%81%A6%E3%80%8D%E3%81%AF%E9%80%86%E5%8A%B9%E6%9E%9C) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F78145&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F78145)
