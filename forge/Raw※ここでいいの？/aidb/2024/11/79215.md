---
source_url: https://ai-data-base.com/archives/79215
captured_at: 2026-01-18T23:29:27.723126
title: "Gemini-1.5-proやGPT-4o-miniなどの性能を上回るLLaVA-o1（11Bパラメータ）"
publish_date: 2024.11.25
tags: ["画像認識 20", "サーベイ 37", "安全性 39", "政治・社会 29", "分析 54", "オープンソース 25", "マルチモーダル 23", "SE 9", "テクニカルレポート 15", "手法", "手法 426", "エージェント 128", "画像認識", "金融・経済 10", "RAG 50", "ベンチマーク・リソース 22", "オープンソース", "ファインチューニング 16", "LLM", "ロボット 6", "プロンプト技術 156", "ハルシネーション 16", "コーディング 56", "LLM 659", "ペルソナ・シミュレーション 36", "ポジション 8", "セキュリティ 16", "実証 137", "エンタメ・アート 23", "製造・デザイン 9", "教育・キャリア 9", "画像生成 9", "音声 8", "医療・ヘルスケア 33"]
conversion_method: fast_collect_v1
batch_id: 1
---

# Gemini-1.5-proやGPT-4o-miniなどの性能を上回るLLaVA-o1（11Bパラメータ）

2024.11.262025.12.22

[深堀り解説](https://ai-data-base.com/archives/category/deep-dive)

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAQAAQMAAABF07nAAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAJZJREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAewEEHgABB9i6GAAAAABJRU5ErkJggg==) ![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_eye.jpg)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F79215&text=Gemini-1.5-pro%E3%82%84GPT-4o-mini%E3%81%AA%E3%81%A9%E3%81%AE%E6%80%A7%E8%83%BD%E3%82%92%E4%B8%8A%E5%9B%9E%E3%82%8BLLaVA-o1%EF%BC%8811B%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%EF%BC%89) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F79215&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F79215)

[手法](https://ai-data-base.com/archives/type-tag/method)

[LLM](https://ai-data-base.com/archives/tech-tag/llm)[オープンソース](https://ai-data-base.com/archives/tech-tag/%e3%82%aa%e3%83%bc%e3%83%97%e3%83%b3%e3%82%bd%e3%83%bc%e3%82%b9)[画像認識](https://ai-data-base.com/archives/tech-tag/%e7%94%bb%e5%83%8f%e8%aa%8d%e8%ad%98)

本記事では、視覚と言語を組み合わせたマルチモーダルLLMの推論能力を大きく向上させた新しい研究を紹介します。

これまでの視覚言語モデルは一般的に論理的な推論を苦手としており、また推論過程でエラーを起こしやすいという問題を抱えていました。そこで研究チームは、人間のように段階的に考えを組み立てていく新しいアプローチを開発し、その有効性を実証しました。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_thum2-1024x576.png)

**本記事の関連研究**

  * [画像分析機能を持つオープンソースLLM『LLaVA-1.5』登場。手持ちの画像を分析可能。GPT-4Vとの違い](https://ai-data-base.com/archives/56440)

  * [マルチモーダルLLMの高難易度ベンチマーク『MMMU-Pro』で明らかになったこと](https://ai-data-base.com/archives/75326)

  * [マルチモーダルLLMは従来手法よりゼロショット画像分類の精度が大幅に向上 Googleが報告](https://ai-data-base.com/archives/70709)

  * [GPT-4o、Gemini、Claude 3などにおける「長いプロンプトのマルチモーダルタスク」性能を測定した結果](https://ai-data-base.com/archives/69354)




## 背景

視覚は世界を理解し認知能力を拡張するために言語と同様に重要な要素とされています。そのため、言語と視覚を統合しながら推論するマルチモーダルモデルの開発は重要な課題とされています。

通常、視覚言語モデル（VLM）は論理的推論を必要とするタスクは得意としていません。Chain-of-Thought（ステップバイステップの思考）を導入すると性能は向上するものの、多くのVLMは依然として推論過程でエラーや幻覚出力（事実とは異なる回答）を生成するという課題を抱えています。

研究チームの分析によると、上記の問題の主な原因は、既存のVLMの推論プロセスが十分に構造化されていないことにあるようです。

推論プロセスの構造化に成功している例といえばOpenAI o1です。しかしo1の技術的詳細はブラックボックスのままです。

そこで今回研究者らは、VLMが自律的にステップバイステップの推論を行う能力を向上させるアプローチを新たに考えることにしました。そうして生まれたのがLLaVA-o1と呼ばれる方法論です。

LLaVA-o1は特定の単一モデルを呼称するものではなく、ベースモデルをトレーニングするフレームワークそのものです。なおLlama-3.2VをベースモデルとしたLLaVA-o1モデルは実際に開発されました。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAi4AAAG0AQMAAAAvkT4kAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAADRJREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgGsBeOwAAfpzPGMAAAAASUVORK5CYII=)![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_1.png)LLaVA-o1が、より大規模なオープンソースモデルや一部のクローズドソースモデルを上回る性能を示すグラフ
