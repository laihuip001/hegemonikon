---
source_url: https://ai-data-base.com/archives/86361
captured_at: 2026-01-18T23:30:13.729297
title: "推論時のトークン数を80%以上削減しながら出力精度を保つプロンプト手法の新提案"
publish_date: 2025.03.16
tags: ["画像認識 20", "サーベイ 37", "安全性 39", "政治・社会 29", "分析 54", "オープンソース 25", "プロンプト技術", "マルチモーダル 23", "SE 9", "テクニカルレポート 15", "手法", "手法 426", "エージェント 128", "金融・経済 10", "RAG 50", "ベンチマーク・リソース 22", "ファインチューニング 16", "LLM", "ロボット 6", "プロンプト技術 156", "ハルシネーション 16", "LLM 660", "コーディング 56", "ペルソナ・シミュレーション 36", "ポジション 8", "セキュリティ 16", "実証 137", "エンタメ・アート 23", "製造・デザイン 9", "教育・キャリア 9", "画像生成 9", "音声 8", "医療・ヘルスケア 33"]
conversion_method: fast_collect_v1
batch_id: 1
---

# 推論時のトークン数を80%以上削減しながら出力精度を保つプロンプト手法の新提案

2025.03.172025.12.22

[深堀り解説](https://ai-data-base.com/archives/category/deep-dive)

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAQAAQMAAABF07nAAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAJZJREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAewEEHgABB9i6GAAAAABJRU5ErkJggg==) ![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361_eye.png)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F86361&text=%E6%8E%A8%E8%AB%96%E6%99%82%E3%81%AE%E3%83%88%E3%83%BC%E3%82%AF%E3%83%B3%E6%95%B0%E3%82%9280%25%E4%BB%A5%E4%B8%8A%E5%89%8A%E6%B8%9B%E3%81%97%E3%81%AA%E3%81%8C%E3%82%89%E5%87%BA%E5%8A%9B%E7%B2%BE%E5%BA%A6%E3%82%92%E4%BF%9D%E3%81%A4%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88%E6%89%8B%E6%B3%95%E3%81%AE%E6%96%B0%E6%8F%90%E6%A1%88) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F86361&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F86361)

[手法](https://ai-data-base.com/archives/type-tag/method)

[LLM](https://ai-data-base.com/archives/tech-tag/llm)[プロンプト技術](https://ai-data-base.com/archives/tech-tag/prompt)

本記事では、LLMが行う推論プロセスの効率性を向上させる新たな研究を紹介します。

プロンプト手法としても推論モデルの内部動作としてもよく使用されているCoTは、推論過程を詳しく書き出すため、正確性は高まるものの処理時間が長くなりがちです。そこで本研究は、人間が自然に行うような簡潔なメモを取る形で、効率的な推論を実現する手法を提案しています。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361_thum2-1024x576.png)

**本記事の関連研究**

  * [三段論法でLLMの推論能力を高める プロンプト手法の新提案](https://ai-data-base.com/archives/82746)

  * [LLMには正解例だけでなく、「よくある間違い例」と理由も一緒に教えるのが有効](https://ai-data-base.com/archives/77507)

  * [LLMの「自己対話」により複雑な問題の解決能力を飛躍的に向上させる手法『Iteration of Thought』](https://ai-data-base.com/archives/76134)




## 背景

LLMは、複雑な問題を段階的に詳しく解きほぐすことで優れた結果を示しています。これは「Chain-of-Thought（CoT）」という手法で、問題を細分化し、一つひとつ丁寧に説明するように答えを導く方法です。多くの推論モデル（o1やDeepSeek-R1など）の内部ではCoTのような段階的な推論プロセスが展開する傾向があります。

しかし、CoTには問題点もあります。CoTを用いた推論は、途中の説明が長く、使われるトークン数（文字数）が非常に多いため、計算時間が長くなりコストも高くなります。特にリアルタイム性が求められる場面では、このような冗長性が大きな障壁となっていました。

人間が実際に問題を解く時を考えると、私たちは全ての細部を詳細に書き出すことはほとんどありません。むしろ、重要な要素だけを短くメモしながら、最小限の情報で効率よく考えを進めています。つまり、長く詳細な説明はせず、核心部分だけを短く書き留めるというやり方です。

そこで今回Zoomの研究者らは、LLMにも人間が自然に行っているような「簡潔で核心的な推論方法」を取り入れようと考えました。この考えに基づき、従来の詳細な説明をする代わりに、必要最小限の短い推論メモを作成しながら回答を導く新たな方法が提案されました。

考案された手法は、推論プロセスの中で余分な説明を省き、本当に必要な情報だけを短くまとめます。実験の結果、CoTと同程度かそれ以上の正確さを維持したまま、トークン数を大幅に削減し、計算時間やコストを著しく抑えることが可能になりました。

プロンプトテンプレートを参照して試せるため、ぜひ確認してみてください。以下で詳しく紹介します。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信



[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F86361&text=%E6%8E%A8%E8%AB%96%E6%99%82%E3%81%AE%E3%83%88%E3%83%BC%E3%82%AF%E3%83%B3%E6%95%B0%E3%82%9280%25%E4%BB%A5%E4%B8%8A%E5%89%8A%E6%B8%9B%E3%81%97%E3%81%AA%E3%81%8C%E3%82%89%E5%87%BA%E5%8A%9B%E7%B2%BE%E5%BA%A6%E3%82%92%E4%BF%9D%E3%81%A4%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88%E6%89%8B%E6%B3%95%E3%81%AE%E6%96%B0%E6%8F%90%E6%A1%88) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F86361&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F86361)
