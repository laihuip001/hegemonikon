---
source_url: https://ai-data-base.com/archives/99123
captured_at: 2026-01-18T23:30:47.946069
title: "学習なしでLLMを強くするための「文脈を育てる」という発想"
publish_date: 2025.12.15
tags: ["画像認識 20", "サーベイ 37", "RAG", "安全性 39", "政治・社会 29", "分析 54", "オープンソース 25", "マルチモーダル 23", "SE 9", "テクニカルレポート 15", "手法", "手法 426", "エージェント 128", "金融・経済 10", "RAG 50", "ベンチマーク・リソース 22", "ファインチューニング 16", "LLM", "ロボット 6", "プロンプト技術 156", "ハルシネーション 16", "LLM 660", "コーディング 56", "ペルソナ・シミュレーション 36", "ポジション 8", "セキュリティ 16", "実証 137", "エンタメ・アート 23", "製造・デザイン 9", "教育・キャリア 9", "画像生成 9", "音声 8", "医療・ヘルスケア 33"]
conversion_method: fast_collect_v1
batch_id: 1
---

# 学習なしでLLMを強くするための「文脈を育てる」という発想

2025.12.162025.12.18

[深堀り解説](https://ai-data-base.com/archives/category/deep-dive)

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABDgAAAQ4AQMAAADW3v7MAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAKVJREFUGBntwTEBAAAAwiD7p14JT2AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAXAU93gABbLlRWwAAAABJRU5ErkJggg==) ![](https://ai-data-base.com/wp-content/uploads/2025/12/AIDB_eye_99123.png)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F99123&text=%E5%AD%A6%E7%BF%92%E3%81%AA%E3%81%97%E3%81%A7LLM%E3%82%92%E5%BC%B7%E3%81%8F%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AE%E3%80%8C%E6%96%87%E8%84%88%E3%82%92%E8%82%B2%E3%81%A6%E3%82%8B%E3%80%8D%E3%81%A8%E3%81%84%E3%81%86%E7%99%BA%E6%83%B3) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F99123&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F99123)

[手法](https://ai-data-base.com/archives/type-tag/method)

[LLM](https://ai-data-base.com/archives/tech-tag/llm)[RAG](https://ai-data-base.com/archives/tech-tag/rag)

本記事では、LLMの性能を、追加の学習なしで向上させる新しいアプローチとして「文脈を育てる」という考え方を紹介します。

最近では、モデルの性能を高めるためには追加学習や微調整が定番となっていますが、これには多くの時間やコストがかかります。そこで、失敗した回答の例を集め、それらに共通するパターンを整理しておくというRAGの手法が提案されています。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-data-base.com/wp-content/uploads/2025/12/AIDB_99123-1024x576.png)

**本記事の関連研究**

  * [LLMを新しいタスクに順応させる「文脈内学習」における効率的なコンテキストの作り方](https://ai-data-base.com/archives/96070)

  * [文脈内学習は「少数事例からの単純な学習だけでなく、言語モデルが持つ幅広い適応能力」](https://ai-data-base.com/archives/80765)

  * [LLMにプロンプトのみで仮想的な強化学習を発生させる方法](https://ai-data-base.com/archives/91141)




## 背景

仕事でLLMを使っていると、同じモデルでも「少し指示を変えるだけで」結果が大きく変わることがあります。うまくいった理由はまだ振り返りやすいですが、うまくいかなかった原因は見過ごされがちです。

そもそも、LLMを業務に合わせて強化する方法には大きく分けて二つあります。一つはモデルそのものを調整する追加学習の方法で、「教師あり微調整」と呼ばれています。これは効果が出やすい反面、モデルの重みに手を加える必要があるため計算コストが高く、もともと備わっていた知識や得意分野が失われるリスクもあります。

もう一つの方法は、モデル自体を変更せずに、入力文の工夫によって出力をコントロールするやり方です。これは「文脈内学習」と呼ばれ、プロンプトに例やルールを書き込むことで、その場でやり方を覚えたように動かすものです。ただしこの方法には、プロンプト設計に大きく左右されやすく、幅広く効果を出すのが難しいという課題があります。

最近では文脈そのものを自動的に生成・更新する研究が活発になっています。しかし、既存のアプローチには二つの課題があります。一つは、個々の失敗例に引っ張られてノイズが混じりやすいこと。もう一つは、改善案を追加する際に「本当に良くなったか」を確かめずに情報を積み上げてしまい、結局、役に立たないどころか逆効果の内容が蓄積してしまうという点です。

そこで本記事では、「文脈は完成されたものではなく、少しずつ育てていくもの」と捉えた手法を取り上げます。単に成功例を増やすのではなく、失敗から共通点を見つけてメモにし、それを慎重に選んで文脈に残す。以下で詳しく見ていきましょう。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信



[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F99123&text=%E5%AD%A6%E7%BF%92%E3%81%AA%E3%81%97%E3%81%A7LLM%E3%82%92%E5%BC%B7%E3%81%8F%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AE%E3%80%8C%E6%96%87%E8%84%88%E3%82%92%E8%82%B2%E3%81%A6%E3%82%8B%E3%80%8D%E3%81%A8%E3%81%84%E3%81%86%E7%99%BA%E6%83%B3) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F99123&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F99123)
