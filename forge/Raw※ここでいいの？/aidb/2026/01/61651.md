---
source_url: https://ai-data-base.com/archives/61651
captured_at: 2026-01-19T07:31:48.395506
title: "LLMの内部状態を観察することで「出力がハルシネーションか否かを判別する」手法『LLMファクトスコープ』 - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

大規模言語モデル（LLM）は「ハルシネーション」と呼ばれる、事実とは異なる出力を生成することが問題視されています。

**関連研究** ：[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ](https://ai-data-
base.com/archives/58767)

ハルシネーションへの対策は、出力を外部データと照合するなどが一般的ですが、対処療法と言えなくもありません。

今回研究者らは、LLMが出力を生成する際に「事実と非事実で異なる内部状態を示す」という仮説に基づき、新しい検証アプローチ『LLMファクトスコープ』を開発しました。実験では、96%以上の精度で事実が判別できたと示されています。

本記事では、課題、アプローチの概要、実験結果を紹介します。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2024/01/AIDB_61651_thum-1024x576.jpg)

