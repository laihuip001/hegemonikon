---
source_url: https://ai-data-base.com/archives/80765
captured_at: 2026-01-19T07:35:12.921027
title: "文脈内学習は「少数事例からの単純な学習だけでなく、言語モデルが持つ幅広い適応能力」 - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

本記事では、言語モデルの文脈内学習について、DeepMindの研究者たちが提案する新しい理論的枠組みを紹介します。

これまで、文脈内学習は主に「少数の例から学習する能力」として議論されてきましたが、実際には指示に従う能力や役割演技、時系列データの予測など、より幅広い現象を含んでいます。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2024/12/AIDB_80765-1024x576.jpg)

**発表者情報**

  * 研究者：Andrew Kyle Lampinen et al.

  * 研究機関：Google DeepMind

**文脈内学習の関連研究**

  * [LLMのプロンプトに数百から数千の例を含める超長尺のコンテキスト内学習（In-context learning）とファインチューニングの性能比較](https://ai-data-base.com/archives/68564)

  * [Claude 3などのLLMはコンテキスト内学習によって線形回帰・非線形回帰問題タスクもこなす](https://ai-data-base.com/archives/67496)

  * [コンテキスト内で重要な情報同士が離れすぎるとLLMの性能は大幅に下がる](https://ai-data-base.com/archives/77563)

## 背景

LLMが持つ「文脈から学習する能力」に大きな注目が集まっています。これはどういうものかと言うと、「新しいことを学ぶ方法を学ぶ」能力です。

もともとAIの分野では、研究者らは言語モデルのメモリや情報処理の仕組みを工夫することで、新しい課題への対応力を高めようとしてきました。そしてTransformerベースのモデルでは、少数の例を見ただけで新しい課題に取り組めるようになり、まるで「少しヒントを与えるだけで自分で考えて解決できる」ような能力を示すようになりました。

この能力の源泉として、学習データに含まれる特徴的なパターン、例えばデータの突発的な集中や繰り返し現れる類似構造などが重要な役割を果たしていると考えられています。しかし、これまでの研究は主に「少数の例から正解を教わりながら学習する」という限定的な枠組みに焦点が当てられてきました。

実際には言語モデルは、指示に従って行動を変えたり、特定の役割を演じたり、時系列データを予測したりするなど、より幅広い学習能力を持っています。これらの能力が互いに独立したものなのか、それとも統一的な学習の枠組みで説明できるのかは、まだ明らかになっていません。

そこでGoogle
DeepMindの研究者らは、言語モデルの文脈学習能力を包括的に理解するため、学習の内容、方法、応用という複数の観点から検討を行いました。本稿では、言語モデルが示す多様な学習能力の本質に迫ります。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信

[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

