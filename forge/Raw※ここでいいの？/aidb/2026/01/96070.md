---
source_url: https://ai-data-base.com/archives/96070
captured_at: 2026-01-19T07:26:42.369158
title: "LLMを新しいタスクに順応させる「文脈内学習」における効率的なコンテキストの作り方 - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

LLMを特定のタスクに使う際、いくつかの例を見せるだけで推論させる「文脈内学習」という技術があります。モデルを再訓練する必要がなく、とても便利な方法です。最近では、LLMが一度に処理できる文脈が大幅に長くなったため、数百個もの例を見せる「多ショット文脈内学習」が可能になり、性能も向上することが分かってきました。

しかし大量の例を毎回処理するのはデメリットもあるため、同様の性能を維持しながらトークン効率の良い方法が求められています。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2025/10/AIDB_96070-1024x576.png)

**本記事の関連研究**

  * [プロンプトに例を多く載せるほど、どんなタスクでも性能が上がるのか？DeepMindによる『Many-shot Learning』の実験結果](https://ai-data-base.com/archives/67883)

  * [スタンフォード大学の研究者ら、GPT-4oとGemini1.5 Proで「マルチモーダルモデルにおける『Many-Shot』の効果」を検証](https://ai-data-base.com/archives/69211)

## 背景

LLMを特定のタスクに活用する方法として、「文脈内学習」（英文では「In-Context
Learning」）があります。これは何かというと、モデルに具体的な例をいくつか見せることで、そのタスクのやり方を理解させる方法です。例えば、英語を日本語に翻訳するタスクであれば、いくつかの翻訳例を見せてから「では、この新しい英文を翻訳してください」と頼むわけです。この方法の素晴らしいところは、モデル自体を再訓練する必要がないということです。

しかし、ここに大きな問題があります。多数の例を毎回モデルに読ませるということは、それだけコストが高くなるということです。例えば、150個の例を含む入力は、数万トークン、場合によっては数十万トークンにもなります。トークンというのは、テキストの最小単位のようなもので、計算量や料金の基準になります。このような長い入力を処理するには、時間もお金も大量にかかってしまうのです。また、精度面にも悪い影響があるかもしれません。

そこで本記事では、多数の例を活用した文脈内学習を効率化する手法について調べました。

以下で詳しくお話しします。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信

[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

