# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- hermeneus/src/ CCL ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£
"""
CCL Dispatch â€” CCL å¼ã®æ¤œçŸ¥ãƒ»ãƒ‘ãƒ¼ã‚¹ãƒ»æ§‹é€ è¡¨ç¤ºã‚’ç’°å¢ƒå¼·åˆ¶ã™ã‚‹ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ

æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã® AI ãŒ CCL å¼ã‚’å—ã‘å–ã£ãŸã¨ã:
  python hermeneus/src/dispatch.py '{CCLå¼}'

Step 0: HermÄ“neus ãƒ‘ãƒ¼ã‚¹ (ç’°å¢ƒå¼·åˆ¶)
Step 1: AST æ§‹é€ è¡¨ç¤º
Step 2: å®Ÿè¡Œè¨ˆç”»ã®ææ¡ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå‡ºåŠ›

Usage:
    python hermeneus/src/dispatch.py '/dia+~*/noe'
    python hermeneus/src/dispatch.py '{(/dia+~*/noe)~*/pan+}~*{(/dia+~*/noe)~*\\pan+}'
"""

import sys
import os
from pathlib import Path
from typing import Any, Dict, List, Optional

# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ‘ã‚¹è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    from typing import TypedDict
except ImportError:
    from typing_extensions import TypedDict


# PURPOSE: G2 â€” dispatch() æˆ»ã‚Šå€¤ã®å‹å®šç¾©
class RouteContext(TypedDict, total=False):
    """Aristos L3 ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ–‡è„ˆã€‚"""
    source: str
    target: str
    route: List[str]
    depth_level: int
    wf_count: int


class DispatchResult(TypedDict, total=False):
    """dispatch() é–¢æ•°ã®æˆ»ã‚Šå€¤å‹ã€‚

    total=False ã«ã™ã‚‹ã“ã¨ã§ã€å…¨ã‚­ãƒ¼ãŒã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ã«ãªã‚‹ã€‚
    ã“ã‚Œã«ã‚ˆã‚Šæ®µéšçš„ã«ã‚­ãƒ¼ã‚’è¨­å®šã™ã‚‹ dispatch() ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨æ•´åˆã™ã‚‹ã€‚
    """
    success: bool
    ccl: str
    ast: Any                            # CCLParser ã® AST ãƒãƒ¼ãƒ‰
    tree: str                           # AST æœ¨æ§‹é€ ã®ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
    workflows: List[str]                # æŠ½å‡ºã•ã‚ŒãŸ WF ID (e.g. ["/noe", "/dia"])
    wf_paths: Dict[str, str]            # WF ID â†’ çµ¶å¯¾ãƒ‘ã‚¹
    wf_submodules: Dict[str, List[str]] # WF ID â†’ ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
    wf_summaries: Dict[str, Dict[str, Any]]  # WF ID â†’ è¦ç´„æƒ…å ±
    plan_template: str                  # å®Ÿè¡Œè¨ˆç”»ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    macro_plan: Optional[Dict[str, Any]]     # ãƒã‚¯ãƒ­å®Ÿè¡Œè¨ˆç”»
    error: Optional[str]                     # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    exhaustive_warnings: List[str]      # ç¶²ç¾…æ€§ãƒã‚§ãƒƒã‚¯è­¦å‘Š
    parallel_warnings: List[str]        # ä¸¦åˆ—å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯è­¦å‘Š
    route_context: RouteContext         # Aristos ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ–‡è„ˆ


# PURPOSE: AST ã‚’ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆä»˜ãã§æœ¨æ§‹é€ è¡¨ç¤º
def format_ast_tree(node, indent=0) -> str:
    """AST ã‚’ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆä»˜ãã§æœ¨æ§‹é€ è¡¨ç¤º"""
    from hermeneus.src.ccl_ast import (
        Workflow, Oscillation, Fusion, Sequence, ConvergenceLoop,
        ColimitExpansion, ForLoop, IfCondition, WhileLoop
    )
    
    prefix = "  " * indent
    lines = []
    
    if isinstance(node, Oscillation):
        mode = "~*" if node.convergent else ("~!" if node.divergent else "~")
        lines.append(f"{prefix}Oscillation ({mode})")
        lines.append(f"{prefix}  left:")
        lines.append(format_ast_tree(node.left, indent + 2))
        lines.append(f"{prefix}  right:")
        lines.append(format_ast_tree(node.right, indent + 2))
    elif isinstance(node, ColimitExpansion):
        lines.append(f"{prefix}ColimitExpansion (\\)")
        lines.append(f"{prefix}  body:")
        lines.append(format_ast_tree(node.body, indent + 2))
    elif isinstance(node, Fusion):
        lines.append(f"{prefix}Fusion (*)")
        lines.append(f"{prefix}  left:")
        lines.append(format_ast_tree(node.left, indent + 2))
        lines.append(f"{prefix}  right:")
        lines.append(format_ast_tree(node.right, indent + 2))
    elif isinstance(node, Sequence):
        lines.append(f"{prefix}Sequence (_)")
        for i, step in enumerate(node.steps):
            lines.append(f"{prefix}  step {i+1}:")
            lines.append(format_ast_tree(step, indent + 2))
    elif isinstance(node, ConvergenceLoop):
        lines.append(f"{prefix}ConvergenceLoop (>>)")
        lines.append(f"{prefix}  body:")
        lines.append(format_ast_tree(node.body, indent + 2))
        lines.append(f"{prefix}  cond: {node.condition.var} {node.condition.op} {node.condition.value}")
    elif isinstance(node, Workflow):
        ops = ""
        if node.operators:
            from hermeneus.src.ccl_ast import OpType
            ops_map = {
                OpType.DEEPEN: "+", OpType.CONDENSE: "-",
                OpType.ASCEND: "^", OpType.EXPAND: "!",
                OpType.QUERY: "?", OpType.INVERT: "\\",
                OpType.DIFF: "'",
            }
            ops = "".join(ops_map.get(op, "") for op in node.operators)
        lines.append(f"{prefix}Workflow: /{node.id}{ops}")
    else:
        lines.append(f"{prefix}{type(node).__name__}: {node}")
    
    return "\n".join(lines)


# PURPOSE: AST ã‹ã‚‰å…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ ID ã‚’å†å¸°çš„ã«æŠ½å‡º
def extract_workflows(node) -> list:
    """AST ã‹ã‚‰å…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ ID ã‚’å†å¸°çš„ã«æŠ½å‡º"""
    from hermeneus.src.ccl_ast import (
        Workflow, Oscillation, Fusion, Sequence, ConvergenceLoop,
        ColimitExpansion
    )
    wfs = []
    if isinstance(node, Workflow):
        wfs.append(f"/{node.id}")
    elif isinstance(node, Oscillation):
        wfs.extend(extract_workflows(node.left))
        wfs.extend(extract_workflows(node.right))
    elif isinstance(node, ColimitExpansion):
        wfs.extend(extract_workflows(node.body))
    elif isinstance(node, Fusion):
        wfs.extend(extract_workflows(node.left))
        wfs.extend(extract_workflows(node.right))
    elif isinstance(node, Sequence):
        for step in node.steps:
            wfs.extend(extract_workflows(step))
    elif isinstance(node, ConvergenceLoop):
        wfs.extend(extract_workflows(node.body))
    return wfs


# PURPOSE: AST å†…ã®æ¡ä»¶åˆ†å²ã®ç¶²ç¾…æ€§ã‚’ãƒã‚§ãƒƒã‚¯ (Pepsis Rust Phase 2 â€” exhaustive_check.md)
def exhaustive_check(node, depth=0) -> list[str]:
    """AST ã‚’å†å¸°èµ°æŸ»ã—ã€/dia+ ã‚’å«ã‚€å¼ã§æ¡ä»¶åˆ†å²ã®ç¶²ç¾…æ€§ã‚’æ¤œè¨¼ã€‚

    Rust ã® exhaustive pattern matching ã«ç€æƒ³ã‚’å¾—ãŸè¨­è¨ˆã€‚
    I: ãŒã‚ã‚Œã° E: (else) ãŒå¿…é ˆã€‚EI: ãƒã‚§ãƒ¼ãƒ³ã‚‚ E: ã§çµ‚ç«¯ã™ã¹ãã€‚

    Returns:
        list of warning strings (ç©ºãªã‚‰å•é¡Œãªã—)
    """
    from hermeneus.src.ccl_ast import (
        Workflow, Oscillation, Fusion, Sequence, ConvergenceLoop,
        ColimitExpansion, ForLoop, IfCondition, WhileLoop,
        TaggedBlock, Pipeline, Parallel, OpType
    )

    warnings = []

    if isinstance(node, IfCondition):
        # I: ãŒã‚ã‚‹ãŒ E: ãŒãªã„ â†’ éç¶²ç¾…çš„
        if node.else_branch is None:
            cond_str = f"{node.condition.var} {node.condition.op} {node.condition.value}"
            warnings.append(
                f"âš ï¸ [exhaustive] I:[{cond_str}] ã« E:{{}} (else) ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
                f" å…¨ã‚±ãƒ¼ã‚¹ã‚’ç¶²ç¾…ã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
            )
        else:
            # else_branch ã‚‚å†å¸°ãƒã‚§ãƒƒã‚¯
            warnings.extend(exhaustive_check(node.else_branch, depth + 1))
        # then_branch ã‚‚å†å¸°ãƒã‚§ãƒƒã‚¯
        warnings.extend(exhaustive_check(node.then_branch, depth + 1))

    elif isinstance(node, Sequence):
        for step in node.steps:
            warnings.extend(exhaustive_check(step, depth + 1))
    elif isinstance(node, Oscillation):
        warnings.extend(exhaustive_check(node.left, depth + 1))
        warnings.extend(exhaustive_check(node.right, depth + 1))
    elif isinstance(node, Fusion):
        warnings.extend(exhaustive_check(node.left, depth + 1))
        warnings.extend(exhaustive_check(node.right, depth + 1))
    elif isinstance(node, ColimitExpansion):
        warnings.extend(exhaustive_check(node.body, depth + 1))
    elif isinstance(node, ConvergenceLoop):
        warnings.extend(exhaustive_check(node.body, depth + 1))
    elif isinstance(node, ForLoop):
        warnings.extend(exhaustive_check(node.body, depth + 1))
    elif isinstance(node, WhileLoop):
        warnings.extend(exhaustive_check(node.body, depth + 1))
    elif isinstance(node, TaggedBlock):
        warnings.extend(exhaustive_check(node.body, depth + 1))
    elif isinstance(node, Pipeline):
        for step in node.steps:
            warnings.extend(exhaustive_check(step, depth + 1))
    elif isinstance(node, Parallel):
        for branch in node.branches:
            warnings.extend(exhaustive_check(branch, depth + 1))

    return warnings


# PURPOSE: ä¸¦åˆ—å®Ÿè¡Œ (||) ãƒãƒ¼ãƒ‰ã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ (Pepsis Rust Phase 2 â€” parallel_model.md)
def parallel_safety_check(node, depth=0) -> list[str]:
    """AST ã‚’å†å¸°èµ°æŸ»ã—ã€|| ãƒãƒ¼ãƒ‰ã®å®‰å…¨æ€§ã‚’æ¤œè¨¼ã€‚

    Rust ã® Send/Sync ç‰¹æ€§ã«ç€æƒ³ã‚’å¾—ãŸè¨­è¨ˆã€‚
    åŒä¸€ WF ãŒè¤‡æ•°ãƒ–ãƒ©ãƒ³ãƒã«å‡ºç¾ã™ã‚‹å ´åˆã€ãƒ‡ãƒ¼ã‚¿ç«¶åˆã®å¯èƒ½æ€§ã‚’è­¦å‘Šã€‚

    Returns:
        list of warning strings (ç©ºãªã‚‰å•é¡Œãªã—)
    """
    from hermeneus.src.ccl_ast import (
        Workflow, Oscillation, Fusion, Sequence, ConvergenceLoop,
        ColimitExpansion, ForLoop, IfCondition, WhileLoop,
        TaggedBlock, Pipeline, Parallel, OpType
    )

    warnings = []

    if isinstance(node, Parallel):
        # å„ãƒ–ãƒ©ãƒ³ãƒã‹ã‚‰ WF ID ã‚’åé›†
        branch_wfs = []
        for branch in node.branches:
            wfs = set(extract_workflows(branch))
            branch_wfs.append(wfs)

        # ãƒ–ãƒ©ãƒ³ãƒé–“ã®é‡è¤‡ WF ã‚’æ¤œå‡º
        for i in range(len(branch_wfs)):
            for j in range(i + 1, len(branch_wfs)):
                shared = branch_wfs[i] & branch_wfs[j]
                if shared:
                    shared_str = ", ".join(sorted(shared))
                    warnings.append(
                        f"âš ï¸ [parallel] || ãƒ–ãƒ©ãƒ³ãƒ {i+1} ã¨ {j+1} ã§åŒä¸€ WF ({shared_str}) ãŒé‡è¤‡ã€‚"
                        f" ãƒ‡ãƒ¼ã‚¿ç«¶åˆã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚`*` ã§å…±æœ‰å‚ç…§ã«ã™ã‚‹ã‹ã€ç‹¬ç«‹ã—ãŸ WF ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚"
                    )

        # å„ãƒ–ãƒ©ãƒ³ãƒã‚‚å†å¸°ãƒã‚§ãƒƒã‚¯
        for branch in node.branches:
            warnings.extend(parallel_safety_check(branch, depth + 1))

    elif isinstance(node, Sequence):
        for step in node.steps:
            warnings.extend(parallel_safety_check(step, depth + 1))
    elif isinstance(node, Oscillation):
        warnings.extend(parallel_safety_check(node.left, depth + 1))
        warnings.extend(parallel_safety_check(node.right, depth + 1))
    elif isinstance(node, Fusion):
        warnings.extend(parallel_safety_check(node.left, depth + 1))
        warnings.extend(parallel_safety_check(node.right, depth + 1))
    elif isinstance(node, ColimitExpansion):
        warnings.extend(parallel_safety_check(node.body, depth + 1))
    elif isinstance(node, ConvergenceLoop):
        warnings.extend(parallel_safety_check(node.body, depth + 1))
    elif isinstance(node, ForLoop):
        warnings.extend(parallel_safety_check(node.body, depth + 1))
    elif isinstance(node, WhileLoop):
        warnings.extend(parallel_safety_check(node.body, depth + 1))
    elif isinstance(node, TaggedBlock):
        warnings.extend(parallel_safety_check(node.body, depth + 1))
    elif isinstance(node, Pipeline):
        for step in node.steps:
            warnings.extend(parallel_safety_check(step, depth + 1))
    elif isinstance(node, IfCondition):
        warnings.extend(parallel_safety_check(node.then_branch, depth + 1))
        if node.else_branch:
            warnings.extend(parallel_safety_check(node.else_branch, depth + 1))

    return warnings


# PURPOSE: WF ID â†’ .agent/workflows/*.md ã®çµ¶å¯¾ãƒ‘ã‚¹ã«è§£æ±ºã€‚
def resolve_wf_paths(wf_ids: list[str]) -> dict[str, str]:
    """WF ID â†’ .agent/workflows/*.md ã®çµ¶å¯¾ãƒ‘ã‚¹ã«è§£æ±ºã€‚

    /dia â†’ dia.md, /noe â†’ noe.md ã®ã‚ˆã†ã«å¯¾å¿œã€‚
    å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯é™¤å¤–ã€‚

    Returns:
        {"/dia": "/absolute/path/.agent/workflows/dia.md", ...}
    """
    project_root = Path(__file__).parent.parent.parent
    wf_dir = project_root / ".agent" / "workflows"
    paths = {}
    for wf_id in wf_ids:
        clean = wf_id.lstrip("/")
        wf_path = wf_dir / f"{clean}.md"
        if wf_path.exists():
            paths[wf_id] = str(wf_path.resolve())
        else:
            # ã‚¨ã‚¤ãƒªã‚¢ã‚¹æ¤œç´¢: boot+ â†’ boot, dia+ â†’ dia ãªã©
            # (æ¼”ç®—å­ä»˜ãã®å ´åˆã€ãƒ™ãƒ¼ã‚¹åã§æ¤œç´¢)
            base = clean.rstrip("+-^!?'")
            base_path = wf_dir / f"{base}.md"
            if base_path.exists():
                paths[wf_id] = str(base_path.resolve())
    return paths


# PURPOSE: WF å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ§‹é€ çš„è¦ç´„ã‚’è‡ªå‹•æŠ½å‡º (L1 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè‡ªå‹•å……å¡«)
def resolve_wf_summaries(wf_paths: dict[str, str]) -> dict[str, dict]:
    """WF å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ purpose / phases / output_hint ã‚’æŠ½å‡ºã€‚

    æŠ½å‡ºå…ƒ:
      - YAML frontmatter ã® description: â†’ purpose (fallback)
      - blockquote `> **ç›®çš„**:` â†’ purpose (å„ªå…ˆ)
      - `## å‡¦ç†ãƒ•ãƒ­ãƒ¼` or `PHASE N` è¦‹å‡ºã— â†’ phases
      - `## å‡ºåŠ›å½¢å¼` â†’ output_hint

    Returns:
        {"/noe": {"purpose": "...", "phases": [...], "output_hint": "..."}, ...}
    """
    import re
    import yaml as _yaml

    summaries: dict[str, dict] = {}

    for wf_id, wf_path_str in wf_paths.items():
        summary: dict = {"purpose": "", "phases": [], "output_hint": ""}

        try:
            content = Path(wf_path_str).read_text(encoding="utf-8")
        except Exception:
            summaries[wf_id] = summary
            continue

        # --- 1. YAML frontmatter ã‹ã‚‰ description ã‚’æŠ½å‡º ---
        fm_match = re.match(r"^---\n(.*?)\n---", content, re.DOTALL)
        if fm_match:
            try:
                fm = _yaml.safe_load(fm_match.group(1))
                if isinstance(fm, dict) and fm.get("description"):
                    summary["purpose"] = fm["description"]
            except Exception:
                pass

        # --- 2. blockquote `> **ç›®çš„**:` ã§ä¸Šæ›¸ã (ã‚ˆã‚Šå…·ä½“çš„) ---
        # frontmatter ç›´å¾Œ ã€œ æœ€åˆã® ## ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—ã¾ã§ã«é™å®š
        # (å„ STEP/PHASE å†…ã®å±€æ‰€çš„ãª `> **ç›®çš„**:` ã‚’æ‹¾ã‚ãªã„)
        body = content[fm_match.end():] if fm_match else content
        # æœ€åˆã® "## " è¦‹å‡ºã—ã®å‰ã¾ã§ã‚’å†’é ­å®šç¾©ãƒ–ãƒ­ãƒƒã‚¯ã¨ã™ã‚‹
        intro_lines = []
        for line in body.split("\n"):
            if line.startswith("## "):
                break
            intro_lines.append(line)
        intro_block = "\n".join(intro_lines)
        purpose_match = re.search(
            r">\s*\*\*ç›®çš„\*\*\s*[:ï¼š]\s*(.+)", intro_block
        )
        if purpose_match:
            summary["purpose"] = purpose_match.group(1).strip()

        # --- 3. PHASE / STEP è¡Œã‚’æŠ½å‡º ---
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: "N. **PHASE N" or "N. **STEP N" (numbered list items)
        phase_pattern = re.compile(
            r"^\d+\.\s+\*\*(?:PHASE|STEP)\s+[\d.]+[\w]*\s*(?:â€”|:|-)\s*(.+?)\*\*",
            re.MULTILINE,
        )
        phases = phase_pattern.findall(content)
        if phases:
            summary["phases"] = [p.strip().rstrip("*") for p in phases]

        # fallback: `## PHASE N` è¦‹å‡ºã—
        if not summary["phases"]:
            heading_pattern = re.compile(
                r"^##\s+PHASE\s+\d+\s*(?:â€”|:|-)\s*(.+)", re.MULTILINE
            )
            summary["phases"] = [
                h.strip() for h in heading_pattern.findall(content)
            ]

        # --- 4. å‡ºåŠ›å½¢å¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å†’é ­ã‚’å–å¾— ---
        output_match = re.search(
            r"##\s*å‡ºåŠ›å½¢å¼\s*\n((?:.*\n){1,5})", content
        )
        if output_match:
            hint = output_match.group(1).strip()
            # ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ (```) ã‚„ç©ºè¡Œã‚’é™¤å¤–
            hint_lines = [
                l for l in hint.split("\n")
                if l.strip()
                and not l.strip().startswith("```")
                and "---" not in l
            ]
            if hint_lines:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ãŒã‚ã‚Œã°ãã‚Œã ã‘ã€ãªã‘ã‚Œã°æœ€åˆã®è¡Œ
                table_lines = [l for l in hint_lines if l.strip().startswith("|")]
                hint = table_lines[0] if table_lines else hint_lines[0]
                summary["output_hint"] = hint.strip()[:120]

        summaries[wf_id] = summary

    return summaries


# PURPOSE: WF å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ã‚¹ã‚’æŠ½å‡ºã€‚
def resolve_submodules(wf_paths: dict[str, str]) -> dict[str, list[str]]:
    """WF å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ã‚¹ã‚’æŠ½å‡ºã€‚

    WF ã® md ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿ã€## ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã®
    Markdown ãƒªãƒ³ã‚¯ [name](../path) ã‚’æ¤œå‡ºã—ã¦çµ¶å¯¾ãƒ‘ã‚¹ã«è§£æ±ºã™ã‚‹ã€‚

    Returns:
        {"/bye": ["/abs/path/value-pitch.md", "/abs/path/pitch_gallery.md"], ...}
    """
    import re
    submodules: dict[str, list[str]] = {}

    # Markdown ãƒªãƒ³ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³: [text](relative/path.md)
    link_pattern = re.compile(r'\[([^\]]+)\]\(([^)]+\.md)\)')

    for wf_id, wf_path_str in wf_paths.items():
        wf_path = Path(wf_path_str)
        subs: list[str] = []

        try:
            content = wf_path.read_text(encoding='utf-8')
        except Exception:
            continue

        # ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        in_submodule_section = False
        for line in content.split('\n'):
            if line.strip().startswith('## ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«') or line.strip().startswith('## Sub'):
                in_submodule_section = True
                continue
            if in_submodule_section and line.strip().startswith('## '):
                break  # æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å…¥ã£ãŸ
            if in_submodule_section:
                for match in link_pattern.finditer(line):
                    rel_path = match.group(2)
                    # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«è§£æ±º
                    abs_path = (wf_path.parent / rel_path).resolve()
                    if abs_path.exists():
                        subs.append(str(abs_path))

        if subs:
            submodules[wf_id] = subs

    return submodules


# PURPOSE: CCL å¼ã‚’ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ: ãƒ‘ãƒ¼ã‚¹ â†’ æ§‹é€ è¡¨ç¤º â†’ å®Ÿè¡Œè¨ˆç”»ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
def dispatch(ccl_expr: str) -> DispatchResult:
    """CCL å¼ã‚’ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ: ãƒ‘ãƒ¼ã‚¹ â†’ æ§‹é€ è¡¨ç¤º â†’ å®Ÿè¡Œè¨ˆç”»ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

    v3.0: @macro æ¤œå‡ºæ™‚ã« MacroExecutor ã‚’è‡ªå‹•å®Ÿè¡Œã—ã€
    ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆæ¸¬ + é€†ä¼æ’­ã®çµæœã‚’ plan_template ã«åŸ‹ã‚è¾¼ã‚€ã€‚
    ã“ã‚Œã«ã‚ˆã‚Šã€Œæ„å¿—ã‚ˆã‚Šç’°å¢ƒã€(ç¬¬é›¶åŸå‰‡) ãŒé”æˆã•ã‚Œã‚‹ã€‚

    Returns:
        DispatchResult: TypedDict â€” success, ast, tree, workflows,
                        wf_paths, wf_submodules, plan_template, macro_plan, error ç­‰
    """
    from hermeneus.src.parser import CCLParser as _Parser

    parser = _Parser()
    result: DispatchResult = {  # type: ignore[typeddict-item]
        "success": False,
        "ccl": ccl_expr,
        "ast": None,
        "tree": "",
        "workflows": [],
        "wf_paths": {},
        "wf_submodules": {},
        "wf_summaries": {},
        "plan_template": "",
        "macro_plan": None,
        "error": None,
    }


    # Step 0: ãƒ‘ãƒ¼ã‚¹
    try:
        ast = parser.parse(ccl_expr)
        result["ast"] = ast
        result["success"] = True
    except Exception as e:
        result["error"] = str(e)
        return result

    # Step 1: æœ¨æ§‹é€ è¡¨ç¤º
    result["tree"] = format_ast_tree(ast)

    # Step 2: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æŠ½å‡º + ãƒ‘ã‚¹è§£æ±º + è¦ç´„æŠ½å‡º
    result["workflows"] = extract_workflows(ast)
    result["wf_paths"] = resolve_wf_paths(result["workflows"])
    result["wf_submodules"] = resolve_submodules(result["wf_paths"])
    result["wf_summaries"] = resolve_wf_summaries(result["wf_paths"])

    # Step 2.3: ç¶²ç¾…æ€§ãƒã‚§ãƒƒã‚¯ (Pepsis Rust â€” exhaustive_check)
    exhaustive_warnings = exhaustive_check(ast)
    result["exhaustive_warnings"] = exhaustive_warnings

    # Step 2.4: ä¸¦åˆ—å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ (Pepsis Rust â€” parallel_safety_check)
    parallel_warnings = parallel_safety_check(ast)
    result["parallel_warnings"] = parallel_warnings

    # Step 2.5: ãƒã‚¯ãƒ­è‡ªå‹•å®Ÿè¡Œè¨ˆç”» (L1 ç’°å¢ƒåˆ¶ç´„)
    macro_section = ""
    if ccl_expr.strip().startswith("@"):
        try:
            from hermeneus.src.macro_executor import MacroExecutor
            executor = MacroExecutor()
            macro_result = executor.execute(ccl_expr)
            result["macro_plan"] = macro_result

            # ãƒã‚¯ãƒ­å®Ÿè¡Œè¨ˆç”»ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
            lines = [
                f"ã€ãƒã‚¯ãƒ­å®Ÿè¡Œè¨ˆç”»ã€‘@macro â†’ AST walk (è‡ªå‹•ç”Ÿæˆ)",
                f"  å±•é–‹: {macro_result.expanded_ccl}",
                f"  ã‚¹ãƒ†ãƒƒãƒ—æ•°: {len(macro_result.steps)}",
                f"  ç¢ºä¿¡åº¦: {macro_result.final_confidence:.0%}",
            ]
            if macro_result.bottleneck_step:
                lines.append(
                    f"  âš ï¸ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: {macro_result.bottleneck_step} "
                    f"(gradient={macro_result.gradient_map.get(macro_result.bottleneck_step, 0):.2f})"
                )
            lines.append("  å®Ÿè¡Œé †åº:")
            for i, step in enumerate(macro_result.steps, 1):
                lines.append(f"    {i}. {step.node_id} (Î”Îµ={step.entropy_reduction:+.2f})")
            lines.append("  â†’ ä¸Šè¨˜é †åºã§å„ WF å®šç¾©ã‚’ view_file ã—ã€é †æ¬¡å®Ÿè¡Œã›ã‚ˆ")

            macro_section = "\n".join(lines)
        except Exception as e:
            macro_section = f"ã€ãƒã‚¯ãƒ­å®Ÿè¡Œè¨ˆç”»ã€‘âš ï¸ MacroExecutor ã‚¨ãƒ©ãƒ¼: {e}"

    # Step 3: å°„ææ¡ˆã®è‡ªå‹•ç”Ÿæˆ (BC-8 å¼•åŠ›åŒ–)
    morphism_section = ""
    try:
        from mekhane.taxis.morphism_proposer import parse_trigonon, format_proposal
        for wf_id, wf_path in result["wf_paths"].items():
            trigonon = parse_trigonon(Path(wf_path))
            if trigonon:
                proposal = format_proposal(
                    wf_id.lstrip("/"), trigonon, confidence=None
                )
                morphism_section += f"\n{proposal}\n"
    except Exception:
        morphism_section = "\n  (å°„ææ¡ˆã®è‡ªå‹•ç”Ÿæˆã«å¤±æ•— â€” æ‰‹å‹•ã§ trigonon ã‚’ç¢ºèª)\n"

    # Step 4: å®Ÿè¡Œè¨ˆç”»ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    wf_list = ", ".join(result["workflows"])

    # view_file ã‚³ãƒãƒ³ãƒ‰ä¸€è¦§ (Agent ãŒã‚³ãƒ”ãƒšã§é–‹ã‘ã‚‹)
    view_lines = []
    for wf_id, wf_path in result["wf_paths"].items():
        view_lines.append(f"  view_file {wf_path}")
        # ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚ã‚Œã°éšå±¤è¡¨ç¤º
        subs = result["wf_submodules"].get(wf_id, [])
        for i, sub_path in enumerate(subs):
            prefix = "â””â”€â”€" if i == len(subs) - 1 else "â”œâ”€â”€"
            sub_name = Path(sub_path).name
            view_lines.append(f"    {prefix} view_file {sub_path}  ({sub_name})")
    view_cmds = "\n".join(view_lines)
    if not view_cmds:
        view_cmds = "  (WF å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)"

    # ãƒã‚¯ãƒ­ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã° plan_template ã®å…ˆé ­ã«æŒ¿å…¥
    macro_block = f"\n{macro_section}\n" if macro_section else ""

    # Step 5: å®Ÿè¡Œè¨ˆç”»ã®è‡ªå‹•å……å¡« (L1 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè‡ªå‹•å……å¡«)
    execution_plan_lines = []
    wf_summaries = result["wf_summaries"]
    for i, wf_id in enumerate(result["workflows"], 1):
        summary = wf_summaries.get(wf_id, {})
        purpose = summary.get("purpose", "")
        phases = summary.get("phases", [])
        output_hint = summary.get("output_hint", "")

        line = f"  Step {i}: {wf_id}"
        if purpose:
            line += f"\n    ç›®çš„: {purpose}"
        if phases:
            phase_str = " â†’ ".join(phases[:5])  # æœ€å¤§5ãƒ•ã‚§ãƒ¼ã‚º
            line += f"\n    ãƒ•ã‚§ãƒ¼ã‚º: {phase_str}"
        if output_hint:
            line += f"\n    å‡ºåŠ›: {output_hint}"
        execution_plan_lines.append(line)

    if execution_plan_lines:
        execution_plan = "\n".join(execution_plan_lines)
    else:
        execution_plan = "  (WF è¦ç´„ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ â€” view_file ã§ç¢ºèªã—ã¦ãã ã•ã„)"

    # Step 6: æ·±åº¦ãƒ¬ãƒ™ãƒ«åˆ¤å®š (CCL æ´¾ç”Ÿã‹ã‚‰æ¨å®š)
    # "+" â†’ L3, ç„¡å° â†’ L2, "-" â†’ L1
    has_plus = "+" in ccl_expr
    has_minus = "-" in ccl_expr and ">" not in ccl_expr  # >> ã¯é™¤å¤–
    if has_plus:
        depth_level = 3
    elif has_minus:
        depth_level = 1
    else:
        depth_level = 2
    result["depth_level"] = depth_level

    # Step 6.1: Adaptive Depth ãƒˆãƒªã‚¬ãƒ¼ (BC-18 v3.5)
    result["adaptive_depth"] = {
        "current_level": depth_level,
        "triggers": [
            {"condition": "BC-14 FaR confidence <50% x2", "action": "propose L+1"},
            {"condition": "AMP loop Stage 3â†’1 x2", "action": "force L+1"},
            {"condition": "Creator explicit request", "action": "immediate L+1"},
        ],
    }

    # UML ã‚»ã‚¯ã‚·ãƒ§ãƒ³: L2+ ã®ã¿
    if depth_level >= 2:
        uml_pre = """ã€UML Pre-checkã€‘(WF å®Ÿè¡Œå‰ã«å›ç­”)
  S1 [O1]: å…¥åŠ›ã‚’æ­£ã—ãç†è§£ã—ãŸã‹ï¼Ÿ â†’ (å›ç­”)
  S2 [A1]: ç¬¬ä¸€å°è±¡ãƒ»ç›´æ„Ÿã¯ã©ã†ã‹ï¼Ÿ â†’ (å›ç­”)"""
        uml_post = """ã€UML Post-checkã€‘(WF å®Ÿè¡Œå¾Œã«å›ç­”)
  S3 [A2]: æ‰¹åˆ¤çš„ã«å†è©•ä¾¡ã—ãŸã‹ï¼Ÿ â†’ (å›ç­”)
  S4 [O4]: æ±ºå®šã¯å¦¥å½“ã‹ï¼Ÿ èª¬æ˜ã§ãã‚‹ã‹ï¼Ÿ â†’ (å›ç­”)
  S5 [A4]: ç¢ºä¿¡åº¦ã¯é©åˆ‡ã‹ï¼Ÿ éä¿¡ã—ã¦ã„ãªã„ã‹ï¼Ÿ (FP 32.5%) â†’ (å›ç­”)"""
    else:
        uml_pre = ""
        uml_post = ""

    # å°„ææ¡ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³: L2+ ã®ã¿ã€ã‹ã¤æ”¹å–„ç‰ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    if depth_level >= 2 and morphism_section.strip():
        morphism_block = f"""ã€å°„ææ¡ˆ @completeã€‘(WF å®Œäº†æ™‚ã«ä»¥ä¸‹ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨)
{morphism_section}"""
    else:
        morphism_block = ""

    # Step 7: æ¼”ç®—å­è­¦å‘Šã®ç”Ÿæˆ (spec_injector + failure_db é€£æº)
    warnings_block = ""
    quiz_block = ""
    try:
        from mekhane.ccl.spec_injector import (
            get_warnings_for_expr, get_warned_operators, SpecInjector
        )
        # 7a: æ—¢çŸ¥ã®å±é™ºãƒ‘ã‚¿ãƒ¼ãƒ³è­¦å‘Š
        op_warnings = get_warnings_for_expr(ccl_expr)
        already_warned = get_warned_operators(ccl_expr)

        # 7b: failure_db ã‹ã‚‰ã®éå»å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³è­¦å‘Š (æ¼”ç®—å­ãƒ™ãƒ¼ã‚¹é‡è¤‡æ’é™¤)
        try:
            from mekhane.ccl.learning.failure_db import get_failure_db
            db = get_failure_db()
            db_warnings = db.get_warnings(ccl_expr)
            for w in db_warnings:
                if w.operator not in already_warned:
                    op_warnings.append(f"âš ï¸ [{w.severity}] {w.operator}: {w.message}")
                    already_warned.add(w.operator)
        except (ImportError, Exception):
            pass

        if op_warnings:
            warnings_block = "ã€âš ï¸ æ¼”ç®—å­æ³¨æ„ã€‘\n" + "\n".join(f"  {w}" for w in op_warnings)

        # 7c: å±é™ºæ¼”ç®—å­å«æœ‰æ™‚ã®ã¿ç†è§£ç¢ºèªã‚¯ã‚¤ã‚ºã‚’æ³¨å…¥
        dangerous_ops = {'!', '*^', '\\'}
        injector = SpecInjector()
        detected_ops = injector.parse_operators(ccl_expr)
        # parse_operators ãŒè¤‡åˆæ¼”ç®—å­ã‚‚æ¤œå‡ºã™ã‚‹ãŸã‚ã€ç›´æ¥ & ã§åˆ¤å®š
        quiz_target = detected_ops & dangerous_ops
        if quiz_target:
            quiz_block = injector.generate_quiz(quiz_target)
            # G4: ã‚¯ã‚¤ã‚ºåŠ¹æœãƒ­ã‚° â€” ç”Ÿæˆã‚’è¨˜éŒ²
            try:
                from mekhane.ccl.learning.quiz_logger import get_quiz_logger
                ql = get_quiz_logger()
                result["quiz_entry_id"] = ql.log_quiz_generated(  # type: ignore[typeddict-unknown-key]
                    ccl_expr=ccl_expr,
                    operators=quiz_target,
                )
            except (ImportError, Exception):
                pass
    except ImportError:
        pass  # spec_injector ãŒåˆ©ç”¨ä¸å¯ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹ç¯‰ (ç©ºã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é™¤å¤–ã—ã¦çµ„ã¿ç«‹ã¦)
    sections = [
        f"ã€CCLã€‘{ccl_expr}",
    ]
    if warnings_block:
        sections.append(warnings_block)
    sections += [
        f"ã€æ§‹é€ ã€‘\n{result['tree']}",
        f"ã€é–¢é€£WFã€‘{wf_list}",
    ]

    # ç¶²ç¾…æ€§ + ä¸¦åˆ—å®‰å…¨æ€§ã®è­¦å‘Šã‚’æ³¨å…¥
    safety_warnings = result.get("exhaustive_warnings", []) + result.get("parallel_warnings", [])
    if safety_warnings:
        safety_block = "ã€ğŸ¦€ Pepsis Safety Checkã€‘\n" + "\n".join(f"  {w}" for w in safety_warnings)
        sections.append(safety_block)

    sections += [
        f"ã€WFå®šç¾©ã€‘ä»¥ä¸‹ã‚’ view_file ã§é–‹ãã“ã¨:\n{view_cmds}{macro_block}",
    ]
    if uml_pre:
        sections.append(uml_pre)
    sections.append(f"ã€å®Ÿè¡Œè¨ˆç”»ã€‘(AST é †åºã«åŸºã¥ãè‡ªå‹•ç”Ÿæˆ)\n{execution_plan}")
    if quiz_block:
        sections.append(f"ã€ç†è§£ç¢ºèªã€‘\n{quiz_block}")
    sections.append("ã€/dia åè«–ã€‘(AI ãŒæœ€ä½1ã¤ã®æ‡¸å¿µã‚’æç¤º)")
    if uml_post:
        sections.append(uml_post)
    if morphism_block:
        sections.append(morphism_block)
    # Adaptive Depth: L1 ä»¥ä¸‹ã®ã¨ãæ·±åº¦ä¸Šæ˜‡ãƒˆãƒªã‚¬ãƒ¼ã‚’è¡¨ç¤º
    if depth_level <= 1:
        ad_section = """ã€ğŸ“ˆ Adaptive Depthã€‘(å®Ÿè¡Œä¸­ã«ä»¥ä¸‹ã‚’æ¤œçŸ¥ã—ãŸã‚‰æ·±åº¦ã‚’ä¸Šã’ã‚ˆ)
  ãƒ»BC-14 FaR ç¢ºä¿¡åº¦ <50% ãŒ 2 å›é€£ç¶š â†’ L+1 ã‚’ææ¡ˆ
  ãƒ»AMP ãƒ«ãƒ¼ãƒ— Stage 3â†’1 ãŒ 2 å›ç™ºå‹• â†’ L+1 ã‚’å¼·åˆ¶
  ãƒ»Creator ãŒã€Œã‚‚ã£ã¨æ·±ãã€ã¨æŒ‡ç¤º â†’ å³åº§ã« L+1"""
        sections.append(ad_section)
    # Step 8: ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†ã®ç’°å¢ƒå¼·åˆ¶ (Aristos L3)
    if len(result["workflows"]) >= 2:
        fb_section = """ã€ğŸ“Š Aristos Feedbackã€‘(WF å®Œäº†å¾Œã«å®Ÿè¡Œ)
  ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å“è³ªã‚’è¨˜éŒ²ã›ã‚ˆ:
  ```python
  from aristos.route_feedback import log_from_dispatch
  log_from_dispatch(dispatch_result, actual_time_min=X, had_errors=False)
  ```
  â†’ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ PT æœ€é©åŒ– (L3 GA) ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ãªã‚‹"""
        sections.append(fb_section)

        # route_context ã‚’ result ã«ä»˜åŠ 
        result["route_context"] = {
            "source": result["workflows"][0].lstrip("/"),
            "target": result["workflows"][-1].lstrip("/"),
            "route": [w.lstrip("/") for w in result["workflows"]],
            "depth_level": depth_level,
            "wf_count": len(result["workflows"]),
        }

    sections.append("â†’ ã“ã‚Œã§é€²ã‚ã¦ã‚ˆã„ã§ã™ã‹ï¼Ÿ")

    tmpl = "\n".join(sections)
    result["plan_template"] = tmpl

    return result


# PURPOSE: CLI ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
def main():
    """CLI ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    if len(sys.argv) < 2:
        print("Usage: python hermeneus/src/dispatch.py '<CCLå¼>'")
        print("Example: python hermeneus/src/dispatch.py '/dia+~/noe'")
        print("Example: python hermeneus/src/dispatch.py '(/dia+~/noe)~/pan+'")
        sys.exit(1)

    ccl_expr = sys.argv[1]

    print(f"{'='*60}")
    print(f"  HermÄ“neus CCL Dispatch")
    print(f"  å…¥åŠ›: {ccl_expr}")
    print(f"{'='*60}")
    print()

    # å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆå›é¿: dispatch() å†…ã§ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    result = dispatch(ccl_expr)

    if not result["success"]:
        print(f"âŒ Parse Error: {result['error']}")
        print()
        print("ãƒ‘ãƒ¼ã‚µãƒ¼æ‹¡å¼µãŒå¿…è¦ã‹ã€å¼ã®ä¿®æ­£ãŒå¿…è¦ã§ã™ã€‚")
        print("Creator ã«å ±å‘Šã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    print("âœ… ãƒ‘ãƒ¼ã‚¹æˆåŠŸ")
    print()
    print("â”€â”€ AST æ§‹é€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(result["tree"])
    print()
    print(f"â”€â”€ é–¢é€£ WF: {', '.join(result['workflows'])} â”€â”€")
    print()

    # WF å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    if result["wf_paths"]:
        print("â”€â”€ WF å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ« (view_file ã§é–‹ã‘) â”€â”€â”€â”€")
        for wf_id, path in result["wf_paths"].items():
            print(f"  {wf_id} â†’ {path}")
        print()

    print("â”€â”€ å®Ÿè¡Œè¨ˆç”»ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(result["plan_template"])
    print()
    print("â”€" * 60)
    print("â†‘ ã“ã®å‡ºåŠ›ã‚’åŸºã« AST é †åºã§ WF ã‚’å®Ÿè¡Œã›ã‚ˆã€‚")


if __name__ == "__main__":
    main()

