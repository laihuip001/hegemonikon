# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- mekhane/anamnesis/
"""
PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å­˜åœ¨ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„

P3 â†’ è¨˜æ†¶ã®æ°¸ç¶šåŒ–ãŒå¿…è¦
   â†’ æ°¸ç¶šåŒ–ã•ã‚ŒãŸçŸ¥è­˜ã¨ã®å¯¾è©±ãŒå¿…è¦
   â†’ gnosis_chat.py ãŒæ‹…ã†

Q.E.D.

---

GnÅsis Chat â€” NotebookLM çš„ ãƒ­ãƒ¼ã‚«ãƒ« RAG å¯¾è©±ã‚¨ãƒ³ã‚¸ãƒ³

Architecture:
  Query â†’ GPU Embedding (BGE-small, 133MB)
       â†’ LanceDB ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ (Top-K)
       â†’ Qwen 2.5 3B Instruct (4bit, ~2.5GB)
       â†’ ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¯¾è©±ã§å›ç­”ç”Ÿæˆ

VRAM Budget:
  BGE-small embedding: ~133MB
  Qwen 2.5 3B (4bit): ~2.5GB
  Total: ~2.6GB / 8GB RTX 2070 SUPER

Knowledge Sources:
  - GnÅsis è«–æ–‡ (LanceDB papers table)
  - Handoff å¼•ç¶™æ›¸ (mneme sessions)
  - KI/Insight (mneme sessions)
  - Kernel ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ (kernel/)
  - ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚° (mneme sessions)
"""

import sys
import time
from pathlib import Path
from typing import Optional

# Hegemonikon root
_THIS_DIR = Path(__file__).parent
_HEGEMONIKON_ROOT = _THIS_DIR.parent.parent

# Knowledge source paths
MNEME_SESSIONS = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "sessions"
MNEME_KNOWLEDGE = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "knowledge"
KERNEL_DIR = _HEGEMONIKON_ROOT / "kernel"
HANDOFF_DIR = _HEGEMONIKON_ROOT / "docs" / "handoff"


class ConversationHistory:
    """ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¯¾è©±ã®å±¥æ­´ç®¡ç†."""

    def __init__(self, max_turns: int = 5):
        self.max_turns = max_turns
        self.turns: list[dict] = []

    def add(self, role: str, content: str):
        """ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ ."""
        self.turns.append({"role": role, "content": content})
        # æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ã‚’è¶…ãˆãŸã‚‰å¤ã„ã‚‚ã®ã‚’å‰Šé™¤
        if len(self.turns) > self.max_turns * 2:
            self.turns = self.turns[-(self.max_turns * 2):]

    def format_for_prompt(self) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ."""
        if not self.turns:
            return ""
        parts = []
        for t in self.turns:
            if t["role"] == "user":
                parts.append(f"<|im_start|>user\n{t['content']}<|im_end|>")
            else:
                parts.append(f"<|im_start|>assistant\n{t['content']}<|im_end|>")
        return "\n".join(parts)

    def clear(self):
        self.turns.clear()

    @property
    def turn_count(self) -> int:
        return len(self.turns) // 2


class Reranker:
    """Cross-encoder Reranker for precision improvement.

    Strategy: bi-encoder ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚§ãƒƒãƒ â†’ cross-encoder ã§ re-score
    â†’ top-k ã«çµã‚‹ã€‚ç²¾åº¦ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã€‚

    VRAM: ~50MB (cross-encoder-ms-marco-MiniLM-L-6-v2)
    """

    DEFAULT_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"

    def __init__(self, model_name: Optional[str] = None):
        self.model_name = model_name or self.DEFAULT_MODEL
        self._model = None

    def _load(self):
        if self._model is not None:
            return
        from sentence_transformers import CrossEncoder
        self._model = CrossEncoder(self.model_name, device="cuda")
        print(f"[Reranker] Loaded ({self.model_name})", flush=True)

    # Cross-encoder score threshold (Layer 2)
    # MiniLM scores are relative, not absolute.
    # Used only as a secondary noise filter.
    SCORE_THRESHOLD = -4.0

    def rerank(
        self, query: str, results: list[dict], top_k: int = 5
    ) -> list[dict]:
        """æ¤œç´¢çµæœã‚’ cross-encoder ã§å†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° + é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿.

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            results: bi-encoder ã®æ¤œç´¢çµæœ
            top_k: è¿”ã™ä»¶æ•°

        Returns:
            re-scored, filtered & sorted results (top_k)
        """
        if not results:
            return results

        self._load()

        # (query, document) ãƒšã‚¢ã‚’ä½œæˆ
        pairs = []
        for r in results:
            table = r.get("_source_table", "unknown")
            if table == "knowledge":
                text = r.get("content", r.get("abstract", ""))[:512]
            else:
                text = f"{r.get('title', '')} {r.get('abstract', '')[:400]}"
            pairs.append((query, text))

        # Cross-encoder scoring
        scores = self._model.predict(pairs)

        # ã‚¹ã‚³ã‚¢ã‚’çµæœã«ä»˜ä¸
        for r, score in zip(results, scores):
            r["_rerank_score"] = float(score)

        # Layer 2: cross-encoder é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿
        filtered = [
            r for r in results
            if r.get("_rerank_score", -999) > self.SCORE_THRESHOLD
        ]

        # re-rank score ã§ã‚½ãƒ¼ãƒˆ (é«˜ã„æ–¹ãŒè‰¯ã„)
        filtered.sort(key=lambda r: r.get("_rerank_score", -999), reverse=True)

        return filtered[:top_k]


class KnowledgeIndexer:
    """HegemonikÃ³n å…¨çŸ¥è­˜ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£."""

    @staticmethod
    def discover_knowledge_files() -> list[dict]:
        """å…¨çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹.

        Returns:
            list of dict with keys: path, source_type, title
        """
        files = []

        # 1. Handoff files (mneme/sessions)
        if MNEME_SESSIONS.exists():
            for f in sorted(MNEME_SESSIONS.glob("handoff_*.md")):
                files.append({
                    "path": f,
                    "source_type": "handoff",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 2. Session logs (mneme/sessions)
        if MNEME_SESSIONS.exists():
            for f in sorted(MNEME_SESSIONS.glob("2026-*_conv_*.md")):
                files.append({
                    "path": f,
                    "source_type": "session",
                    "title": f.stem.split("_", 3)[-1].replace("_", " ")
                    if "_" in f.stem else f.stem,
                })

        # 3. KI / Insight files (mneme/sessions)
        if MNEME_SESSIONS.exists():
            for f in sorted(MNEME_SESSIONS.glob("insight_*.md")):
                files.append({
                    "path": f,
                    "source_type": "ki",
                    "title": f.stem.replace("_", " ").title(),
                })
            for f in sorted(MNEME_SESSIONS.glob("weekly_review_*.md")):
                files.append({
                    "path": f,
                    "source_type": "review",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 4. Knowledge (mneme/knowledge/)
        if MNEME_KNOWLEDGE.exists():
            for f in sorted(MNEME_KNOWLEDGE.glob("*.md")):
                files.append({
                    "path": f,
                    "source_type": "ki",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 5. Handoff (docs/)
        if HANDOFF_DIR.exists():
            for f in sorted(HANDOFF_DIR.glob("*.md")):
                files.append({
                    "path": f,
                    "source_type": "handoff",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 6. Kernel docs
        if KERNEL_DIR.exists():
            for f in sorted(KERNEL_DIR.glob("*.md")):
                files.append({
                    "path": f,
                    "source_type": "kernel",
                    "title": f.stem.replace("_", " ").title(),
                })
            # CEP
            cep_dir = KERNEL_DIR / "cep"
            if cep_dir.exists():
                for f in sorted(cep_dir.glob("*.md")):
                    files.append({
                        "path": f,
                        "source_type": "kernel",
                        "title": f"CEP: {f.stem}",
                    })

        return files

    @staticmethod
    def _chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰² (overlap ä»˜ã)."""
        if len(text) <= chunk_size:
            return [text]

        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]

            # æ–‡ã®é€”ä¸­ã§åˆ‡ã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹
            if end < len(text):
                # æœ€å¾Œã®æ”¹è¡Œã§åˆ‡ã‚‹
                last_nl = chunk.rfind("\n")
                if last_nl > chunk_size // 2:
                    end = start + last_nl + 1
                    chunk = text[start:end]

            chunks.append(chunk.strip())
            start = end - overlap

        return [c for c in chunks if c]

    @staticmethod
    def index_knowledge(force_reindex: bool = False) -> int:
        """å…¨çŸ¥è­˜ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ .

        Returns:
            è¿½åŠ ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
        """
        from mekhane.anamnesis.index import GnosisIndex, LANCE_DIR
        from mekhane.anamnesis.models.paper import Paper

        index = GnosisIndex()

        # æ—¢å­˜ã® knowledge ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç¢ºèª
        table_name = "knowledge"
        existing_keys = set()

        if not force_reindex and table_name in index.db.table_names():
            try:
                table = index.db.open_table(table_name)
                df = table.search().select(["primary_key"]).limit(None).to_pandas()
                existing_keys = set(df["primary_key"].tolist())
                print(f"[Knowledge] Existing: {len(existing_keys)} chunks")
            except Exception:
                pass

        files = KnowledgeIndexer.discover_knowledge_files()
        print(f"[Knowledge] Discovered {len(files)} knowledge files")

        # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        embedder = index._get_embedder()
        data = []
        skipped = 0

        for finfo in files:
            try:
                text = finfo["path"].read_text(encoding="utf-8")
            except Exception:
                continue

            chunks = KnowledgeIndexer._chunk_text(text)

            for ci, chunk in enumerate(chunks):
                primary_key = f"{finfo['source_type']}:{finfo['path'].stem}:{ci}"

                if primary_key in existing_keys:
                    skipped += 1
                    continue

                data.append({
                    "primary_key": primary_key,
                    "title": finfo["title"],
                    "source": finfo["source_type"],
                    "abstract": chunk[:300],
                    "content": chunk,
                    "authors": "",
                    "doi": "",
                    "arxiv_id": "",
                    "url": str(finfo["path"]),
                    "citations": 0,
                })

        if not data:
            print(f"[Knowledge] No new documents (skipped {skipped})")
            return 0

        # ãƒãƒƒãƒ embedding
        print(f"[Knowledge] Embedding {len(data)} chunks...", flush=True)
        BATCH_SIZE = 32
        for i in range(0, len(data), BATCH_SIZE):
            batch = data[i:i + BATCH_SIZE]
            texts = [d["content"][:512] for d in batch]
            vectors = embedder.embed_batch(texts)

            for d, v in zip(batch, vectors):
                d["vector"] = v

            done = min(i + BATCH_SIZE, len(data))
            print(f"  Embedded {done}/{len(data)}...", flush=True)

        # LanceDB ã«è¿½åŠ 
        if table_name in index.db.table_names():
            table = index.db.open_table(table_name)
            table.add(data)
        else:
            index.db.create_table(table_name, data=data)

        print(f"[Knowledge] Added {len(data)} chunks (skipped {skipped})")
        return len(data)


class GnosisChat:
    """GnÅsis å¯¾è©±å‹ RAG ã‚¨ãƒ³ã‚¸ãƒ³.

    NotebookLM ã®ã‚ˆã†ã«ã€è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦å¯¾è©±ã™ã‚‹ã€‚
    å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ« (ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å‹•ä½œå¯èƒ½)ã€‚

    3å±¤ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³é˜²å¾¡:
      Layer 1: bi-encoder distance threshold
      Layer 2: cross-encoder score threshold
      Layer 3: confidence assessment â†’ prompt adaptation
    """

    DEFAULT_MODEL = "Qwen/Qwen2.5-3B-Instruct"

    # Layer 1: bi-encoder distance threshold
    # LanceDB L2 distance on normalized vectors:
    #   0 = identical, ~1.0 = unrelated, ~1.41 = opposite
    DISTANCE_THRESHOLD = 0.85

    # Confidence levels
    CONFIDENCE_HIGH = "high"
    CONFIDENCE_MEDIUM = "medium"
    CONFIDENCE_LOW = "low"
    CONFIDENCE_NONE = "none"

    def __init__(
        self,
        model_id: Optional[str] = None,
        top_k: int = 5,
        max_new_tokens: int = 512,
        search_knowledge: bool = True,
        search_papers: bool = True,
        use_reranker: bool = True,
    ):
        self.model_id = model_id or self.DEFAULT_MODEL
        self.top_k = top_k
        self.max_new_tokens = max_new_tokens
        self.search_knowledge = search_knowledge
        self.search_papers = search_papers
        self.use_reranker = use_reranker

        self._index = None
        self._model = None
        self._tokenizer = None
        self._reranker = Reranker() if use_reranker else None
        self.history = ConversationHistory(max_turns=5)

    def _load_index(self):
        """GnÅsis ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ (GPU embedding)."""
        if self._index is not None:
            return
        from mekhane.anamnesis.index import GnosisIndex
        self._index = GnosisIndex()
        print("[GnÅsis Chat] Index loaded", flush=True)

    def _unload_embedder(self):
        """VRAM ã‚¿ã‚¤ãƒ ã‚·ã‚§ã‚¢: Embedder ã‚’è§£æ”¾ã—ã¦ LLM ç”¨ã« VRAM ã‚’ç¢ºä¿.

        BGE-m3 (2.3GB) + Qwen 3B (2.1GB) = 4.4GB > 8GB GPU
        æ¤œç´¢å®Œäº†å¾Œã« Embedder ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ LLM ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã«ã™ã‚‹ã€‚
        """
        import gc
        import torch
        if self._index is not None:
            embedder = self._index._get_embedder()
            if hasattr(embedder, '_st_model') and embedder._st_model is not None:
                del embedder._st_model
                embedder._st_model = None
                embedder._use_gpu = False
            # Reranker ã‚‚è§£æ”¾
            if self._reranker and self._reranker._model is not None:
                del self._reranker._model
                self._reranker._model = None
            gc.collect()
            torch.cuda.empty_cache()
            vram_mb = torch.cuda.memory_allocated() / 1e6
            print(f"[GnÅsis Chat] Embedder unloaded ({vram_mb:.0f}MB VRAM)", flush=True)

    def _load_model(self):
        """LLM ã‚’ãƒ­ãƒ¼ãƒ‰ (4bité‡å­åŒ– on GPU)."""
        if self._model is not None:
            return

        # VRAM ã‚¿ã‚¤ãƒ ã‚·ã‚§ã‚¢: Embedder ã‚’å…ˆã«è§£æ”¾
        self._unload_embedder()

        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )

        print(f"[GnÅsis Chat] Loading {self.model_id}...", flush=True)
        self._tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        self._model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            quantization_config=bnb_config,
            device_map="auto",
        )

        vram_mb = torch.cuda.memory_allocated() / 1e6
        print(f"[GnÅsis Chat] Model loaded ({vram_mb:.0f}MB VRAM)", flush=True)

    def _retrieve(self, query: str) -> list[dict]:
        """å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ + é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ + rerank."""
        self._load_index()
        results = []
        fetch_k = self.top_k * 3 if self._reranker else self.top_k

        # Papers table
        if self.search_papers:
            try:
                paper_results = self._index.search(query, k=fetch_k)
                for r in paper_results:
                    r["_source_table"] = "papers"
                results.extend(paper_results)
            except Exception:
                pass

        # Knowledge table
        if self.search_knowledge:
            try:
                if "knowledge" in self._index.db.table_names():
                    embedder = self._index._get_embedder()
                    qvec = embedder.embed(query)
                    table = self._index.db.open_table("knowledge")
                    k_results = table.search(qvec).limit(fetch_k).to_list()
                    for r in k_results:
                        r["_source_table"] = "knowledge"
                    results.extend(k_results)
            except Exception:
                pass

        # Layer 1: bi-encoder è·é›¢é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿
        before_filter = len(results)
        results = [
            r for r in results
            if r.get("_distance", 999) < self.DISTANCE_THRESHOLD
        ]
        if before_filter > 0 and len(results) == 0:
            print(f"[GnÅsis Chat] Layer 1: all {before_filter} results filtered "
                  f"(min_dist > {self.DISTANCE_THRESHOLD})", flush=True)

        results.sort(key=lambda r: r.get("_distance", 999))

        # Layer 2: Reranker
        if self._reranker and results:
            results = self._reranker.rerank(query, results, top_k=self.top_k)
        else:
            results = results[:self.top_k]

        return results

    def _assess_confidence(self, results: list[dict]) -> str:
        """Layer 3: æ¤œç´¢çµæœã®å“è³ªã‹ã‚‰ç¢ºä¿¡åº¦ã‚’åˆ¤å®š.

        è·é›¢ (bi-encoder) ã®ã¿ã§åˆ¤å®šã™ã‚‹ã€‚
        cross-encoder ã‚¹ã‚³ã‚¢ã¯ relative ranking ç”¨ã§ã‚ã‚Š absolute åˆ¤å®šã«ã¯ä¸é©ã€‚

        BGE-m3 è·é›¢ã‚¹ã‚±ãƒ¼ãƒ« (L2, normalized):
          - é–¢é€£: 0.5-0.8
          - æ›–æ˜§: 0.8-0.9
          - ç„¡é–¢é€£: >0.9 (DISTANCE_THRESHOLD ã§é™¤å»æ¸ˆã¿)
        """
        if not results:
            return self.CONFIDENCE_NONE

        distances = [r.get("_distance", 999) for r in results]
        min_dist = min(distances)
        avg_dist = sum(distances) / len(distances)
        n = len(results)

        if min_dist < 0.6 and n >= 3 and avg_dist < 0.75:
            return self.CONFIDENCE_HIGH
        elif min_dist < 0.7:
            return self.CONFIDENCE_HIGH
        elif min_dist < 0.8:
            return self.CONFIDENCE_MEDIUM
        else:
            return self.CONFIDENCE_LOW

    def _build_context(self, results: list[dict]) -> str:
        """æ¤œç´¢çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’æ§‹ç¯‰."""
        context_parts = []
        for i, r in enumerate(results, 1):
            title = r.get("title", "Untitled")
            source = r.get("source", "unknown")
            table = r.get("_source_table", "unknown")

            # knowledge ãƒ†ãƒ¼ãƒ–ãƒ«ã®å ´åˆã¯ content ã‚’ä½¿ç”¨
            if table == "knowledge":
                content = r.get("content", r.get("abstract", ""))[:600]
            else:
                content = r.get("abstract", "")[:500]

            authors = r.get("authors", "")[:80]
            dist = r.get("_distance", 0)

            context_parts.append(
                f"[{i}] [{source}] {title}\n"
                f"    Relevance: {1 - dist:.2f}\n"
                f"    {content}"
            )
        return "\n\n".join(context_parts)

    def _generate(self, prompt: str) -> str:
        """LLM ã§å›ç­”ã‚’ç”Ÿæˆ."""
        import torch

        self._load_model()

        inputs = self._tokenizer(
            prompt, return_tensors="pt", truncation=True, max_length=2048
        ).to(self._model.device)

        with torch.no_grad():
            outputs = self._model.generate(
                **inputs,
                max_new_tokens=self.max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.15,
                pad_token_id=self._tokenizer.eos_token_id,
            )

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»ã—ã¦å›ç­”ã®ã¿è¿”ã™
        input_len = inputs["input_ids"].shape[1]
        answer_tokens = outputs[0][input_len:]
        return self._tokenizer.decode(answer_tokens, skip_special_tokens=True)

    def ask(self, question: str) -> dict:
        """è³ªå•ã«å›ç­”ã™ã‚‹ (RAG + ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ + 3å±¤é˜²å¾¡)."""
        # 1. Retrieve (Layer 1 + 2)
        t0 = time.time()
        results = self._retrieve(question)
        retrieval_time = time.time() - t0

        # 2. Layer 3: Confidence
        confidence = self._assess_confidence(results)
        context = self._build_context(results)

        # 3. Confidence-adaptive response
        if confidence == self.CONFIDENCE_NONE:
            answer = (
                "âš ï¸ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«ã“ã®è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n\n"
                "ä»¥ä¸‹ã‚’ãŠè©¦ã—ãã ã•ã„:\n"
                "- åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è³ªå•ã™ã‚‹\n"
                "- `/index` ã§çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã™ã‚‹\n"
                "- é–¢é€£ã™ã‚‹è«–æ–‡ã‚’ `collect` ã§è¿½åŠ ã™ã‚‹"
            )
            generation_time = 0
        else:
            conf_instr = {
                self.CONFIDENCE_HIGH: "æ¤œç´¢çµæœã«ååˆ†ãªæƒ…å ±ãŒã‚ã‚Šã¾ã™ã€‚è‡ªä¿¡ã‚’æŒã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚",
                self.CONFIDENCE_MEDIUM: (
                    "æ¤œç´¢çµæœã«éƒ¨åˆ†çš„ãªæƒ…å ±ãŒã‚ã‚Šã¾ã™ã€‚"
                    "ä¸ç¢ºå®Ÿãªéƒ¨åˆ†ã¯ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«ååˆ†ãªæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚"
                ),
                self.CONFIDENCE_LOW: (
                    "æ¤œç´¢çµæœã¨ã®é–¢é€£æ€§ãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
                    "æ¨æ¸¬ã‚„å‰µä½œã¯çµ¶å¯¾ã«ã—ãªã„ã§ãã ã•ã„ã€‚"
                    "å›ç­”å†’é ­ã«ã€âš ï¸ é–¢é€£æ€§ãŒä½ã„æƒ…å ±ã«åŸºã¥ãå›ç­”ã§ã™ã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"
                    "ç­”ãˆã‚‰ã‚Œãªã„å ´åˆã¯ã€ååˆ†ãªæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨æ­£ç›´ã«è¿”ã—ã¦ãã ã•ã„ã€‚"
                ),
            }.get(confidence, "")

            system_prompt = (
                "ã‚ãªãŸã¯ HegemonikÃ³n ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å¯¾è©±ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
                "ä»¥ä¸‹ã®æ¤œç´¢çµæœã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
                f"{conf_instr}\n"
                "å›ç­”ã¯ç°¡æ½”ã§æ§‹é€ çš„ã«ã—ã¦ãã ã•ã„ã€‚\n"
                "å¼•ç”¨ã™ã‚‹å ´åˆã¯ [ç•ªå·] ã®å½¢å¼ã§å‚ç…§å…ƒã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚\n"
                "æ¤œç´¢çµæœã«ãªã„æƒ…å ±ã‚’å‰µä½œãƒ»æ¨æ¸¬ã—ãªã„ã§ãã ã•ã„ã€‚"
            )

            prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
            history_text = self.history.format_for_prompt()
            if history_text:
                prompt += history_text + "\n"
            prompt += (
                f"<|im_start|>user\n"
                f"## æ¤œç´¢çµæœ (é–¢é€£æ–‡æ›¸)\n\n{context}\n\n"
                f"## è³ªå•\n{question}<|im_end|>\n"
                f"<|im_start|>assistant\n"
            )

            t0 = time.time()
            answer = self._generate(prompt)
            generation_time = time.time() - t0

        self.history.add("user", question)
        self.history.add("assistant", answer.strip())

        sources = []
        for r in results:
            sources.append({
                "title": r.get("title", "Untitled")[:80],
                "source": r.get("source", "unknown"),
                "table": r.get("_source_table", "unknown"),
                "url": r.get("url", ""),
                "distance": round(r.get("_distance", 0), 4),
                "rerank_score": r.get("_rerank_score"),
            })

        return {
            "answer": answer.strip(),
            "sources": sources,
            "confidence": confidence,
            "retrieval_time": round(retrieval_time, 3),
            "generation_time": round(generation_time, 1),
            "context_docs": len(results),
            "turn": self.history.turn_count,
        }

    def interactive(self):
        """å¯¾è©±ãƒ«ãƒ¼ãƒ— (REPL)."""
        print("\n" + "=" * 60)
        print("  GnÅsis Chat â€” HegemonikÃ³n ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å¯¾è©±")
        print(f"  Model: {self.model_id}")
        print("  Commands: /quit, /sources, /stats, /clear, /index")
        print("=" * 60)

        last_result = None

        while True:
            try:
                turn = self.history.turn_count
                question = input(f"\n[{turn}] â“ ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\nbye.")
                break

            if not question:
                continue

            if question in ("/quit", "/q", "/exit"):
                print("bye.")
                break

            if question == "/clear":
                self.history.clear()
                print("ğŸ”„ ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                continue

            if question == "/sources" and last_result:
                print("\nğŸ“š Sources:")
                for i, s in enumerate(last_result["sources"], 1):
                    icon = {"papers": "ğŸ“„", "knowledge": "ğŸ§ "}.get(
                        s["table"], "ğŸ“"
                    )
                    print(f"  [{i}] {icon} [{s['source']}] {s['title']}")
                    print(f"      Relevance: {s['relevance']}")
                    if s["url"]:
                        print(f"      {s['url']}")
                continue

            if question == "/stats":
                self._load_index()
                stats = self._index.stats()
                print(f"\nğŸ“Š Papers: {stats['total']}")
                for src, cnt in stats.get("sources", {}).items():
                    print(f"   {src}: {cnt}")

                # Knowledge table stats
                if "knowledge" in self._index.db.table_names():
                    kt = self._index.db.open_table("knowledge")
                    kdf = kt.to_pandas()
                    print(f"\nğŸ§  Knowledge chunks: {len(kdf)}")
                    for src, cnt in kdf["source"].value_counts().items():
                        print(f"   {src}: {cnt}")
                continue

            if question == "/index":
                print("ğŸ“ Indexing knowledge files...", flush=True)
                added = KnowledgeIndexer.index_knowledge()
                print(f"âœ… Indexed {added} new chunks")
                continue

            # Ask
            print("ğŸ” Searching...", flush=True)
            result = self.ask(question)
            last_result = result

            print(f"\nğŸ’¡ [{result['turn']}] Answer "
                  f"({result['generation_time']}s, "
                  f"{result['context_docs']} docs):\n")
            print(result["answer"])
            print(f"\nğŸ“š {len(result['sources'])} sources (/sources for details)")


def cmd_chat(args) -> int:
    """CLI entry point for chat command."""
    chat = GnosisChat(
        top_k=args.top_k,
        max_new_tokens=args.max_tokens,
    )

    if args.index:
        print("ğŸ“ Indexing knowledge files...", flush=True)
        added = KnowledgeIndexer.index_knowledge()
        print(f"âœ… Indexed {added} new chunks")
        if not args.question:
            return 0

    if args.question:
        result = chat.ask(args.question)
        conf_icon = {"high": "ğŸŸ¢", "medium": "ğŸŸ¡", "low": "ğŸŸ ", "none": "ğŸ”´"}.get(
            result.get("confidence", ""), "âšª")
        print(f"\n{conf_icon} Confidence: {result.get('confidence', '?')}")
        print(f"\nğŸ’¡ Answer:\n\n{result['answer']}")
        print(f"\n---")
        print(f"ğŸ“š Sources ({result['context_docs']} docs, "
              f"retrieval: {result['retrieval_time']}s, "
              f"generation: {result['generation_time']}s):")
        for i, s in enumerate(result["sources"], 1):
            icon = {"papers": "ğŸ“„", "knowledge": "ğŸ§ "}.get(s["table"], "ğŸ“")
            d = s.get('distance', 0)
            rs = s.get('rerank_score')
            score_str = f" rs={rs:.1f}" if rs is not None else ""
            print(f"  [{i}] {icon} [{s['source']}] {s['title']} (d={d:.4f}{score_str})")
            if s["url"]:
                print(f"      {s['url']}")
        return 0
    else:
        chat.interactive()
        return 0
