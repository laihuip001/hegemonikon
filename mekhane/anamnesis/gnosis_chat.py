# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- mekhane/anamnesis/
"""
PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å­˜åœ¨ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„

P3 â†’ è¨˜æ†¶ã®æ°¸ç¶šåŒ–ãŒå¿…è¦
   â†’ æ°¸ç¶šåŒ–ã•ã‚ŒãŸçŸ¥è­˜ã¨ã®å¯¾è©±ãŒå¿…è¦
   â†’ gnosis_chat.py ãŒæ‹…ã†

Q.E.D.

---

GnÅsis Chat â€” NotebookLM çš„ ãƒ­ãƒ¼ã‚«ãƒ« RAG å¯¾è©±ã‚¨ãƒ³ã‚¸ãƒ³

Architecture:
  Query â†’ GPU Embedding (BGE-small, 133MB)
       â†’ LanceDB ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ (Top-K)
       â†’ Qwen 2.5 3B Instruct (4bit, ~2.5GB)
       â†’ ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¯¾è©±ã§å›ç­”ç”Ÿæˆ

VRAM Budget:
  BGE-small embedding: ~133MB
  Qwen 2.5 3B (4bit): ~2.5GB
  Total: ~2.6GB / 8GB RTX 2070 SUPER

Knowledge Sources:
  - GnÅsis è«–æ–‡ (LanceDB papers table)
  - Handoff å¼•ç¶™æ›¸ (mneme sessions)
  - KI/Insight (mneme sessions)
  - Kernel ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ (kernel/)
  - ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚° (mneme sessions)
"""

import sys
import time
from pathlib import Path
from typing import Optional
from mekhane.anamnesis.lancedb_compat import get_table_names

# Hegemonikon root
_THIS_DIR = Path(__file__).parent
_HEGEMONIKON_ROOT = _THIS_DIR.parent.parent

# Knowledge source paths
_MNEME_ROOT = Path.home() / "oikos" / "mneme" / ".hegemonikon"
MNEME_SESSIONS = _MNEME_ROOT / "sessions"
MNEME_KNOWLEDGE = _MNEME_ROOT / "knowledge"
MNEME_HANDOFFS = _MNEME_ROOT / "handoffs"
MNEME_DOXA = _MNEME_ROOT / "doxa"
MNEME_WORKFLOWS = _MNEME_ROOT / "workflows"
MNEME_RESEARCH = _MNEME_ROOT / "research"
MNEME_XSERIES = _MNEME_ROOT / "x-series"
KERNEL_DIR = _HEGEMONIKON_ROOT / "kernel"
HANDOFF_DIR = _HEGEMONIKON_ROOT / "docs" / "handoff"


# PURPOSE: ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¯¾è©±ã®å±¥æ­´ç®¡ç†.
class ConversationHistory:
    """ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¯¾è©±ã®å±¥æ­´ç®¡ç†."""

    # PURPOSE: ConversationHistory ã®åˆæœŸåŒ– â€” ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ .
    def __init__(self, max_turns: int = 5):
        self.max_turns = max_turns
        self.turns: list[dict] = []

    # PURPOSE: ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ .
    def add(self, role: str, content: str):
        """ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ ."""
        self.turns.append({"role": role, "content": content})
        # æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ã‚’è¶…ãˆãŸã‚‰å¤ã„ã‚‚ã®ã‚’å‰Šé™¤
        if len(self.turns) > self.max_turns * 2:
            self.turns = self.turns[-(self.max_turns * 2):]

    # PURPOSE: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ.
    def format_for_prompt(self) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ."""
        if not self.turns:
            return ""
        parts = []
        for t in self.turns:
            if t["role"] == "user":
                parts.append(f"<|im_start|>user\n{t['content']}<|im_end|>")
            else:
                parts.append(f"<|im_start|>assistant\n{t['content']}<|im_end|>")
        return "\n".join(parts)

    # PURPOSE: çŠ¶æ…‹ã®ãƒªã‚»ãƒƒãƒˆã¨å†åˆæœŸåŒ–
    def clear(self):
        self.turns.clear()

    # PURPOSE: gnosis_chat ã® turn count å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    @property
# PURPOSE: Cross-encoder Reranker for precision improvement.
    # PURPOSE: Cross-encoder Reranker for precision improvement. Strategy: bi-encoder ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚§ãƒƒãƒ
    def turn_count(self) -> int:
        return len(self.turns) // 2


# PURPOSE: ã®çµ±ä¸€çš„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®Ÿç¾ã™ã‚‹
class Reranker:
    """Cross-encoder Reranker for precision improvement.

    Strategy: bi-encoder ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚§ãƒƒãƒ â†’ cross-encoder ã§ re-score
    â†’ top-k ã«çµã‚‹ã€‚ç²¾åº¦ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã€‚

    VRAM: ~50MB (cross-encoder-ms-marco-MiniLM-L-6-v2)
    """

    DEFAULT_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"

    # PURPOSE: Reranker ã®æ§‹æˆã¨ä¾å­˜é–¢ä¿‚ã®åˆæœŸåŒ–
    def __init__(self, model_name: Optional[str] = None):
        self.model_name = model_name or self.DEFAULT_MODEL
        self._model = None

    # PURPOSE: cross-encoder ãƒ¢ãƒ‡ãƒ«ã®é…å»¶ãƒ­ãƒ¼ãƒ‰ â€” ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ã
    def _load(self):
        if self._model is not None:
            return
        if getattr(self, "_failed", False):
            return  # æ—¢ã«å¤±æ•—æ¸ˆã¿ â€” ãƒ­ãƒ¼ãƒ‰å†è©¦è¡Œã—ãªã„
        import os
        from sentence_transformers import CrossEncoder
        # Strategy: ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆ â†’ ãƒªãƒ¢ãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ â†’ ç„¡åŠ¹åŒ–
        for attempt, offline in enumerate(["1", ""]):
            try:
                if offline:
                    os.environ["HF_HUB_OFFLINE"] = "1"
                    os.environ["TRANSFORMERS_OFFLINE"] = "1"
                self._model = CrossEncoder(self.model_name, device="cuda")
                print(f"[Reranker] Loaded ({self.model_name},"
                      f" offline={bool(offline)})", flush=True)
                return
            except Exception as e:
                if attempt == 0:
                    # ãƒ­ãƒ¼ã‚«ãƒ«å¤±æ•— â†’ ãƒªãƒ¢ãƒ¼ãƒˆè©¦è¡Œ
                    os.environ.pop("HF_HUB_OFFLINE", None)
                    os.environ.pop("TRANSFORMERS_OFFLINE", None)
                    continue
                # å…¨è©¦è¡Œå¤±æ•—
                print(f"[Reranker] âš ï¸ Failed to load ({e}), "
                      f"falling back to bi-encoder only", flush=True)
                self._failed = True
                return

    # Cross-encoder score threshold (Layer 2)
    # MiniLM scores are relative, not absolute.
    # Used only as a secondary noise filter.
    SCORE_THRESHOLD = -4.0

    # PURPOSE: æ¤œç´¢çµæœã‚’ cross-encoder ã§å†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° + é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿.
    def rerank(
        self, query: str, results: list[dict], top_k: int = 5
    ) -> list[dict]:
        """æ¤œç´¢çµæœã‚’ cross-encoder ã§å†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° + é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿.

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            results: bi-encoder ã®æ¤œç´¢çµæœ
            top_k: è¿”ã™ä»¶æ•°

        Returns:
            re-scored, filtered & sorted results (top_k)
        """
        if not results:
            return results

        self._load()

        # Reranker ãƒ­ãƒ¼ãƒ‰å¤±æ•—æ™‚ã¯ bi-encoder çµæœã‚’ãã®ã¾ã¾è¿”ã™
        if self._model is None:
            return sorted(results, key=lambda r: r.get("_distance", 999))[:top_k]

        # (query, document) ãƒšã‚¢ã‚’ä½œæˆ
        pairs = []
        for r in results:
            table = r.get("_source_table", "unknown")
            if table == "knowledge":
                text = r.get("content", r.get("abstract", ""))[:512]
            else:
                text = f"{r.get('title', '')} {r.get('abstract', '')[:400]}"
            pairs.append((query, text))

        # Cross-encoder scoring
        scores = self._model.predict(pairs)

        # ã‚¹ã‚³ã‚¢ã‚’çµæœã«ä»˜ä¸
        for r, score in zip(results, scores):
            r["_rerank_score"] = float(score)

        # Layer 2: cross-encoder é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿
        filtered = [
            r for r in results
            if r.get("_rerank_score", -999) > self.SCORE_THRESHOLD
# PURPOSE: HegemonikÃ³n å…¨çŸ¥è­˜ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£.
        ]

        # re-rank score ã§ã‚½ãƒ¼ãƒˆ (é«˜ã„æ–¹ãŒè‰¯ã„)
        filtered.sort(key=lambda r: r.get("_rerank_score", -999), reverse=True)

        return filtered[:top_k]


# PURPOSE: HegemonikÃ³n å…¨çŸ¥è­˜ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
class KnowledgeIndexer:
    """HegemonikÃ³n å…¨çŸ¥è­˜ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£."""

    # PURPOSE: gnosis_chat ã® discover knowledge files å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    @staticmethod
    # PURPOSE: å…¨çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹.
    def discover_knowledge_files() -> list[dict]:
        """å…¨çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹.

        Returns:
            list of dict with keys: path, source_type, title
        """
        files = []

        # 1. Handoff files (mneme/sessions)
        if MNEME_SESSIONS.exists():
            for f in sorted(MNEME_SESSIONS.glob("handoff_*.md")):
                files.append({
                    "path": f,
                    "source_type": "handoff",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 2. Session logs (mneme/sessions)
        if MNEME_SESSIONS.exists():
            for f in sorted(MNEME_SESSIONS.glob("2026-*_conv_*.md")):
                files.append({
                    "path": f,
                    "source_type": "session",
                    "title": f.stem.split("_", 3)[-1].replace("_", " ")
                    if "_" in f.stem else f.stem,
                })

        # 3. KI / Insight files (mneme/sessions)
        if MNEME_SESSIONS.exists():
            for f in sorted(MNEME_SESSIONS.glob("insight_*.md")):
                files.append({
                    "path": f,
                    "source_type": "ki",
                    "title": f.stem.replace("_", " ").title(),
                })
            for f in sorted(MNEME_SESSIONS.glob("weekly_review_*.md")):
                files.append({
                    "path": f,
                    "source_type": "review",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 4. Knowledge (mneme/knowledge/)
        if MNEME_KNOWLEDGE.exists():
            for f in sorted(MNEME_KNOWLEDGE.glob("*.md")):
                files.append({
                    "path": f,
                    "source_type": "ki",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 5. Handoff (docs/)
        if HANDOFF_DIR.exists():
            for f in sorted(HANDOFF_DIR.glob("*.md")):
                files.append({
                    "path": f,
                    "source_type": "handoff",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 6. Mneme Handoffs (handoffs/ dir)
        if MNEME_HANDOFFS.exists():
            for f in sorted(MNEME_HANDOFFS.glob("*.md")):
                files.append({
                    "path": f,
                    "source_type": "handoff",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 7. Doxa (belief records)
        if MNEME_DOXA.exists():
            for f in sorted(MNEME_DOXA.rglob("*.md")):
                files.append({
                    "path": f,
                    "source_type": "doxa",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 8. Workflow artifacts
        if MNEME_WORKFLOWS.exists():
            for f in sorted(MNEME_WORKFLOWS.glob("*.md")):
                files.append({
                    "path": f,
                    "source_type": "workflow",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 9. Research notes
        if MNEME_RESEARCH.exists():
            for f in sorted(MNEME_RESEARCH.glob("*.md")):
                files.append({
                    "path": f,
                    "source_type": "research",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 10. X-series relation maps
        if MNEME_XSERIES.exists():
            for f in sorted(MNEME_XSERIES.glob("*.md")):
                files.append({
                    "path": f,
                    "source_type": "xseries",
                    "title": f.stem.replace("_", " ").title(),
                })

        # 11. Kernel docs
        if KERNEL_DIR.exists():
            for f in sorted(KERNEL_DIR.glob("*.md")):
                files.append({
                    "path": f,
                    "source_type": "kernel",
                    "title": f.stem.replace("_", " ").title(),
                })
            # CEP
            cep_dir = KERNEL_DIR / "cep"
            if cep_dir.exists():
                for f in sorted(cep_dir.glob("*.md")):
                    files.append({
                        "path": f,
                        "source_type": "kernel",
                        "title": f"CEP: {f.stem}",
                    })

        return files

    @staticmethod
    # PURPOSE: ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯å¢ƒç•Œã§åˆ†å‰² (Markdown ã‚»ã‚¯ã‚·ãƒ§ãƒ³å¯¾å¿œ).
    def _chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> list[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯å¢ƒç•Œã§åˆ†å‰².

        æˆ¦ç•¥:
          1. Markdown ## ãƒ˜ãƒƒãƒ€ãƒ¼ã§ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰² (ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯å˜ä½)
          2. ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒ chunk_size ã‚’è¶…ãˆã‚‹å ´åˆã€æ®µè½ (ç©ºè¡Œ) ã§åˆ†å‰²
          3. ãã‚Œã§ã‚‚è¶…ãˆã‚‹å ´åˆã€æ–‡ã®é€”ä¸­ã§åˆ‡ã‚‰ãªã„ã‚ˆã†æ”¹è¡Œã§åˆ†å‰²
        """
        if len(text) <= chunk_size:
            return [text]

        import re

        # Phase 1: Markdown ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰² (##, ###, ----)
        sections = re.split(r'\n(?=#{1,4}\s|\-{3,})', text)

        chunks = []
        for section in sections:
            section = section.strip()
            if not section:
                continue

            if len(section) <= chunk_size:
                chunks.append(section)
            else:
                # Phase 2: ç©ºè¡Œã§æ®µè½åˆ†å‰²
                paragraphs = re.split(r'\n\s*\n', section)
                buffer = ""
                for para in paragraphs:
                    para = para.strip()
                    if not para:
                        continue

                    if len(buffer) + len(para) + 2 <= chunk_size:
                        buffer = f"{buffer}\n\n{para}" if buffer else para
                    else:
                        if buffer:
                            chunks.append(buffer)
                        # Phase 3: æ®µè½è‡ªä½“ãŒå¤§ãã„å ´åˆã€æ”¹è¡Œã§åˆ‡ã‚‹
                        if len(para) > chunk_size:
                            sub_chunks = KnowledgeIndexer._split_long_text(
                                para, chunk_size, overlap
                            )
                            chunks.extend(sub_chunks)
                        else:
                            buffer = para
                            continue
                        buffer = ""
                if buffer:
                    chunks.append(buffer)

        return [c for c in chunks if c and len(c) > 20]

    @staticmethod
    # PURPOSE: [L2-auto] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ”¹è¡Œã§åˆ‡ã‚‹å›ºå®šã‚µã‚¤ã‚ºåˆ†å‰².
    def _split_long_text(text: str, chunk_size: int, overlap: int) -> list[str]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ”¹è¡Œã§åˆ‡ã‚‹å›ºå®šã‚µã‚¤ã‚ºåˆ†å‰²."""
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]

            if end < len(text):
                last_nl = chunk.rfind("\n")
                if last_nl > chunk_size // 2:
                    end = start + last_nl + 1
                    chunk = text[start:end]

            chunks.append(chunk.strip())
            # Guard: start ã¯å¿…ãšå‰é€²ã™ã‚‹ (overlap > effective chunk æ™‚ã®ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢)
            start = max(start + 1, end - overlap)
        return [c for c in chunks if c]

    @staticmethod
    # PURPOSE: [L2-auto] embedding ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰ (ã‚¿ã‚¤ãƒˆãƒ« + ã‚½ãƒ¼ã‚¹ + ã‚³ãƒ³ãƒ†ãƒ³ãƒ„).
    def _build_embedding_text(title: str, source: str, content: str) -> str:
        """embedding ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰ (ã‚¿ã‚¤ãƒˆãƒ« + ã‚½ãƒ¼ã‚¹ + ã‚³ãƒ³ãƒ†ãƒ³ãƒ„).

        æ¤œç´¢ç²¾åº¦å‘ä¸Šã®ãŸã‚ã€æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä»˜ä¸ã€‚
        """
        prefix = f"[{source}] {title}\n"
        max_content = 500 - len(prefix)
        return prefix + content[:max_content]

    # PURPOSE: gnosis_chat ã® index knowledge å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    @staticmethod
    # PURPOSE: å…¨çŸ¥è­˜ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ .
    def index_knowledge(force_reindex: bool = False) -> int:
        """å…¨çŸ¥è­˜ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ .

        Returns:
            è¿½åŠ ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
        """
        from mekhane.anamnesis.index import GnosisIndex, LANCE_DIR
        from mekhane.anamnesis.models.paper import Paper

        index = GnosisIndex()

        # æ—¢å­˜ã® knowledge ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç¢ºèª
        table_name = "knowledge"
        existing_keys = set()

        if not force_reindex and table_name in get_table_names(index.db):
            try:
                table = index.db.open_table(table_name)
                df = table.search().select(["primary_key"]).limit(None).to_pandas()
                existing_keys = set(df["primary_key"].tolist())
                print(f"[Knowledge] Existing: {len(existing_keys)} chunks")
            except Exception:
                pass

        files = KnowledgeIndexer.discover_knowledge_files()
        print(f"[Knowledge] Discovered {len(files)} knowledge files")

        # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        embedder = index._get_embedder()
        data = []
        skipped = 0

        for finfo in files:
            try:
                text = finfo["path"].read_text(encoding="utf-8")
            except Exception:
                continue

            chunks = KnowledgeIndexer._chunk_text(text)

            for ci, chunk in enumerate(chunks):
                primary_key = f"{finfo['source_type']}:{finfo['path'].stem}:{ci}"

                if primary_key in existing_keys:
                    skipped += 1
                    continue

                data.append({
                    "primary_key": primary_key,
                    "title": finfo["title"],
                    "source": finfo["source_type"],
                    "abstract": chunk[:300],
                    "content": chunk,
                    "authors": "",
                    "doi": "",
                    "arxiv_id": "",
                    "url": str(finfo["path"]),
                    "citations": 0,
                })

        if not data:
            print(f"[Knowledge] No new documents (skipped {skipped})")
            return 0

        # ãƒãƒƒãƒ embedding
        print(f"[Knowledge] Embedding {len(data)} chunks...", flush=True)
        BATCH_SIZE = 32
        for i in range(0, len(data), BATCH_SIZE):
            batch = data[i:i + BATCH_SIZE]
            texts = [
                KnowledgeIndexer._build_embedding_text(
                    d["title"], d["source"], d["content"]
                )
                for d in batch
            ]
            vectors = embedder.embed_batch(texts)

            for d, v in zip(batch, vectors):
                d["vector"] = v

            done = min(i + BATCH_SIZE, len(data))
            print(f"  Embedded {done}/{len(data)}...", flush=True)

# PURPOSE: GnÅsis å¯¾è©±å‹ RAG ã‚¨ãƒ³ã‚¸ãƒ³.
        # LanceDB ã«è¿½åŠ 
        if table_name in get_table_names(index.db):
            table = index.db.open_table(table_name)
            table.add(data)
        else:
            index.db.create_table(table_name, data=data)

        print(f"[Knowledge] Added {len(data)} chunks (skipped {skipped})")
        return len(data)


# PURPOSE: GnÅsis å¯¾è©±å‹ RAG ã‚¨ãƒ³ã‚¸ãƒ³
class GnosisChat:
    """GnÅsis å¯¾è©±å‹ RAG ã‚¨ãƒ³ã‚¸ãƒ³.

    NotebookLM ã®ã‚ˆã†ã«ã€è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦å¯¾è©±ã™ã‚‹ã€‚
    å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ« (ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å‹•ä½œå¯èƒ½)ã€‚

    3å±¤ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³é˜²å¾¡:
      Layer 1: bi-encoder distance threshold
      Layer 2: cross-encoder score threshold
      Layer 3: confidence assessment â†’ prompt adaptation

    Layer 4: Prompt-level steering
      System prompt ã« contrastive guidance ã‚’æ³¨å…¥ã—ã¦æŒ¯ã‚‹èˆã„ã‚’åˆ¶å¾¡ã€‚
    """

    DEFAULT_MODEL = "Qwen/Qwen2.5-3B-Instruct"

    # Layer 1: bi-encoder distance threshold
    # LanceDB L2 distance on normalized vectors:
    #   0 = identical, ~1.0 = unrelated, ~1.41 = opposite
    DISTANCE_THRESHOLD = 0.85

    # Papers: cross-language gap (+0.21~0.28) ã‚’è€ƒæ…®ã—ã¦ç·©å’Œ
    PAPERS_DISTANCE_THRESHOLD = 0.95

    # Prompt-level steering profiles
    STEERING_PROFILES = {
        "hegemonikon": (
            "\n## æŒ¯ã‚‹èˆã„æŒ‡é‡ (Steering)\n"
            "- ä¸ç¢ºå®Ÿãªå ´åˆã¯ã€ç¢ºä¿¡åº¦ãŒä½ã„ã§ã™ãŒ...ã€ã¨å‰ç½®ãã™ã‚‹\n"
            "- è³ªå•ã«ç­”ãˆã‚‹å‰ã«ã€å›ç­”ã®å‰ææ¡ä»¶ã‚’æ˜ç¤ºã™ã‚‹\n"
            "- è¤‡æ•°ã®è§£é‡ˆãŒå¯èƒ½ãªå ´åˆã¯ã€ãã‚Œãã‚Œã®å¯èƒ½æ€§ã‚’åˆ—æŒ™ã™ã‚‹\n"
            "- æ½œåœ¨çš„ãªãƒªã‚¹ã‚¯ã‚„æ³¨æ„ç‚¹ã«æ°—ã¥ã„ãŸå ´åˆã¯ã€ç©æ¥µçš„ã«æŒ‡æ‘˜ã™ã‚‹\n"
            "- å›ç­”ãŒçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã®ã¿ã«åŸºã¥ãã“ã¨ã‚’æ„è­˜ã—ã€æ¨æ¸¬ã¨äº‹å®Ÿã‚’æ˜ç¢ºã«åŒºåˆ¥ã™ã‚‹\n"
        ),
        "neutral": "",  # No steering
        "academic": (
            "\n## æŒ¯ã‚‹èˆã„æŒ‡é‡ (Steering)\n"
            "- å­¦è¡“çš„ãªæ­£ç¢ºã•ã‚’æœ€å„ªå…ˆã™ã‚‹\n"
            "- ä¸»å¼µã«ã¯å¿…ãšæ ¹æ‹  [ç•ªå·] ã‚’ä»˜ä¸ã™ã‚‹\n"
            "- å¯¾ç«‹ã™ã‚‹è¦‹è§£ãŒã‚ã‚‹å ´åˆã¯ä¸¡æ–¹ã‚’æç¤ºã™ã‚‹\n"
            "- æ–¹æ³•è«–çš„ãªé™ç•Œã‚’æŒ‡æ‘˜ã™ã‚‹\n"
        ),
    }

    # Confidence levels
    CONFIDENCE_HIGH = "high"
    CONFIDENCE_MEDIUM = "medium"
    CONFIDENCE_LOW = "low"
    CONFIDENCE_NONE = "none"

    # PURPOSE: çŸ¥è­˜åŸºç›¤ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
    def __init__(
        self,
        model_id: Optional[str] = None,
        top_k: int = 5,
        max_new_tokens: int = 512,
        search_knowledge: bool = True,
        search_papers: bool = True,
        use_reranker: bool = True,
        steering_profile: str = "hegemonikon",
    ):
        self.model_id = model_id or self.DEFAULT_MODEL
        self.top_k = top_k
        self.max_new_tokens = max_new_tokens
        self.search_knowledge = search_knowledge
        self.search_papers = search_papers
        self.use_reranker = use_reranker
        self.steering_profile = steering_profile

        self._index = None
        self._model = None
        self._tokenizer = None
        self._reranker = Reranker() if use_reranker else None
        self.history = ConversationHistory(max_turns=5)

    # PURPOSE: GnÅsis ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ (GPU embedding).
    def _load_index(self):
        """GnÅsis ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ (GPU embedding)."""
        if self._index is not None:
            return
        from mekhane.anamnesis.index import GnosisIndex
        self._index = GnosisIndex()
        print("[GnÅsis Chat] Index loaded", flush=True)

    # PURPOSE: VRAM ã‚¿ã‚¤ãƒ ã‚·ã‚§ã‚¢: Embedder ã‚’è§£æ”¾ã—ã¦ LLM ç”¨ã« VRAM ã‚’ç¢ºä¿.
    def _unload_embedder(self):
        """VRAM ã‚¿ã‚¤ãƒ ã‚·ã‚§ã‚¢: Embedder ã‚’è§£æ”¾ã—ã¦ LLM ç”¨ã« VRAM ã‚’ç¢ºä¿.

        BGE-m3 (2.3GB) + Qwen 3B (2.1GB) = 4.4GB > 8GB GPU
        æ¤œç´¢å®Œäº†å¾Œã« Embedder ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ LLM ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã«ã™ã‚‹ã€‚
        """
        import gc
        import torch
        if self._index is not None:
            embedder = self._index._get_embedder()
            if hasattr(embedder, '_st_model') and embedder._st_model is not None:
                del embedder._st_model
                embedder._st_model = None
                embedder._use_gpu = False
            # Reranker ã‚‚è§£æ”¾
            if self._reranker and self._reranker._model is not None:
                del self._reranker._model
                self._reranker._model = None
            gc.collect()
            torch.cuda.empty_cache()
            vram_mb = torch.cuda.memory_allocated() / 1e6
            print(f"[GnÅsis Chat] Embedder unloaded ({vram_mb:.0f}MB VRAM)", flush=True)

    # PURPOSE: LLM ã‚’ãƒ­ãƒ¼ãƒ‰ (4bité‡å­åŒ– on GPU).
    def _load_model(self):
        """LLM ã‚’ãƒ­ãƒ¼ãƒ‰ (4bité‡å­åŒ– on GPU)."""
        if self._model is not None:
            return

        # VRAM ã‚¿ã‚¤ãƒ ã‚·ã‚§ã‚¢: Embedder ã‚’å…ˆã«è§£æ”¾
        self._unload_embedder()

        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )

        print(f"[GnÅsis Chat] Loading {self.model_id}...", flush=True)
        self._tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        self._model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            quantization_config=bnb_config,
            device_map="auto",
        )

        vram_mb = torch.cuda.memory_allocated() / 1e6
        print(f"[GnÅsis Chat] Model loaded ({vram_mb:.0f}MB VRAM)", flush=True)

    # PURPOSE: å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ + é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ + rerank.
    def _retrieve(self, query: str) -> list[dict]:
        """å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ + é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ + rerank."""
        self._load_index()
        results = []
        fetch_k = self.top_k * 3 if self._reranker else self.top_k

        # Papers table
        if self.search_papers:
            try:
                paper_results = self._index.search(query, k=fetch_k)
                for r in paper_results:
                    r["_source_table"] = "papers"
                results.extend(paper_results)
            except Exception:
                pass

        # Knowledge table
        if self.search_knowledge:
            try:
                if "knowledge" in get_table_names(self._index.db):
                    embedder = self._index._get_embedder()
                    # Dimension check: knowledge table may be indexed with a different model
                    table = self._index.db.open_table("knowledge")
                    from mekhane.anamnesis.index import _get_vector_dimension
                    table_dim = _get_vector_dimension(table)
                    embedder_dim = getattr(embedder, '_dimension', 0)
                    if table_dim and embedder_dim and table_dim != embedder_dim:
                        print(
                            f"[GnÅsis Chat] âš ï¸ Knowledge table dimension mismatch: "
                            f"table={table_dim}, embedder={embedder_dim} "
                            f"({'ONNX fallback' if getattr(embedder, '_is_onnx_fallback', False) else embedder.model_name}). "
                            f"Skipping knowledge search.",
                            flush=True,
                        )
                    else:
                        qvec = embedder.embed(query)
                        k_results = table.search(qvec).limit(fetch_k).to_list()
                        for r in k_results:
                            r["_source_table"] = "knowledge"
                        results.extend(k_results)
            except Exception:
                pass

        # Layer 1: bi-encoder è·é›¢é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ (papers/knowledge ã§é–¾å€¤åˆ†é›¢)
        before_filter = len(results)
        results = [
            r for r in results
            if r.get("_distance", 999) < (
                self.PAPERS_DISTANCE_THRESHOLD
                if r.get("_source_table") == "papers"
                else self.DISTANCE_THRESHOLD
            )
        ]
        if before_filter > 0 and len(results) == 0:
            print(f"[GnÅsis Chat] Layer 1: all {before_filter} results filtered "
                  f"(knowledge>{self.DISTANCE_THRESHOLD}, "
                  f"papers>{self.PAPERS_DISTANCE_THRESHOLD})", flush=True)

        results.sort(key=lambda r: r.get("_distance", 999))

        # Layer 2: Reranker
        if self._reranker and results:
            results = self._reranker.rerank(query, results, top_k=self.top_k)
        else:
            results = results[:self.top_k]

        return results

    # PURPOSE: Layer 3: æ¤œç´¢çµæœã®å“è³ªã‹ã‚‰ç¢ºä¿¡åº¦ã‚’åˆ¤å®š.
    def _assess_confidence(self, results: list[dict]) -> str:
        """Layer 3: æ¤œç´¢çµæœã®å“è³ªã‹ã‚‰ç¢ºä¿¡åº¦ã‚’åˆ¤å®š.

        è·é›¢ (bi-encoder) ã®ã¿ã§åˆ¤å®šã™ã‚‹ã€‚
        cross-encoder ã‚¹ã‚³ã‚¢ã¯ relative ranking ç”¨ã§ã‚ã‚Š absolute åˆ¤å®šã«ã¯ä¸é©ã€‚

        BGE-m3 è·é›¢ã‚¹ã‚±ãƒ¼ãƒ« (L2, normalized):
          - é–¢é€£: 0.5-0.8
          - æ›–æ˜§: 0.8-0.9
          - ç„¡é–¢é€£: >0.9 (DISTANCE_THRESHOLD ã§é™¤å»æ¸ˆã¿)
        """
        if not results:
            return self.CONFIDENCE_NONE

        distances = [r.get("_distance", 999) for r in results]
        min_dist = min(distances)
        avg_dist = sum(distances) / len(distances)
        n = len(results)

        if min_dist < 0.6 and n >= 3 and avg_dist < 0.75:
            return self.CONFIDENCE_HIGH
        elif min_dist < 0.7:
            return self.CONFIDENCE_HIGH
        elif min_dist < 0.8:
            return self.CONFIDENCE_MEDIUM
        else:
            return self.CONFIDENCE_LOW

    # PURPOSE: æ¤œç´¢çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’æ§‹ç¯‰ (Source Grounding å¼·åŒ–).
    def _build_context(self, results: list[dict]) -> str:
        """æ¤œç´¢çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’æ§‹ç¯‰.

        Source Grounding: å„å¼•ç”¨ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã«ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨
        ãƒãƒ£ãƒ³ã‚¯ ID ã‚’ä»˜ä¸ã—ã€å›ç­”ã®è¿½è·¡å¯èƒ½æ€§ã‚’ç¢ºä¿ã™ã‚‹ã€‚
        """
        context_parts = []
        for i, r in enumerate(results, 1):
            title = r.get("title", "Untitled")
            source = r.get("source", "unknown")
            table = r.get("_source_table", "unknown")

            # knowledge ãƒ†ãƒ¼ãƒ–ãƒ«ã®å ´åˆã¯ content ã‚’ä½¿ç”¨
            if table == "knowledge":
                content = r.get("content", r.get("abstract", ""))[:600]
            else:
                content = r.get("abstract", "")[:500]

            authors = r.get("authors", "")[:80]
            dist = r.get("_distance", 0)

            # Source Grounding: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨ chunk_id ã§è¿½è·¡å¯èƒ½ã«ã™ã‚‹
            primary_key = r.get("primary_key", "")
            url = r.get("url", "")
            source_loc = ""
            if primary_key:
                source_loc = f"    Source: {primary_key}\n"
            elif url:
                source_loc = f"    Source: {url}\n"

            context_parts.append(
                f"[{i}] [{source}] {title}\n"
                f"    Relevance: {1 - dist:.2f}\n"
                f"{source_loc}"
                f"    {content}"
            )
        return "\n\n".join(context_parts)

    # PURPOSE: LLM ã§å›ç­”ã‚’ç”Ÿæˆ.
    def _generate(self, prompt: str) -> str:
        """LLM ã§å›ç­”ã‚’ç”Ÿæˆ."""
        import torch

        self._load_model()

        inputs = self._tokenizer(
            prompt, return_tensors="pt", truncation=True, max_length=2048
        ).to(self._model.device)

        with torch.no_grad():
            outputs = self._model.generate(
                **inputs,
                max_new_tokens=self.max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.15,
                pad_token_id=self._tokenizer.eos_token_id,
            )

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»ã—ã¦å›ç­”ã®ã¿è¿”ã™
        input_len = inputs["input_ids"].shape[1]
        answer_tokens = outputs[0][input_len:]
        return self._tokenizer.decode(answer_tokens, skip_special_tokens=True)

    # PURPOSE: è³ªå•ã«å›ç­”ã™ã‚‹ (RAG + ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ + 3å±¤é˜²å¾¡).
    def ask(self, question: str) -> dict:
        """è³ªå•ã«å›ç­”ã™ã‚‹ (RAG + ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ + 3å±¤é˜²å¾¡)."""
        # 1. Retrieve (Layer 1 + 2)
        t0 = time.time()
        results = self._retrieve(question)
        retrieval_time = time.time() - t0

        # 2. Layer 3: Confidence
        confidence = self._assess_confidence(results)
        context = self._build_context(results)

        # 3. Confidence-adaptive response
        if confidence == self.CONFIDENCE_NONE:
            answer = (
                "ğŸ“‹ **ã‚½ãƒ¼ã‚¹ã«æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“**\n\n"
                "ã“ã®è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ã¯ã€ç¾åœ¨ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"
                "GnÅsis ã¯ã‚½ãƒ¼ã‚¹ã«åŸºã¥ãå›ç­”ã®ã¿ã‚’æä¾›ã—ã€æ¨æ¸¬ã¯è¡Œã„ã¾ã›ã‚“ã€‚\n\n"
                "**å¯¾å‡¦æ³•:**\n"
                "- ğŸ”„ åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è³ªå•ã™ã‚‹\n"
                "- ğŸ“¥ `/index` ã§çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã™ã‚‹\n"
                "- ğŸ“„ `collect` ã‚³ãƒãƒ³ãƒ‰ã§é–¢é€£ã™ã‚‹è«–æ–‡ã‚’è¿½åŠ ã™ã‚‹\n"
                "- ğŸ” è³ªå•ã®ç¯„å›²ã‚’ç‹­ã‚ã‚‹ï¼ˆå…·ä½“çš„ãªãƒˆãƒ”ãƒƒã‚¯åã‚’å«ã‚ã‚‹ï¼‰"
            )
            generation_time = 0
        else:
            conf_instr = {
                self.CONFIDENCE_HIGH: "æ¤œç´¢çµæœã«ååˆ†ãªæƒ…å ±ãŒã‚ã‚Šã¾ã™ã€‚è‡ªä¿¡ã‚’æŒã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚",
                self.CONFIDENCE_MEDIUM: (
                    "æ¤œç´¢çµæœã«éƒ¨åˆ†çš„ãªæƒ…å ±ãŒã‚ã‚Šã¾ã™ã€‚"
                    "ä¸ç¢ºå®Ÿãªéƒ¨åˆ†ã¯ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«ååˆ†ãªæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚"
                ),
                self.CONFIDENCE_LOW: (
                    "æ¤œç´¢çµæœã¨ã®é–¢é€£æ€§ãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
                    "æ¨æ¸¬ã‚„å‰µä½œã¯çµ¶å¯¾ã«ã—ãªã„ã§ãã ã•ã„ã€‚"
                    "å›ç­”å†’é ­ã«ã€âš ï¸ é–¢é€£æ€§ãŒä½ã„æƒ…å ±ã«åŸºã¥ãå›ç­”ã§ã™ã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"
                    "ç­”ãˆã‚‰ã‚Œãªã„å ´åˆã¯ã€ååˆ†ãªæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨æ­£ç›´ã«è¿”ã—ã¦ãã ã•ã„ã€‚"
                ),
            }.get(confidence, "")

            system_prompt = (
                "ã‚ãªãŸã¯ HegemonikÃ³n ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å¯¾è©±ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
                "ä»¥ä¸‹ã®æ¤œç´¢çµæœã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
                f"{conf_instr}\n"
                "å›ç­”ã¯ç°¡æ½”ã§æ§‹é€ çš„ã«ã—ã¦ãã ã•ã„ã€‚\n"
                "å¼•ç”¨ã™ã‚‹å ´åˆã¯ [ç•ªå·] ã®å½¢å¼ã§å‚ç…§å…ƒã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚\n"
                "æ¤œç´¢çµæœã«ãªã„æƒ…å ±ã‚’å‰µä½œãƒ»æ¨æ¸¬ã—ãªã„ã§ãã ã•ã„ã€‚"
                + self.STEERING_PROFILES.get(self.steering_profile, "")
            )

            prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
            history_text = self.history.format_for_prompt()
            if history_text:
                prompt += history_text + "\n"
            prompt += (
                f"<|im_start|>user\n"
                f"## æ¤œç´¢çµæœ (é–¢é€£æ–‡æ›¸)\n\n{context}\n\n"
                f"## è³ªå•\n{question}<|im_end|>\n"
                f"<|im_start|>assistant\n"
            )

            t0 = time.time()
            answer = self._generate(prompt)
            generation_time = time.time() - t0

        self.history.add("user", question)
        self.history.add("assistant", answer.strip())

        sources = []
        for r in results:
            sources.append({
                "title": r.get("title", "Untitled")[:80],
                "source": r.get("source", "unknown"),
                "table": r.get("_source_table", "unknown"),
                "url": r.get("url", ""),
                "primary_key": r.get("primary_key", ""),
                "content_snippet": (r.get("content", r.get("abstract", "")))[:200],
                "distance": round(r.get("_distance", 0), 4),
                "relevance": round(1 - r.get("_distance", 0), 4),
                "rerank_score": r.get("_rerank_score"),
            })

        return {
            "answer": answer.strip(),
            "sources": sources,
            "confidence": confidence,
            "retrieval_time": round(retrieval_time, 3),
            "generation_time": round(generation_time, 1),
            "context_docs": len(results),
            "turn": self.history.turn_count,
        }

    # PURPOSE: æ¤œç´¢ã®ã¿å®Ÿè¡Œ (LLM ä¸ä½¿ç”¨). Claude (IDE) ãŒ Generation ã™ã‚‹ç”¨
    def retrieve_only(self, query: str) -> dict:
        """æ¤œç´¢ã®ã¿å®Ÿè¡Œ (LLM ä¸ä½¿ç”¨). Claude (IDE) ãŒ Generation ã™ã‚‹ç”¨."""
        t0 = time.time()
        results = self._retrieve(query)
        retrieval_time = time.time() - t0
        confidence = self._assess_confidence(results)
        context = self._build_context(results)

        sources = []
        for r in results:
            sources.append({
                "title": r.get("title", "Untitled")[:80],
                "source": r.get("source", "unknown"),
                "table": r.get("_source_table", "unknown"),
                "url": r.get("url", ""),
                "distance": round(r.get("_distance", 0), 4),
                "rerank_score": r.get("_rerank_score"),
            })

        return {
            "context": context,
            "sources": sources,
            "confidence": confidence,
            "retrieval_time": round(retrieval_time, 3),
            "context_docs": len(results),
        }

    # PURPOSE: å¯¾è©±ãƒ«ãƒ¼ãƒ— (REPL).
    def interactive(self):
        """å¯¾è©±ãƒ«ãƒ¼ãƒ— (REPL)."""
        print("\n" + "=" * 60)
        print("  GnÅsis Chat â€” HegemonikÃ³n ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å¯¾è©±")
        print(f"  Model: {self.model_id}")
        print(f"  Steering: {self.steering_profile}")
        print("  Commands: /quit, /sources, /stats, /clear, /index, /steering")
        print("=" * 60)

        last_result = None

        while True:
            try:
                turn = self.history.turn_count
                question = input(f"\n[{turn}] â“ ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\nbye.")
                break

            if not question:
                continue

            if question in ("/quit", "/q", "/exit"):
                print("bye.")
                break

            if question == "/clear":
                self.history.clear()
                print("ğŸ”„ ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                continue

            if question == "/sources" and last_result:
                print("\nğŸ“š Sources:")
                for i, s in enumerate(last_result["sources"], 1):
                    icon = {"papers": "ğŸ“„", "knowledge": "ğŸ§ "}.get(
                        s["table"], "ğŸ“"
                    )
                    print(f"  [{i}] {icon} [{s['source']}] {s['title']}")
                    print(f"      Relevance: {s['relevance']}")
                    if s["url"]:
                        print(f"      {s['url']}")
                continue

            if question == "/stats":
                self._load_index()
                stats = self._index.stats()
                print(f"\nğŸ“Š Papers: {stats['total']}")
                for src, cnt in stats.get("sources", {}).items():
                    print(f"   {src}: {cnt}")

                # Knowledge table stats
                if "knowledge" in get_table_names(self._index.db):
                    kt = self._index.db.open_table("knowledge")
                    kdf = kt.to_pandas()
                    print(f"\nğŸ§  Knowledge chunks: {len(kdf)}")
                    for src, cnt in kdf["source"].value_counts().items():
# PURPOSE: CLI entry point for chat command.
                        print(f"   {src}: {cnt}")
                continue

            if question == "/index":
                print("ğŸ“ Indexing knowledge files...", flush=True)
                added = KnowledgeIndexer.index_knowledge()
                print(f"âœ… Indexed {added} new chunks")
                continue

            if question.startswith("/steering"):
                parts = question.split()
                if len(parts) == 1:
                    print(f"ğŸ§­ Current: {self.steering_profile}")
                    print(f"   Available: {', '.join(self.STEERING_PROFILES.keys())}")
                elif parts[1] in self.STEERING_PROFILES:
                    self.steering_profile = parts[1]
                    print(f"ğŸ§­ Switched to: {self.steering_profile}")
                else:
                    print(f"âŒ Unknown: {parts[1]}. Available: {', '.join(self.STEERING_PROFILES.keys())}")
                continue

            # Ask
            print("ğŸ” Searching...", flush=True)
            result = self.ask(question)
            last_result = result

            print(f"\nğŸ’¡ [{result['turn']}] Answer "
                  f"({result['generation_time']}s, "
                  f"{result['context_docs']} docs):\n")
            print(result["answer"])
            print(f"\nğŸ“š {len(result['sources'])} sources (/sources for details)")


# PURPOSE: CLI entry point for chat command
def cmd_chat(args) -> int:
    """CLI entry point for chat command."""
    chat = GnosisChat(
        top_k=args.top_k,
        max_new_tokens=args.max_tokens,
        steering_profile=args.steering,
    )

    if args.index:
        print("ğŸ“ Indexing knowledge files...", flush=True)
        added = KnowledgeIndexer.index_knowledge()
        print(f"âœ… Indexed {added} new chunks")
        if not args.question:
            return 0

    if args.question:
        result = chat.ask(args.question)
        conf_icon = {"high": "ğŸŸ¢", "medium": "ğŸŸ¡", "low": "ğŸŸ ", "none": "ğŸ”´"}.get(
            result.get("confidence", ""), "âšª")
        print(f"\n{conf_icon} Confidence: {result.get('confidence', '?')}")
        print(f"\nğŸ’¡ Answer:\n\n{result['answer']}")
        print(f"\n---")
        print(f"ğŸ“š Sources ({result['context_docs']} docs, "
              f"retrieval: {result['retrieval_time']}s, "
              f"generation: {result['generation_time']}s):")
        for i, s in enumerate(result["sources"], 1):
            icon = {"papers": "ğŸ“„", "knowledge": "ğŸ§ "}.get(s["table"], "ğŸ“")
            d = s.get('distance', 0)
            rs = s.get('rerank_score')
            score_str = f" rs={rs:.1f}" if rs is not None else ""
            print(f"  [{i}] {icon} [{s['source']}] {s['title']} (d={d:.4f}{score_str})")
            if s["url"]:
                print(f"      {s['url']}")
        return 0
    else:
        chat.interactive()
        return 0


# PURPOSE: CLI entry point for retrieve command (no LLM)
def cmd_retrieve(args) -> int:
    """CLI entry point for retrieve command (no LLM)."""
    chat = GnosisChat(top_k=args.top_k)

    result = chat.retrieve_only(args.query)

    conf_icon = {"high": "ğŸŸ¢", "medium": "ğŸŸ¡", "low": "ğŸŸ ", "none": "ğŸ”´"}.get(
        result.get("confidence", ""), "âšª")

    print(f"\n{conf_icon} Confidence: {result['confidence']} "
          f"({result['context_docs']} docs, {result['retrieval_time']}s)")

    if result["context"]:
        print(f"\n{'=' * 60}")
        print("ğŸ“š Retrieved Context")
        print(f"{'=' * 60}\n")
        print(result["context"])
        print(f"\n{'=' * 60}")

    print("\nğŸ“‘ Sources:")
    for i, s in enumerate(result["sources"], 1):
        icon = {"papers": "ğŸ“„", "knowledge": "ğŸ§ "}.get(s["table"], "ğŸ“")
        d = s.get("distance", 0)
        rs = s.get("rerank_score")
        score_str = f" rs={rs:.1f}" if rs is not None else ""
        print(f"  [{i}] {icon} [{s['source']}] {s['title']} (d={d:.4f}{score_str})")
        if s["url"]:
            print(f"      {s['url']}")

    return 0
