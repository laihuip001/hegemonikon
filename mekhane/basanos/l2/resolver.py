#!/usr/bin/env python3
# PROOF: [L2/Basanos] <- mekhane/basanos/l2/ A0->BasanosLogic
# PURPOSE: Basanos L3 Ëá™ÂãïËß£Ê±∫„É´„Éº„Éó ‚Äî deficit‚ÜíÂïè„ÅÑ‚ÜíËß£Ê±∫Á≠ñ„ÅÆËá™ÂãïÁîüÊàê
# REASON: deficit „ÇíÊ§úÂá∫„Åô„Çã„Å†„Åë„Åß„Å™„Åè„ÄÅËß£Ê±∫„Å∏„ÅÆÈÅìÁ≠ã„ÇíËá™ÂãïÊèêÊ°à„Åô„Çã„Åü„ÇÅ
"""Auto-resolution loop for Basanos L3.

Takes detected deficits, generates questions via L2, and uses LLM
to propose resolution strategies. Supports both Cortex (Gemini) and
local analysis fallbacks.
"""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Optional

from mekhane.basanos.l2.models import Deficit, Question

logger = logging.getLogger(__name__)


@dataclass
class Resolution:
    """Proposed resolution for a deficit."""

    question: Question
    strategy: str  # proposed resolution strategy
    confidence: float  # 0.0 - 1.0
    actions: list[str] = field(default_factory=list)  # concrete action items
    references: list[str] = field(default_factory=list)  # relevant files/docs
    status: str = "proposed"  # proposed, accepted, rejected, applied

    def to_dict(self) -> dict[str, Any]:
        """Serialize to JSON-safe dict."""
        return {
            "question": self.question.text,
            "deficit_type": self.question.deficit.type.value,
            "strategy": self.strategy,
            "confidence": self.confidence,
            "actions": self.actions,
            "references": self.references,
            "status": self.status,
        }


class Resolver:
    """L3 auto-resolution engine.

    Generates resolution strategies for deficits using:
    1. Rule-based heuristics (fast, no LLM)
    2. LLM-assisted analysis (via Cortex/Ochema, if available)
    """

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self._heuristics = self._build_heuristics()

    def _build_heuristics(self) -> dict[str, Any]:
        """Build rule-based resolution heuristics per deficit type."""
        return {
            "Œ∑": {
                "strategy_template": (
                    "Â§ñÈÉ®Ë´ñÊñá„Äå{source}„Äç„ÅÆÊ¶ÇÂøµ„Çí HGK „Å´Âê∏Âèé„Åô„Çã„ÄÇ"
                    "Èñ¢ÈÄ£„Åô„Çã Series „ÇíÁâπÂÆö„Åó„ÄÅkernel/ „Å´ÂÆöÁæ©„ÇíËøΩÂä†„Åô„Çã„Åã„ÄÅ"
                    "Êó¢Â≠ò„ÅÆÂÆöÁêÜ„Å´Á¥ê‰ªò„Åë„Çã„ÄÇ"
                ),
                "actions": [
                    "/eat „ÅßÂ§ñÈÉ®Ê¶ÇÂøµ„ÇíÊ∂àÂåñ",
                    "kernel/ „Å´Êñ∞ÂÆöÁæ©ËøΩÂä† or Êó¢Â≠òÂÆöÁêÜ„Å´Ê≥®ÈáàËøΩÂä†",
                    "/dia „ÅßÂê∏ÂèéÂæå„ÅÆÊï¥ÂêàÊÄß„ÇíÊ§úË®º",
                ],
                "confidence": 0.6,
            },
            "Œµ-impl": {
                "strategy_template": (
                    "kernel/{source} „ÅÆÂÆöÁêÜ {target} „Å´ÂØæÂøú„Åô„ÇãÂÆüË£Ö„Çí "
                    "mekhane/ „Å´‰ΩúÊàê„Åô„Çã„ÄÇWF ÂÆöÁæ©„ÇÇÈÄ£Âãï„Åó„Å¶ËøΩÂä†„ÄÇ"
                ),
                "actions": [
                    "mekhane/ „Å´ÂÆüË£Ö„É¢„Ç∏„É•„Éº„É´‰ΩúÊàê",
                    "PROOF.md „ÇíÂêå„Éá„Ç£„É¨„ÇØ„Éà„É™„Å´ÈÖçÁΩÆ",
                    ".agent/workflows/ „Å´ÂØæÂøú WF ‰ΩúÊàê",
                    "/dendron „ÅßÂ≠òÂú®Ë®ºÊòé„ÉÅ„Çß„ÉÉ„ÇØ",
                ],
                "confidence": 0.7,
            },
            "Œµ-just": {
                "strategy_template": (
                    "{source} „ÅÆ‰∏ªÂºµ„Äå{target}„Äç„Å´Â≠¶Ë°ìÁöÑÊ†πÊã†„Çí‰ªò‰∏é„Åô„Çã„ÄÇ"
                    "Gn≈çsis Ê§úÁ¥¢„ÅßÈñ¢ÈÄ£Ë´ñÊñá„ÇíÊé¢Á¥¢„Åó„ÄÅÊ†πÊã†„ÇíÊòéÁ§∫„Åô„Çã„ÄÇ"
                ),
                "actions": [
                    "/sop „ÅßÈñ¢ÈÄ£Ë´ñÊñá„ÇíË™øÊüª",
                    "kernel/ „Éï„Ç°„Ç§„É´„Å´ references „Çª„ÇØ„Ç∑„Éß„É≥ËøΩÂä†",
                    "Gn≈çsis KB „Å´Ë´ñÊñá„ÇíÁôªÈå≤",
                ],
                "confidence": 0.5,
            },
            "ŒîŒµ/Œît": {
                "strategy_template": (
                    "ÊúÄËøë„ÅÆÂ§âÊõ¥„Å´„Çà„Çã {source} „Å® {target} „ÅÆ‰∏çÊï¥Âêà„ÇíËß£Ê∂à„Åô„Çã„ÄÇ"
                    "ÊÑèÂõ≥ÁöÑ„Å™Â§âÊõ¥„Åß„ÅÇ„Çå„Å∞„ÄÅÈñ¢ÈÄ£„Éï„Ç°„Ç§„É´„ÇíÈÄ£ÂãïÊõ¥Êñ∞„Åô„Çã„ÄÇ"
                ),
                "actions": [
                    "git log „ÅßÂ§âÊõ¥„ÅÆÊÑèÂõ≥„ÇíÁ¢∫Ë™ç",
                    "Èñ¢ÈÄ£„Åô„Çã kernel/ or mekhane/ „Éï„Ç°„Ç§„É´„ÇíÈÄ£ÂãïÊõ¥Êñ∞",
                    "/vet „ÅßÂ§âÊõ¥„ÅÆÊï¥ÂêàÊÄß„ÇíÂÜçÊ§úË®º",
                ],
                "confidence": 0.5,
            },
        }

    def resolve_heuristic(self, deficit: Deficit) -> Resolution:
        """Generate resolution using rule-based heuristics (no LLM)."""
        question = deficit.to_question()
        type_key = deficit.type.value
        heuristic = self._heuristics.get(type_key, {})

        strategy_template = heuristic.get(
            "strategy_template",
            "{source} „Å® {target} „ÅÆ„Ç∫„É¨„ÇíÊâãÂãï„ÅßËß£Ê∂à„Åô„Çã„ÄÇ",
        )
        strategy = strategy_template.format(
            source=deficit.source,
            target=deficit.target,
        )

        return Resolution(
            question=question,
            strategy=strategy,
            confidence=heuristic.get("confidence", 0.3),
            actions=heuristic.get("actions", []),
            references=self._find_references(deficit),
            status="proposed",
        )

    def _find_references(self, deficit: Deficit) -> list[str]:
        """Find relevant files for a deficit."""
        refs: list[str] = []

        # Check if source file exists
        source_path = self.project_root / deficit.source
        if source_path.exists():
            refs.append(str(source_path))

        # Check for related kernel files
        if deficit.target:
            for kernel_file in (self.project_root / "kernel").glob("*.md"):
                if deficit.target.lower() in kernel_file.stem.lower():
                    refs.append(str(kernel_file))

        return refs[:5]  # Limit references

    def resolve_batch(
        self,
        deficits: list[Deficit],
        max_resolutions: int = 10,
    ) -> list[Resolution]:
        """Resolve multiple deficits, prioritized by severity.

        Args:
            deficits: List of deficits to resolve
            max_resolutions: Maximum number of resolutions to generate

        Returns:
            List of Resolution objects, sorted by confidence (desc)
        """
        # Sort by severity (highest first)
        sorted_deficits = sorted(deficits, key=lambda d: d.severity, reverse=True)
        resolutions: list[Resolution] = []

        for deficit in sorted_deficits[:max_resolutions]:
            try:
                resolution = self.resolve_heuristic(deficit)
                resolutions.append(resolution)
            except Exception as exc:
                logger.warning("Failed to resolve deficit: %s ‚Äî %s", deficit.description, exc)

        # Sort by confidence (highest first)
        resolutions.sort(key=lambda r: r.confidence, reverse=True)
        return resolutions

    async def resolve_with_llm(
        self,
        deficit: Deficit,
        context: str = "",
    ) -> Resolution:
        """Resolve deficit using LLM via Cortex API (async).

        Falls back to heuristic if LLM is unavailable.
        """
        question = deficit.to_question()

        prompt = (
            f"Hegemonik√≥n „Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅÆÊßãÈÄ†ÁöÑÊ¨†Èô•„ÇíÂàÜÊûê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n"
            f"### Ê¨†Èô•„ÅÆÁ®ÆÈ°û: {deficit.type.value}\n"
            f"### Âïè„ÅÑ: {question.text}\n"
            f"### Ë™¨Êòé: {deficit.description}\n"
            f"### „ÇΩ„Éº„Çπ: {deficit.source}\n"
            f"### „Çø„Éº„Ç≤„ÉÉ„Éà: {deficit.target}\n"
            f"### ÈáçÂ§ßÂ∫¶: {deficit.severity:.1f}\n"
        )
        if context:
            prompt += f"\n### ËøΩÂä†„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà:\n{context}\n"
        prompt += (
            "\n‰ª•‰∏ã„ÅÆÂΩ¢Âºè„ÅßËß£Ê±∫Á≠ñ„ÇíÊèêÊ°à„Åó„Å¶„Åè„Å†„Åï„ÅÑ:\n"
            "1. **Êà¶Áï•**: 1-2Êñá„ÅßËß£Ê±∫„ÅÆÊñπÂêëÊÄß\n"
            "2. **„Ç¢„ÇØ„Ç∑„Éß„É≥**: ÂÖ∑‰ΩìÁöÑ„Å™ÊâãÈ†Ü„É™„Çπ„Éà (3-5È†ÖÁõÆ)\n"
            "3. **Á¢∫‰ø°Â∫¶**: 0.0-1.0\n"
        )

        try:
            from mekhane.ochema.cortex import CortexClient

            client = CortexClient()
            response = await client.generate(
                prompt,
                model="gemini-2.0-flash",
                max_tokens=1024,
            )

            # Parse structured response
            text = response.get("text", "") if isinstance(response, dict) else str(response)

            return Resolution(
                question=question,
                strategy=text[:500],
                confidence=0.7,
                actions=self._extract_actions(text),
                references=self._find_references(deficit),
                status="proposed",
            )
        except Exception as exc:
            logger.info("LLM resolution unavailable (%s), falling back to heuristic", exc)
            return self.resolve_heuristic(deficit)

    def _extract_actions(self, text: str) -> list[str]:
        """Extract action items from LLM response text."""
        actions: list[str] = []
        for line in text.split("\n"):
            line = line.strip()
            if line and (line.startswith("-") or line.startswith("‚Ä¢") or
                         (len(line) > 2 and line[0].isdigit() and line[1] in ".)")):
                actions.append(line.lstrip("-‚Ä¢0123456789.) "))
        return actions[:5]


def print_resolutions(resolutions: list[Resolution]) -> None:
    """Display resolutions in formatted output."""
    if not resolutions:
        print("\n‚úÖ Ëß£Ê±∫ÊèêÊ°à„Å™„ÅóÔºàdeficit „ÅåÊ§úÂá∫„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„ÇìÔºâ")
        return

    print(f"\n\033[1m‚îÅ‚îÅ‚îÅ Basanos L3: Ëá™ÂãïËß£Ê±∫ÊèêÊ°à ({len(resolutions)} ‰ª∂) ‚îÅ‚îÅ‚îÅ\033[0m\n")

    for i, r in enumerate(resolutions, 1):
        conf_icon = "üü¢" if r.confidence >= 0.7 else "üü°" if r.confidence >= 0.4 else "üî¥"
        print(f"  {conf_icon} \033[1mR{i}\033[0m [{r.question.deficit.type.value}] conf={r.confidence:.1f}")
        print(f"     Q: {r.question.text}")
        print(f"     ‚Üí {r.strategy}")
        if r.actions:
            print(f"     „Ç¢„ÇØ„Ç∑„Éß„É≥:")
            for a in r.actions:
                print(f"       ‚Ä¢ {a}")
        if r.references:
            print(f"     \033[2mÂèÇÁÖß: {', '.join(r.references[:3])}\033[0m")
        print()
