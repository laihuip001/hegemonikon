#!/usr/bin/env python3
# PROOF: [L2/Mekhane] <- mekhane/l2/ A0->Auto->AddedByCI
# PURPOSE: Basanos L3 è‡ªå‹•è§£æ±ºãƒ«ãƒ¼ãƒ— â€” deficitâ†’å•ã„â†’è§£æ±ºç­–ã®è‡ªå‹•ç”Ÿæˆ
# REASON: deficit ã‚’æ¤œå‡ºã™ã‚‹ã ã‘ã§ãªãã€è§£æ±ºã¸ã®é“ç­‹ã‚’è‡ªå‹•ææ¡ˆã™ã‚‹ãŸã‚
"""Auto-resolution loop for Basanos L3.

Takes detected deficits, generates questions via L2, and uses LLM
to propose resolution strategies. Supports both Cortex (Gemini) and
local analysis fallbacks.
"""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Optional

from mekhane.basanos.l2.models import Deficit, Question

logger = logging.getLogger(__name__)


@dataclass
class Resolution:
    """Proposed resolution for a deficit."""

    question: Question
    strategy: str  # proposed resolution strategy
    confidence: float  # 0.0 - 1.0
    actions: list[str] = field(default_factory=list)  # concrete action items
    references: list[str] = field(default_factory=list)  # relevant files/docs
    status: str = "proposed"  # proposed, accepted, rejected, applied

    def to_dict(self) -> dict[str, Any]:
        """Serialize to JSON-safe dict."""
        return {
            "question": self.question.text,
            "deficit_type": self.question.deficit.type.value,
            "strategy": self.strategy,
            "confidence": self.confidence,
            "actions": self.actions,
            "references": self.references,
            "status": self.status,
        }


class Resolver:
    """L3 auto-resolution engine.

    Generates resolution strategies for deficits using:
    1. Rule-based heuristics (fast, no LLM)
    2. LLM-assisted analysis (via Cortex/Ochema, if available)
    """

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self._heuristics = self._build_heuristics()

    def _build_heuristics(self) -> dict[str, Any]:
        """Build rule-based resolution heuristics per deficit type."""
        return {
            "Î·": {
                "strategy_template": (
                    "å¤–éƒ¨è«–æ–‡ã€Œ{source}ã€ã®æ¦‚å¿µã‚’ HGK ã«å¸åã™ã‚‹ã€‚"
                    "é–¢é€£ã™ã‚‹ Series ã‚’ç‰¹å®šã—ã€kernel/ ã«å®šç¾©ã‚’è¿½åŠ ã™ã‚‹ã‹ã€"
                    "æ—¢å­˜ã®å®šç†ã«ç´ä»˜ã‘ã‚‹ã€‚"
                ),
                "actions": [
                    "/eat ã§å¤–éƒ¨æ¦‚å¿µã‚’æ¶ˆåŒ–",
                    "kernel/ ã«æ–°å®šç¾©è¿½åŠ  or æ—¢å­˜å®šç†ã«æ³¨é‡ˆè¿½åŠ ",
                    "/dia ã§å¸åå¾Œã®æ•´åˆæ€§ã‚’æ¤œè¨¼",
                ],
                "confidence": 0.6,
            },
            "Îµ-impl": {
                "strategy_template": (
                    "kernel/{source} ã®å®šç† {target} ã«å¯¾å¿œã™ã‚‹å®Ÿè£…ã‚’ "
                    "mekhane/ ã«ä½œæˆã™ã‚‹ã€‚WF å®šç¾©ã‚‚é€£å‹•ã—ã¦è¿½åŠ ã€‚"
                ),
                "actions": [
                    "mekhane/ ã«å®Ÿè£…ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ",
                    "PROOF.md ã‚’åŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®",
                    ".agent/workflows/ ã«å¯¾å¿œ WF ä½œæˆ",
                    "/dendron ã§å­˜åœ¨è¨¼æ˜ãƒã‚§ãƒƒã‚¯",
                ],
                "confidence": 0.7,
            },
            "Îµ-just": {
                "strategy_template": (
                    "{source} ã®ä¸»å¼µã€Œ{target}ã€ã«å­¦è¡“çš„æ ¹æ‹ ã‚’ä»˜ä¸ã™ã‚‹ã€‚"
                    "GnÅsis æ¤œç´¢ã§é–¢é€£è«–æ–‡ã‚’æ¢ç´¢ã—ã€æ ¹æ‹ ã‚’æ˜ç¤ºã™ã‚‹ã€‚"
                ),
                "actions": [
                    "/sop ã§é–¢é€£è«–æ–‡ã‚’èª¿æŸ»",
                    "kernel/ ãƒ•ã‚¡ã‚¤ãƒ«ã« references ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¿½åŠ ",
                    "GnÅsis KB ã«è«–æ–‡ã‚’ç™»éŒ²",
                ],
                "confidence": 0.5,
            },
            "Î”Îµ/Î”t": {
                "strategy_template": (
                    "æœ€è¿‘ã®å¤‰æ›´ã«ã‚ˆã‚‹ {source} ã¨ {target} ã®ä¸æ•´åˆã‚’è§£æ¶ˆã™ã‚‹ã€‚"
                    "æ„å›³çš„ãªå¤‰æ›´ã§ã‚ã‚Œã°ã€é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é€£å‹•æ›´æ–°ã™ã‚‹ã€‚"
                ),
                "actions": [
                    "git log ã§å¤‰æ›´ã®æ„å›³ã‚’ç¢ºèª",
                    "é–¢é€£ã™ã‚‹ kernel/ or mekhane/ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é€£å‹•æ›´æ–°",
                    "/vet ã§å¤‰æ›´ã®æ•´åˆæ€§ã‚’å†æ¤œè¨¼",
                ],
                "confidence": 0.5,
            },
        }

    def resolve_heuristic(self, deficit: Deficit) -> Resolution:
        """Generate resolution using rule-based heuristics (no LLM)."""
        question = deficit.to_question()
        type_key = deficit.type.value
        heuristic = self._heuristics.get(type_key, {})

        strategy_template = heuristic.get(
            "strategy_template",
            "{source} ã¨ {target} ã®ã‚ºãƒ¬ã‚’æ‰‹å‹•ã§è§£æ¶ˆã™ã‚‹ã€‚",
        )
        strategy = strategy_template.format(
            source=deficit.source,
            target=deficit.target,
        )

        return Resolution(
            question=question,
            strategy=strategy,
            confidence=heuristic.get("confidence", 0.3),
            actions=heuristic.get("actions", []),
            references=self._find_references(deficit),
            status="proposed",
        )

    def _find_references(self, deficit: Deficit) -> list[str]:
        """Find relevant files for a deficit."""
        refs: list[str] = []

        # Check if source file exists
        source_path = self.project_root / deficit.source
        if source_path.exists():
            refs.append(str(source_path))

        # Check for related kernel files
        if deficit.target:
            for kernel_file in (self.project_root / "kernel").glob("*.md"):
                if deficit.target.lower() in kernel_file.stem.lower():
                    refs.append(str(kernel_file))

        return refs[:5]  # Limit references

    def resolve_batch(
        self,
        deficits: list[Deficit],
        max_resolutions: int = 10,
    ) -> list[Resolution]:
        """Resolve multiple deficits, prioritized by severity.

        Args:
            deficits: List of deficits to resolve
            max_resolutions: Maximum number of resolutions to generate

        Returns:
            List of Resolution objects, sorted by confidence (desc)
        """
        # Sort by severity (highest first)
        sorted_deficits = sorted(deficits, key=lambda d: d.severity, reverse=True)
        resolutions: list[Resolution] = []

        for deficit in sorted_deficits[:max_resolutions]:
            try:
                resolution = self.resolve_heuristic(deficit)
                resolutions.append(resolution)
            except Exception as exc:
                logger.warning("Failed to resolve deficit: %s â€” %s", deficit.description, exc)

        # Sort by confidence (highest first)
        resolutions.sort(key=lambda r: r.confidence, reverse=True)
        return resolutions

    async def resolve_with_llm(
        self,
        deficit: Deficit,
        context: str = "",
    ) -> Resolution:
        """Resolve deficit using LLM via Cortex API (async).

        Falls back to heuristic if LLM is unavailable.
        """
        question = deficit.to_question()

        prompt = (
            f"HegemonikÃ³n ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®æ§‹é€ çš„æ¬ é™¥ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚\n\n"
            f"### æ¬ é™¥ã®ç¨®é¡: {deficit.type.value}\n"
            f"### å•ã„: {question.text}\n"
            f"### èª¬æ˜: {deficit.description}\n"
            f"### ã‚½ãƒ¼ã‚¹: {deficit.source}\n"
            f"### ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {deficit.target}\n"
            f"### é‡å¤§åº¦: {deficit.severity:.1f}\n"
        )
        if context:
            prompt += f"\n### è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}\n"
        prompt += (
            "\nä»¥ä¸‹ã®å½¢å¼ã§è§£æ±ºç­–ã‚’ææ¡ˆã—ã¦ãã ã•ã„:\n"
            "1. **æˆ¦ç•¥**: 1-2æ–‡ã§è§£æ±ºã®æ–¹å‘æ€§\n"
            "2. **ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**: å…·ä½“çš„ãªæ‰‹é †ãƒªã‚¹ãƒˆ (3-5é …ç›®)\n"
            "3. **ç¢ºä¿¡åº¦**: 0.0-1.0\n"
        )

        try:
            from mekhane.ochema.cortex import CortexClient

            client = CortexClient()
            response = await client.generate(
                prompt,
                model="gemini-2.0-flash",
                max_tokens=1024,
            )

            # Parse structured response
            text = response.get("text", "") if isinstance(response, dict) else str(response)

            return Resolution(
                question=question,
                strategy=text[:500],
                confidence=0.7,
                actions=self._extract_actions(text),
                references=self._find_references(deficit),
                status="proposed",
            )
        except Exception as exc:
            logger.info("LLM resolution unavailable (%s), falling back to heuristic", exc)
            return self.resolve_heuristic(deficit)

    def _extract_actions(self, text: str) -> list[str]:
        """Extract action items from LLM response text."""
        actions: list[str] = []
        for line in text.split("\n"):
            line = line.strip()
            if line and (line.startswith("-") or line.startswith("â€¢") or
                         (len(line) > 2 and line[0].isdigit() and line[1] in ".)")):
                actions.append(line.lstrip("-â€¢0123456789.) "))
        return actions[:5]


def print_resolutions(resolutions: list[Resolution]) -> None:
    """Display resolutions in formatted output."""
    if not resolutions:
        print("\nâœ… è§£æ±ºææ¡ˆãªã—ï¼ˆdeficit ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰")
        return

    print(f"\n\033[1mâ”â”â” Basanos L3: è‡ªå‹•è§£æ±ºææ¡ˆ ({len(resolutions)} ä»¶) â”â”â”\033[0m\n")

    for i, r in enumerate(resolutions, 1):
        conf_icon = "ğŸŸ¢" if r.confidence >= 0.7 else "ğŸŸ¡" if r.confidence >= 0.4 else "ğŸ”´"
        print(f"  {conf_icon} \033[1mR{i}\033[0m [{r.question.deficit.type.value}] conf={r.confidence:.1f}")
        print(f"     Q: {r.question.text}")
        print(f"     â†’ {r.strategy}")
        if r.actions:
            print(f"     ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
            for a in r.actions:
                print(f"       â€¢ {a}")
        if r.references:
            print(f"     \033[2må‚ç…§: {', '.join(r.references[:3])}\033[0m")
        print()