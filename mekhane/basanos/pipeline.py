# PROOF: [L1/å®šç†] <- mekhane/basanos/ VISION.md Aâ†’Bâ†’C ã®å…·ä½“åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""
DailyReviewPipeline â€” Basanos L0 â†’ Synteleia L1 â†’ Jules L2 â†’ Feedback

VISION.md Layer A (Immunitas) â†’ Layer B (Nous) â†’ Layer C (Pronoia) ã®
å…·ä½“çš„å®Ÿè£…ã€‚è¨­è¨ˆåˆ¤æ–­ã¯ FEP Ï€(Îµ) ã®æ“ä½œåŒ–:
- L2 ç™ºå‹• = CRITICAL/HIGH (= Îµ ãŒé–¾å€¤è¶…é)
- é‡ã¿ = Ï€ ã®è“„ç©ã§è‡ªå‹•èª¿æ•´
- æ®µéšçš„å®Ÿè£… = ç´¯ç©åŸå‰‡
"""

import json
import logging
import os
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from mekhane.basanos.ai_auditor import AIAuditor, AuditResult as BasanosResult, Issue, Severity
from mekhane.synteleia.base import (
    AuditIssue,
    AuditResult as SynteleiaResult,
    AuditSeverity,
    AuditTarget,
    AuditTargetType,
)

logger = logging.getLogger(__name__)

# Default path
ROTATION_STATE_PATH = Path(__file__).parent.parent.parent / "synergeia" / "basanos_rotation_state.json"


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Adapter: Basanos Issue â†’ Synteleia AuditIssue
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Severity mapping
_SEVERITY_MAP = {
    Severity.CRITICAL: AuditSeverity.CRITICAL,
    Severity.HIGH: AuditSeverity.HIGH,
    Severity.MEDIUM: AuditSeverity.MEDIUM,
    Severity.LOW: AuditSeverity.LOW,
}


def basanos_issue_to_synteleia(issue: Issue, file_path: str = "") -> AuditIssue:
    """Basanos Issue â†’ Synteleia AuditIssue ã«å¤‰æ›ã€‚"""
    return AuditIssue(
        agent=f"basanos/{issue.code}",
        code=issue.code,
        severity=_SEVERITY_MAP.get(issue.severity, AuditSeverity.LOW),
        message=issue.message,
        location=f"{file_path}:{issue.line}" if file_path else f"L{issue.line}",
        suggestion=issue.suggestion,
    )


def basanos_to_synteleia_target(basanos_result: BasanosResult) -> AuditTarget:
    """Basanos AuditResult â†’ Synteleia AuditTarget ã«å¤‰æ›ã€‚"""
    content = basanos_result.file_path.read_text(encoding="utf-8")
    return AuditTarget(
        content=content,
        target_type=AuditTargetType.CODE,
        source=str(basanos_result.file_path),
        metadata={
            "basanos_issues": len(basanos_result.issues),
            "has_critical": basanos_result.has_critical,
            "has_high": basanos_result.has_high,
        },
    )


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Domain Weights â€” Ï€(Îµ) ã®è“„ç©
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@dataclass
class DomainWeight:
    """ãƒ‰ãƒ¡ã‚¤ãƒ³ã®é‡ã¿ â€” éå»ã®å•é¡Œé »åº¦ã«åŸºã¥ãç²¾åº¦åŠ é‡ã€‚"""
    name: str
    weight: float = 1.0  # 1.0 = æ¨™æº–ã€>1 = è¦æ³¨æ„ã€<1 = å®‰å®š
    last_issues: int = 0  # å‰å›ã® issue æ•°
    total_issues: int = 0  # ç´¯è¨ˆ issue æ•°
    last_reviewed: Optional[str] = None


@dataclass
class RotationState:
    """basanos_rotation_state.json ã®æ‹¡å¼µç‰ˆã€‚"""
    domains: Dict[str, DomainWeight] = field(default_factory=dict)
    cycle: int = 0
    last_date: str = ""

    @classmethod
    def load(cls, path: Path = ROTATION_STATE_PATH) -> "RotationState":
        """JSON ã‹ã‚‰èª­è¾¼ã€‚æ—§å½¢å¼ã«ã‚‚å¯¾å¿œã€‚"""
        if not path.exists():
            return cls()

        with open(path) as f:
            data = json.load(f)

        state = cls(cycle=data.get("cycle", 0), last_date=data.get("last_date", ""))

        # Handle both old format (last_domains list) and new format (domains dict)
        if "domains" in data and isinstance(data["domains"], dict):
            # New weighted format
            for name, info in data["domains"].items():
                state.domains[name] = DomainWeight(
                    name=name,
                    weight=info.get("weight", 1.0),
                    last_issues=info.get("last_issues", 0),
                    total_issues=info.get("total_issues", 0),
                    last_reviewed=info.get("last_reviewed"),
                )
        elif "last_domains" in data:
            # Old format â€” migrate
            for name in data["last_domains"]:
                state.domains[name] = DomainWeight(name=name)

        return state

    def save(self, path: Path = ROTATION_STATE_PATH) -> None:
        """JSON ã«ä¿å­˜ã€‚"""
        data = {
            "domains": {
                name: {
                    "weight": dw.weight,
                    "last_issues": dw.last_issues,
                    "total_issues": dw.total_issues,
                    "last_reviewed": dw.last_reviewed,
                }
                for name, dw in self.domains.items()
            },
            "cycle": self.cycle,
            "last_date": self.last_date,
            # Backward compat â€” keep last_domains list
            "last_domains": list(self.domains.keys()),
        }
        with open(path, "w") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def select_domains(self, n: int = 3) -> List[str]:
        """é‡ã¿ä»˜ãã§ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é¸æŠã€‚é‡ã¿ãŒé«˜ã„ = Îµ ãŒå¤§ãã„ = å„ªå…ˆã€‚"""
        if not self.domains:
            return []
        sorted_domains = sorted(
            self.domains.values(),
            key=lambda d: d.weight,
            reverse=True,
        )
        return [d.name for d in sorted_domains[:n]]

    def update_weights(self, domain: str, issue_count: int, decay: float = 0.9) -> None:
        """Ï€(Îµ) æ›´æ–°: å•é¡ŒãŒå¤šã„ â†’ é‡ã¿ä¸Šæ˜‡ã€å°‘ãªã„ â†’ æ¸›è¡°ã€‚"""
        if domain not in self.domains:
            self.domains[domain] = DomainWeight(name=domain)

        dw = self.domains[domain]
        dw.last_issues = issue_count
        dw.total_issues += issue_count
        dw.last_reviewed = datetime.now().strftime("%Y-%m-%d")

        # Weight update: exponential moving average
        # More issues â†’ higher weight (needs more attention)
        if issue_count > 0:
            dw.weight = min(3.0, dw.weight + 0.1 * issue_count)
        else:
            dw.weight = max(0.3, dw.weight * decay)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Pipeline Result
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@dataclass
class PipelineResult:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®çµæœã€‚"""
    files_scanned: int = 0
    l0_issues: List[Dict[str, Any]] = field(default_factory=list)  # Basanos findings
    l1_results: List[Dict[str, Any]] = field(default_factory=list)  # Synteleia findings
    l2_triggered: bool = False  # Jules deep review triggered?
    l2_session_id: Optional[str] = None  # Jules session ID if triggered
    domains_reviewed: List[str] = field(default_factory=list)

    @property
    def needs_l2(self) -> bool:
        """L2 ç™ºå‹•æ¡ä»¶: CRITICAL or HIGH ãŒå­˜åœ¨ = Îµ ãŒé–¾å€¤è¶…éã€‚"""
        return any(
            i.get("severity") in ("critical", "high")
            for i in self.l0_issues
        )

    def to_jules_prompt(self, max_issues: int = 10, context_lines: int = 5) -> str:
        """Jules ã«æ¸¡ã™æ·±æ˜ã‚Šãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã€‚

        Args:
            max_issues: å«ã‚ã‚‹æœ€å¤§ issue æ•°
            context_lines: å•é¡Œç®‡æ‰€ã®å‰å¾Œã«å«ã‚ã‚‹è¡Œæ•°
        """
        critical_high = [
            i for i in self.l0_issues
            if i.get("severity") in ("critical", "high")
        ][:max_issues]

        prompt_parts = [
            "## Deep Review Request (è‡ªå‹•ç”Ÿæˆ)",
            "",
            "Basanos L0 + Synteleia L1 ã®åˆ†æã§ä»¥ä¸‹ã®é‡è¦ãªå•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚",
            "å„å•é¡Œã«ã¤ã„ã¦æ ¹æœ¬åŸå› ã‚’åˆ†æã—ã€ä¿®æ­£æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚",
            "",
            "### æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ",
            "",
        ]

        for i, issue in enumerate(critical_high, 1):
            location = issue.get("location", "?")
            prompt_parts.append(
                f"{i}. **[{issue.get('severity', '?').upper()}] {issue.get('code', '?')}** "
                f"at `{location}`"
            )
            prompt_parts.append(f"   {issue.get('message', '')}")
            if issue.get("suggestion"):
                prompt_parts.append(f"   â†’ ææ¡ˆ: {issue['suggestion']}")

            # Attach source code context
            snippet = self._extract_snippet(location, context_lines)
            if snippet:
                prompt_parts.append("")
                prompt_parts.append(f"   ```python")
                prompt_parts.append(snippet)
                prompt_parts.append(f"   ```")

            prompt_parts.append("")

        return "\n".join(prompt_parts)

    @staticmethod
    def _extract_snippet(location: str, context_lines: int = 5) -> str:
        """location æ–‡å­—åˆ— (e.g. "path/to/file.py:42") ã‹ã‚‰ã‚³ãƒ¼ãƒ‰æ–­ç‰‡ã‚’æŠ½å‡ºã€‚"""
        try:
            if ":" not in location:
                return ""
            parts = location.rsplit(":", 1)
            file_path = Path(parts[0])
            line_no = int(parts[1])

            if not file_path.exists() or file_path.stat().st_size > 500_000:
                return ""

            lines = file_path.read_text("utf-8").splitlines()
            start = max(0, line_no - context_lines - 1)
            end = min(len(lines), line_no + context_lines)

            numbered = []
            for idx in range(start, end):
                marker = ">>>" if idx == line_no - 1 else "   "
                numbered.append(f"   {marker} {idx + 1:4d} | {lines[idx]}")

            return "\n".join(numbered)
        except Exception:
            return ""


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# DailyReviewPipeline â€” Main Orchestrator
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

class DailyReviewPipeline:
    """
    Daily Review Pipeline â€” VISION.md Aâ†’Bâ†’C ã®å…·ä½“åŒ–ã€‚

    L0 (Basanos): AST-based static analysis
    L1 (Synteleia): Cognitive auditing (pattern-based agents)
    L2 (Jules): Deep LLM review (triggered by CRITICAL/HIGH)
    FB: Feedback to rotation weights
    """

    def __init__(
        self,
        project_root: Optional[Path] = None,
        rotation_state_path: Path = ROTATION_STATE_PATH,
        enable_l2: bool = True,
    ):
        self.project_root = project_root or Path.cwd()
        self.rotation_state = RotationState.load(rotation_state_path)
        self.rotation_state_path = rotation_state_path
        self.enable_l2 = enable_l2
        self.auditor = AIAuditor(strict=False)  # CRITICAL/HIGH only
        self._synteleia = None

    @property
    def synteleia(self):
        """Lazy-load Synteleia orchestrator."""
        if self._synteleia is None:
            from mekhane.synteleia.orchestrator import SynteleiaOrchestrator
            self._synteleia = SynteleiaOrchestrator()
        return self._synteleia

    def run(
        self,
        files: Optional[List[Path]] = None,
        domains: Optional[List[str]] = None,
        dry_run: bool = False,
    ) -> PipelineResult:
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã€‚

        Args:
            files: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ (None = git diff ã‹ã‚‰è‡ªå‹•æ¤œå‡º)
            domains: ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‰ãƒ¡ã‚¤ãƒ³ (None = é‡ã¿ä»˜ãè‡ªå‹•é¸æŠ)
            dry_run: True = L2 ç™ºå‹•ã›ãšçµæœã®ã¿è¿”ã™
        """
        result = PipelineResult()

        # Resolve domains
        if domains is None:
            domains = self.rotation_state.select_domains(n=3)
        result.domains_reviewed = domains

        # Resolve files
        if files is None:
            files = self._discover_changed_files()

        logger.info(f"Pipeline: {len(files)} files, domains={domains}")

        # â”€â”€ L0: Basanos static analysis â”€â”€
        for file_path in files:
            if not file_path.suffix == ".py":
                continue
            try:
                basanos_result = self.auditor.audit_file(file_path)
                result.files_scanned += 1

                for issue in basanos_result.issues:
                    result.l0_issues.append({
                        "code": issue.code,
                        "name": issue.name,
                        "severity": issue.severity.value,
                        "line": issue.line,
                        "message": issue.message,
                        "suggestion": issue.suggestion,
                        "file": str(file_path),
                        "location": f"{file_path}:{issue.line}",
                    })

            except Exception as e:
                logger.warning(f"L0 audit failed for {file_path}: {e}")

        # â”€â”€ L1: Synteleia cognitive audit (for files with L0 issues) â”€â”€
        files_with_issues = {
            i["file"] for i in result.l0_issues
            if i["severity"] in ("critical", "high")
        }
        for file_str in files_with_issues:
            try:
                file_path = Path(file_str)
                basanos_result = self.auditor.audit_file(file_path)
                target = basanos_to_synteleia_target(basanos_result)
                synteleia_result = self.synteleia.audit(target)

                result.l1_results.append({
                    "file": file_str,
                    "passed": synteleia_result.passed,
                    "issues": len(synteleia_result.all_issues),
                    "summary": synteleia_result.summary,
                })

            except Exception as e:
                logger.warning(f"L1 audit failed for {file_str}: {e}")

        # â”€â”€ L2: Jules deep review (if CRITICAL/HIGH detected) â”€â”€
        if result.needs_l2 and self.enable_l2 and not dry_run:
            result.l2_triggered = True
            result.l2_session_id = self._trigger_jules_review(result)
            # Register session for feedback tracking
            if result.l2_session_id:
                self._register_jules_session(result)

        # â”€â”€ Auto-expand: æ–°ãƒ‰ãƒ¡ã‚¤ãƒ³è‡ªå‹•æ¤œå‡º â”€â”€
        self._auto_expand_domains(result)

        # â”€â”€ FB: Feedback loop â€” update weights â”€â”€
        self._update_feedback(result, domains)

        # â”€â”€ Notify: Sympatheia é€šçŸ¥ â”€â”€
        self._notify_result(result)

        # â”€â”€ Persist: ãƒ¬ãƒãƒ¼ãƒˆæ°¸ç¶šåŒ– â”€â”€
        if not dry_run:
            self._persist_report(result)

        # â”€â”€ Trend: è“„ç©ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å­¦ç¿’ (G7/G8) â”€â”€
        if not dry_run:
            self._apply_trends()

        # â”€â”€ Git: churn äºˆå…†æ¤œçŸ¥ â”€â”€
        if not dry_run:
            self._git_risk_check(result)

        # â”€â”€ Jules Feedback: L2çµæœâ†’L0ç²¾åº¦èª¿æ•´ â”€â”€
        if not dry_run:
            self._collect_jules_feedback()

        return result

    def _discover_changed_files(self) -> List[Path]:
        """git diff ã‹ã‚‰å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã€‚"""
        import subprocess
        try:
            output = subprocess.run(
                ["git", "diff", "--name-only", "HEAD~1"],
                capture_output=True, text=True,
                cwd=self.project_root,
            )
            if output.returncode == 0:
                return [
                    self.project_root / f.strip()
                    for f in output.stdout.strip().split("\n")
                    if f.strip() and f.strip().endswith(".py")
                ]
        except Exception as e:
            logger.warning(f"git diff failed: {e}")
        return []

    def _auto_expand_domains(self, result: PipelineResult) -> None:
        """L0 çµæœã‹ã‚‰æœªç™»éŒ²ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’è‡ªå‹•è¿½åŠ ã€‚"""
        # AI-XXX code â†’ name mapping for domain categorization
        CATEGORY_MAP = {
            "Naming": "Naming",
            "API": "API",
            "Type": "Types",
            "Logic": "Logic",
            "Incomplete": "Completeness",
            "Context": "Context",
            "Pattern": "Patterns",
            "Contradiction": "Logic",
            "Security": "Security",
            "Input": "Validation",
            "Boundary": "Boundary",
            "Async": "Async",
            "Concurrency": "Concurrency",
            "Comment": "Documentation",
            "Copy": "DRY",
            "Dead": "DeadCode",
            "Magic": "Magic",
            "Hardcoded": "Config",
        }
        for issue in result.l0_issues:
            name = issue.get("name", "")
            # Extract first word as category key
            key = name.split()[0] if name else ""
            domain = CATEGORY_MAP.get(key, "")
            if domain and domain not in self.rotation_state.domains:
                self.rotation_state.domains[domain] = DomainWeight(name=domain, weight=0.8)
                logger.info(f"Auto-expanded domain: {domain} (from {name})")

    def _trigger_jules_review(self, result: PipelineResult) -> Optional[str]:
        """Jules API çµŒç”±ã§æ·±æ˜ã‚Šãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç™ºå‹•ã€‚"""
        try:
            import asyncio
            from mekhane.symploke.jules_client import JulesClient

            # API key from environment
            api_key = None
            for i in range(1, 10):
                key = os.environ.get(f"JULES_API_KEY_{i:02d}")
                if key:
                    api_key = key
                    break

            if not api_key:
                logger.warning("L2: No JULES_API_KEY_XX found, skipping")
                return None

            repo = os.environ.get("JULES_REPO", "laihuip001/oikos")
            prompt = result.to_jules_prompt()
            logger.info(f"L2 triggered: {len(result.l0_issues)} issues â†’ Jules ({repo})")

            async def _create():
                client = JulesClient(api_key)
                source = f"sources/github/{repo}"
                session = await client.create_session(prompt, source, "main")
                return session.id

            session_id = asyncio.run(_create())
            logger.info(f"L2 session created: {session_id}")
            return session_id

        except ImportError:
            logger.warning("L2: JulesClient not available")
            return None
        except Exception as e:
            logger.warning(f"Jules trigger failed: {e}")
            return None

    def _apply_trends(self) -> None:
        """G7/G8: è“„ç©ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å­¦ç¿’ã‚’é©ç”¨ã€‚"""
        try:
            from mekhane.basanos.trend_analyzer import TrendAnalyzer

            analyzer = TrendAnalyzer(days=14)
            changes = analyzer.apply_to_rotation(self.rotation_state)

            if changes.get("weight_adjustments"):
                self.rotation_state.save(self.rotation_state_path)
                logger.info(f"Trend learning: {len(changes['weight_adjustments'])} adjustments")
            else:
                logger.debug("Trend learning: no adjustments needed")

        except Exception as e:
            logger.warning(f"Trend analysis failed (non-fatal): {e}")

    def _register_jules_session(self, result: PipelineResult) -> None:
        """Jules ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ feedback è¿½è·¡ã«ç™»éŒ²ã€‚"""
        try:
            from mekhane.basanos.jules_feedback import JulesFeedback

            fb = JulesFeedback()
            critical_issues = [
                i for i in result.l0_issues
                if i.get("severity") in ("critical", "high")
            ]
            fb.register_session(result.l2_session_id, critical_issues)
        except Exception as e:
            logger.debug(f"Jules session registration skipped: {e}")

    def _collect_jules_feedback(self) -> None:
        """å‰å›ã® Jules ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’å›åã— L0 ç²¾åº¦ã‚’èª¿æ•´ã€‚"""
        try:
            from mekhane.basanos.jules_feedback import JulesFeedback

            fb = JulesFeedback()
            completed = fb.collect_completed()

            if completed:
                changes = fb.apply_to_rotation(self.rotation_state)
                if changes.get("adjustments_applied"):
                    self.rotation_state.save(self.rotation_state_path)
                    logger.info(f"Jules feedback: {len(changes['adjustments_applied'])} adjustments")
        except Exception as e:
            logger.debug(f"Jules feedback collection skipped: {e}")

    def _git_risk_check(self, result: PipelineResult) -> None:
        """Git churn + TrendAnalyzer ã®äº¤å·®ã§äºˆå…†æ¤œçŸ¥ã€‚"""
        try:
            from mekhane.basanos.git_metrics import GitMetrics

            gm = GitMetrics(repo_root=self.project_root, days=14)
            risky = gm.risky_files(top_n=5)

            if risky:
                # TrendAnalyzer ã® hot files ã¨äº¤å·®ã•ã›ã‚‹
                try:
                    from mekhane.basanos.trend_analyzer import TrendAnalyzer
                    ta = TrendAnalyzer(days=14)
                    hot_paths = [fp.path for fp in ta.hot_files(top_n=10)]
                    overlaps = gm.hotspot_overlaps(hot_paths)
                except Exception:
                    overlaps = []

                git_summary = gm.summary()
                result.l0_issues.append({
                    "file": "",
                    "name": "Git Risk Alert",
                    "code": "GIT-001",
                    "severity": "info",
                    "location": "",
                    "description": git_summary,
                    "risky_files": [{"path": fc.path, "risk": round(fc.risk_score, 1)} for fc in risky[:3]],
                    "hotspot_overlaps": overlaps,
                })
                if overlaps:
                    logger.warning(f"âš ï¸ Hotspot overlap: {overlaps}")

        except Exception as e:
            logger.debug(f"Git risk check skipped: {e}")

    def _update_feedback(self, result: PipelineResult, domains: List[str]) -> None:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—: é‡ã¿ã‚’æ›´æ–°ã—ã¦ä¿å­˜ã€‚"""
        # Count issues per domain (approximate mapping)
        issue_count = len(result.l0_issues)

        for domain in domains:
            # Simple heuristic: distribute issues across reviewed domains
            domain_issues = issue_count // max(len(domains), 1)
            self.rotation_state.update_weights(domain, domain_issues)

        # Update cycle
        self.rotation_state.cycle += 1
        self.rotation_state.last_date = datetime.now().strftime("%Y-%m-%d")

        # Save
        self.rotation_state.save(self.rotation_state_path)

    def _notify_result(self, result: PipelineResult) -> None:
        """Sympatheia notifications.jsonl ã«çµæœã‚’é€ä¿¡ã€‚"""
        try:
            import asyncio
            from mekhane.api.routes.sympatheia import _send_notification
            level = "HIGH" if result.needs_l2 else "INFO"
            icon = "ğŸš¨" if result.needs_l2 else "ğŸ“‹"

            async def run_notify():
                await _send_notification(
                    source="DailyReview",
                    level=level,
                    title=f"{icon} Daily Review: {result.files_scanned} files, {len(result.l0_issues)} issues",
                    body=self.summary(result),
                    data={
                        "files_scanned": result.files_scanned,
                        "l0_issues": len(result.l0_issues),
                        "l2_triggered": result.l2_triggered,
                        "domains": result.domains_reviewed,
                    },
                )

            try:
                loop = asyncio.get_running_loop()
                # Keep a reference to the task to prevent it from being garbage collected
                task = loop.create_task(run_notify())
                if not hasattr(self, "_bg_tasks"):
                    self._bg_tasks = set()
                self._bg_tasks.add(task)
                task.add_done_callback(self._bg_tasks.discard)
            except RuntimeError:
                asyncio.run(run_notify())

        except Exception as e:
            logger.warning(f"Sympatheia notification failed: {e}")

    def _persist_report(self, result: PipelineResult) -> Optional[Path]:
        """daily_reviews/YYYY-MM-DD.json ã«çµæœã‚’ä¿å­˜ã€‚"""
        try:
            report_dir = Path.home() / "oikos/mneme/.hegemonikon/daily_reviews"
            report_dir.mkdir(parents=True, exist_ok=True)
            today = datetime.now().strftime("%Y-%m-%d")
            report_file = report_dir / f"{today}.json"

            report = {
                "timestamp": datetime.now().isoformat(),
                "files_scanned": result.files_scanned,
                "l0_issues": result.l0_issues,
                "l1_results": result.l1_results,
                "l2_triggered": result.l2_triggered,
                "l2_session_id": result.l2_session_id,
                "domains_reviewed": result.domains_reviewed,
                "needs_l2": result.needs_l2,
                "summary": self.summary(result),
            }

            # Append mode: same day runs get merged
            if report_file.exists():
                existing = json.loads(report_file.read_text("utf-8"))
                if isinstance(existing, list):
                    existing.append(report)
                else:
                    existing = [existing, report]
                report_file.write_text(
                    json.dumps(existing, ensure_ascii=False, indent=2), "utf-8"
                )
            else:
                report_file.write_text(
                    json.dumps(report, ensure_ascii=False, indent=2), "utf-8"
                )

            logger.info(f"Report saved: {report_file}")
            return report_file
        except Exception as e:
            logger.warning(f"Report persist failed: {e}")
            return None


    def summary(self, result: PipelineResult) -> str:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆã€‚"""
        lines = [
            "â”â”â” Daily Review Pipeline â”â”â”",
            f"ğŸ“ Files scanned: {result.files_scanned}",
            f"ğŸ” L0 (Basanos): {len(result.l0_issues)} issues",
            f"ğŸ§  L1 (Synteleia): {len(result.l1_results)} files reviewed",
            f"ğŸ¤– L2 (Jules): {'triggered' if result.l2_triggered else 'not needed'}",
            f"ğŸ“Š Domains: {', '.join(result.domains_reviewed)}",
        ]

        # Issue breakdown
        severity_counts = {}
        for issue in result.l0_issues:
            sev = issue["severity"]
            severity_counts[sev] = severity_counts.get(sev, 0) + 1

        if severity_counts:
            lines.append("â”Œâ”€ Issue Breakdown â”€â”")
            for sev in ["critical", "high", "medium", "low"]:
                if sev in severity_counts:
                    icon = {"critical": "ğŸ”´", "high": "ğŸŸ ", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}[sev]
                    lines.append(f"â”‚ {icon} {sev}: {severity_counts[sev]}")
            lines.append("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

        return "\n".join(lines)
