# PROOF: [L1/å®šç†] <- mekhane/basanos/ VISION.md G7/G8 ã®å…·ä½“åŒ–
"""
TrendAnalyzer â€” daily_reviews/ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ã€Œè¨˜æ†¶ã™ã‚‹å…ç–«ã€ã€‚

G7: ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥é‡ã¿ãƒãƒˆãƒªã‚¯ã‚¹ (file_heat â†’ RotationState)
G8: FEP Ï€(Îµ) å‹•çš„é–¾å€¤ (category_velocity â†’ threshold èª¿æ•´)

è¨­è¨ˆåŸå‰‡:
- ç©ºãƒ‡ãƒ¼ã‚¿ã§ã‚‚ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãªã„ (graceful degradation)
- åˆæˆãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆå¯èƒ½ (fixture first)
- æ¼¸é€²çš„å­¦ç¿’ (1æ—¥åˆ†ã§ã‚‚æœ‰ç”¨ã€è“„ç©ã§ç²¾åº¦å‘ä¸Š)
"""

import json
import logging
import math
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)

DAILY_REVIEWS_DIR = Path.home() / "oikos/mneme/.hegemonikon/daily_reviews"


# PURPOSE: ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®issueå±¥æ­´ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã€‚
@dataclass
class FileProfile:
    """ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®issueå±¥æ­´ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã€‚"""

    path: str
    total_issues: int = 0
    issue_types: Dict[str, int] = field(default_factory=dict)
    first_seen: str = ""
    last_seen: str = ""
    streak: int = 0  # consecutive days with issues
    days_active: int = 0  # total days with issues

    # PURPOSE: ãƒ’ãƒ¼ãƒˆã‚¹ã‚³ã‚¢: issues/day Ã— streak Ã— recency_decayã€‚
    @property
    def heat(self) -> float:
        """ãƒ’ãƒ¼ãƒˆã‚¹ã‚³ã‚¢: issues/day Ã— streak Ã— recency_decayã€‚

        FEP è§£é‡ˆ: äºˆæ¸¬èª¤å·®ãŒç¹°ã‚Šè¿”ã—ç™ºç”Ÿã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¯
        ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°ãŒå¿…è¦ = é‡ç‚¹ç›£è¦–å¯¾è±¡ã€‚
        """
        if self.days_active == 0:
            return 0.0

        issues_per_day = self.total_issues / max(self.days_active, 1)
        streak_factor = math.log2(self.streak + 1) + 1  # 1.0 â†’ 1.0, 3 â†’ 2.0, 7 â†’ 3.0
        recency = self._recency_decay()

        return issues_per_day * streak_factor * recency

    def _recency_decay(self, half_life_days: int = 7) -> float:
        """æœ€çµ‚æ¤œå‡ºæ—¥ã‹ã‚‰ã®æŒ‡æ•°æ¸›è¡°ã€‚half_life_days ã§åŠæ¸›ã€‚"""
        if not self.last_seen:
            return 0.0
        try:
            last = datetime.strptime(self.last_seen, "%Y-%m-%d")
            days_ago = (datetime.now() - last).days
            return math.exp(-0.693 * days_ago / half_life_days)  # ln(2) â‰ˆ 0.693
        except ValueError:
            return 0.5


# PURPOSE: daily_reviews/ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æã™ã‚‹ã€‚
class TrendAnalyzer:
    """daily_reviews/ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æã™ã‚‹ã€‚

    Usage:
        analyzer = TrendAnalyzer()
        profiles = analyzer.file_profiles()
        hot = analyzer.hot_files(top_n=5)
        thresholds = analyzer.suggest_thresholds()
    """

    def __init__(
        self,
        reviews_dir: Path = DAILY_REVIEWS_DIR,
        days: int = 14,
    ):
        self.reviews_dir = reviews_dir
        self.days = days
        self._reports: Optional[List[dict]] = None
        self._dates: Optional[List[str]] = None

    # PURPOSE: daily_reviews/ ã‹ã‚‰éå»Næ—¥åˆ†ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚€ã€‚
    def load_reports(self) -> List[dict]:
        """daily_reviews/ ã‹ã‚‰éå»Næ—¥åˆ†ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚€ã€‚"""
        if self._reports is not None:
            return self._reports

        reports = []
        if not self.reviews_dir.exists():
            self._reports = reports
            self._dates = []
            return reports

        cutoff = datetime.now() - timedelta(days=self.days)
        dates = []

        for json_file in sorted(self.reviews_dir.glob("*.json")):
            try:
                date_str = json_file.stem  # YYYY-MM-DD
                file_date = datetime.strptime(date_str, "%Y-%m-%d")
                if file_date < cutoff:
                    continue

                data = json.loads(json_file.read_text("utf-8"))
                # Normalize: single report â†’ list
                if isinstance(data, dict):
                    data = [data]

                for report in data:
                    report["_date"] = date_str
                    reports.append(report)

                dates.append(date_str)
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"Skipping {json_file}: {e}")

        self._reports = reports
        self._dates = sorted(set(dates))
        logger.info(f"Loaded {len(reports)} reports from {len(self._dates)} days")
        return reports

    # PURPOSE: ãƒ¬ãƒãƒ¼ãƒˆãŒå­˜åœ¨ã™ã‚‹æ—¥ä»˜ã®ãƒªã‚¹ãƒˆã€‚
    @property
    def dates(self) -> List[str]:
        """ãƒ¬ãƒãƒ¼ãƒˆãŒå­˜åœ¨ã™ã‚‹æ—¥ä»˜ã®ãƒªã‚¹ãƒˆã€‚"""
        if self._dates is None:
            self.load_reports()
        return self._dates or []

    # PURPOSE: ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é›†è¨ˆã€‚
    def file_profiles(self) -> Dict[str, FileProfile]:
        """ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é›†è¨ˆã€‚"""
        reports = self.load_reports()
        profiles: Dict[str, FileProfile] = {}
        # Track per-date file presence for streak calculation
        file_dates: Dict[str, set] = {}

        for report in reports:
            date = report.get("_date", "")
            for issue in report.get("l0_issues", []):
                file_path = issue.get("file", "")
                if not file_path:
                    continue

                if file_path not in profiles:
                    profiles[file_path] = FileProfile(
                        path=file_path,
                        first_seen=date,
                    )
                    file_dates[file_path] = set()

                p = profiles[file_path]
                p.total_issues += 1
                p.last_seen = max(p.last_seen, date) if p.last_seen else date

                # Category counting
                name = issue.get("name", "Unknown")
                category = name.split()[0] if name else "Unknown"
                p.issue_types[category] = p.issue_types.get(category, 0) + 1

                file_dates[file_path].add(date)

        # Calculate days_active and streak
        all_dates = self.dates
        for file_path, p in profiles.items():
            p.days_active = len(file_dates.get(file_path, set()))
            p.streak = self._calculate_streak(
                file_dates.get(file_path, set()), all_dates
            )

        return profiles

    def _calculate_streak(self, file_dates: set, all_dates: List[str]) -> int:
        """æœ€æ–°æ—¥ã‹ã‚‰ã®é€£ç¶šæ¤œå‡ºæ—¥æ•°ã‚’è¨ˆç®—ã€‚"""
        if not file_dates or not all_dates:
            return 0

        streak = 0
        for date in reversed(all_dates):
            if date in file_dates:
                streak += 1
            else:
                break

        return streak

    # PURPOSE: ãƒ’ãƒ¼ãƒˆã‚¹ã‚³ã‚¢ä¸Šä½ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™ã€‚
    def hot_files(self, top_n: int = 10) -> List[FileProfile]:
        """ãƒ’ãƒ¼ãƒˆã‚¹ã‚³ã‚¢ä¸Šä½ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™ã€‚"""
        profiles = self.file_profiles()
        ranked = sorted(profiles.values(), key=lambda p: p.heat, reverse=True)
        return ranked[:top_n]

    # PURPOSE: ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æ—¥æ¬¡issueæ•°æ¨ç§»ã€‚
    def category_trends(self) -> Dict[str, List[int]]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æ—¥æ¬¡issueæ•°æ¨ç§»ã€‚

        Returns:
            {category: [day1_count, day2_count, ...]}
        """
        reports = self.load_reports()
        all_dates = self.dates

        # Initialize
        daily: Dict[str, Dict[str, int]] = {}  # {category: {date: count}}

        for report in reports:
            date = report.get("_date", "")
            for issue in report.get("l0_issues", []):
                name = issue.get("name", "Unknown")
                category = name.split()[0] if name else "Unknown"
                if category not in daily:
                    daily[category] = {}
                daily[category][date] = daily[category].get(date, 0) + 1

        # Convert to ordered lists
        result: Dict[str, List[int]] = {}
        for category, date_counts in daily.items():
            result[category] = [date_counts.get(d, 0) for d in all_dates]

        return result

    # PURPOSE: ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®issueå¢—æ¸›é€Ÿåº¦ (issues/day)ã€‚
    def category_velocity(self) -> Dict[str, float]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®issueå¢—æ¸›é€Ÿåº¦ (issues/day)ã€‚

        æ­£ = å¢—åŠ å‚¾å‘ã€è²  = æ¸›å°‘å‚¾å‘ã€‚
        ç·šå½¢å›å¸°ã®å‚¾ãã§ç®—å‡ºã€‚
        """
        trends = self.category_trends()
        velocities: Dict[str, float] = {}

        for category, counts in trends.items():
            if len(counts) < 2:
                velocities[category] = 0.0
                continue

            # Simple linear regression slope
            n = len(counts)
            x_mean = (n - 1) / 2.0
            y_mean = sum(counts) / n

            numerator = sum((i - x_mean) * (c - y_mean) for i, c in enumerate(counts))
            denominator = sum((i - x_mean) ** 2 for i in range(n))

            if denominator == 0:
                velocities[category] = 0.0
            else:
                velocities[category] = round(numerator / denominator, 4)

        return velocities

    # PURPOSE: G8: ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æ¨å¥¨é–¾å€¤ã‚’ç®—å‡ºã€‚
    def suggest_thresholds(self) -> Dict[str, float]:
        """G8: ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æ¨å¥¨é–¾å€¤ã‚’ç®—å‡ºã€‚

        FEP Ï€(Îµ): ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ã®ã‚«ãƒ†ã‚´ãƒªã¯ç²¾åº¦ Ï€ ã‚’ä¸Šã’ã‚‹ (= é–¾å€¤ã‚’ä¸‹ã’ã‚‹)ã€‚
        """
        velocity = self.category_velocity()
        thresholds: Dict[str, float] = {}

        for category, v in velocity.items():
            # Base threshold: 1.0
            # Rising velocity â†’ lower threshold (more sensitive)
            # Falling velocity â†’ higher threshold (less sensitive)
            adjustment = -0.1 * v  # velocity 1.0/day â†’ threshold -0.1
            threshold = max(0.3, min(1.5, 1.0 + adjustment))
            thresholds[category] = round(threshold, 3)

        return thresholds

    # PURPOSE: G7: åˆ†æçµæœã‚’ RotationState ã«åæ˜ ã€‚
    def apply_to_rotation(self, state: "RotationState") -> Dict[str, Any]:
        """G7: åˆ†æçµæœã‚’ RotationState ã«åæ˜ ã€‚

        Returns:
            å¤‰æ›´ã®ã‚µãƒãƒª dictã€‚
        """
        from mekhane.basanos.pipeline import DomainWeight

        changes: Dict[str, Any] = {"hot_files": [], "weight_adjustments": {}}

        # 1. Hot files â†’ ãƒ‰ãƒ¡ã‚¤ãƒ³é‡ã¿ã‚’ä¸Šã’ã‚‹
        hot = self.hot_files(top_n=5)
        for fp in hot:
            changes["hot_files"].append({"path": fp.path, "heat": round(fp.heat, 3)})

            # Top issue type â†’ domain weight increase
            if fp.issue_types:
                top_category = max(fp.issue_types, key=fp.issue_types.get)
                if top_category in state.domains:
                    old_w = state.domains[top_category].weight
                    boost = min(0.3, fp.heat * 0.1)
                    new_w = min(2.0, old_w + boost)
                    state.domains[top_category].weight = round(new_w, 3)
                    changes["weight_adjustments"][top_category] = {
                        "old": old_w,
                        "new": round(new_w, 3),
                        "reason": f"hot file: {fp.path}",
                    }

        # 2. Category velocity â†’ ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰ã®é‡ã¿ã‚’ä¸‹ã’ã‚‹
        velocity = self.category_velocity()
        for category, v in velocity.items():
            if v < -0.5 and category in state.domains:
                old_w = state.domains[category].weight
                new_w = max(0.1, old_w - 0.1)
                state.domains[category].weight = round(new_w, 3)
                if category not in changes["weight_adjustments"]:
                    changes["weight_adjustments"][category] = {
                        "old": old_w,
                        "new": round(new_w, 3),
                        "reason": f"declining trend (v={v})",
                    }

        logger.info(
            f"Trend applied: {len(changes['hot_files'])} hot files, "
            f"{len(changes['weight_adjustments'])} weight changes"
        )
        return changes

    # PURPOSE: åˆ†æçµæœã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã€‚
    def summary(self) -> str:
        """åˆ†æçµæœã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã€‚"""
        reports = self.load_reports()
        if not reports:
            return "No data available yet."

        profiles = self.file_profiles()
        hot = self.hot_files(top_n=3)
        velocity = self.category_velocity()

        lines = [
            f"ğŸ“Š Trend Analysis ({len(self.dates)} days, {len(reports)} reports)",
            f"   Files tracked: {len(profiles)}",
        ]

        if hot:
            lines.append("   ğŸ”¥ Hot files:")
            for fp in hot:
                lines.append(f"      {fp.path} (heat={fp.heat:.2f}, streak={fp.streak})")

        rising = {k: v for k, v in velocity.items() if v > 0.3}
        if rising:
            lines.append("   ğŸ“ˆ Rising categories:")
            for cat, v in sorted(rising.items(), key=lambda x: -x[1]):
                lines.append(f"      {cat}: +{v:.2f}/day")

        return "\n".join(lines)
