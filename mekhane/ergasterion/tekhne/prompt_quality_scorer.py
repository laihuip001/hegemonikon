#!/usr/bin/env python3
# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- mekhane/ergasterion/tekhne/ A0â†’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ªå®šé‡åŒ–ãŒå¿…è¦â†’prompt_quality_scorerãŒæ‹…ã†
"""
Prompt Quality Scorer â€” ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å“è³ªã‚’å®šé‡çš„ã«ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

4æ¬¡å…ƒã‚¹ã‚³ã‚¢ä½“ç³»:
  - Structure (æ§‹é€ ): YAML frontmatter, å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³å……è¶³ç‡
  - Safety (å®‰å…¨æ€§): æ•µå¯¾çš„å…¥åŠ›å¯¾ç­–, ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«è¨˜è¿°
  - Completeness (å®Œæˆåº¦): Edge Cases, Fallback, å®šé‡æŒ‡æ¨™
  - Archetype Fit (é©åˆåº¦): Archetype å¿…é ˆæŠ€è¡“/ç¦å¿Œã¨ã®æ•´åˆæ€§

Usage:
  python prompt_quality_scorer.py <filepath>
  python prompt_quality_scorer.py --batch ".agent/skills/*/SKILL.md" --min-score 50
"""

from __future__ import annotations

import argparse
import glob
import json
import re
import sys
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Optional


# === Score Data Structures ===

# PURPOSE: DimensionScore ã®æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹
@dataclass
class DimensionScore:
    """Individual dimension score with details."""
    score: int  # 0-100
    max_score: int
    checks_passed: list[str] = field(default_factory=list)
    checks_failed: list[str] = field(default_factory=list)
    suggestions: list[str] = field(default_factory=list)

    # PURPOSE: prompt_quality_scorer ã® normalized å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    @property
    def normalized(self) -> int:
        """Normalize to 0-100 scale."""
        if self.max_score == 0:
            return 0
        return min(100, int((self.score / self.max_score) * 100))


# PURPOSE: QualityReport ã®æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹
@dataclass
class QualityReport:
    """Complete quality assessment report."""
    filepath: str
    structure: DimensionScore
    safety: DimensionScore
    completeness: DimensionScore
    archetype_fit: DimensionScore
    detected_format: str  # "skill" | "prompt" | "sage" | "unknown"
    detected_archetype: Optional[str] = None

    # PURPOSE: prompt_quality_scorer ã® total å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    @property
    def total(self) -> int:
        """Weighted total score (Structure 30, Safety 20, Completeness 30, Fit 20)."""
        return int(
            self.structure.normalized * 0.30
            + self.safety.normalized * 0.20
            + self.completeness.normalized * 0.30
            + self.archetype_fit.normalized * 0.20
        )

    # PURPOSE: prompt_quality_scorer ã® grade å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    @property
    def grade(self) -> str:
        t = self.total
        if t >= 90:
            return "S"
        elif t >= 80:
            return "A"
        elif t >= 70:
            return "B"
        elif t >= 60:
            return "C"
        elif t >= 50:
            return "D"
        else:
            return "F"

    # PURPOSE: prompt_quality_scorer ã® to dict å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    def to_dict(self) -> dict:
        return {
            "filepath": self.filepath,
            "total": self.total,
            "grade": self.grade,
            "detected_format": self.detected_format,
            "detected_archetype": self.detected_archetype,
            "dimensions": {
                "structure": {
                    "score": self.structure.normalized,
                    "passed": self.structure.checks_passed,
                    "failed": self.structure.checks_failed,
                    "suggestions": self.structure.suggestions,
                },
                "safety": {
                    "score": self.safety.normalized,
                    "passed": self.safety.checks_passed,
                    "failed": self.safety.checks_failed,
                    "suggestions": self.safety.suggestions,
                },
                "completeness": {
                    "score": self.completeness.normalized,
                    "passed": self.completeness.checks_passed,
                    "failed": self.completeness.checks_failed,
                    "suggestions": self.completeness.suggestions,
                },
                "archetype_fit": {
                    "score": self.archetype_fit.normalized,
                    "passed": self.archetype_fit.checks_passed,
                    "failed": self.archetype_fit.checks_failed,
                    "suggestions": self.archetype_fit.suggestions,
                },
            },
        }


# === Format Detection ===

# PURPOSE: prompt_quality_scorer ã® detect format å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
def detect_format(content: str) -> str:
    """Detect prompt format: skill, prompt, sage, or unknown."""
    if content.strip().startswith("---") and "name:" in content[:500]:
        return "skill"
    if content.strip().startswith("//") or content.strip().startswith("#prompt"):
        return "prompt"
    if "<module_config>" in content or "<instruction>" in content:
        return "sage"
    return "unknown"


# PURPOSE: prompt_quality_scorer ã® extract frontmatter å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
def extract_frontmatter(content: str) -> dict:
    """Extract YAML frontmatter from SKILL.md format."""
    match = re.match(r"^---\s*\n(.*?)\n---", content, re.DOTALL)
    if not match:
        return {}
    try:
        import yaml
        return yaml.safe_load(match.group(1)) or {}
    except Exception:
        return {}


# === Archetype Detection ===

ARCHETYPE_KEYWORDS = {
    "Precision": ["ç²¾åº¦", "accuracy", "precision", "èª¤ç­”ç‡", "æ¤œè¨¼", "verification",
                  "CoVe", "WACK", "confidence"],
    "Speed": ["é€Ÿåº¦", "speed", "ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·", "latency", "fast", "é«˜é€Ÿ", "compression"],
    "Autonomy": ["è‡ªå¾‹", "autonomy", "autonomous", "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ", "agent",
                 "ReAct", "Reflexion", "fallback"],
    "Creative": ["å‰µé€ ", "creative", "å¤šæ§˜æ€§", "diversity", "ã‚¢ã‚¤ãƒ‡ã‚¢", "idea",
                 "temperature", "brainstorm"],
    "Safety": ["å®‰å…¨", "safety", "ãƒªã‚¹ã‚¯", "risk", "guard", "URIAL",
               "injection", "harmful"],
}


# PURPOSE: prompt_quality_scorer ã® detect archetype å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
def detect_archetype(content: str) -> Optional[str]:
    """Detect the most likely archetype from content."""
    scores: dict[str, int] = {}
    content_lower = content.lower()
    for archetype, keywords in ARCHETYPE_KEYWORDS.items():
        scores[archetype] = sum(1 for kw in keywords if kw.lower() in content_lower)
    if not any(scores.values()):
        return None
    return max(scores, key=scores.get)


# === Dimension Checkers ===

# PURPOSE: structure ã‚’æ¤œè¨¼ã™ã‚‹
def check_structure(content: str, fmt: str) -> DimensionScore:
    """Check structural quality of the prompt."""
    score = 0
    max_score = 0
    passed = []
    failed = []
    suggestions = []

    if fmt == "skill":
        # YAML frontmatter
        max_score += 20
        fm = extract_frontmatter(content)
        if fm:
            score += 10
            passed.append("YAML frontmatter present")
            if fm.get("name"):
                score += 5
                passed.append("name field exists")
            else:
                failed.append("name field missing in frontmatter")
                suggestions.append("frontmatter ã« name: ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
            if fm.get("description"):
                score += 5
                passed.append("description field exists")
            else:
                failed.append("description field missing")
                suggestions.append("frontmatter ã« description: ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
        else:
            failed.append("YAML frontmatter missing")
            suggestions.append("--- ã§å›²ã‚“ã  YAML frontmatter ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")

        # Required sections
        required_sections = [
            ("Overview", 10),
            ("Core Behavior", 15),
            ("Quality Standards", 10),
            ("Edge Cases", 10),
            ("Examples", 10),
        ]
        for section_name, points in required_sections:
            max_score += points
            patterns = [
                f"## {section_name}",
                f"## {section_name.lower()}",
                f"### {section_name}",
            ]
            if any(p in content for p in patterns):
                score += points
                passed.append(f"Section '{section_name}' found")
            else:
                failed.append(f"Section '{section_name}' missing")
                suggestions.append(f"## {section_name} ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")

        # Heading hierarchy
        max_score += 10
        h2_count = len(re.findall(r"^## ", content, re.MULTILINE))
        if h2_count >= 3:
            score += 10
            passed.append(f"Good heading structure ({h2_count} H2 sections)")
        elif h2_count >= 1:
            score += 5
            passed.append(f"Minimal heading structure ({h2_count} H2 sections)")
            suggestions.append("ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹é€ ã‚’ã‚‚ã†å°‘ã—åˆ†å‰²ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨")
        else:
            failed.append("No H2 headings found")
            suggestions.append("## è¦‹å‡ºã—ã§ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ§‹é€ åŒ–ã—ã¦ãã ã•ã„")

        # Content length
        max_score += 15
        word_count = len(content.split())
        if word_count >= 200:
            score += 15
            passed.append(f"Sufficient content length ({word_count} words)")
        elif word_count >= 100:
            score += 8
            passed.append(f"Minimal content length ({word_count} words)")
            suggestions.append("å†…å®¹ã‚’ã‚‚ã†å°‘ã—è©³ç´°ã«è¨˜è¿°ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨")
        else:
            failed.append(f"Content too short ({word_count} words)")
            suggestions.append("æœ€ä½200èªä»¥ä¸Šã®å†…å®¹ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„")

    elif fmt == "prompt":
        # TÃ½pos format checks
        max_score += 40
        if "#prompt" in content:
            score += 10
            passed.append("#prompt directive found")
        else:
            failed.append("#prompt directive missing")
        if "@role:" in content:
            score += 10
            passed.append("@role defined")
        else:
            failed.append("@role missing")
            suggestions.append("@role: ã§ãƒ­ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã¦ãã ã•ã„")
        if "@goal:" in content:
            score += 10
            passed.append("@goal defined")
        else:
            failed.append("@goal missing")
        if "@constraints:" in content:
            score += 10
            passed.append("@constraints defined")
        else:
            failed.append("@constraints missing")

        max_score += 30
        if "@examples:" in content:
            score += 15
            passed.append("@examples provided")
        else:
            failed.append("@examples missing")
            suggestions.append("@examples: ã§å…¥å‡ºåŠ›ä¾‹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
        if "@format:" in content:
            score += 15
            passed.append("@format defined")
        else:
            failed.append("@format missing")

        max_score += 30
        word_count = len(content.split())
        if word_count >= 100:
            score += 30
            passed.append(f"Sufficient content ({word_count} words)")
        elif word_count >= 50:
            score += 15
            passed.append(f"Minimal content ({word_count} words)")
        else:
            failed.append(f"Content too short ({word_count} words)")

    elif fmt == "sage":
        max_score += 50
        for tag, points in [("<module_config>", 10), ("<instruction>", 15),
                            ("<protocol>", 10), ("<constraints>", 10),
                            ("<output_template>", 5)]:
            max_score_adj = points  # already included in 50
            if tag in content:
                score += points
                passed.append(f"XML tag {tag} found")
            else:
                failed.append(f"XML tag {tag} missing")
                suggestions.append(f"{tag} ã‚¿ã‚°ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")

        max_score += 50
        word_count = len(content.split())
        if word_count >= 150:
            score += 50
            passed.append(f"Sufficient content ({word_count} words)")
        elif word_count >= 75:
            score += 25
        else:
            failed.append(f"Content too short ({word_count} words)")

    else:
        max_score = 100
        score = 30  # Unknown format gets base score
        suggestions.append("èªè­˜å¯èƒ½ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (SKILL.md / .prompt / SAGE XML) ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")

    return DimensionScore(score=score, max_score=max(max_score, 1),
                          checks_passed=passed, checks_failed=failed,
                          suggestions=suggestions)


# PURPOSE: safety ã‚’æ¤œè¨¼ã™ã‚‹
def check_safety(content: str) -> DimensionScore:
    """Check safety-related qualities."""
    score = 0
    max_score = 0
    passed = []
    failed = []
    suggestions = []

    # Injection defense
    safety_patterns = [
        ("injection", 15, "Prompt injection defense",
         "prompt injection ã¸ã®é˜²å¾¡ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„"),
        ("guard|guardrail|boundary", 15, "Guardrails defined",
         "ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«/å¢ƒç•Œæ¡ä»¶ã‚’å®šç¾©ã—ã¦ãã ã•ã„"),
        ("error|exception|ã‚¨ãƒ©ãƒ¼|ç•°å¸¸", 10, "Error handling mentioned",
         "ã‚¨ãƒ©ãƒ¼/ç•°å¸¸ç³»ã®å‡¦ç†ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„"),
        ("fallback|ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯|ä»£æ›¿", 10, "Fallback strategy",
         "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ã‚’å®šç¾©ã—ã¦ãã ã•ã„"),
        ("role|persona|ãƒ­ãƒ¼ãƒ«", 10, "Role boundaries defined",
         "ãƒ­ãƒ¼ãƒ«å¢ƒç•Œã‚’æ˜ç¢ºã«å®šç¾©ã—ã¦ãã ã•ã„"),
        ("confidenc|ç¢ºä¿¡|ä¿¡é ¼", 10, "Confidence handling",
         "ç¢ºä¿¡åº¦ã®è¡¨ç¾æ–¹æ³•ã‚’å®šç¾©ã—ã¦ãã ã•ã„"),
        ("refuse|reject|æ‹’å¦|limitations", 10, "Refusal conditions",
         "æ‹’å¦/å›ç­”ä¿ç•™ã®æ¡ä»¶ã‚’å®šç¾©ã—ã¦ãã ã•ã„"),
        ("harmful|æœ‰å®³|unethical", 5, "Harmful content policy",
         "æœ‰å®³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¸ã®ãƒãƒªã‚·ãƒ¼ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„"),
        ("user_input|input.*zone|sanitiz", 10, "Input sanitization",
         "ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®éš”é›¢/ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã‚’è¨˜è¿°ã—ã¦ãã ã•ã„"),
        ("limit|åˆ¶é™|boundary|ä¸Šé™", 5, "Operational limits",
         "å‹•ä½œä¸Šé™/åˆ¶é™ã‚’å®šç¾©ã—ã¦ãã ã•ã„"),
    ]

    for pattern, points, pass_msg, fail_msg in safety_patterns:
        max_score += points
        if re.search(pattern, content, re.IGNORECASE):
            score += points
            passed.append(pass_msg)
        else:
            failed.append(pass_msg + " â€” not found")
            suggestions.append(fail_msg)

    return DimensionScore(score=score, max_score=max(max_score, 1),
                          checks_passed=passed, checks_failed=failed,
                          suggestions=suggestions)


# PURPOSE: completeness ã‚’æ¤œè¨¼ã™ã‚‹
def check_completeness(content: str, fmt: str) -> DimensionScore:
    """Check completeness of the prompt."""
    score = 0
    max_score = 0
    passed = []
    failed = []
    suggestions = []

    # Edge cases / failure scenarios
    max_score += 25
    failure_keywords = re.findall(
        r"(failure|å¤±æ•—|edge.?case|å¢ƒç•Œ|trap|ç½ |worst.?case|æœ€æ‚ª|pre.?mortem)",
        content, re.IGNORECASE
    )
    failure_count = len(failure_keywords)
    if failure_count >= 3:
        score += 25
        passed.append(f"Failure scenarios: {failure_count} mentions (â‰¥3 required)")
    elif failure_count >= 1:
        score += 12
        passed.append(f"Failure scenarios: {failure_count} mentions (need â‰¥3)")
        suggestions.append("å¤±æ•—ã‚±ãƒ¼ã‚¹ã‚’3ã¤ä»¥ä¸Šäºˆæ¸¬ã—ã¦ãã ã•ã„")
    else:
        failed.append("No failure scenarios described")
        suggestions.append("å¤±æ•—ã‚±ãƒ¼ã‚¹ã®äºˆæ¸¬ (Pre-Mortem) ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")

    # Quantitative metrics
    max_score += 20
    quant_patterns = [r"\d+%", r"\d+ç§’", r"\d+ä»¶", r"<\s*\d", r">\s*\d",
                      r"\d+å›", r"score", r"metric"]
    quant_found = sum(1 for p in quant_patterns if re.search(p, content))
    if quant_found >= 3:
        score += 20
        passed.append(f"Quantitative metrics: {quant_found} found")
    elif quant_found >= 1:
        score += 10
        passed.append(f"Minimal quantitative metrics: {quant_found} found")
        suggestions.append("å®šé‡çš„å“è³ªæŒ‡æ¨™ã‚’ã‚‚ã†å°‘ã—è¿½åŠ ã—ã¦ãã ã•ã„")
    else:
        failed.append("No quantitative metrics found")
        suggestions.append("å®šé‡çš„å“è³ªæŒ‡æ¨™ (ç²¾åº¦X%ä»¥ä¸Šã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·Yç§’ä»¥ä¸‹ ç­‰) ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")

    # Examples
    max_score += 20
    example_patterns = [r"example|ä¾‹|input.*output|å…¥åŠ›.*å‡ºåŠ›|usage|ä½¿ç”¨æ–¹æ³•"]
    if any(re.search(p, content, re.IGNORECASE) for p in example_patterns):
        score += 20
        passed.append("Examples/usage section found")
    else:
        failed.append("No examples found")
        suggestions.append("å…¥å‡ºåŠ›ä¾‹ (Examples) ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")

    # Fallback strategy
    max_score += 15
    if re.search(r"fallback|ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯|ä»£æ›¿|escalat|ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", content, re.IGNORECASE):
        score += 15
        passed.append("Fallback/escalation strategy defined")
    else:
        failed.append("No fallback strategy")
        suggestions.append("Fallback/ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥ã‚’å®šç¾©ã—ã¦ãã ã•ã„")

    # Tools / references
    max_score += 10
    if re.search(r"tool|ãƒ„ãƒ¼ãƒ«|reference|å‚ç…§|context|ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ", content, re.IGNORECASE):
        score += 10
        passed.append("Tools/references defined")
    else:
        failed.append("No tools or references mentioned")
        suggestions.append("ä½¿ç”¨ãƒ„ãƒ¼ãƒ«ã‚„å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„")

    # Activation / trigger conditions
    max_score += 10
    if re.search(r"trigger|activation|ç™ºå‹•|èµ·å‹•|æ¡ä»¶", content, re.IGNORECASE):
        score += 10
        passed.append("Activation triggers defined")
    else:
        failed.append("No activation triggers")
        suggestions.append("ç™ºå‹•æ¡ä»¶ (Triggers) ã‚’å®šç¾©ã—ã¦ãã ã•ã„")

    return DimensionScore(score=score, max_score=max(max_score, 1),
                          checks_passed=passed, checks_failed=failed,
                          suggestions=suggestions)


ARCHETYPE_REQUIRED_TECH = {
    "Precision": ["CoVe", "WACK", "Confidence", "æ¤œè¨¼", "verification"],
    "Speed": ["åœ§ç¸®", "compression", "cache", "ã‚­ãƒ£ãƒƒã‚·ãƒ¥", "çŸ­æ–‡"],
    "Autonomy": ["ReAct", "Reflexion", "Fallback", "ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", "Mem0"],
    "Creative": ["Temperature", "SAC", "å¤šæ§˜æ€§", "diversity"],
    "Safety": ["URIAL", "Neutralizing", "Constitutional", "æœ‰å®³", "ãƒ•ã‚£ãƒ«ã‚¿"],
}

ARCHETYPE_FORBIDDEN_TECH = {
    "Precision": ["EmotionPrompt", "é«˜Temperature", "Creative"],
    "Speed": ["Many-shot", "Self-Consistency", "ToT", "æ·±ã„CoT"],
    "Autonomy": ["Abstention", "éåº¦ãª", "äººé–“ç¢ºèªå¿…é ˆ"],
    "Creative": ["Temperature=0", "å³æ ¼ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ¶ç´„"],
    "Safety": ["ãƒ­ãƒ¼ãƒ«é€¸è„±è¨±å®¹", "ç„¡åˆ¶é™ç”Ÿæˆ"],
}


# PURPOSE: archetype fit ã‚’æ¤œè¨¼ã™ã‚‹
def check_archetype_fit(content: str, archetype: Optional[str]) -> DimensionScore:
    """Check archetype-specific fitness."""
    score = 0
    max_score = 0
    passed = []
    failed = []
    suggestions = []

    if not archetype:
        return DimensionScore(
            score=50, max_score=100,
            checks_passed=["No archetype detected â€” using neutral score"],
            checks_failed=[],
            suggestions=["Archetype (Precision/Speed/Autonomy/Creative/Safety) ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„"]
        )

    # Required technologies
    required = ARCHETYPE_REQUIRED_TECH.get(archetype, [])
    max_score += len(required) * 15
    for tech in required:
        if re.search(tech, content, re.IGNORECASE):
            score += 15
            passed.append(f"Required tech '{tech}' found for {archetype}")
        else:
            failed.append(f"Required tech '{tech}' missing for {archetype}")
            suggestions.append(f"{archetype} ã‚¢ãƒ¼ã‚­ã‚¿ã‚¤ãƒ—ã«å¿…è¦ãª '{tech}' ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")

    # Forbidden technologies (negative check)
    forbidden = ARCHETYPE_FORBIDDEN_TECH.get(archetype, [])
    max_score += len(forbidden) * 10
    for tech in forbidden:
        if re.search(tech, content, re.IGNORECASE):
            failed.append(f"Forbidden tech '{tech}' found for {archetype}")
            suggestions.append(f"{archetype} ã‚¢ãƒ¼ã‚­ã‚¿ã‚¤ãƒ—ã§ã¯ '{tech}' ã®ä½¿ç”¨ã‚’é¿ã‘ã¦ãã ã•ã„")
        else:
            score += 10
            passed.append(f"Forbidden tech '{tech}' correctly absent")

    if max_score == 0:
        max_score = 100
        score = 50

    return DimensionScore(score=score, max_score=max(max_score, 1),
                          checks_passed=passed, checks_failed=failed,
                          suggestions=suggestions)


# === Convergence/Divergence Policy ===

# FEP Function axiom: Explore â†” Exploit
# .prompt = precision weighting â†‘ = Exploit optimal, Explore detrimental
CONVERGENT_TASKS = frozenset([
    "data_extraction", "spec_generation", "test_generation",
    "code_formatting", "translation", "schema_validation",
    "jules_coding",
])

DIVERGENT_TASKS = frozenset([
    "brainstorming", "ideation", "exploration",
    "creative_writing", "design_review",
])


# PURPOSE: convergence policy ã‚’æ¤œè¨¼ã™ã‚‹
def check_convergence_policy(archetype: Optional[str], fmt: str) -> list[str]:
    """Check if .prompt format is appropriate for detected archetype.

    Returns list of warnings (empty = no issues).
    FEP basis: Function axiom (Explore â†” Exploit)
    """
    warnings = []
    if fmt == "prompt" and archetype == "Creative":
        warnings.append(
            "âš ï¸ POLICY: Creative archetype + .prompt å½¢å¼ã¯å¤šæ§˜æ€§å–ªå¤±ãƒªã‚¹ã‚¯ã‚ã‚Šã€‚"
            " .prompt ã¯ precision weighting ã‚’ä¸Šã’ã‚‹ãŸã‚ã€æ‹¡æ•£ã‚¿ã‚¹ã‚¯ã«ã¯ä¸å‘ãã€‚"
            " (FEP Function å…¬ç†: Explore â†” Exploit)"
        )
    return warnings


# === Main Scoring ===

# PURPOSE: prompt_quality_scorer ã® score prompt å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
def score_prompt(filepath: str) -> QualityReport:
    """Score a prompt file and return a QualityReport."""
    content = Path(filepath).read_text(encoding="utf-8")
    fmt = detect_format(content)
    archetype = detect_archetype(content)

    report = QualityReport(
        filepath=filepath,
        structure=check_structure(content, fmt),
        safety=check_safety(content),
        completeness=check_completeness(content, fmt),
        archetype_fit=check_archetype_fit(content, archetype),
        detected_format=fmt,
        detected_archetype=archetype,
    )

    # Add convergence/divergence policy warnings
    policy_warnings = check_convergence_policy(archetype, fmt)
    for w in policy_warnings:
        report.archetype_fit.suggestions.append(w)

    return report


# PURPOSE: report ã‚’æ•´å½¢ã™ã‚‹
def format_report(report: QualityReport, verbose: bool = False) -> str:
    """Format a QualityReport as human-readable text."""
    lines = []
    lines.append(f"\n{'='*60}")
    lines.append(f"ğŸ“Š Prompt Quality Score: {report.filepath}")
    lines.append(f"{'='*60}")
    lines.append(f"  Format: {report.detected_format} | Archetype: {report.detected_archetype or 'N/A'}")
    lines.append(f"")
    lines.append(f"  Total: {report.total}/100 (Grade: {report.grade})")
    lines.append(f"")
    lines.append(f"  â”œâ”€ Structure:     {report.structure.normalized:3d}/100")
    lines.append(f"  â”œâ”€ Safety:        {report.safety.normalized:3d}/100")
    lines.append(f"  â”œâ”€ Completeness:  {report.completeness.normalized:3d}/100")
    lines.append(f"  â””â”€ Archetype Fit: {report.archetype_fit.normalized:3d}/100")

    if verbose:
        for dim_name, dim in [("Structure", report.structure),
                               ("Safety", report.safety),
                               ("Completeness", report.completeness),
                               ("Archetype Fit", report.archetype_fit)]:
            if dim.checks_failed or dim.suggestions:
                lines.append(f"\n  [{dim_name}] Improvements:")
                for s in dim.suggestions[:5]:
                    lines.append(f"    â†’ {s}")

    lines.append(f"{'='*60}")
    return "\n".join(lines)


# === CLI ===

# PURPOSE: prompt_quality_scorer ã® main å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
def main():
    parser = argparse.ArgumentParser(description="Prompt Quality Scorer")
    parser.add_argument("filepath", nargs="?", help="Path to prompt file")
    parser.add_argument("--batch", help="Glob pattern for batch scoring")
    parser.add_argument("--min-score", type=int, default=0,
                        help="Minimum score threshold (exit 1 if any below)")
    parser.add_argument("--json", action="store_true", help="Output as JSON")
    parser.add_argument("-v", "--verbose", action="store_true",
                        help="Show detailed suggestions")
    args = parser.parse_args()

    if not args.filepath and not args.batch:
        parser.error("Either filepath or --batch is required")

    files = []
    if args.batch:
        files = sorted(glob.glob(args.batch, recursive=True))
    elif args.filepath:
        files = [args.filepath]

    if not files:
        print("No files found.")
        sys.exit(1)

    reports = []
    failures = []

    for f in files:
        try:
            report = score_prompt(f)
            reports.append(report)
            if report.total < args.min_score:
                failures.append(report)
        except Exception as e:
            print(f"Error scoring {f}: {e}", file=sys.stderr)

    if args.json:
        print(json.dumps([r.to_dict() for r in reports], ensure_ascii=False, indent=2))
    else:
        for r in reports:
            print(format_report(r, verbose=args.verbose))

        if len(reports) > 1:
            avg = sum(r.total for r in reports) / len(reports)
            print(f"\nğŸ“ˆ Batch Summary: {len(reports)} files | Avg: {avg:.1f}/100")
            if failures:
                print(f"âš ï¸  {len(failures)} file(s) below threshold ({args.min_score}):")
                for f in failures:
                    print(f"   {f.filepath}: {f.total}/100 (Grade: {f.grade})")

    if failures:
        sys.exit(1)


if __name__ == "__main__":
    main()
