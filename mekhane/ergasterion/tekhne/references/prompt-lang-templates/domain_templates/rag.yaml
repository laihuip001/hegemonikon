# RAG Domain Template v2.0
# ===================
# 検索拡張プロンプト、ドキュメントQA、知識ベース検索向け
# Enhanced: domain_examples, anti_patterns, output_style 追加

domain: "rag"
applicable_tasks:
  - document_qa
  - knowledge_search
  - citation_generation
  - fact_checking
  - multi_document_synthesis

# --- ドメイン固有の制約 ---
domain_constraints:
  - "回答は必ず検索結果に基づくこと（ハルシネーション禁止）"
  - "情報源を明示的に引用すること（文献番号、URL等）"
  - "検索結果に回答がない場合は「情報なし」と明示"
  - "複数ソースがある場合は信頼度順に整理"
  - "引用は原文をそのまま使い、要約して歪めない"

# --- ドメイン固有の評価軸 ---
domain_rubric:
  - name: "groundedness"
    description: "回答が検索結果に基づいているか"
    scale: "1-5"
    criteria:
      5: "全ての主張に引用あり、原文に忠実"
      3: "主要主張に引用あり、一部推測を含む"
      1: "検索結果と無関係な回答、ハルシネーション"

  - name: "relevance"
    description: "質問への関連性"
    scale: "1-5"
    criteria:
      5: "質問に直接かつ完全に回答"
      3: "部分的に回答、補足が必要"
      1: "質問と無関係な内容"

  - name: "completeness"
    description: "回答の網羅性"
    scale: "1-5"
    criteria:
      5: "関連する全ソースを統合した包括的回答"
      3: "主要ソースのみ参照"
      1: "単一ソースのみで不完全"

# --- ドメイン固有の入出力例 (few-shot) ---
domain_examples:
  - name: "document_qa_example"
    input: |
      以下の検索結果に基づいて質問に回答してください。
      質問: Transformer の自己注意機構の計算量は？
      検索結果:
        [1] Vaswani et al. (2017): "self-attention の計算量は O(n²·d)"
        [2] Kitaev et al. (2020): "Reformer は O(n·log(n)) に削減"
    output: |
      {
        "answer": "標準的な Transformer の自己注意機構の計算量は O(n²·d) です [1]。ただし、Reformer などの効率化手法により O(n·log(n)) まで削減可能です [2]。",
        "citations": [
          {
            "id": "[1]",
            "source": "Vaswani et al. (2017) - Attention Is All You Need",
            "excerpt": "self-attention の計算量は O(n²·d)",
            "relevance_score": 0.95
          },
          {
            "id": "[2]",
            "source": "Kitaev et al. (2020) - Reformer",
            "excerpt": "Reformer は O(n·log(n)) に削減",
            "relevance_score": 0.85
          }
        ],
        "confidence": "high",
        "no_answer_reason": null
      }

  - name: "no_answer_example"
    input: |
      質問: GPT-5 のパラメータ数は？
      検索結果: （関連情報なし）
    output: |
      {
        "answer": "検索結果に GPT-5 のパラメータ数に関する情報は見つかりませんでした。",
        "citations": [],
        "confidence": "low",
        "no_answer_reason": "検索結果に該当する情報なし"
      }

# --- アンチパターン（避けるべき出力パターン） ---
anti_patterns:
  - pattern: "ハルシネーション"
    bad: "GPT-4 のパラメータ数は 1.8 兆です"
    good: "検索結果に GPT-4 のパラメータ数に関する公式情報は見つかりませんでした"
  - pattern: "引用なし回答"
    bad: "Transformer は注意機構を使います"
    good: "Transformer は自己注意機構を使用しています [1]"
  - pattern: "情報の歪曲"
    bad: "論文によると、これが最良の方法です"
    good: "論文 [1] は O(n·log(n)) の手法を提案しています。他のアプローチ [2] も存在します"

# --- 出力スタイル設定 ---
output_style:
  format: "json"
  structure: |
    {
      "answer": "回答本文（必ず引用 [n] を含む）",
      "citations": [
        {
          "id": "[n]",
          "source": "ドキュメント名/URL",
          "excerpt": "引用箇所（原文そのまま）",
          "relevance_score": "0.0-1.0"
        }
      ],
      "confidence": "high | medium | low",
      "no_answer_reason": "null | 理由"
    }
  tone: "客観的。検索結果に存在する情報のみを使用"

# --- @context 推奨設定 ---
context_recommendations:
  - type: "mcp"
    tool: "gnosis.tool(\"search\")"
    priority: "HIGH"
    description: "Gnōsis 知識ベースから関連論文を検索"

  - type: "dir"
    path: "docs/"
    filter: "*.md"
    priority: "MEDIUM"
    description: "ローカルドキュメントを検索"
