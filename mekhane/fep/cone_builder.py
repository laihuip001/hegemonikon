# PROOF: [L2/ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£] <- mekhane/fep/
"""
PROOF: [L2/ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£] ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å­˜åœ¨ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„

A0 â†’ Hub Peras WF (@converge) ã§ Cone ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
   â†’ V[outputs] (åˆ†æ•£åº¦) ã‚’è‡ªå‹•è¨ˆç®—ã—ã€è§£æ¶ˆæ³•ã‚’ææ¡ˆã™ã‚‹
   â†’ C0: Precision Weighting ã§å„å®šç†ã®é‡ã¿ã‚’å‹•çš„ã«æ±ºå®šã™ã‚‹
   â†’ cone_builder.py ãŒæ‹…ã†

Q.E.D.

---

Cone Builder â€” Hub Peras @converge C0-C3 æ”¯æ´ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

Hub WF (/o, /s, /h, /p, /k, /a) ã® @converge C0-C3 ã‚’æ”¯æ´ã™ã‚‹ã€‚
C0: PW æ±ºå®š â†’ C1: Cone æ§‹ç¯‰ + V[outputs] â†’ C2: PW åŠ é‡èåˆ â†’ C3: æ™®éæ€§æ¤œè¨¼

Usage:
    from mekhane.fep.cone_builder import converge

    result = converge(
        series=Series.O,
        outputs={"O1": "æ·±ã„èªè­˜", "O2": "å¼·ã„æ„å¿—", "O3": "é‹­ã„å•ã„", "O4": "ç¢ºå®Ÿãªè¡Œå‹•"},
        pw={"O1": 1.0, "O3": 0.5},  # O1 ã‚’æœ€é‡è¦–ã€O3 ã‚„ã‚„é‡è¦–
    )
    print(result.apex)               # çµ±åˆåˆ¤æ–­
    print(result.dispersion)         # V[outputs]
    print(result.resolution_method)  # simple/pw_weighted/root
    print(result.pw)                 # {'O1': 1.0, 'O3': 0.5}
    print(result.pw_weights)         # æ­£è¦åŒ–æ¸ˆã¿é‡ã¿
"""

from __future__ import annotations

import re
from dataclasses import dataclass
from difflib import SequenceMatcher
from typing import Dict, List, Optional

from mekhane.fep.category import (
    COGNITIVE_TYPES,
    FUNCTORS,
    MORPHISMS,
    NATURAL_TRANSFORMATIONS,
    Cone,
    CognitiveType,
    Functor,
    Morphism,
    NaturalTransformation,
    Series,
    build_cone,
)


# å¦å®šèªãƒ‘ã‚¿ãƒ¼ãƒ³ (æ—¥æœ¬èª + è‹±èª)
_NEGATION_RE = re.compile(
    r"(?:ãªã„|ã—ãªã„|ã§ããªã„|ä¸å¯|å¦å®š|åå¯¾|æ‹’å¦|ä¸­æ­¢"
    r"|stop|no|not|never|don'?t|won'?t|reject|cancel)",
    re.IGNORECASE,
)
# æ–¹å‘æ€§: GO ç³»
_DIR_GO = re.compile(
    r"(?:ã™ã‚‹|é€²ã‚€|é–‹å§‹|GO|yes|accept|approve|keep|continue|å®Ÿè¡Œ|è¿½åŠ )",
    re.IGNORECASE,
)
# æ–¹å‘æ€§: WAIT ç³»
_DIR_WAIT = re.compile(
    r"(?:ã—ãªã„|æ­¢ã‚ã‚‹|ä¸­æ­¢|WAIT|no|reject|cancel|stop|remove|å‰Šé™¤|å»ƒæ­¢)",
    re.IGNORECASE,
)


def _char_bigrams(text: str) -> List[str]:
    """Generate character bigrams from text (whitespace removed).

    Used for Jaccard similarity â€” captures topic overlap even in Japanese
    where SequenceMatcher's character-level matching performs poorly.
    """
    clean = re.sub(r"\s+", "", text)
    return [clean[i:i + 2] for i in range(len(clean) - 1)]


# =============================================================================
# C0: Precision Weighting (PW)
# =============================================================================


# PURPOSE: C0 â€” PW ã®æ­£è¦åŒ–ã€‚raw pw [-1, +1] â†’ èåˆç”¨é‡ã¿ [0, 2]
def normalize_pw(
    outputs: Dict[str, str],
    pw: Optional[Dict[str, float]] = None,
) -> Dict[str, float]:
    """Normalize Precision Weighting for fusion.

    Formula: weight_i = 1 + pw_i
    - pw_i = 0  â†’ weight = 1.0 (neutral, uniform)
    - pw_i = +1 â†’ weight = 2.0 (double emphasis)
    - pw_i = -1 â†’ weight = 0.0 (fully suppressed)

    Returns:
        Dict[str, float]: normalized weights (0.0 - 2.0)
    """
    if pw is None:
        pw = {}

    return {
        tid: 1.0 + max(-1.0, min(1.0, pw.get(tid, 0.0)))
        for tid in outputs
    }


# PURPOSE: PW ãŒå‡ç­‰ã‹ã©ã†ã‹åˆ¤å®šã™ã‚‹
def is_uniform_pw(pw: Optional[Dict[str, float]]) -> bool:
    """Check if precision weighting is uniform (all zero or not specified)."""
    if not pw:
        return True
    return all(abs(v) < 1e-9 for v in pw.values())


# =============================================================================
# C1: å°„ã®å¯¾æ¯” (Contrast) â€” V[outputs]
# =============================================================================


# PURPOSE: @converge C1 â€” å°„ã®å¯¾æ¯” (Contrast): V[outputs] ã‚’è¨ˆç®—ã™ã‚‹
def compute_dispersion(outputs: Dict[str, str]) -> float:
    """Compute V[outputs] â€” the dispersion of theorem outputs.

    Uses pairwise text similarity + negation detection + direction coding
    to estimate how much the 4 outputs agree or contradict.

    Components:
        1. Ensemble similarity (SequenceMatcher + bigram Jaccard) â†’ base dispersion
        2. Negation contradiction â†’ bonus (ãƒ†ã‚­ã‚¹ãƒˆé–“ã§å¦å®šãŒæ··åœ¨)
        3. Direction contradiction â†’ bonus (GO vs WAIT ãŒæ··åœ¨)

    The ensemble approach addresses BS-2 (Japanese short text V being
    systematically high). SequenceMatcher works at character level,
    which is poor for Japanese. Bigram Jaccard captures topic overlap
    via shared character pairs â€” higher for texts about the same topic.

    Returns:
        float: dispersion score (0.0-1.0)
        0.0 = all outputs identical
        1.0 = all outputs completely different
    """
    if not outputs or len(outputs) <= 1:
        return 0.0

    values = list(outputs.values())
    similarities: List[float] = []

    for i in range(len(values)):
        for j in range(i + 1, len(values)):
            # Method 1: SequenceMatcher (character-level)
            seq_ratio = SequenceMatcher(None, values[i], values[j]).ratio()

            # Method 2: Bigram Jaccard (topic-level, better for Japanese)
            bigrams_a = set(_char_bigrams(values[i]))
            bigrams_b = set(_char_bigrams(values[j]))
            if bigrams_a or bigrams_b:
                jaccard = len(bigrams_a & bigrams_b) / len(bigrams_a | bigrams_b)
            else:
                jaccard = 1.0

            # Ensemble: take the max â€” if either method sees similarity,
            # the texts are not contradictory
            similarities.append(max(seq_ratio, jaccard))

    if not similarities:
        return 0.0

    avg_similarity = sum(similarities) / len(similarities)
    base = 1.0 - avg_similarity

    # å¦å®šçŸ›ç›¾ãƒœãƒ¼ãƒŠã‚¹: ä¸€éƒ¨ã ã‘å¦å®šèªãŒã‚ã‚‹ = çŸ›ç›¾
    neg_flags = [bool(_NEGATION_RE.search(v)) for v in values]
    neg_bonus = 0.0
    if any(neg_flags) and not all(neg_flags):
        neg_bonus = 0.15  # å¦å®šã®æ··åœ¨ã§ +0.15

    # æ–¹å‘æ€§çŸ›ç›¾ãƒœãƒ¼ãƒŠã‚¹: GO ã¨ WAIT ãŒæ··åœ¨
    dir_bonus = 0.0
    dirs = []
    for v in values:
        go = bool(_DIR_GO.search(v))
        wait = bool(_DIR_WAIT.search(v))
        if go and not wait:
            dirs.append(1)
        elif wait and not go:
            dirs.append(-1)
        else:
            dirs.append(0)
    non_zero = [d for d in dirs if d != 0]
    if non_zero and any(d > 0 for d in non_zero) and any(d < 0 for d in non_zero):
        dir_bonus = 0.2  # æ–¹å‘æ€§çŸ›ç›¾ã§ +0.2

    return round(min(1.0, base + neg_bonus + dir_bonus), 3)


# =============================================================================
# C2: Cone ã®é ‚ç‚¹æ¢ç´¢ (Resolve) â€” PW åŠ é‡èåˆ
# =============================================================================


# PURPOSE: @converge C2 â€” è§£æ¶ˆæ³•ã‚’åˆ¤å®šã™ã‚‹ (PW è€ƒæ…®)
def resolve_method(
    dispersion: float,
    pw: Optional[Dict[str, float]] = None,
) -> str:
    """Determine resolution method based on V[outputs] + PW.

    | V[outputs] | PW     | Method         |
    |:-----------|:-------|:---------------|
    | > 0.3      | any    | root           |
    | > 0.1      | any    | pw_weighted    |
    | â‰¤ 0.1      | â‰  0    | pw_weighted    |
    | â‰¤ 0.1      | = 0    | simple         |

    Returns:
        str: "simple", "pw_weighted", or "root"
    """
    if dispersion > 0.3:
        return "root"
    elif dispersion > 0.1:
        return "pw_weighted"
    elif not is_uniform_pw(pw):
        return "pw_weighted"
    else:
        return "simple"


# PURPOSE: PW åŠ é‡èåˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¨ˆç®—ã™ã‚‹
def compute_pw_table(
    outputs: Dict[str, str],
    pw: Optional[Dict[str, float]] = None,
) -> List[Dict]:
    """Compute the PW weighting table for each theorem.

    Returns a list of dicts with:
    - theorem_id: str
    - output: str (truncated)
    - pw_raw: float (-1 to +1)
    - weight: float (0 to 2, normalized)
    - weight_pct: float (percentage contribution)
    """
    weights = normalize_pw(outputs, pw)
    total = sum(weights.values())
    if total == 0:
        total = 1.0  # avoid division by zero

    table = []
    for tid, output in outputs.items():
        raw = (pw or {}).get(tid, 0.0)
        w = weights[tid]
        table.append({
            "theorem_id": tid,
            "output": output[:50] + "..." if len(output) > 50 else output,
            "pw_raw": raw,
            "weight": w,
            "weight_pct": round(w / total * 100, 1),
        })
    return table


# =============================================================================
# Main: converge() â€” C0-C3 ä¸€æ‹¬å®Ÿè¡Œ
# =============================================================================


# PURPOSE: @converge C0-C3 ã‚’ä¸€æ‹¬å®Ÿè¡Œã™ã‚‹
def converge(
    series: Series,
    outputs: Dict[str, str],
    pw: Optional[Dict[str, float]] = None,
    apex: Optional[str] = None,
    confidence: float = 0.0,
    context: Optional[str] = None,
    agent: object = None,
) -> Cone:
    """Execute @converge C0-C3 and return a fully populated Cone.

    This is the main entry point for Hub Peras workflows.

    Args:
        series: Which series (O/S/H/P/K/A)
        outputs: Dict mapping theorem_id -> output string
        pw: Precision Weighting dict {theorem_id: weight}.
            weight âˆˆ [-1, +1]. 0 = neutral, +1 = emphasize, -1 = suppress.
            None or empty = uses pw_adapter cascade to auto-resolve.
        apex: Optional pre-computed integrated judgment
        confidence: Optional confidence score (0-100)
        context: Optional natural language context for PW inference
        agent: Optional HegemonikÃ³nFEPAgent for PW derivation

    Returns:
        Cone with C0 pw, C1 projections, C2 resolution, C3 universality

    Formula (C2 weighted fusion):
        çµ±åˆå‡ºåŠ› = Î£(å®šç†_i Ã— (1 + pw_i)) / Î£(1 + pw_i)
    """
    # C0: Precision Weighting (pw_adapter cascade)
    if pw is not None:
        cone_pw = pw
    else:
        try:
            from mekhane.fep.pw_adapter import resolve_pw
            cone_pw = resolve_pw(
                series.name,
                context=context,
                agent=agent,
            )
        except ImportError:
            cone_pw = {}

    # C1: Build Cone with projections
    cone = build_cone(series, outputs)
    cone.pw = {k: max(-1.0, min(1.0, v)) for k, v in cone_pw.items()}

    # C1: Compute dispersion
    cone.dispersion = compute_dispersion(outputs)

    # C2: Determine resolution method (PW-aware)
    cone.resolution_method = resolve_method(cone.dispersion, cone_pw)

    # C2: Set apex if provided
    if apex:
        cone.apex = apex

    # C3: Cone å“è³ªè©•ä¾¡ (è‡ªå‹•è¨ˆç®—)
    if confidence > 0:
        cone.confidence = confidence  # å¤–éƒ¨æŒ‡å®šã‚’å„ªå…ˆ
    else:
        # è‡ªå‹•è¨ˆç®—: base = (1 - dispersion) * 100
        base_conf = (1.0 - cone.dispersion) * 100.0
        # å¦å®šãƒšãƒŠãƒ«ãƒ†ã‚£: apex ã¨ projection ã®çŸ›ç›¾
        penalty = 0.0
        if cone.apex:
            apex_neg = bool(_NEGATION_RE.search(cone.apex))
            for proj in cone.projections:
                if proj.output and bool(_NEGATION_RE.search(proj.output)) != apex_neg:
                    penalty += 5.0
        cone.confidence = max(0.0, min(100.0, base_conf - penalty))

    cone.is_universal = cone.dispersion <= 0.1 and cone.confidence >= 70.0

    return cone


# =============================================================================
# Display
# =============================================================================


# PURPOSE: å…¨ Series ã®åœè«–çš„ä½ç½®ã‚’è¡¨ç¤ºã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
def describe_cone(cone: Cone) -> str:
    """Format a Cone as human-readable text for WF output."""
    has_pw = not is_uniform_pw(cone.pw)

    # Header
    lines = [
        f"## Cone: {cone.series.value}-series",
        "",
    ]

    # C0: PW section (if non-uniform)
    if has_pw:
        lines.extend([
            "### C0: Precision Weighting",
            "",
        ])
        table = compute_pw_table(
            {p.theorem_id: p.output for p in cone.projections},
            cone.pw,
        )
        lines.append("| Theorem | pw | Weight | % |")
        lines.append("|:--------|:--:|:------:|:--:|")
        for row in table:
            pw_str = f"+{row['pw_raw']}" if row["pw_raw"] > 0 else str(row["pw_raw"])
            lines.append(
                f"| {row['theorem_id']} | {pw_str} | {row['weight']:.1f} | {row['weight_pct']}% |"
            )
        lines.append("")

    # C1: Projections
    lines.extend([
        "### C1: å°„ã®å¯¾æ¯”",
        "",
        "| Theorem | Hom Label | Output |",
        "|:--------|:----------|:-------|",
    ])
    for proj in cone.projections:
        lines.append(f"| {proj.theorem_id} | {proj.hom_label} | {proj.output} |")

    # C2: Resolution
    lines.extend([
        "",
        f"**V[outputs]** = {cone.dispersion:.3f}"
        + (" âœ…" if cone.dispersion <= 0.1
           else " âš ï¸" if cone.dispersion <= 0.3
           else " ğŸ”´"),
        f"**Resolution** = {cone.resolution_method}",
        f"**Apex** = {cone.apex or '(æœªè¨­å®š)'}",
    ])

    # C3: Cone å“è³ªè©•ä¾¡
    lines.extend([
        f"**Confidence** = {cone.confidence:.0f}%",
        f"**Universal** = {'âœ… Yes' if cone.is_universal else 'âŒ No'}",
    ])
    if cone.series == Series.S and cone.dispersion > 0.1:
        lines.append("âš ï¸ **Devil's Advocate æ¨å¥¨** (S-series, V > 0.1)")

    return "\n".join(lines)


# =============================================================================
# Natural Transformation Verification
# =============================================================================


@dataclass
class NaturalityResult:
    """Result of verifying the naturality condition for a natural transformation.

    Naturality condition: G(f) âˆ˜ Î±_X = Î±_Y âˆ˜ F(f)
    For each morphism f: X â†’ Y in the source category.
    """

    transformation_name: str
    is_natural: bool  # All checks passed
    checks: List[Dict]  # Individual check results
    violations: List[str]  # Human-readable violation descriptions

    @property
    def summary(self) -> str:
        """One-line summary."""
        n_pass = sum(1 for c in self.checks if c["pass"])
        return (
            f"{self.transformation_name}: "
            f"{n_pass}/{len(self.checks)} checks passed"
            + ("" if self.is_natural else f" â€” {len(self.violations)} violations")
        )


# PURPOSE: Verify naturality condition for a natural transformation
def verify_naturality(
    nt: NaturalTransformation,
    source_functor: Optional[Functor] = None,
    target_functor: Optional[Functor] = None,
) -> NaturalityResult:
    """Verify the naturality condition: G(f) âˆ˜ Î±_X = Î±_Y âˆ˜ F(f).

    For each morphism f: X â†’ Y in the source functor's morphism_map,
    checks that the naturality square commutes:

        F(X) --Î±_X--> G(X)
         |              |
       F(f)           G(f)
         |              |
         v              v
        F(Y) --Î±_Y--> G(Y)

    Args:
        nt: The natural transformation Î±: F â‡’ G
        source_functor: F (auto-resolved from FUNCTORS if not given)
        target_functor: G (auto-resolved from FUNCTORS if not given)

    Returns:
        NaturalityResult with per-morphism check details
    """
    # Auto-resolve functors
    if source_functor is None:
        source_functor = FUNCTORS.get(nt.source_functor.lower())
    if target_functor is None:
        target_functor = FUNCTORS.get(nt.target_functor.lower())

    checks: List[Dict] = []
    violations: List[str] = []

    # If we can't resolve both functors, check component consistency only
    if source_functor is None or target_functor is None:
        # Fallback: verify that all components map to valid theorems
        for src_obj, tgt_obj in nt.components.items():
            is_valid = tgt_obj in COGNITIVE_TYPES
            check = {
                "source_obj": src_obj,
                "target_obj": tgt_obj,
                "type": "component_validity",
                "pass": is_valid,
            }
            checks.append(check)
            if not is_valid:
                violations.append(
                    f"Component Î±_{src_obj} = {tgt_obj}: "
                    f"target {tgt_obj} is not a valid theorem"
                )
        return NaturalityResult(
            transformation_name=nt.name,
            is_natural=len(violations) == 0,
            checks=checks,
            violations=violations,
        )

    # Full naturality check: for each morphism in source functor
    for mor_id, mapped_mor_id in source_functor.morphism_map.items():
        # Parse source morphism X â†’ Y
        parts = mor_id.replace("â†’", "->").split("->")
        if len(parts) != 2:
            # Not a parseable morphism, skip
            continue

        src_x, src_y = parts[0].strip(), parts[1].strip()

        # Î±_X: component at X
        alpha_x = nt.component_at(src_x)
        # Î±_Y: component at Y
        alpha_y = nt.component_at(src_y)

        # F(f): how the source functor maps the morphism
        f_of_f = mapped_mor_id

        # G(f): how the target functor maps the same morphism
        # (if target_functor has a morphism_map)
        g_of_f = (
            target_functor.morphism_map.get(mor_id)
            if target_functor
            else None
        )

        # Check: both components must exist
        if alpha_x is None or alpha_y is None:
            check = {
                "morphism": mor_id,
                "source_obj": src_x,
                "target_obj": src_y,
                "alpha_x": alpha_x,
                "alpha_y": alpha_y,
                "type": "component_missing",
                "pass": False,
            }
            checks.append(check)
            missing = src_x if alpha_x is None else src_y
            violations.append(
                f"Morphism {mor_id}: component Î±_{missing} is undefined"
            )
            continue

        # Structural commutativity check:
        # G(f) âˆ˜ Î±_X and Î±_Y âˆ˜ F(f) should arrive at the same target theorem
        # In our registry, Î± maps source objects to HGK theorems.
        # The naturality square commutes if:
        #   mapping(Î±_X) through the HGK morphism structure = Î±_Y
        #
        # Operational check: does a morphism path exist from Î±_X to Î±_Y
        # in the MORPHISMS registry?
        path_exists = any(
            m.source == alpha_x and m.target == alpha_y
            for m in MORPHISMS.values()
        )

        check = {
            "morphism": mor_id,
            "source_obj": src_x,
            "target_obj": src_y,
            "alpha_x": alpha_x,
            "alpha_y": alpha_y,
            "f_of_f": f_of_f,
            "g_of_f": g_of_f,
            "path_exists": path_exists,
            "type": "naturality_square",
            "pass": path_exists,
        }
        checks.append(check)

        if not path_exists:
            violations.append(
                f"Morphism {mor_id}: no path from Î±_{src_x}={alpha_x} "
                f"to Î±_{src_y}={alpha_y} in MORPHISMS. "
                f"Naturality square may not commute."
            )

    return NaturalityResult(
        transformation_name=nt.name,
        is_natural=len(violations) == 0,
        checks=checks,
        violations=violations,
    )


# PURPOSE: Classify a theorem's cognitive type (Understanding/Reasoning/Bridge)
def classify_cognitive_type(theorem_id: str) -> CognitiveType:
    """Look up the cognitive type for a theorem.

    Args:
        theorem_id: e.g. "O1", "A1", "K4"

    Returns:
        CognitiveType enum value

    Raises:
        KeyError: if theorem_id is not in the registry
    """
    return COGNITIVE_TYPES[theorem_id]


# PURPOSE: Check if a morphism crosses the Understanding/Reasoning boundary
def is_cross_boundary_morphism(
    source_id: str, target_id: str
) -> Optional[str]:
    """Determine if a morphism crosses the U/R boundary.

    Returns:
        "Uâ†’R" if source is Understanding and target is Reasoning
        "Râ†’U" if source is Reasoning and target is Understanding
        None if both are same type or if either is Bridge/Mixed
    """
    src_type = COGNITIVE_TYPES.get(source_id)
    tgt_type = COGNITIVE_TYPES.get(target_id)

    if src_type is None or tgt_type is None:
        return None

    u_types = {CognitiveType.UNDERSTANDING, CognitiveType.BRIDGE_U_TO_R}
    r_types = {CognitiveType.REASONING, CognitiveType.BRIDGE_R_TO_U}

    if src_type in u_types and tgt_type in r_types:
        return "Uâ†’R"
    elif src_type in r_types and tgt_type in u_types:
        return "Râ†’U"
    return None


# =============================================================================
# CLI Entry Point â€” @converge turbo blocks call this
# =============================================================================


def _parse_pw(pw_str: str) -> Dict[str, float]:
    """Parse PW string: 'O1:0.5,O3:-0.5' â†’ {'O1': 0.5, 'O3': -0.5}

    Invalid values are silently skipped (defensive parsing).
    """
    if not pw_str:
        return {}
    result = {}
    for pair in pw_str.split(","):
        pair = pair.strip()
        if ":" in pair:
            k, v = pair.split(":", 1)
            try:
                result[k.strip()] = float(v.strip())
            except ValueError:
                pass  # Skip invalid values (e.g. "O1:abc")
    return result


def _parse_outputs(args: list) -> Dict[str, str]:
    """Parse 'O1=æ·±ã„èªè­˜ O2=å¼·ã„æ„å¿—' â†’ {'O1': 'æ·±ã„èªè­˜', ...}"""
    result = {}
    for arg in args:
        if "=" in arg:
            k, v = arg.split("=", 1)
            result[k.strip()] = v.strip()
    return result


if __name__ == "__main__":
    import argparse
    import sys

    parser = argparse.ArgumentParser(
        description="Cone Builder â€” Hub Peras @converge C0-C3",
        usage="python -m mekhane.fep.cone_builder --series O O1='å‡ºåŠ›1' O2='å‡ºåŠ›2' ...",
    )
    parser.add_argument(
        "--series", "-s",
        required=True,
        choices=["O", "S", "H", "P", "K", "A"],
        help="Series to build cone for",
    )
    parser.add_argument(
        "--pw",
        default="",
        help="Precision Weighting: 'O1:0.5,O3:-0.5'",
    )
    parser.add_argument(
        "--apex",
        default=None,
        help="Pre-computed apex (integrated judgment)",
    )
    parser.add_argument(
        "--confidence",
        type=float,
        default=0.0,
        help="Confidence score (0-100)",
    )
    parser.add_argument(
        "outputs",
        nargs="*",
        help="Theorem outputs: O1='èªè­˜ã®çµè«–' O2='æ„å¿—ã®çµè«–' ...",
    )

    args = parser.parse_args()

    series = Series[args.series]
    pw = _parse_pw(args.pw)
    outputs = _parse_outputs(args.outputs)

    if not outputs:
        print("âš ï¸ No outputs provided. Use: O1='å‡ºåŠ›1' O2='å‡ºåŠ›2' ...", file=sys.stderr)
        sys.exit(1)

    cone = converge(series, outputs, pw=pw or None, apex=args.apex, confidence=args.confidence)
    print(describe_cone(cone))

