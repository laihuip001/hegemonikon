# PROOF: [L2/ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£] <- mekhane/fep/
"""
PROOF: [L2/ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£] ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å­˜åœ¨ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„

A0 â†’ Hub Peras WF (@converge) ã§ Cone ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
   â†’ V[outputs] (åˆ†æ•£åº¦) ã‚’è‡ªå‹•è¨ˆç®—ã—ã€è§£æ¶ˆæ³•ã‚’ææ¡ˆã™ã‚‹
   â†’ C0: Precision Weighting ã§å„å®šç†ã®é‡ã¿ã‚’å‹•çš„ã«æ±ºå®šã™ã‚‹
   â†’ cone_builder.py ãŒæ‹…ã†

Q.E.D.

---

Cone Builder â€” Hub Peras @converge C0-C3 æ”¯æ´ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

Hub WF (/o, /s, /h, /p, /k, /a) ã® @converge C0-C3 ã‚’æ”¯æ´ã™ã‚‹ã€‚
C0: PW æ±ºå®š â†’ C1: Cone æ§‹ç¯‰ + V[outputs] â†’ C2: PW åŠ é‡èåˆ â†’ C3: æ™®éæ€§æ¤œè¨¼

Usage:
    from mekhane.fep.cone_builder import converge

    result = converge(
        series=Series.O,
        outputs={"O1": "æ·±ã„èªè­˜", "O2": "å¼·ã„æ„å¿—", "O3": "é‹­ã„å•ã„", "O4": "ç¢ºå®Ÿãªè¡Œå‹•"},
        pw={"O1": 1.0, "O3": 0.5},  # O1 ã‚’æœ€é‡è¦–ã€O3 ã‚„ã‚„é‡è¦–
    )
    print(result.apex)               # çµ±åˆåˆ¤æ–­
    print(result.dispersion)         # V[outputs]
    print(result.resolution_method)  # simple/pw_weighted/root
    print(result.pw)                 # {'O1': 1.0, 'O3': 0.5}
    print(result.pw_weights)         # æ­£è¦åŒ–æ¸ˆã¿é‡ã¿
"""

from __future__ import annotations

import re
from dataclasses import dataclass
from difflib import SequenceMatcher
from typing import Dict, List, Optional

from mekhane.fep.category import (
    COGNITIVE_TYPES,
    FUNCTORS,
    MORPHISMS,
    NATURAL_TRANSFORMATIONS,
    SERIES_ENRICHMENTS,
    Cone,
    CognitiveType,
    Enrichment,
    EnrichmentType,
    Functor,
    Morphism,
    NaturalTransformation,
    Series,
    build_cone,
)


# å¦å®šèªãƒ‘ã‚¿ãƒ¼ãƒ³ (æ—¥æœ¬èª + è‹±èª)
_NEGATION_RE = re.compile(
    r"(?:ãªã„|ã—ãªã„|ã§ããªã„|ä¸å¯|å¦å®š|åå¯¾|æ‹’å¦|ä¸­æ­¢"
    r"|stop|no|not|never|don'?t|won'?t|reject|cancel)",
    re.IGNORECASE,
)
# æ–¹å‘æ€§: GO ç³»
_DIR_GO = re.compile(
    r"(?:ã™ã‚‹|é€²ã‚€|é–‹å§‹|GO|yes|accept|approve|keep|continue|å®Ÿè¡Œ|è¿½åŠ )",
    re.IGNORECASE,
)
# æ–¹å‘æ€§: WAIT ç³»
_DIR_WAIT = re.compile(
    r"(?:ã—ãªã„|æ­¢ã‚ã‚‹|ä¸­æ­¢|WAIT|no|reject|cancel|stop|remove|å‰Šé™¤|å»ƒæ­¢)",
    re.IGNORECASE,
)


def _char_bigrams(text: str) -> List[str]:
    """Generate character bigrams from text (whitespace removed).

    Used for Jaccard similarity â€” captures topic overlap even in Japanese
    where SequenceMatcher's character-level matching performs poorly.
    """
    clean = re.sub(r"\s+", "", text)
    return [clean[i:i + 2] for i in range(len(clean) - 1)]


# Japanese morphological tokenization (janome, graceful fallback)
_JANOME_TOKENIZER = None
_JANOME_AVAILABLE = False

try:
    from janome.tokenizer import Tokenizer as _JanomeTokenizer
    _JANOME_TOKENIZER = _JanomeTokenizer()
    _JANOME_AVAILABLE = True
except ImportError:
    pass  # janome not installed; falls back to char bigrams

# Content word POS prefixes (nouns, verbs, adjectives)
_CONTENT_POS = ("åè©", "å‹•è©", "å½¢å®¹è©")
# Suffix/non-independent POS to exclude
_EXCLUDE_POS_DETAIL = ("æ¥å°¾", "éè‡ªç«‹", "æ•°")


def _tokenize_ja(text: str) -> List[str]:
    """Tokenize Japanese text into content words using janome.

    Extracts base forms of nouns, verbs, and adjectives,
    filtering out suffixes (çš„, ã•, æ€§) and non-independent words.

    Returns:
        List of base-form content words.
        Empty list if janome is not available.
    """
    if not _JANOME_AVAILABLE or _JANOME_TOKENIZER is None:
        return []

    tokens = []
    for tok in _JANOME_TOKENIZER.tokenize(text):
        pos = tok.part_of_speech
        # Must be content word
        if not pos.startswith(_CONTENT_POS):
            continue
        # Exclude suffixes and non-independent forms
        if any(excl in pos for excl in _EXCLUDE_POS_DETAIL):
            continue
        # Use base_form for normalization (æ·±ã„â†’æ·±ã„, ãªã‚‹â†’ãªã‚‹)
        base = tok.base_form if tok.base_form != "*" else tok.surface
        if len(base) >= 2 or not base.isascii():  # skip 1-char ASCII
            tokens.append(base)
    return tokens


# =============================================================================
# C0: Precision Weighting (PW)
# =============================================================================


# PURPOSE: C0 â€” PW ã®æ­£è¦åŒ–ã€‚raw pw [-1, +1] â†’ èåˆç”¨é‡ã¿ [0, 2]
def normalize_pw(
    outputs: Dict[str, str],
    pw: Optional[Dict[str, float]] = None,
) -> Dict[str, float]:
    """Normalize Precision Weighting for fusion.

    Formula: weight_i = 1 + pw_i
    - pw_i = 0  â†’ weight = 1.0 (neutral, uniform)
    - pw_i = +1 â†’ weight = 2.0 (double emphasis)
    - pw_i = -1 â†’ weight = 0.0 (fully suppressed)

    Returns:
        Dict[str, float]: normalized weights (0.0 - 2.0)
    """
    if pw is None:
        pw = {}

    return {
        tid: 1.0 + max(-1.0, min(1.0, pw.get(tid, 0.0)))
        for tid in outputs
    }


# PURPOSE: PW ãŒå‡ç­‰ã‹ã©ã†ã‹åˆ¤å®šã™ã‚‹
def is_uniform_pw(pw: Optional[Dict[str, float]]) -> bool:
    """Check if precision weighting is uniform (all zero or not specified)."""
    if not pw:
        return True
    return all(abs(v) < 1e-9 for v in pw.values())


# =============================================================================
# C1: å°„ã®å¯¾æ¯” (Contrast) â€” V[outputs]
# =============================================================================


# PURPOSE: @converge C1 â€” å°„ã®å¯¾æ¯” (Contrast): V[outputs] ã‚’è¨ˆç®—ã™ã‚‹
def compute_dispersion(outputs: Dict[str, str]) -> float:
    """Compute V[outputs] â€” the dispersion of theorem outputs.

    Uses pairwise text similarity + negation detection + direction coding
    to estimate how much the 4 outputs agree or contradict.

    Components:
        1. Ensemble similarity (SequenceMatcher + bigram Jaccard) â†’ base dispersion
        2. Negation contradiction â†’ bonus (ãƒ†ã‚­ã‚¹ãƒˆé–“ã§å¦å®šãŒæ··åœ¨)
        3. Direction contradiction â†’ bonus (GO vs WAIT ãŒæ··åœ¨)

    The ensemble approach addresses BS-2 (Japanese short text V being
    systematically high). SequenceMatcher works at character level,
    which is poor for Japanese. Bigram Jaccard captures topic overlap
    via shared character pairs â€” higher for texts about the same topic.

    Returns:
        float: dispersion score (0.0-1.0)
        0.0 = all outputs identical
        1.0 = all outputs completely different
    """
    if not outputs or len(outputs) <= 1:
        return 0.0

    values = list(outputs.values())
    similarities: List[float] = []

    for i in range(len(values)):
        for j in range(i + 1, len(values)):
            # Method 1: SequenceMatcher (character-level)
            seq_ratio = SequenceMatcher(None, values[i], values[j]).ratio()

            # Method 2: Bigram Jaccard (topic-level, better for Japanese)
            bigrams_a = set(_char_bigrams(values[i]))
            bigrams_b = set(_char_bigrams(values[j]))
            if bigrams_a or bigrams_b:
                jaccard = len(bigrams_a & bigrams_b) / len(bigrams_a | bigrams_b)
            else:
                jaccard = 1.0

            # Method 3: Morphological Jaccard (semantic-level, best for Japanese)
            # Uses content words (nouns/verbs/adjectives) in base form.
            morph_jaccard = 0.0
            if _JANOME_AVAILABLE:
                words_a = set(_tokenize_ja(values[i]))
                words_b = set(_tokenize_ja(values[j]))
                if words_a or words_b:
                    morph_jaccard = len(words_a & words_b) / len(words_a | words_b)
                else:
                    morph_jaccard = 1.0

            # Ensemble: take the max â€” if any method sees similarity,
            # the texts are not contradictory
            similarities.append(max(seq_ratio, jaccard, morph_jaccard))

    if not similarities:
        return 0.0

    avg_similarity = sum(similarities) / len(similarities)
    base = 1.0 - avg_similarity

    # å¦å®šçŸ›ç›¾ãƒœãƒ¼ãƒŠã‚¹: ä¸€éƒ¨ã ã‘å¦å®šèªãŒã‚ã‚‹ = çŸ›ç›¾
    neg_flags = [bool(_NEGATION_RE.search(v)) for v in values]
    neg_bonus = 0.0
    if any(neg_flags) and not all(neg_flags):
        neg_bonus = 0.15  # å¦å®šã®æ··åœ¨ã§ +0.15

    # æ–¹å‘æ€§çŸ›ç›¾ãƒœãƒ¼ãƒŠã‚¹: GO ã¨ WAIT ãŒæ··åœ¨
    dir_bonus = 0.0
    dirs = []
    for v in values:
        go = bool(_DIR_GO.search(v))
        wait = bool(_DIR_WAIT.search(v))
        if go and not wait:
            dirs.append(1)
        elif wait and not go:
            dirs.append(-1)
        else:
            dirs.append(0)
    non_zero = [d for d in dirs if d != 0]
    if non_zero and any(d > 0 for d in non_zero) and any(d < 0 for d in non_zero):
        dir_bonus = 0.2  # æ–¹å‘æ€§çŸ›ç›¾ã§ +0.2

    return round(min(1.0, base + neg_bonus + dir_bonus), 3)


# =============================================================================
# C2: Cone ã®é ‚ç‚¹æ¢ç´¢ (Resolve) â€” PW åŠ é‡èåˆ
# =============================================================================


# PURPOSE: @converge C2 â€” è§£æ¶ˆæ³•ã‚’åˆ¤å®šã™ã‚‹ (PW è€ƒæ…®)
def resolve_method(
    dispersion: float,
    pw: Optional[Dict[str, float]] = None,
) -> str:
    """Determine resolution method based on V[outputs] + PW.

    | V[outputs] | PW     | Method         |
    |:-----------|:-------|:---------------|
    | > 0.3      | any    | root           |
    | > 0.1      | any    | pw_weighted    |
    | â‰¤ 0.1      | â‰  0    | pw_weighted    |
    | â‰¤ 0.1      | = 0    | simple         |

    Returns:
        str: "simple", "pw_weighted", or "root"
    """
    if dispersion > 0.3:
        return "root"
    elif dispersion > 0.1:
        return "pw_weighted"
    elif not is_uniform_pw(pw):
        return "pw_weighted"
    else:
        return "simple"


# PURPOSE: PW åŠ é‡èåˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¨ˆç®—ã™ã‚‹
def compute_pw_table(
    outputs: Dict[str, str],
    pw: Optional[Dict[str, float]] = None,
) -> List[Dict]:
    """Compute the PW weighting table for each theorem.

    Returns a list of dicts with:
    - theorem_id: str
    - output: str (truncated)
    - pw_raw: float (-1 to +1)
    - weight: float (0 to 2, normalized)
    - weight_pct: float (percentage contribution)
    """
    weights = normalize_pw(outputs, pw)
    total = sum(weights.values())
    if total == 0:
        total = 1.0  # avoid division by zero

    table = []
    for tid, output in outputs.items():
        raw = (pw or {}).get(tid, 0.0)
        w = weights[tid]
        table.append({
            "theorem_id": tid,
            "output": output[:50] + "..." if len(output) > 50 else output,
            "pw_raw": raw,
            "weight": w,
            "weight_pct": round(w / total * 100, 1),
        })
    return table


# =============================================================================
# Enrichment-specific Cone post-processing
# =============================================================================


# PURPOSE: Apply enrichment-specific behavior to a Cone
def apply_enrichment(cone: Cone) -> None:
    """Apply enrichment-specific post-processing to a Cone.

    Each enrichment type modifies the Cone in ways specific to its
    mathematical structure:

    - End: V > 0.5 â†’ suggest O3+ (self-referential deepening)
    - Met: Lower Devil's Advocate threshold (0.1 â†’ 0.08)
    - Prob: Detect valence bias (>75% same direction â†’ flag)
    - Temp: Urgency boost for high-PW items
    - Fuzzy: Add confidence grading (tentative/justified/certain)
    - Set: No enrichment â†’ no-op

    Mutates cone in-place. All changes are recorded in cone metadata.
    """
    if cone.enrichment is None:
        return

    etype = cone.enrichment.type

    if etype == EnrichmentType.MET:
        # Met-enrichment: stricter dispersion threshold for S-series
        # S-series already has Devil's Advocate at V > 0.1,
        # Met-enrichment lowers effective threshold by boosting dispersion
        # interpretation: treat V > 0.08 as "needs attention"
        if 0.08 < cone.dispersion <= 0.1 and cone.resolution_method == "simple":
            cone.resolution_method = "pw_weighted"

    elif etype == EnrichmentType.END:
        # End-enrichment: V > 0.5 â†’ self-referential feedback
        # Adds a hint that /o* (meta-query) should be invoked
        if cone.dispersion > 0.5 and not cone.apex:
            cone.apex = "[V>0.5: /o* è‡ªå·±å‚ç…§ã‚’æ¨å¥¨ â€” èªçŸ¥å±¤ãŒè‡ªèº«ã‚’å•ã„ç›´ã™å¿…è¦ã‚ã‚Š]"

    elif etype == EnrichmentType.PROB:
        # Prob-enrichment: detect valence bias
        # If >75% of outputs share the same direction, flag as biased
        values = [p.output for p in cone.projections if p.output]
        if values:
            go_count = sum(1 for v in values if _DIR_GO.search(v))
            wait_count = sum(1 for v in values if _DIR_WAIT.search(v))
            total = len(values)
            if total > 0:
                bias_ratio = max(go_count, wait_count) / total
                if bias_ratio >= 0.75 and cone.confidence > 0:
                    # Bias detected: reduce confidence slightly
                    cone.confidence = max(0.0, cone.confidence - 10.0)

    elif etype == EnrichmentType.TEMP:
        # Temp-enrichment: urgency boost
        # If any projection contains urgency markers, boost non-uniform PW
        urgency_markers = ("urgent", "ç·Šæ€¥", "è‡³æ€¥", "ä»Šã™ã", "deadline", "æœŸé™")
        has_urgency = any(
            any(m in p.output.lower() for m in urgency_markers)
            for p in cone.projections if p.output
        )
        if has_urgency and cone.resolution_method == "simple":
            cone.resolution_method = "pw_weighted"

    elif etype == EnrichmentType.FUZZY:
        # Fuzzy-enrichment: confidence grading
        # Map raw confidence to tentative/justified/certain
        if cone.confidence < 50:
            cone.resolution_method = (
                cone.resolution_method + " [tent]"
                if "[tent]" not in cone.resolution_method
                else cone.resolution_method
            )
        elif cone.confidence < 80:
            cone.resolution_method = (
                cone.resolution_method + " [just]"
                if "[just]" not in cone.resolution_method
                else cone.resolution_method
            )
        else:
            cone.resolution_method = (
                cone.resolution_method + " [cert]"
                if "[cert]" not in cone.resolution_method
                else cone.resolution_method
            )

    # SET: no-op (container series, no enrichment needed)


# =============================================================================
# Main: converge() â€” C0-C3 ä¸€æ‹¬å®Ÿè¡Œ
# =============================================================================


# PURPOSE: @converge C0-C3 ã‚’ä¸€æ‹¬å®Ÿè¡Œã™ã‚‹
def converge(
    series: Series,
    outputs: Dict[str, str],
    pw: Optional[Dict[str, float]] = None,
    apex: Optional[str] = None,
    confidence: float = 0.0,
    context: Optional[str] = None,
    agent: object = None,
) -> Cone:
    """Execute @converge C0-C3 and return a fully populated Cone.

    This is the main entry point for Hub Peras workflows.

    Args:
        series: Which series (O/S/H/P/K/A)
        outputs: Dict mapping theorem_id -> output string
        pw: Precision Weighting dict {theorem_id: weight}.
            weight âˆˆ [-1, +1]. 0 = neutral, +1 = emphasize, -1 = suppress.
            None or empty = uses pw_adapter cascade to auto-resolve.
        apex: Optional pre-computed integrated judgment
        confidence: Optional confidence score (0-100)
        context: Optional natural language context for PW inference
        agent: Optional HegemonikÃ³nFEPAgent for PW derivation

    Returns:
        Cone with C0 pw, C1 projections, C2 resolution, C3 universality

    Formula (C2 weighted fusion):
        çµ±åˆå‡ºåŠ› = Î£(å®šç†_i Ã— (1 + pw_i)) / Î£(1 + pw_i)
    """
    # C0: Precision Weighting (pw_adapter cascade)
    if pw is not None:
        cone_pw = pw
    else:
        try:
            from mekhane.fep.pw_adapter import resolve_pw
            cone_pw = resolve_pw(
                series.name,
                context=context,
                agent=agent,
            )
        except ImportError:
            cone_pw = {}

    # C1: Build Cone with projections
    cone = build_cone(series, outputs)
    cone.pw = {k: max(-1.0, min(1.0, v)) for k, v in cone_pw.items()}

    # C1: Compute dispersion
    cone.dispersion = compute_dispersion(outputs)

    # C2: Determine resolution method (PW-aware)
    cone.resolution_method = resolve_method(cone.dispersion, cone_pw)

    # C2: Set apex if provided
    if apex:
        cone.apex = apex

    # C3: Cone å“è³ªè©•ä¾¡ (è‡ªå‹•è¨ˆç®—)
    if confidence > 0:
        cone.confidence = confidence  # å¤–éƒ¨æŒ‡å®šã‚’å„ªå…ˆ
    else:
        # è‡ªå‹•è¨ˆç®—: base = (1 - dispersion) * 100
        base_conf = (1.0 - cone.dispersion) * 100.0
        # å¦å®šãƒšãƒŠãƒ«ãƒ†ã‚£: apex ã¨ projection ã®çŸ›ç›¾
        penalty = 0.0
        if cone.apex:
            apex_neg = bool(_NEGATION_RE.search(cone.apex))
            for proj in cone.projections:
                if proj.output and bool(_NEGATION_RE.search(proj.output)) != apex_neg:
                    penalty += 5.0
        cone.confidence = max(0.0, min(100.0, base_conf - penalty))

    cone.is_universal = cone.dispersion <= 0.1 and cone.confidence >= 70.0

    # Typed Enrichment: auto-assign from SERIES_ENRICHMENTS
    cone.enrichment = SERIES_ENRICHMENTS.get(series)

    # Apply enrichment-specific behavior
    apply_enrichment(cone)

    return cone


# =============================================================================
# Display
# =============================================================================


# PURPOSE: å…¨ Series ã®åœè«–çš„ä½ç½®ã‚’è¡¨ç¤ºã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
def describe_cone(cone: Cone) -> str:
    """Format a Cone as human-readable text for WF output."""
    has_pw = not is_uniform_pw(cone.pw)

    # Header
    lines = [
        f"## Cone: {cone.series.value}-series",
        "",
    ]

    # C0: PW section (if non-uniform)
    if has_pw:
        lines.extend([
            "### C0: Precision Weighting",
            "",
        ])
        table = compute_pw_table(
            {p.theorem_id: p.output for p in cone.projections},
            cone.pw,
        )
        lines.append("| Theorem | pw | Weight | % |")
        lines.append("|:--------|:--:|:------:|:--:|")
        for row in table:
            pw_str = f"+{row['pw_raw']}" if row["pw_raw"] > 0 else str(row["pw_raw"])
            lines.append(
                f"| {row['theorem_id']} | {pw_str} | {row['weight']:.1f} | {row['weight_pct']}% |"
            )
        lines.append("")

    # C1: Projections
    lines.extend([
        "### C1: å°„ã®å¯¾æ¯”",
        "",
        "| Theorem | Hom Label | Output |",
        "|:--------|:----------|:-------|",
    ])
    for proj in cone.projections:
        lines.append(f"| {proj.theorem_id} | {proj.hom_label} | {proj.output} |")

    # C2: Resolution
    lines.extend([
        "",
        f"**V[outputs]** = {cone.dispersion:.3f}"
        + (" âœ…" if cone.dispersion <= 0.1
           else " âš ï¸" if cone.dispersion <= 0.3
           else " ğŸ”´"),
        f"**Resolution** = {cone.resolution_method}",
        f"**Apex** = {cone.apex or '(æœªè¨­å®š)'}",
    ])

    # C3: Cone å“è³ªè©•ä¾¡
    lines.extend([
        f"**Confidence** = {cone.confidence:.0f}%",
        f"**Universal** = {'âœ… Yes' if cone.is_universal else 'âŒ No'}",
    ])
    if cone.series == Series.S and cone.dispersion > 0.1:
        lines.append("âš ï¸ **Devil's Advocate æ¨å¥¨** (S-series, V > 0.1)")

    # Enrichment section
    if cone.enrichment is not None:
        enr = cone.enrichment
        lines.append("")
        lines.append("### Enrichment")
        lines.append("")
        if enr.type == EnrichmentType.SET:
            lines.append(f"**Type** = {enr.type.value} (å™¨ â€” enrichment ä¸è¦)")
        else:
            kalon_str = f"{enr.kalon:.2f}" if enr.kalon is not None else "â€”"
            lines.append(f"**Type** = {enr.type.value}-enrichment (Kalon {kalon_str})")
        lines.append(f"**Concept** = {enr.concept}")
        if enr.structures:
            lines.append("")
            for s in enr.structures:
                lines.append(f"- {s}")

    return "\n".join(lines)


# =============================================================================
# Natural Transformation Verification
# =============================================================================


# PURPOSE: Result of verifying the naturality condition for a natural transformation
@dataclass
class NaturalityResult:
    """Result of verifying the naturality condition for a natural transformation.

    Naturality condition: G(f) âˆ˜ Î±_X = Î±_Y âˆ˜ F(f)
    For each morphism f: X â†’ Y in the source category.
    """

    transformation_name: str
    is_natural: bool  # All checks passed
    checks: List[Dict]  # Individual check results
    violations: List[str]  # Human-readable violation descriptions

    # PURPOSE: One-line summary
    @property
    def summary(self) -> str:
        """One-line summary."""
        n_pass = sum(1 for c in self.checks if c["pass"])
        return (
            f"{self.transformation_name}: "
            f"{n_pass}/{len(self.checks)} checks passed"
            + ("" if self.is_natural else f" â€” {len(self.violations)} violations")
        )


# PURPOSE: BFS composite morphism path search (/m dia+ P4)
# C3 fix: Module-level adjacency cache (MORPHISMS is constant)
_MORPHISM_ADJACENCY: Optional[Dict[str, List[str]]] = None


def _get_morphism_adjacency() -> Dict[str, List[str]]:
    """Get or build the morphism adjacency graph (cached)."""
    global _MORPHISM_ADJACENCY
    if _MORPHISM_ADJACENCY is None:
        adj: Dict[str, List[str]] = {}
        for m in MORPHISMS.values():
            adj.setdefault(m.source, []).append(m.target)
        _MORPHISM_ADJACENCY = adj
    return _MORPHISM_ADJACENCY


def _has_morphism_path(source: str, target: str, max_depth: int = 4) -> bool:
    """Check if a morphism path exists from source to target in MORPHISMS.

    Uses BFS to find composite paths (e.g., O1â†’O2â†’O4) in addition to
    direct morphisms. Bounded by max_depth to prevent graph explosion.

    Args:
        source: Source theorem ID (e.g., "O1")
        target: Target theorem ID (e.g., "O4")
        max_depth: Maximum BFS depth (default 4)

    Returns:
        True if any path (direct or composite) exists
    """
    if source == target:
        return True

    adjacency = _get_morphism_adjacency()

    # BFS
    visited: set = {source}
    frontier: List[str] = [source]
    for _depth in range(max_depth):
        next_frontier: List[str] = []
        for node in frontier:
            for neighbor in adjacency.get(node, []):
                if neighbor == target:
                    return True
                if neighbor not in visited:
                    visited.add(neighbor)
                    next_frontier.append(neighbor)
        frontier = next_frontier
        if not frontier:
            break

    return False


# PURPOSE: Verify naturality condition for a natural transformation
def verify_naturality(
    nt: NaturalTransformation,
    source_functor: Optional[Functor] = None,
    target_functor: Optional[Functor] = None,
) -> NaturalityResult:
    """Verify the naturality condition: G(f) âˆ˜ Î±_X = Î±_Y âˆ˜ F(f).

    For each morphism f: X â†’ Y in the source functor's morphism_map,
    checks that the naturality square commutes:

        F(X) --Î±_X--> G(X)
         |              |
       F(f)           G(f)
         |              |
         v              v
        F(Y) --Î±_Y--> G(Y)

    Args:
        nt: The natural transformation Î±: F â‡’ G
        source_functor: F (auto-resolved from FUNCTORS if not given)
        target_functor: G (auto-resolved from FUNCTORS if not given)

    Returns:
        NaturalityResult with per-morphism check details
    """
    # Auto-resolve functors
    if source_functor is None:
        source_functor = FUNCTORS.get(nt.source_functor.lower())
    if target_functor is None:
        target_functor = FUNCTORS.get(nt.target_functor.lower())

    checks: List[Dict] = []
    violations: List[str] = []

    # If we can't resolve both functors, check component consistency only
    if source_functor is None or target_functor is None:
        # Fallback: verify that all components map to valid theorems
        for src_obj, tgt_obj in nt.components.items():
            is_valid = tgt_obj in COGNITIVE_TYPES
            check = {
                "source_obj": src_obj,
                "target_obj": tgt_obj,
                "type": "component_validity",
                "pass": is_valid,
            }
            checks.append(check)
            if not is_valid:
                violations.append(
                    f"Component Î±_{src_obj} = {tgt_obj}: "
                    f"target {tgt_obj} is not a valid theorem"
                )
        return NaturalityResult(
            transformation_name=nt.name,
            is_natural=len(violations) == 0,
            checks=checks,
            violations=violations,
        )

    # Full naturality check: for each morphism in source functor
    for mor_id, mapped_mor_id in source_functor.morphism_map.items():
        # Parse source morphism X â†’ Y
        parts = mor_id.replace("â†’", "->").split("->")
        if len(parts) != 2:
            # Not a parseable morphism, skip
            continue

        src_x, src_y = parts[0].strip(), parts[1].strip()

        # Î±_X: component at X
        alpha_x = nt.component_at(src_x)
        # Î±_Y: component at Y
        alpha_y = nt.component_at(src_y)

        # F(f): how the source functor maps the morphism
        f_of_f = mapped_mor_id

        # G(f): how the target functor maps the same morphism
        # (if target_functor has a morphism_map)
        g_of_f = (
            target_functor.morphism_map.get(mor_id)
            if target_functor
            else None
        )

        # Check: both components must exist
        if alpha_x is None or alpha_y is None:
            check = {
                "morphism": mor_id,
                "source_obj": src_x,
                "target_obj": src_y,
                "alpha_x": alpha_x,
                "alpha_y": alpha_y,
                "type": "component_missing",
                "pass": False,
            }
            checks.append(check)
            missing = src_x if alpha_x is None else src_y
            violations.append(
                f"Morphism {mor_id}: component Î±_{missing} is undefined"
            )
            continue

        # Structural commutativity check (/m dia+ P4):
        # G(f) âˆ˜ Î±_X and Î±_Y âˆ˜ F(f) should arrive at the same target theorem
        # In our registry, Î± maps source objects to HGK theorems.
        # The naturality square commutes if:
        #   mapping(Î±_X) through the HGK morphism structure = Î±_Y
        #
        # Operational check: does a morphism path exist from Î±_X to Î±_Y
        # in the MORPHISMS registry?  Includes composite paths via BFS.
        path_exists = _has_morphism_path(alpha_x, alpha_y, max_depth=4)

        check = {
            "morphism": mor_id,
            "source_obj": src_x,
            "target_obj": src_y,
            "alpha_x": alpha_x,
            "alpha_y": alpha_y,
            "f_of_f": f_of_f,
            "g_of_f": g_of_f,
            "path_exists": path_exists,
            "type": "naturality_square",
            "pass": path_exists,
        }
        checks.append(check)

        if not path_exists:
            violations.append(
                f"Morphism {mor_id}: no path from Î±_{src_x}={alpha_x} "
                f"to Î±_{src_y}={alpha_y} in MORPHISMS (direct or composite). "
                f"Naturality square may not commute."
            )

    return NaturalityResult(
        transformation_name=nt.name,
        is_natural=len(violations) == 0,
        checks=checks,
        violations=violations,
    )


# PURPOSE: Classify a theorem's cognitive type (Understanding/Reasoning/Bridge)
def classify_cognitive_type(theorem_id: str) -> CognitiveType:
    """Look up the cognitive type for a theorem.

    Args:
        theorem_id: e.g. "O1", "A1", "K4"

    Returns:
        CognitiveType enum value

    Raises:
        KeyError: if theorem_id is not in the registry
    """
    return COGNITIVE_TYPES[theorem_id]


# PURPOSE: Check if a morphism crosses the Understanding/Reasoning boundary
def is_cross_boundary_morphism(
    source_id: str, target_id: str
) -> Optional[str]:
    """Determine if a morphism crosses the U/R boundary.

    BRIDGE classification rationale (F6):
    - BRIDGE_U_TO_R (A1 Pathos) is grouped with U_types because it
      *originates from* Understanding. A1 transforms raw emotion
      (understood) into precision (reasoning). The morphism A1â†’S1
      crosses Uâ†’R because A1 is the "departure side" of understanding.
    - BRIDGE_R_TO_U (A3 GnÅmÄ“) is grouped with R_types because it
      *originates from* Reasoning. A3 distills methodical analysis
      into insight (understanding).
    - This "origin-side" classification aligns with how MP (Wang & Zhao
      2023) treats metacognitive transitions: the bridge belongs to
      the side it departs from, not the side it arrives at.

    Returns:
        "Uâ†’R" if source is Understanding and target is Reasoning
        "Râ†’U" if source is Reasoning and target is Understanding
        None if both are same type or if either is MIXED
    """
    src_type = COGNITIVE_TYPES.get(source_id)
    tgt_type = COGNITIVE_TYPES.get(target_id)

    if src_type is None or tgt_type is None:
        return None

    # BRIDGE belongs to its origin side (departure classification)
    u_types = {CognitiveType.UNDERSTANDING, CognitiveType.BRIDGE_U_TO_R}
    r_types = {CognitiveType.REASONING, CognitiveType.BRIDGE_R_TO_U}

    if src_type in u_types and tgt_type in r_types:
        return "Uâ†’R"
    elif src_type in r_types and tgt_type in u_types:
        return "Râ†’U"
    return None


# =============================================================================
# CLI Entry Point â€” @converge turbo blocks call this
# =============================================================================


def _parse_pw(pw_str: str) -> Dict[str, float]:
    """Parse PW string: 'O1:0.5,O3:-0.5' â†’ {'O1': 0.5, 'O3': -0.5}

    Invalid values are logged and skipped (DAPL: DefensiveFallback â†’ cleaned).
    """
    if not pw_str:
        return {}
    result = {}
    for pair in pw_str.split(","):
        pair = pair.strip()
        if ":" in pair:
            k, v = pair.split(":", 1)
            try:
                result[k.strip()] = float(v.strip())
            except (ValueError, TypeError):
                import logging
                logging.getLogger(__name__).warning(
                    "PW parse error: %r (key=%r, value=%r) â€” skipped",
                    pair, k.strip(), v.strip(),
                )
    return result


def _parse_outputs(args: list) -> Dict[str, str]:
    """Parse 'O1=æ·±ã„èªè­˜ O2=å¼·ã„æ„å¿—' â†’ {'O1': 'æ·±ã„èªè­˜', ...}"""
    result = {}
    for arg in args:
        if "=" in arg:
            k, v = arg.split("=", 1)
            result[k.strip()] = v.strip()
    return result


if __name__ == "__main__":
    import argparse
    import json
    import sys
    from pathlib import Path

    parser = argparse.ArgumentParser(
        description="Cone Builder â€” Hub Peras @converge C0-C3",
        usage="python -m mekhane.fep.cone_builder --series O O1='å‡ºåŠ›1' O2='å‡ºåŠ›2' ...",
    )
    parser.add_argument(
        "--series", "-s",
        required=True,
        choices=["O", "S", "H", "P", "K", "A"],
        help="Series to build cone for",
    )
    parser.add_argument(
        "--pw",
        default="",
        help="Precision Weighting: 'O1:0.5,O3:-0.5'",
    )
    parser.add_argument(
        "--file", "-f",
        default=None,
        help="JSON file from wf_env_bridge export (overrides positional outputs)",
    )
    parser.add_argument(
        "--apex",
        default=None,
        help="Pre-computed apex (integrated judgment)",
    )
    parser.add_argument(
        "--confidence",
        type=float,
        default=0.0,
        help="Confidence score (0-100)",
    )
    parser.add_argument(
        "outputs",
        nargs="*",
        help="Theorem outputs: O1='èªè­˜ã®çµè«–' O2='æ„å¿—ã®çµè«–' ...",
    )

    args = parser.parse_args()

    series = Series[args.series]

    # --file mode: load outputs + pw from wf_env_bridge JSON export
    if args.file:
        try:
            file_data = json.loads(Path(args.file).read_text(encoding="utf-8"))
        except (OSError, json.JSONDecodeError) as e:
            print(f"âš ï¸ Failed to read --file {args.file}: {e}", file=sys.stderr)
            sys.exit(1)
        outputs = file_data.get("outputs", {})
        pw = file_data.get("pw", {})
        # CLI --pw overrides file pw if explicitly given
        if args.pw:
            pw = _parse_pw(args.pw)
    else:
        pw = _parse_pw(args.pw)
        outputs = _parse_outputs(args.outputs)

    if not outputs:
        print("âš ï¸ No outputs provided. Use: O1='å‡ºåŠ›1' O2='å‡ºåŠ›2' ... or --file", file=sys.stderr)
        sys.exit(1)

    cone = converge(series, outputs, pw=pw or None, apex=args.apex, confidence=args.confidence)
    print(describe_cone(cone))

    # F4: Cone Consumer â€” active inference recommendation
    from mekhane.fep.cone_consumer import advise
    advice = advise(cone)
    print()
    print(f"ğŸ§­ **Next Action**: {advice.action}")
    if advice.suggested_wf:
        print(f"   æ¨å¥¨WF: {advice.suggested_wf}")
    print(f"   ç†ç”±: {advice.reason}")
    if advice.next_steps:
        for step in advice.next_steps:
            print(f"   â†’ {step}")
    print(f"   urgency: {advice.urgency:.1f}")

