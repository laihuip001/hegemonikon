# PROOF: [L1/FEP] <- mekhane/fep/
# PURPOSE: 96 è¦ç´ ä½“ç³»ã® Theorem-Level Attractor â€” èªçŸ¥ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿
"""
Theorem-Level Attractor Engine

24 å®šç†ã‚’ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ç©ºé–“ä¸Šã® attractor ã¨ã—ã¦å®šç¾©ã—ã€
72 X-series morphism ã‚’é·ç§»è¡Œåˆ—ã¨ã—ã¦ GPU ä¸Šã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ã€‚

ç†è«–çš„æ ¹æ‹ :
- Spisak & Friston 2025: FEP â†’ è‡ªå·±ç›´äº¤åŒ–ã™ã‚‹ attractor network
- HegemonikÃ³n v3.3: 7å…¬ç† + 24å®šç† + 72é–¢ä¿‚ = 96è¦ç´ ä½“ç³»

Usage:
    from mekhane.fep.theorem_attractor import TheoremAttractor
    ta = TheoremAttractor()
    result = ta.diagnose("ãªãœã“ã®è¨­è¨ˆãŒä»Šå¿…è¦ãªã®ã‹")
    flow = ta.simulate_flow("ãªãœã“ã®è¨­è¨ˆãŒä»Šå¿…è¦ãªã®ã‹", steps=10)
    basins = ta.detect_basins(n_samples=10000)
"""

from __future__ import annotations

import math
import time
from dataclasses import dataclass, field
from typing import Optional

import numpy as np

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False


# ---------------------------------------------------------------------------
# 24 Theorem Definitions
# ---------------------------------------------------------------------------

# PURPOSE: å„å®šç†ã®æœ¬è³ªã‚’æ‰ãˆã‚‹å®šç¾©ãƒ†ã‚­ã‚¹ãƒˆ (bge embedding ç”¨ã€è‹±èª)
# NOTE: WF ã® description ã‹ã‚‰æŠ½å‡º + æœ¬è³ªã‚’è‹±èªã§è¨˜è¿°
THEOREM_DEFINITIONS: dict[str, dict] = {
    # O-series: Ousia (æœ¬è³ª)
    "O1": {
        "name": "NoÄ“sis (æ·±ã„èªè­˜)",
        "series": "O",
        "command": "/noe",
        "definition": (
            "Deep cognition, intuitive insight. Recursive self-evidencing. "
            "Premise destruction. Zero-point design. Graph-of-Thought analysis. "
            "The deepest layer of understanding. Why does this truly exist?"
        ),
    },
    "O2": {
        "name": "BoulÄ“sis (æ„å¿—)",
        "series": "O",
        "command": "/bou",
        "definition": (
            "Will, purpose, goal clarification. What do you truly want? "
            "From pure ideal to practical objective. Desire, volition, akrasia. "
            "Priority setting and trade-off analysis."
        ),
    },
    "O3": {
        "name": "ZÄ“tÄ“sis (æ¢æ±‚)",
        "series": "O",
        "command": "/zet",
        "definition": (
            "Inquiry, question discovery. What should be asked? "
            "Finding the seed of the question. Five Whys. Root cause exploration. "
            "Spike and proof-of-concept investigation."
        ),
    },
    "O4": {
        "name": "Energeia (è¡Œç‚º)",
        "series": "O",
        "command": "/ene",
        "definition": (
            "Action, actualization. Turning will into reality. "
            "6-stage execution framework. Feature flags. Staged deployment. "
            "Making things happen. Implementation and delivery."
        ),
    },
    # S-series: Schema (æ§˜æ…‹)
    "S1": {
        "name": "Metron (å°ºåº¦)",
        "series": "S",
        "command": "/met",
        "definition": (
            "Scale, granularity, measurement. How large or small? "
            "Scope determination. Level of abstraction. Resolution setting. "
            "Zoom in or zoom out decision."
        ),
    },
    "S2": {
        "name": "MekhanÄ“ (æ–¹æ³•)",
        "series": "S",
        "command": "/mek",
        "definition": (
            "Method, mechanism, skill arrangement. How to build? "
            "Workflow generation and diagnosis. Tool selection. "
            "Architecture design. Blueprint creation."
        ),
    },
    "S3": {
        "name": "Stathmos (åŸºæº–)",
        "series": "S",
        "command": "/sta",
        "definition": (
            "Standard, benchmark, evaluation criteria. What defines quality? "
            "Acceptance criteria. Performance metrics. Quality gate. "
            "Success criteria definition."
        ),
    },
    "S4": {
        "name": "Praxis (å®Ÿè·µ)",
        "series": "S",
        "command": "/pra",
        "definition": (
            "Practice, value realization. How to deliver value? "
            "Method selection for implementation. Hands-on execution. "
            "Turning design into working system."
        ),
    },
    # H-series: HormÄ“ (å‹•æ©Ÿ)
    "H1": {
        "name": "Propatheia (å‰æ„Ÿæƒ…)",
        "series": "H",
        "command": "/pro",
        "definition": (
            "Pre-emotion, initial impulse, gut feeling. First reaction. "
            "Intuitive tendency. Instinctive response before rational evaluation. "
            "What is your immediate feeling about this?"
        ),
    },
    "H2": {
        "name": "Pistis (ç¢ºä¿¡)",
        "series": "H",
        "command": "/pis",
        "definition": (
            "Conviction, confidence level. How sure are you? "
            "Trust assessment. Reliability evaluation. Epistemic humility. "
            "Certainty vs uncertainty measurement."
        ),
    },
    "H3": {
        "name": "Orexis (æ¬²æ±‚)",
        "series": "H",
        "command": "/ore",
        "definition": (
            "Desire, value tendency. What do you value? "
            "Appetite for change. Motivation assessment. "
            "Passion and drive evaluation. Value alignment check."
        ),
    },
    "H4": {
        "name": "Doxa (ä¿¡å¿µ)",
        "series": "H",
        "command": "/dox",
        "definition": (
            "Belief, opinion, conviction record. What do you believe? "
            "Belief persistence and recording. Worldview documentation. "
            "Assumption tracking and updating."
        ),
    },
    # P-series: PerigraphÄ“ (æ¡ä»¶)
    "P1": {
        "name": "KhÅra (å ´)",
        "series": "P",
        "command": "/kho",
        "definition": (
            "Space, field, domain. Where does this apply? "
            "Scope definition. Boundary setting. Markov blanket delineation. "
            "Context and environment specification."
        ),
    },
    "P2": {
        "name": "Hodos (é“)",
        "series": "P",
        "command": "/hod",
        "definition": (
            "Path, route, trajectory. Which way to go? "
            "Route planning. Step sequence. Roadmap creation. "
            "Migration path and transition strategy."
        ),
    },
    "P3": {
        "name": "Trokhia (è»Œé“)",
        "series": "P",
        "command": "/tro",
        "definition": (
            "Orbit, cycle, iteration scope. How does this repeat? "
            "Application range. Feedback loop. Sprint cycle. "
            "Iterative refinement pattern."
        ),
    },
    "P4": {
        "name": "TekhnÄ“ (æŠ€æ³•)",
        "series": "P",
        "command": "/tek",
        "definition": (
            "Technique, craft, specific tool choice. Which technique? "
            "Tool selection. Technology choice. Implementation technique. "
            "Craft and artisanship in execution."
        ),
    },
    # K-series: Kairos (æ–‡è„ˆ)
    "K1": {
        "name": "Eukairia (å¥½æ©Ÿ)",
        "series": "K",
        "command": "/euk",
        "definition": (
            "Opportunity, right timing. Is now the right moment? "
            "Window of opportunity detection. Timing assessment. "
            "Readiness evaluation. Strategic timing."
        ),
    },
    "K2": {
        "name": "Chronos (æ™‚é–“)",
        "series": "K",
        "command": "/chr",
        "definition": (
            "Time, deadline, temporal constraint. How much time? "
            "Schedule evaluation. Time pressure assessment. "
            "Duration estimation. Calendar awareness."
        ),
    },
    "K3": {
        "name": "Telos (ç›®çš„)",
        "series": "K",
        "command": "/tel",
        "definition": (
            "Purpose, end goal, teleological check. Why this goal? "
            "Means-end inversion prevention. Purpose validation. "
            "Are you solving the right problem?"
        ),
    },
    "K4": {
        "name": "Sophia (çŸ¥æµ)",
        "series": "K",
        "command": "/sop",
        "definition": (
            "Wisdom, research, deep investigation. What does the evidence say? "
            "Literature review. Academic inquiry. Expert consultation. "
            "Evidence-based decision making."
        ),
    },
    # A-series: Akribeia (ç²¾åº¦)
    "A1": {
        "name": "Pathos (æƒ…å¿µ)",
        "series": "A",
        "command": "/pat",
        "definition": (
            "Meta-emotion, emotional evaluation. What are you feeling about your feelings? "
            "Dual tendency assessment. Emotional bias detection. "
            "Sentiment meta-analysis."
        ),
    },
    "A2": {
        "name": "Krisis (åˆ¤å®š)",
        "series": "A",
        "command": "/dia",
        "definition": (
            "Judgment, critical assessment, decision. Is this correct? "
            "Adversarial review. Devil's advocate. Quality verification. "
            "Pass/fail determination. Accuracy validation."
        ),
    },
    "A3": {
        "name": "GnÅmÄ“ (æ ¼è¨€)",
        "series": "A",
        "command": "/gno",
        "definition": (
            "Maxim, principle extraction, lesson learned. What is the rule? "
            "Pattern recognition. Law derivation. Wisdom distillation. "
            "Converting experience into reusable principles."
        ),
    },
    "A4": {
        "name": "EpistÄ“mÄ“ (çŸ¥è­˜)",
        "series": "A",
        "command": "/epi",
        "definition": (
            "Knowledge, belief-to-knowledge promotion. Is this proven? "
            "Evidence-based knowledge establishment. Verification and validation. "
            "Transforming opinion into established fact."
        ),
    },
}

# PURPOSE: X-series morphism å®šç¾© â€” 24 å®šç†é–“ã®é·ç§»é–¢ä¿‚
# WF frontmatter ã® morphisms ã‹ã‚‰æ©Ÿæ¢°çš„ã«æ§‹ç¯‰
# å„å®šç†ã¯ 2 ã¤ã® Series (8 theorems) ã¸ã®å°„ã‚’æŒã¤ = å®šç†ã‚ãŸã‚Šæœ€å¤§ 8 å°„
MORPHISM_MAP: dict[str, list[str]] = {
    # O-series >>S, >>H (Pure, anchor_via=[])
    "O1": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    "O2": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    "O3": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    "O4": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    # S-series >>H, >>K (anchor_via=[O, P])
    "S1": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    "S2": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    "S3": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    "S4": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    # H-series >>S, >>K (anchor_via=[O, A])
    "H1": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    "H2": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    "H3": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    "H4": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    # P-series >>S, >>K (anchor_via=[S, K])
    "P1": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    "P2": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    "P3": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    "P4": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    # K-series >>S, >>H (anchor_via=[P, A])
    "K1": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    "K2": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    "K3": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    "K4": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    # A-series >>H, >>K (anchor_via=[H, K])
    "A1": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    "A2": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    "A3": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    "A4": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
}

# Theorem keys in canonical order
THEOREM_KEYS = [
    "O1", "O2", "O3", "O4",
    "S1", "S2", "S3", "S4",
    "H1", "H2", "H3", "H4",
    "P1", "P2", "P3", "P4",
    "K1", "K2", "K3", "K4",
    "A1", "A2", "A3", "A4",
]
assert len(THEOREM_KEYS) == 24


# ---------------------------------------------------------------------------
# Data Classes
# ---------------------------------------------------------------------------

# PURPOSE: å®šç†ãƒ¬ãƒ™ãƒ«ã® attractor åæŸçµæœ
@dataclass
class TheoremResult:
    """å®šç†ãƒ¬ãƒ™ãƒ«ã® attractor åæŸçµæœ"""
    theorem: str
    name: str
    series: str
    similarity: float
    command: str

    def __repr__(self) -> str:
        return f"âŸ¨{self.theorem}: {self.name} | sim={self.similarity:.3f}âŸ©"


# PURPOSE: X-series flow simulation ã®å„ã‚¹ãƒ†ãƒƒãƒ—
@dataclass
class FlowState:
    """X-series flow simulation ã®å„ã‚¹ãƒ†ãƒƒãƒ—"""
    step: int
    activation: np.ndarray  # (24,)
    top_theorems: list[tuple[str, float]]  # [(theorem, activation), ...]

    def __repr__(self) -> str:
        tops = ", ".join(f"{t}={v:.3f}" for t, v in self.top_theorems[:3])
        return f"âŸ¨Step {self.step}: {tops}âŸ©"


# PURPOSE: Flow simulation ã®å®Œå…¨ãªçµæœ
@dataclass
class FlowResult:
    """Flow simulation ã®å®Œå…¨ãªçµæœ"""
    initial_similarities: list[tuple[str, float]]
    states: list[FlowState]
    converged_at: int  # åæŸã‚¹ãƒ†ãƒƒãƒ— (-1 = æœªåæŸ)
    final_theorems: list[tuple[str, float]]

    def __repr__(self) -> str:
        tops = "+".join(t for t, _ in self.final_theorems[:3])
        return f"âŸ¨Flow: {tops} | converged={self.converged_at}âŸ©"


# PURPOSE: Monte Carlo basin detection ã®çµæœ
@dataclass
class BasinResult:
    """Monte Carlo basin detection ã®çµæœ"""
    n_samples: int
    basin_sizes: dict[str, int]  # {theorem: count}
    basin_fractions: dict[str, float]  # {theorem: fraction}
    elapsed: float

    def __repr__(self) -> str:
        top = sorted(self.basin_fractions.items(), key=lambda x: x[1], reverse=True)[:5]
        tops = ", ".join(f"{t}={v:.1%}" for t, v in top)
        return f"âŸ¨Basins({self.n_samples}): {tops}âŸ©"


# PURPOSE: Q2 â€” 24 å®šç†ã®ç¢ºç‡åˆ†å¸ƒ (èªçŸ¥ã®é…åˆ)
@dataclass
class TheoremMixture:
    """24 å®šç†ã®ç¢ºç‡åˆ†å¸ƒ (é…åˆ).

    argmax çš„ãªã€Œ1ã¤ã®ç­”ãˆã€ã§ã¯ãªãã€
    å…¥åŠ›ãŒã©ã®å®šç†ã«ã©ã‚Œã ã‘å¼•ã‹ã‚Œã¦ã„ã‚‹ã‹ã®å…¨ä½“åƒã‚’æä¾›ã™ã‚‹ã€‚
    """
    distribution: dict[str, float]      # {theorem: probability}, åˆè¨ˆâ‰ˆ1.0
    top_theorems: list[TheoremResult]   # top-K (suggest ã¨åŒã˜)
    entropy: float                      # æ­£è¦åŒ– entropy (0=æ”¯é…çš„, 1=å®Œå…¨å‡ä¸€)
    dominant_series: str                # æœ€ã‚‚æ”¯é…çš„ãª Series
    series_distribution: dict[str, float]  # Series åˆ¥é›†ç´„ {O: 0.3, S: 0.2, ...}
    temperature: float                  # ä½¿ç”¨ã•ã‚ŒãŸ temperature

    def __repr__(self) -> str:
        top = sorted(self.distribution.items(), key=lambda x: x[1], reverse=True)[:3]
        tops = " + ".join(f"{t}({v:.0%})" for t, v in top)
        return f"âŸ¨Mixture: {tops} | H={self.entropy:.2f} | dom={self.dominant_series}âŸ©"


# ---------------------------------------------------------------------------
# TheoremAttractor
# ---------------------------------------------------------------------------

# PURPOSE: 24 å®šç†ãƒ¬ãƒ™ãƒ«ã® Attractor Engine + X-series Flow Simulator
class TheoremAttractor:
    """24 å®šç†ãƒ¬ãƒ™ãƒ«ã® Attractor Engine + X-series Flow Simulator

    Usage:
        ta = TheoremAttractor()

        # 1. å®šç†ãƒ¬ãƒ™ãƒ«å¼•åŠ›è¨ˆç®—
        results = ta.suggest("ãªãœã“ã®è¨­è¨ˆãŒä»Šå¿…è¦ãªã®ã‹")
        # â†’ [âŸ¨O1: NoÄ“sis | sim=0.42âŸ©, âŸ¨S2: MekhanÄ“ | sim=0.41âŸ©, ...]

        # 2. X-series flow simulation
        flow = ta.simulate_flow("ãªãœã“ã®è¨­è¨ˆãŒä»Šå¿…è¦ãªã®ã‹", steps=10)
        # â†’ èªçŸ¥ã®è»Œé“: O1 â†’ S2 (via X-OS) â†’ ...

        # 3. Monte Carlo basin detection (GPU)
        basins = ta.detect_basins(n_samples=10000)
        # â†’ å„å®šç†ã® basin ã‚µã‚¤ã‚ºåˆ†å¸ƒ
    """

    def __init__(self, force_cpu: bool = False):
        self._embedder = None
        self._proto_tensor = None  # (24, D) GPU tensor
        self._transition_matrix = None  # (24, 24) GPU tensor
        self._device = None
        self._force_cpu = force_cpu
        self._initialized = False

    # --- Initialization ---

    def _ensure_initialized(self) -> None:
        if self._initialized:
            return

        from mekhane.anamnesis.index import Embedder
        self._embedder = Embedder(force_cpu=self._force_cpu)

        # 24 å®šç†ã® prototype embedding
        texts = [THEOREM_DEFINITIONS[k]["definition"] for k in THEOREM_KEYS]
        embeddings = self._embedder.embed_batch(texts)
        proto_matrix = np.array(embeddings, dtype=np.float32)

        # X-series é·ç§»è¡Œåˆ— (24Ã—24) â€” Q5: cosine sim based weighting
        T = np.zeros((24, 24), dtype=np.float32)
        key_to_idx = {k: i for i, k in enumerate(THEOREM_KEYS)}

        # Prototype é–“ã® cosine similarity â†’ é·ç§»ã®é‡ã¿
        proto_norms = np.linalg.norm(proto_matrix, axis=1, keepdims=True)
        proto_norms[proto_norms == 0] = 1
        proto_normed = proto_matrix / proto_norms

        for src, targets in MORPHISM_MAP.items():
            src_idx = key_to_idx[src]
            for tgt in targets:
                tgt_idx = key_to_idx[tgt]
                # cosine sim â†’ clamp [0.1, 1.0]
                weight = float(proto_normed[src_idx] @ proto_normed[tgt_idx])
                T[src_idx, tgt_idx] = max(0.1, weight)

        # Row-normalize (ç¢ºç‡é·ç§»è¡Œåˆ—ã«ã™ã‚‹)
        row_sums = T.sum(axis=1, keepdims=True)
        row_sums[row_sums == 0] = 1
        T = T / row_sums

        # Sinkhorn æ­£è¦åŒ–: doubly stochastic ã«è¿‘ä¼¼
        # (in-degree ã®åã‚Šã«ã‚ˆã‚‹ S/H ã¸ã®éé›†ä¸­ã‚’ç·©å’Œ)
        for _ in range(10):
            col_sums = T.sum(axis=0, keepdims=True)
            col_sums[col_sums == 0] = 1
            T = T / col_sums  # column normalize
            row_sums = T.sum(axis=1, keepdims=True)
            row_sums[row_sums == 0] = 1
            T = T / row_sums  # row normalize

        # è‡ªå·±ãƒ«ãƒ¼ãƒ—ã®è¿½åŠ  (å®‰å®šåŒ–)
        alpha = 0.3  # 30% è‡ªå·±ä¿æŒ
        T = (1 - alpha) * T + alpha * np.eye(24, dtype=np.float32)

        # GPU tensor åŒ–
        if TORCH_AVAILABLE:
            from mekhane.fep.gpu import get_device, to_tensor
            self._device = get_device(force_cpu=self._force_cpu)
            self._proto_tensor = to_tensor(proto_matrix, self._device)
            self._transition_matrix = to_tensor(T, self._device)
            print(f"[TheoremAttractor] GPU mode ({self._device}), "
                  f"{len(THEOREM_KEYS)} theorems, "
                  f"{sum(len(v) for v in MORPHISM_MAP.values())} morphisms",
                  flush=True)
        else:
            self._proto_tensor = proto_matrix
            self._transition_matrix = T
            print("[TheoremAttractor] CPU mode", flush=True)

        self._initialized = True

    # --- 1. Theorem-Level Attractor ---

    # PURPOSE: å…¥åŠ›ã«æœ€ã‚‚å¼•åŠ›ã®å¼·ã„å®šç†ã‚’è¿”ã™
    def suggest(self, user_input: str, top_k: int = 5) -> list[TheoremResult]:
        """å…¥åŠ›ã«æœ€ã‚‚å¼•åŠ›ã®å¼·ã„å®šç†ã‚’è¿”ã™."""
        self._ensure_initialized()
        sims = self._compute_similarities(user_input)

        results = []
        for theorem, sim in sorted(sims, key=lambda x: x[1], reverse=True)[:top_k]:
            defn = THEOREM_DEFINITIONS[theorem]
            results.append(TheoremResult(
                theorem=theorem,
                name=defn["name"],
                series=defn["series"],
                similarity=sim,
                command=defn["command"],
            ))
        return results

    # --- 1b. Mixture Diagnosis (Q2) ---

    # PURPOSE: Q2 â€” 24 å®šç†ã®é…åˆã‚’ç¢ºç‡åˆ†å¸ƒã¨ã—ã¦å‡ºåŠ›
    def diagnose_mixture(
        self, user_input: str, temperature: float = 0.05, top_k: int = 5,
    ) -> TheoremMixture:
        """24 å®šç†ã®é…åˆã‚’ç¢ºç‡åˆ†å¸ƒã¨ã—ã¦å‡ºåŠ›.

        argmax (suggest) ãŒã€Œä½•ã€ã‚’è¿”ã™ã®ã«å¯¾ã—ã€
        mixture ã¯ã€Œã©ã‚Œãã‚‰ã„ã®å¼·ã•ã§ã€ã‚’è¿”ã™ã€‚

        Args:
            temperature: ä½ã„=å°–ã£ãŸåˆ†å¸ƒ, é«˜ã„=å¹³å¦ã€‚
                0.05 default â€” similarity range ãŒç‹­ã„ (0.35-0.48) ãŸã‚
                é«˜ã„ T ã§ã¯å‡ä¸€åˆ†å¸ƒã«ãªã‚Šæƒ…å ±é‡ã‚¼ãƒ­ã«ãªã‚‹ã€‚
            top_k: top_theorems ã«å«ã‚ã‚‹æ•°ã€‚
        """
        self._ensure_initialized()
        sims = self._compute_similarities(user_input)

        # similarity â†’ probability distribution (softmax)
        sim_values = np.array(
            [s for _, s in sorted(sims, key=lambda x: THEOREM_KEYS.index(x[0]))],
            dtype=np.float32,
        )
        probs = self._softmax(sim_values, temperature=temperature)

        # distribution dict
        distribution = {k: float(probs[i]) for i, k in enumerate(THEOREM_KEYS)}

        # Series é›†ç´„
        series_dist: dict[str, float] = {}
        for k, p in distribution.items():
            s = THEOREM_DEFINITIONS[k]["series"]
            series_dist[s] = series_dist.get(s, 0.0) + p
        dominant_series = max(series_dist, key=series_dist.get)  # type: ignore

        # Shannon entropy (æ­£è¦åŒ–: 0=æ”¯é…çš„, 1=å®Œå…¨å‡ä¸€)
        max_entropy = math.log(24)
        entropy = -sum(p * math.log(p + 1e-12) for p in probs)
        norm_entropy = entropy / max_entropy if max_entropy > 0 else 0.0

        # top-K TheoremResults
        top_results = self.suggest(user_input, top_k=top_k)

        return TheoremMixture(
            distribution=distribution,
            top_theorems=top_results,
            entropy=round(norm_entropy, 4),
            dominant_series=dominant_series,
            series_distribution={s: round(v, 4) for s, v in series_dist.items()},
            temperature=temperature,
        )

    # --- 1c. Basin Separation (Q1) ---

    # PURPOSE: Q1 â€” 24 å®šç†é–“ã®åˆ†é›¢åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    def basin_separation(self) -> dict:
        """24 å®šç†é–“ã® embedding è·é›¢è¡Œåˆ— + åˆ†é›¢åº¦ã®ä½ã„ãƒšã‚¢ã‚’å ±å‘Š.

        Returns:
            distance_matrix: (24, 24) cosine distance
            closest_pairs: æœ€ã‚‚è¿‘ã„ 5 ãƒšã‚¢
            avg_separation: å¹³å‡åˆ†é›¢åº¦
            min_separation: æœ€å°åˆ†é›¢åº¦
        """
        self._ensure_initialized()
        proto = self._proto_tensor
        if hasattr(proto, 'cpu'):
            proto = proto.cpu().numpy()

        # L2 æ­£è¦åŒ–
        norms = np.linalg.norm(proto, axis=1, keepdims=True)
        norms[norms == 0] = 1
        normed = proto / norms

        # Cosine distance matrix: 1 - cosine_sim
        sim_matrix = normed @ normed.T
        dist_matrix = 1.0 - sim_matrix

        # ä¸Šä¸‰è§’ã® distance ã‚’åé›† (è‡ªåˆ†è‡ªèº«ã‚’é™¤ã)
        pairs = []
        for i in range(24):
            for j in range(i + 1, 24):
                pairs.append((THEOREM_KEYS[i], THEOREM_KEYS[j], float(dist_matrix[i, j])))

        pairs.sort(key=lambda x: x[2])
        dists = [p[2] for p in pairs]

        return {
            "distance_matrix": dist_matrix,
            "closest_pairs": pairs[:5],
            "farthest_pairs": pairs[-5:],
            "avg_separation": float(np.mean(dists)),
            "min_separation": float(min(dists)) if dists else 0,
            "max_separation": float(max(dists)) if dists else 0,
        }

    # --- 2. X-series Flow Simulation ---

    # PURPOSE: å…¥åŠ›ã®åˆæœŸ activation ã‚’ X-series é·ç§»è¡Œåˆ—ã§ä¼æ’­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    def simulate_flow(
        self,
        user_input: str,
        steps: int = 10,
        convergence_threshold: float = 0.001,
    ) -> FlowResult:
        """å…¥åŠ›ã®åˆæœŸ activation ã‚’ X-series é·ç§»è¡Œåˆ—ã§ä¼æ’­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³."""
        self._ensure_initialized()

        # åˆæœŸ activation = cosine similarity
        sims = self._compute_similarities(user_input)
        initial = np.array([s for _, s in sorted(sims, key=lambda x: THEOREM_KEYS.index(x[0]))],
                           dtype=np.float32)

        # Softmax ã§ç¢ºç‡åˆ†å¸ƒåŒ–
        initial = self._softmax(initial, temperature=0.5)

        if TORCH_AVAILABLE and self._device is not None and self._device.type == "cuda":
            states = self._simulate_gpu(initial, steps, convergence_threshold)
        else:
            states = self._simulate_cpu(initial, steps, convergence_threshold)

        # åæŸåˆ¤å®š
        converged_at = -1
        for i in range(1, len(states)):
            diff = np.abs(states[i].activation - states[i-1].activation).max()
            if diff < convergence_threshold:
                converged_at = i
                break

        final_tops = states[-1].top_theorems

        return FlowResult(
            initial_similarities=sorted(sims, key=lambda x: x[1], reverse=True),
            states=states,
            converged_at=converged_at,
            final_theorems=final_tops,
        )

    def _simulate_gpu(self, initial: np.ndarray, steps: int, threshold: float) -> list[FlowState]:
        """GPU è¡Œåˆ—ç©ã§ãƒ•ãƒ­ãƒ¼ä¼æ’­."""
        from mekhane.fep.gpu import to_tensor
        state = to_tensor(initial, self._device)
        T = self._transition_matrix
        states = [self._make_flow_state(0, initial)]

        for step in range(1, steps + 1):
            state = state @ T
            # Re-normalize
            state = state / state.sum()
            state_np = state.cpu().numpy()
            states.append(self._make_flow_state(step, state_np))

            # Early convergence check
            if step > 1:
                diff = np.abs(state_np - states[-2].activation).max()
                if diff < threshold:
                    break

        return states

    def _simulate_cpu(self, initial: np.ndarray, steps: int, threshold: float) -> list[FlowState]:
        """CPU è¡Œåˆ—ç©ã§ãƒ•ãƒ­ãƒ¼ä¼æ’­."""
        state = initial.copy()
        T = self._transition_matrix if isinstance(self._transition_matrix, np.ndarray) \
            else self._transition_matrix.cpu().numpy()
        states = [self._make_flow_state(0, state)]

        for step in range(1, steps + 1):
            state = state @ T
            state = state / state.sum()
            states.append(self._make_flow_state(step, state.copy()))

            if step > 1:
                diff = np.abs(state - states[-2].activation).max()
                if diff < threshold:
                    break

        return states

    # --- 3. Monte Carlo Basin Detection ---

    # PURPOSE: ãƒ©ãƒ³ãƒ€ãƒ  embedding ã§ãƒãƒƒãƒ basin detection â€” GPU ã®çœŸã®å±…å ´æ‰€
    def detect_basins(self, n_samples: int = 10000) -> BasinResult:
        """ãƒ©ãƒ³ãƒ€ãƒ  embedding ã§ãƒãƒƒãƒ basin detection â€” GPU ã®çœŸã®å±…å ´æ‰€.

        å„ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«ã®æœ€ã‚‚è¿‘ã„å®šç† (argmax of cosine similarity) ã‚’è¨ˆç®—ã€‚
        flow ã¯é©ç”¨ã—ãªã„: ã“ã‚Œã¯ semantic space ä¸Šã®ã€Œå½±éŸ¿åœã€ã‚’æ¸¬å®šã™ã‚‹ã€‚
        """
        self._ensure_initialized()
        t0 = time.time()

        if TORCH_AVAILABLE and self._device is not None and self._device.type == "cuda":
            result = self._detect_basins_gpu(n_samples)
        else:
            result = self._detect_basins_cpu(n_samples)

        result.elapsed = time.time() - t0
        return result

    def _detect_basins_gpu(self, n_samples: int) -> BasinResult:
        """GPU ãƒãƒƒãƒ Monte Carlo: (N, D) @ (D, 24) â†’ argmax."""
        import torch
        from mekhane.fep.gpu import batch_cosine_similarity

        D = self._proto_tensor.shape[1]

        # ãƒ©ãƒ³ãƒ€ãƒ  embedding ç”Ÿæˆ (unit sphere ä¸Š)
        random_vecs = torch.randn(n_samples, D, device=self._device, dtype=torch.float32)
        random_vecs = torch.nn.functional.normalize(random_vecs, p=2, dim=-1)

        # ãƒãƒƒãƒ cosine similarity: (N, D) @ (D, 24) â†’ (N, 24)
        sims = batch_cosine_similarity(random_vecs, self._proto_tensor)

        # Argmax: å„ã‚µãƒ³ãƒ—ãƒ«ã®æœ€è¿‘æ¥å®šç†
        basin_indices = sims.argmax(dim=-1).cpu().numpy()

        basin_sizes = {}
        for idx in basin_indices:
            theorem = THEOREM_KEYS[idx]
            basin_sizes[theorem] = basin_sizes.get(theorem, 0) + 1

        basin_fractions = {k: v / n_samples for k, v in basin_sizes.items()}

        return BasinResult(
            n_samples=n_samples,
            basin_sizes=basin_sizes,
            basin_fractions=basin_fractions,
            elapsed=0,
        )

    def _detect_basins_cpu(self, n_samples: int) -> BasinResult:
        """CPU fallback."""
        proto = self._proto_tensor if isinstance(self._proto_tensor, np.ndarray) \
            else self._proto_tensor.cpu().numpy()
        D = proto.shape[1]

        random_vecs = np.random.randn(n_samples, D).astype(np.float32)
        norms = np.linalg.norm(random_vecs, axis=1, keepdims=True)
        random_vecs = random_vecs / norms

        proto_norm = proto / np.linalg.norm(proto, axis=1, keepdims=True)
        sims = random_vecs @ proto_norm.T  # (N, 24)

        basin_indices = sims.argmax(axis=1)

        basin_sizes = {}
        for idx in basin_indices:
            theorem = THEOREM_KEYS[idx]
            basin_sizes[theorem] = basin_sizes.get(theorem, 0) + 1

        basin_fractions = {k: v / n_samples for k, v in basin_sizes.items()}

        return BasinResult(
            n_samples=n_samples,
            basin_sizes=basin_sizes,
            basin_fractions=basin_fractions,
            elapsed=0,
        )

    # --- Internal ---

    def _compute_similarities(self, user_input: str) -> list[tuple[str, float]]:
        """å…¨ 24 å®šç†ã® similarity ã‚’è¨ˆç®—."""
        input_emb = np.array(self._embedder.embed(user_input), dtype=np.float32)

        if TORCH_AVAILABLE and self._device is not None and self._device.type == "cuda":
            from mekhane.fep.gpu import to_tensor, batch_cosine_similarity
            query = to_tensor(input_emb, self._device)
            sims = batch_cosine_similarity(query, self._proto_tensor)
            sims_np = sims.cpu().numpy()
            return [(k, float(sims_np[i])) for i, k in enumerate(THEOREM_KEYS)]
        else:
            proto = self._proto_tensor if isinstance(self._proto_tensor, np.ndarray) \
                else self._proto_tensor.cpu().numpy()
            proto_norm = proto / np.linalg.norm(proto, axis=1, keepdims=True)
            input_norm = input_emb / np.linalg.norm(input_emb)
            sims = input_norm @ proto_norm.T
            return [(k, float(sims[i])) for i, k in enumerate(THEOREM_KEYS)]

    @staticmethod
    def _softmax(x: np.ndarray, temperature: float = 1.0) -> np.ndarray:
        e = np.exp((x - x.max()) / temperature)
        return e / e.sum()

    @staticmethod
    def _softmax_batch(x: np.ndarray, temperature: float = 1.0) -> np.ndarray:
        e = np.exp((x - x.max(axis=1, keepdims=True)) / temperature)
        return e / e.sum(axis=1, keepdims=True)

    @staticmethod
    def _make_flow_state(step: int, activation: np.ndarray) -> FlowState:
        top_indices = np.argsort(activation)[::-1][:5]
        top_theorems = [(THEOREM_KEYS[i], float(activation[i])) for i in top_indices]
        return FlowState(step=step, activation=activation.copy(), top_theorems=top_theorems)


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

# PURPOSE: CLI: python -m mekhane.fep.theorem_attractor "å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ"
def main() -> None:
    """CLI: python -m mekhane.fep.theorem_attractor \"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ\" """
    import sys

    if len(sys.argv) < 2:
        print("Usage: python -m mekhane.fep.theorem_attractor <input_text>")
        print("       python -m mekhane.fep.theorem_attractor --basins [N]")
        sys.exit(1)

    if sys.argv[1] == "--basins":
        n = int(sys.argv[2]) if len(sys.argv) > 2 else 10000
        ta = TheoremAttractor()
        print(f"\nğŸ² Basin Detection (n={n:,})...")
        result = ta.detect_basins(n_samples=n)
        print(f"\n{'='*60}")
        print(f"Basin Map ({result.elapsed:.2f}s)")
        print(f"{'='*60}")
        for theorem in THEOREM_KEYS:
            frac = result.basin_fractions.get(theorem, 0)
            bar = "â–ˆ" * int(frac * 100)
            name = THEOREM_DEFINITIONS[theorem]["name"]
            print(f"  {theorem} {name:20s} {frac:6.1%} {bar}")
        return

    user_input = " ".join(sys.argv[1:])
    ta = TheoremAttractor()

    print(f"\nå…¥åŠ›: {user_input}")
    print("=" * 60)

    # 1. Theorem-level suggest
    results = ta.suggest(user_input, top_k=24)
    print("\nğŸ“Š å…¨ 24 å®šç†ã®å¼•åŠ›ãƒãƒƒãƒ—:")
    for r in results:
        bar = "â–ˆ" * int(r.similarity * 40)
        print(f"  {r.theorem} {r.name:20s} {r.similarity:.3f} {bar}")

    # 2. Flow simulation
    print("\nğŸŒŠ X-series Flow Simulation (10 steps):")
    flow = ta.simulate_flow(user_input, steps=10)
    for state in flow.states:
        tops = ", ".join(f"{t}={v:.3f}" for t, v in state.top_theorems[:3])
        print(f"  Step {state.step:2d}: {tops}")

    if flow.converged_at >= 0:
        print(f"  âœ… Converged at step {flow.converged_at}")
    else:
        print("  â³ Not converged in 10 steps")

    print(f"\nğŸ¯ Final: {' + '.join(t for t, _ in flow.final_theorems[:3])}")


if __name__ == "__main__":
    main()
