# PROOF: [L1/FEP] <- mekhane/fep/
# PURPOSE: 96 è¦ç´ ä½“ç³»ã® Theorem-Level Attractor â€” èªçŸ¥ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿
"""
Theorem-Level Attractor Engine

24 å®šç†ã‚’ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ç©ºé–“ä¸Šã® attractor ã¨ã—ã¦å®šç¾©ã—ã€
72 X-series morphism ã‚’é·ç§»è¡Œåˆ—ã¨ã—ã¦ GPU ä¸Šã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ã€‚

ç†è«–çš„æ ¹æ‹ :
- Spisak & Friston 2025: FEP â†’ è‡ªå·±ç›´äº¤åŒ–ã™ã‚‹ attractor network
- HegemonikÃ³n v3.3: 7å…¬ç† + 24å®šç† + 72é–¢ä¿‚ = 96è¦ç´ ä½“ç³»

Usage:
    from mekhane.fep.theorem_attractor import TheoremAttractor
    ta = TheoremAttractor()
    result = ta.diagnose("ãªãœã“ã®è¨­è¨ˆãŒä»Šå¿…è¦ãªã®ã‹")
    flow = ta.simulate_flow("ãªãœã“ã®è¨­è¨ˆãŒä»Šå¿…è¦ãªã®ã‹", steps=10)
    basins = ta.detect_basins(n_samples=10000)
"""

from __future__ import annotations

import json
import math
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional

import numpy as np

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False


# ---------------------------------------------------------------------------
# 24 Theorem Definitions
# ---------------------------------------------------------------------------

# PURPOSE: å„å®šç†ã®æœ¬è³ªã‚’æ‰ãˆã‚‹å®šç¾©ãƒ†ã‚­ã‚¹ãƒˆ (bge embedding ç”¨ã€è‹±èª)
# NOTE: WF ã® description ã‹ã‚‰æŠ½å‡º + æœ¬è³ªã‚’è‹±èªã§è¨˜è¿°
THEOREM_DEFINITIONS: dict[str, dict] = {
    # O-series: Ousia (æœ¬è³ª)
    "O1": {
        "name": "NoÄ“sis (æ·±ã„èªè­˜)",
        "series": "O",
        "command": "/noe",
        "definition": (
            "Deep cognition, intuitive insight. Recursive self-evidencing. "
            "Premise destruction. Zero-point design. Graph-of-Thought analysis. "
            "The deepest layer of understanding. Why does this truly exist?"
        ),
    },
    "O2": {
        "name": "BoulÄ“sis (æ„å¿—)",
        "series": "O",
        "command": "/bou",
        "definition": (
            "Will, purpose, goal clarification. What do you truly want? "
            "From pure ideal to practical objective. Desire, volition, akrasia. "
            "Priority setting and trade-off analysis."
        ),
    },
    "O3": {
        "name": "ZÄ“tÄ“sis (æ¢æ±‚)",
        "series": "O",
        "command": "/zet",
        "definition": (
            "Inquiry, question discovery. What should be asked? "
            "Finding the seed of the question. Five Whys. Root cause exploration. "
            "Spike and proof-of-concept investigation."
        ),
    },
    "O4": {
        "name": "Energeia (è¡Œç‚º)",
        "series": "O",
        "command": "/ene",
        "definition": (
            "Action, actualization. Turning will into reality. "
            "6-stage execution framework. Feature flags. Staged deployment. "
            "Making things happen. Implementation and delivery."
        ),
    },
    # S-series: Schema (æ§˜æ…‹)
    "S1": {
        "name": "Metron (å°ºåº¦)",
        "series": "S",
        "command": "/met",
        "definition": (
            "Scale, granularity, measurement. How large or small? "
            "Scope determination. Level of abstraction. Resolution setting. "
            "Zoom in or zoom out decision."
        ),
    },
    "S2": {
        "name": "MekhanÄ“ (æ–¹æ³•)",
        "series": "S",
        "command": "/mek",
        "definition": (
            "Method, mechanism, skill arrangement. How to build? "
            "Workflow generation and diagnosis. Tool selection. "
            "Architecture design. Blueprint creation."
        ),
    },
    "S3": {
        "name": "Stathmos (åŸºæº–)",
        "series": "S",
        "command": "/sta",
        "definition": (
            "Standard, benchmark, evaluation criteria. What defines quality? "
            "Acceptance criteria. Performance metrics. Quality gate. "
            "Success criteria definition."
        ),
    },
    "S4": {
        "name": "Praxis (å®Ÿè·µ)",
        "series": "S",
        "command": "/pra",
        "definition": (
            "Practice, value realization. How to deliver value? "
            "Method selection for implementation. Hands-on execution. "
            "Turning design into working system."
        ),
    },
    # H-series: HormÄ“ (å‹•æ©Ÿ)
    "H1": {
        "name": "Propatheia (å‰æ„Ÿæƒ…)",
        "series": "H",
        "command": "/pro",
        "definition": (
            "Pre-emotion, initial impulse, gut feeling. First reaction. "
            "Intuitive tendency. Instinctive response before rational evaluation. "
            "What is your immediate feeling about this?"
        ),
    },
    "H2": {
        "name": "Pistis (ç¢ºä¿¡)",
        "series": "H",
        "command": "/pis",
        "definition": (
            "Conviction, confidence level. How sure are you? "
            "Trust assessment. Reliability evaluation. Epistemic humility. "
            "Certainty vs uncertainty measurement."
        ),
    },
    "H3": {
        "name": "Orexis (æ¬²æ±‚)",
        "series": "H",
        "command": "/ore",
        "definition": (
            "Desire, value tendency. What do you value? "
            "Appetite for change. Motivation assessment. "
            "Passion and drive evaluation. Value alignment check."
        ),
    },
    "H4": {
        "name": "Doxa (ä¿¡å¿µ)",
        "series": "H",
        "command": "/dox",
        "definition": (
            "Belief, opinion, conviction record. What do you believe? "
            "Belief persistence and recording. Worldview documentation. "
            "Assumption tracking and updating."
        ),
    },
    # P-series: PerigraphÄ“ (æ¡ä»¶)
    "P1": {
        "name": "KhÅra (å ´)",
        "series": "P",
        "command": "/kho",
        "definition": (
            "Space, field, domain. Where does this apply? "
            "Scope definition. Boundary setting. Markov blanket delineation. "
            "Context and environment specification."
        ),
    },
    "P2": {
        "name": "Hodos (é“)",
        "series": "P",
        "command": "/hod",
        "definition": (
            "Path, route, trajectory. Which way to go? "
            "Route planning. Step sequence. Roadmap creation. "
            "Migration path and transition strategy."
        ),
    },
    "P3": {
        "name": "Trokhia (è»Œé“)",
        "series": "P",
        "command": "/tro",
        "definition": (
            "Orbit, cycle, iteration scope. How does this repeat? "
            "Application range. Feedback loop. Sprint cycle. "
            "Iterative refinement pattern."
        ),
    },
    "P4": {
        "name": "TekhnÄ“ (æŠ€æ³•)",
        "series": "P",
        "command": "/tek",
        "definition": (
            "Technique, craft, specific tool choice. Which technique? "
            "Tool selection. Technology choice. Implementation technique. "
            "Craft and artisanship in execution."
        ),
    },
    # K-series: Kairos (æ–‡è„ˆ)
    "K1": {
        "name": "Eukairia (å¥½æ©Ÿ)",
        "series": "K",
        "command": "/euk",
        "definition": (
            "Opportunity, right timing. Is now the right moment? "
            "Window of opportunity detection. Timing assessment. "
            "Readiness evaluation. Strategic timing."
        ),
    },
    "K2": {
        "name": "Chronos (æ™‚é–“)",
        "series": "K",
        "command": "/chr",
        "definition": (
            "Time, deadline, temporal constraint. How much time? "
            "Schedule evaluation. Time pressure assessment. "
            "Duration estimation. Calendar awareness."
        ),
    },
    "K3": {
        "name": "Telos (ç›®çš„)",
        "series": "K",
        "command": "/tel",
        "definition": (
            "Purpose, end goal, teleological check. Why this goal? "
            "Means-end inversion prevention. Purpose validation. "
            "Are you solving the right problem?"
        ),
    },
    "K4": {
        "name": "Sophia (çŸ¥æµ)",
        "series": "K",
        "command": "/sop",
        "definition": (
            "Wisdom, research, deep investigation. What does the evidence say? "
            "Literature review. Academic inquiry. Expert consultation. "
            "Evidence-based decision making."
        ),
    },
    # A-series: Akribeia (ç²¾åº¦)
    "A1": {
        "name": "Pathos (æƒ…å¿µ)",
        "series": "A",
        "command": "/pat",
        "definition": (
            "Meta-emotion, emotional evaluation. What are you feeling about your feelings? "
            "Dual tendency assessment. Emotional bias detection. "
            "Sentiment meta-analysis."
        ),
    },
    "A2": {
        "name": "Krisis (åˆ¤å®š)",
        "series": "A",
        "command": "/dia",
        "definition": (
            "Judgment, critical assessment, decision. Is this correct? "
            "Adversarial review. Devil's advocate. Quality verification. "
            "Pass/fail determination. Accuracy validation."
        ),
    },
    "A3": {
        "name": "GnÅmÄ“ (æ ¼è¨€)",
        "series": "A",
        "command": "/gno",
        "definition": (
            "Maxim, principle extraction, lesson learned. What is the rule? "
            "Pattern recognition. Law derivation. Wisdom distillation. "
            "Converting experience into reusable principles."
        ),
    },
    "A4": {
        "name": "EpistÄ“mÄ“ (çŸ¥è­˜)",
        "series": "A",
        "command": "/epi",
        "definition": (
            "Knowledge, belief-to-knowledge promotion. Is this proven? "
            "Evidence-based knowledge establishment. Verification and validation. "
            "Transforming opinion into established fact."
        ),
    },
}

# PURPOSE: X-series morphism å®šç¾© â€” 24 å®šç†é–“ã®é·ç§»é–¢ä¿‚
# WF frontmatter ã® morphisms ã‹ã‚‰æ©Ÿæ¢°çš„ã«æ§‹ç¯‰
# å„å®šç†ã¯ 2 ã¤ã® Series (8 theorems) ã¸ã®å°„ã‚’æŒã¤ = å®šç†ã‚ãŸã‚Šæœ€å¤§ 8 å°„
MORPHISM_MAP: dict[str, list[str]] = {
    # O-series >>S, >>H (Pure, anchor_via=[])
    "O1": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    "O2": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    "O3": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    "O4": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    # S-series >>H, >>K (anchor_via=[O, P])
    "S1": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    "S2": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    "S3": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    "S4": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    # H-series >>S, >>K (anchor_via=[O, A])
    "H1": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    "H2": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    "H3": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    "H4": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    # P-series >>S, >>K (anchor_via=[S, K])
    "P1": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    "P2": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    "P3": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    "P4": ["S1", "S2", "S3", "S4", "K1", "K2", "K3", "K4"],
    # K-series >>S, >>H (anchor_via=[P, A])
    "K1": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    "K2": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    "K3": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    "K4": ["S1", "S2", "S3", "S4", "H1", "H2", "H3", "H4"],
    # A-series >>H, >>K (anchor_via=[H, K])
    "A1": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    "A2": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    "A3": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
    "A4": ["H1", "H2", "H3", "H4", "K1", "K2", "K3", "K4"],
}

# Theorem keys in canonical order
THEOREM_KEYS = [
    "O1", "O2", "O3", "O4",
    "S1", "S2", "S3", "S4",
    "H1", "H2", "H3", "H4",
    "P1", "P2", "P3", "P4",
    "K1", "K2", "K3", "K4",
    "A1", "A2", "A3", "A4",
]
assert len(THEOREM_KEYS) == 24


# ---------------------------------------------------------------------------
# Data Classes
# ---------------------------------------------------------------------------

# PURPOSE: å®šç†ãƒ¬ãƒ™ãƒ«ã® attractor åæŸçµæœ
@dataclass
class TheoremResult:
    """å®šç†ãƒ¬ãƒ™ãƒ«ã® attractor åæŸçµæœ"""
    theorem: str
    name: str
    series: str
    similarity: float
    command: str

    def __repr__(self) -> str:
        return f"âŸ¨{self.theorem}: {self.name} | sim={self.similarity:.3f}âŸ©"


# PURPOSE: X-series flow simulation ã®å„ã‚¹ãƒ†ãƒƒãƒ—
@dataclass
class FlowState:
    """X-series flow simulation ã®å„ã‚¹ãƒ†ãƒƒãƒ—"""
    step: int
    activation: np.ndarray  # (24,)
    top_theorems: list[tuple[str, float]]  # [(theorem, activation), ...]

    def __repr__(self) -> str:
        tops = ", ".join(f"{t}={v:.3f}" for t, v in self.top_theorems[:3])
        return f"âŸ¨Step {self.step}: {tops}âŸ©"


# PURPOSE: Flow simulation ã®å®Œå…¨ãªçµæœ
@dataclass
class FlowResult:
    """Flow simulation ã®å®Œå…¨ãªçµæœ"""
    initial_similarities: list[tuple[str, float]]
    states: list[FlowState]
    converged_at: int  # åæŸã‚¹ãƒ†ãƒƒãƒ— (-1 = æœªåæŸ)
    final_theorems: list[tuple[str, float]]

    def __repr__(self) -> str:
        tops = "+".join(t for t, _ in self.final_theorems[:3])
        return f"âŸ¨Flow: {tops} | converged={self.converged_at}âŸ©"


# PURPOSE: Monte Carlo basin detection ã®çµæœ
@dataclass
class BasinResult:
    """Monte Carlo basin detection ã®çµæœ"""
    n_samples: int
    basin_sizes: dict[str, int]  # {theorem: count}
    basin_fractions: dict[str, float]  # {theorem: fraction}
    elapsed: float

    def __repr__(self) -> str:
        top = sorted(self.basin_fractions.items(), key=lambda x: x[1], reverse=True)[:5]
        tops = ", ".join(f"{t}={v:.1%}" for t, v in top)
        return f"âŸ¨Basins({self.n_samples}): {tops}âŸ©"


# PURPOSE: Q2 â€” 24 å®šç†ã®ç¢ºç‡åˆ†å¸ƒ (èªçŸ¥ã®é…åˆ)
@dataclass
class TheoremMixture:
    """24 å®šç†ã®ç¢ºç‡åˆ†å¸ƒ (é…åˆ).

    argmax çš„ãªã€Œ1ã¤ã®ç­”ãˆã€ã§ã¯ãªãã€
    å…¥åŠ›ãŒã©ã®å®šç†ã«ã©ã‚Œã ã‘å¼•ã‹ã‚Œã¦ã„ã‚‹ã‹ã®å…¨ä½“åƒã‚’æä¾›ã™ã‚‹ã€‚
    """
    distribution: dict[str, float]      # {theorem: probability}, åˆè¨ˆâ‰ˆ1.0
    top_theorems: list[TheoremResult]   # top-K (suggest ã¨åŒã˜)
    entropy: float                      # æ­£è¦åŒ– entropy (0=æ”¯é…çš„, 1=å®Œå…¨å‡ä¸€)
    dominant_series: str                # æœ€ã‚‚æ”¯é…çš„ãª Series
    series_distribution: dict[str, float]  # Series åˆ¥é›†ç´„ {O: 0.3, S: 0.2, ...}
    temperature: float                  # ä½¿ç”¨ã•ã‚ŒãŸ temperature

    def __repr__(self) -> str:
        top = sorted(self.distribution.items(), key=lambda x: x[1], reverse=True)[:3]
        tops = " + ".join(f"{t}({v:.0%})" for t, v in top)
        return f"âŸ¨Mixture: {tops} | H={self.entropy:.2f} | dom={self.dominant_series}âŸ©"


# ---------------------------------------------------------------------------
# Q3: TheoremLogger â€” å®šç†ãƒ¬ãƒ™ãƒ«è¨˜æ†¶
# ---------------------------------------------------------------------------

# PURPOSE: Q3 â€” å®šç†ã®ä½¿ç”¨å±¥æ­´è¿½è·¡ + ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…æ¸›è¡°
class TheoremLogger:
    """å®šç†ã®ä½¿ç”¨å±¥æ­´è¿½è·¡.

    JSONL å½¢å¼ã§ä½¿ç”¨ãƒ­ã‚°ã‚’ä¿å­˜ã—ã€éå°‘ä½¿ç”¨å®šç†ã®ãƒ–ãƒ¼ã‚¹ãƒˆã¨
    ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã®æ—¢ä½¿ç”¨å®šç†ã®æ¸›è¡°ã‚’æä¾›ã™ã‚‹ã€‚
    """
    LOG_DIR = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "logs"

    def __init__(self):
        self.LOG_DIR.mkdir(parents=True, exist_ok=True)

    def _log_path(self) -> Path:
        """ä»Šæ—¥ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹."""
        from datetime import date
        return self.LOG_DIR / f"theorem_log_{date.today().isoformat()}.jsonl"

    def log(self, theorem: str, input_text: str, similarity: float) -> None:
        """ä½¿ç”¨ã‚’è¨˜éŒ²."""
        entry = {
            "ts": time.time(),
            "theorem": theorem,
            "input": input_text[:100],
            "sim": round(similarity, 4),
        }
        try:
            with open(self._log_path(), "a", encoding="utf-8") as f:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        except Exception:
            pass  # Logging failure should not break attractor

    def get_usage_counts(self, days: int = 7) -> dict[str, int]:
        """ç›´è¿‘ N æ—¥é–“ã®å®šç†åˆ¥ä½¿ç”¨å›æ•°."""
        counts: dict[str, int] = {k: 0 for k in THEOREM_KEYS}
        from datetime import date, timedelta
        for d in range(days):
            day = date.today() - timedelta(days=d)
            path = self.LOG_DIR / f"theorem_log_{day.isoformat()}.jsonl"
            if not path.exists():
                continue
            try:
                for line in path.read_text(encoding="utf-8").strip().split("\n"):
                    if not line:
                        continue
                    entry = json.loads(line)
                    t = entry.get("theorem", "")
                    if t in counts:
                        counts[t] += 1
            except Exception:
                continue
        return counts

    def compute_novelty_boost(self, days: int = 7) -> dict[str, float]:
        """éå°‘ä½¿ç”¨å®šç†ã®ãƒ–ãƒ¼ã‚¹ãƒˆä¿‚æ•° (1.0 = å¹³å‡, >1.0 = éå°‘ä½¿ç”¨)."""
        counts = self.get_usage_counts(days)
        total = sum(counts.values())
        if total == 0:
            return {k: 1.0 for k in THEOREM_KEYS}
        avg = total / 24
        boost = {}
        for k in THEOREM_KEYS:
            if counts[k] == 0:
                boost[k] = 1.5  # æœªä½¿ç”¨ â†’ 50% ãƒ–ãƒ¼ã‚¹ãƒˆ
            elif counts[k] < avg:
                boost[k] = 1.0 + 0.5 * (1 - counts[k] / avg)  # 1.0-1.5
            else:
                boost[k] = 1.0  # ååˆ†ä½¿ç”¨æ¸ˆã¿ â†’ ãƒ–ãƒ¼ã‚¹ãƒˆãªã—
        return boost


# ---------------------------------------------------------------------------
# TheoremAttractor
# ---------------------------------------------------------------------------

# PURPOSE: 24 å®šç†ãƒ¬ãƒ™ãƒ«ã® Attractor Engine + X-series Flow Simulator
class TheoremAttractor:
    """24 å®šç†ãƒ¬ãƒ™ãƒ«ã® Attractor Engine + X-series Flow Simulator

    Usage:
        ta = TheoremAttractor()

        # 1. å®šç†ãƒ¬ãƒ™ãƒ«å¼•åŠ›è¨ˆç®—
        results = ta.suggest("ãªãœã“ã®è¨­è¨ˆãŒä»Šå¿…è¦ãªã®ã‹")
        # â†’ [âŸ¨O1: NoÄ“sis | sim=0.42âŸ©, âŸ¨S2: MekhanÄ“ | sim=0.41âŸ©, ...]

        # 2. X-series flow simulation
        flow = ta.simulate_flow("ãªãœã“ã®è¨­è¨ˆãŒä»Šå¿…è¦ãªã®ã‹", steps=10)
        # â†’ èªçŸ¥ã®è»Œé“: O1 â†’ S2 (via X-OS) â†’ ...

        # 3. Monte Carlo basin detection (GPU)
        basins = ta.detect_basins(n_samples=10000)
        # â†’ å„å®šç†ã® basin ã‚µã‚¤ã‚ºåˆ†å¸ƒ
    """

    def __init__(self, force_cpu: bool = False, enable_memory: bool = True):
        self._embedder = None
        self._proto_tensor = None  # (24, D) GPU tensor
        self._transition_matrix = None  # (24, 24) GPU tensor
        self._device = None
        self._force_cpu = force_cpu
        self._initialized = False
        # Q3: å®šç†ãƒ¬ãƒ™ãƒ«è¨˜æ†¶
        self._logger = TheoremLogger() if enable_memory else None
        self._session_used: list[str] = []  # ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§ä½¿ç”¨ã—ãŸå®šç†
        self._decay_factor = 0.7  # ä½¿ç”¨æ¸ˆã¿å®šç†ã® similarity æ¸›è¡°ç‡
        # Q4: multiview prototype
        self._multiview_proto = None  # (24, D) definition+WF
        # Q7: inhibition matrix
        self._inhibition_matrix = None  # (24, 24)

    # --- Initialization ---

    def _ensure_initialized(self) -> None:
        if self._initialized:
            return

        from mekhane.anamnesis.index import Embedder
        self._embedder = Embedder(force_cpu=self._force_cpu)

        # 24 å®šç†ã® prototype embedding
        texts = [THEOREM_DEFINITIONS[k]["definition"] for k in THEOREM_KEYS]
        embeddings = self._embedder.embed_batch(texts)
        proto_matrix = np.array(embeddings, dtype=np.float32)

        # X-series é·ç§»è¡Œåˆ— (24Ã—24) â€” Q5: cosine sim based weighting
        T = np.zeros((24, 24), dtype=np.float32)
        key_to_idx = {k: i for i, k in enumerate(THEOREM_KEYS)}

        # Prototype é–“ã® cosine similarity â†’ é·ç§»ã®é‡ã¿
        proto_norms = np.linalg.norm(proto_matrix, axis=1, keepdims=True)
        proto_norms[proto_norms == 0] = 1
        proto_normed = proto_matrix / proto_norms

        for src, targets in MORPHISM_MAP.items():
            src_idx = key_to_idx[src]
            for tgt in targets:
                tgt_idx = key_to_idx[tgt]
                # cosine sim â†’ clamp [0.1, 1.0]
                weight = float(proto_normed[src_idx] @ proto_normed[tgt_idx])
                T[src_idx, tgt_idx] = max(0.1, weight)

        # Row-normalize (ç¢ºç‡é·ç§»è¡Œåˆ—ã«ã™ã‚‹)
        row_sums = T.sum(axis=1, keepdims=True)
        row_sums[row_sums == 0] = 1
        T = T / row_sums

        # Sinkhorn æ­£è¦åŒ–: doubly stochastic ã«è¿‘ä¼¼
        # (in-degree ã®åã‚Šã«ã‚ˆã‚‹ S/H ã¸ã®éé›†ä¸­ã‚’ç·©å’Œ)
        for _ in range(10):
            col_sums = T.sum(axis=0, keepdims=True)
            col_sums[col_sums == 0] = 1
            T = T / col_sums  # column normalize
            row_sums = T.sum(axis=1, keepdims=True)
            row_sums[row_sums == 0] = 1
            T = T / row_sums  # row normalize

        # è‡ªå·±ãƒ«ãƒ¼ãƒ—ã®è¿½åŠ  (å®‰å®šåŒ–)
        alpha = 0.3  # 30% è‡ªå·±ä¿æŒ
        T = (1 - alpha) * T + alpha * np.eye(24, dtype=np.float32)

        # GPU tensor åŒ–
        if TORCH_AVAILABLE:
            from mekhane.fep.gpu import get_device, to_tensor
            self._device = get_device(force_cpu=self._force_cpu)
            self._proto_tensor = to_tensor(proto_matrix, self._device)
            self._transition_matrix = to_tensor(T, self._device)
            print(f"[TheoremAttractor] GPU mode ({self._device}), "
                  f"{len(THEOREM_KEYS)} theorems, "
                  f"{sum(len(v) for v in MORPHISM_MAP.values())} morphisms",
                  flush=True)
        else:
            self._proto_tensor = proto_matrix
            self._transition_matrix = T
            print("[TheoremAttractor] CPU mode", flush=True)

        self._initialized = True

        # Q4: Multiview prototype (definition 0.7 + WF description 0.3)
        self._multiview_proto = self._build_multiview_proto(proto_matrix)

        # Q7: Inhibition matrix (cosine distance based)
        self._inhibition_matrix = self._build_inhibition_matrix(proto_normed)

    # --- Private Builders (Q4, Q7) ---

    # PURPOSE: Q4 â€” Definition + WF description ã® multiview prototype æ§‹ç¯‰
    def _build_multiview_proto(self, def_matrix: np.ndarray) -> Optional[np.ndarray]:
        """Definition (0.7) + WF description (0.3) ã®åŠ é‡å¹³å‡ embedding.

        WF ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å®šç†ã¯ definition ã®ã¿ (weight=1.0)ã€‚
        """
        wf_dir = Path(__file__).parent.parent.parent / ".agent" / "workflows"
        if not wf_dir.exists():
            return None

        # å„å®šç†ã® WF description ã‚’åé›†
        wf_texts = []
        for k in THEOREM_KEYS:
            cmd = THEOREM_DEFINITIONS[k]["command"].lstrip("/")
            wf_path = wf_dir / f"{cmd}.md"
            desc = ""
            if wf_path.exists():
                try:
                    content = wf_path.read_text(encoding="utf-8")
                    # YAML frontmatter ã® description ã‚’æŠ½å‡º
                    if content.startswith("---"):
                        parts = content.split("---", 2)
                        if len(parts) >= 3:
                            for line in parts[1].strip().split("\n"):
                                if line.startswith("description:"):
                                    desc = line.split(":", 1)[1].strip().strip('"').strip("'")
                                    break
                    if not desc:
                        # Fallback: æœ€åˆã®éç©ºè¡Œ (h1 ä»¥é™)
                        for line in content.split("\n"):
                            stripped = line.strip()
                            if stripped and not stripped.startswith("#") and not stripped.startswith("---"):
                                desc = stripped[:200]
                                break
                except Exception:
                    pass
            wf_texts.append(desc if desc else THEOREM_DEFINITIONS[k]["definition"])

        # WF embedding
        wf_embeddings = self._embedder.embed_batch(wf_texts)
        wf_matrix = np.array(wf_embeddings, dtype=np.float32)

        # Multiview: weighted average
        alpha = 0.7  # definition weight
        multiview = alpha * def_matrix + (1 - alpha) * wf_matrix

        print(f"[TheoremAttractor] Q4: multiview proto built "
              f"(def={alpha:.0%} + wf={1-alpha:.0%})", flush=True)
        return multiview

    # PURPOSE: Q7 â€” æŠ‘åˆ¶è¡Œåˆ—æ§‹ç¯‰ (cosine distance â†’ inhibition strength)
    def _build_inhibition_matrix(self, proto_normed: np.ndarray) -> np.ndarray:
        """Cosine distance > threshold â†’ inhibition strength.

        è·é›¢ãŒå¤§ãã„ã»ã©æŠ‘åˆ¶ãŒå¼·ã„ã€‚åŒä¸€ theorem ã¯ 0ã€‚
        """
        sim_matrix = proto_normed @ proto_normed.T
        dist_matrix = 1.0 - sim_matrix
        np.fill_diagonal(dist_matrix, 0)
        return dist_matrix.astype(np.float32)

    # --- 1. Theorem-Level Attractor ---

    # PURPOSE: å…¥åŠ›ã«æœ€ã‚‚å¼•åŠ›ã®å¼·ã„å®šç†ã‚’è¿”ã™ (Q3: ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…æ¸›è¡°ä»˜ã)
    def suggest(self, user_input: str, top_k: int = 5) -> list[TheoremResult]:
        """å…¥åŠ›ã«æœ€ã‚‚å¼•åŠ›ã®å¼·ã„å®šç†ã‚’è¿”ã™.

        Q3: ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§æ—¢ã«ä½¿ç”¨ã—ãŸå®šç†ã¯ similarity ã‚’ decay_factor (0.7x) ã§æ¸›è¡°ã€‚
        ã“ã‚Œã«ã‚ˆã‚ŠåŒä¸€ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§ç•°ãªã‚‹å®šç†ã¸ã®æ¢ç´¢ã‚’ä¿ƒã™ã€‚
        """
        self._ensure_initialized()
        sims = self._compute_similarities(user_input)

        # Q3: ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…æ¸›è¡°
        if self._session_used:
            sims = [
                (t, s * self._decay_factor if t in self._session_used else s)
                for t, s in sims
            ]

        results = []
        for theorem, sim in sorted(sims, key=lambda x: x[1], reverse=True)[:top_k]:
            defn = THEOREM_DEFINITIONS[theorem]
            results.append(TheoremResult(
                theorem=theorem,
                name=defn["name"],
                series=defn["series"],
                similarity=sim,
                command=defn["command"],
            ))

        # Q3: top-1 ã‚’ãƒ­ã‚° + ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½¿ç”¨æ¸ˆã¿ã«è¿½åŠ 
        if results and self._logger:
            top = results[0]
            self._logger.log(top.theorem, user_input, top.similarity)
            if top.theorem not in self._session_used:
                self._session_used.append(top.theorem)

        return results

    # --- 1b. Mixture Diagnosis (Q2) ---

    # PURPOSE: Q2 â€” 24 å®šç†ã®é…åˆã‚’ç¢ºç‡åˆ†å¸ƒã¨ã—ã¦å‡ºåŠ›
    def diagnose_mixture(
        self, user_input: str, temperature: float = 0.05, top_k: int = 5,
    ) -> TheoremMixture:
        """24 å®šç†ã®é…åˆã‚’ç¢ºç‡åˆ†å¸ƒã¨ã—ã¦å‡ºåŠ›.

        argmax (suggest) ãŒã€Œä½•ã€ã‚’è¿”ã™ã®ã«å¯¾ã—ã€
        mixture ã¯ã€Œã©ã‚Œãã‚‰ã„ã®å¼·ã•ã§ã€ã‚’è¿”ã™ã€‚

        Args:
            temperature: ä½ã„=å°–ã£ãŸåˆ†å¸ƒ, é«˜ã„=å¹³å¦ã€‚
                0.05 default â€” similarity range ãŒç‹­ã„ (0.35-0.48) ãŸã‚
                é«˜ã„ T ã§ã¯å‡ä¸€åˆ†å¸ƒã«ãªã‚Šæƒ…å ±é‡ã‚¼ãƒ­ã«ãªã‚‹ã€‚
            top_k: top_theorems ã«å«ã‚ã‚‹æ•°ã€‚
        """
        self._ensure_initialized()
        sims = self._compute_similarities(user_input)

        # similarity â†’ probability distribution (softmax)
        sim_values = np.array(
            [s for _, s in sorted(sims, key=lambda x: THEOREM_KEYS.index(x[0]))],
            dtype=np.float32,
        )
        probs = self._softmax(sim_values, temperature=temperature)

        # distribution dict
        distribution = {k: float(probs[i]) for i, k in enumerate(THEOREM_KEYS)}

        # Series é›†ç´„
        series_dist: dict[str, float] = {}
        for k, p in distribution.items():
            s = THEOREM_DEFINITIONS[k]["series"]
            series_dist[s] = series_dist.get(s, 0.0) + p
        dominant_series = max(series_dist, key=series_dist.get)  # type: ignore

        # Shannon entropy (æ­£è¦åŒ–: 0=æ”¯é…çš„, 1=å®Œå…¨å‡ä¸€)
        max_entropy = math.log(24)
        entropy = -sum(p * math.log(p + 1e-12) for p in probs)
        norm_entropy = entropy / max_entropy if max_entropy > 0 else 0.0

        # top-K TheoremResults
        top_results = self.suggest(user_input, top_k=top_k)

        return TheoremMixture(
            distribution=distribution,
            top_theorems=top_results,
            entropy=round(norm_entropy, 4),
            dominant_series=dominant_series,
            series_distribution={s: round(v, 4) for s, v in series_dist.items()},
            temperature=temperature,
        )

    # --- 1c. Basin Separation (Q1) ---

    # PURPOSE: Q1 â€” 24 å®šç†é–“ã®åˆ†é›¢åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    def basin_separation(self) -> dict:
        """24 å®šç†é–“ã® embedding è·é›¢è¡Œåˆ— + åˆ†é›¢åº¦ã®ä½ã„ãƒšã‚¢ã‚’å ±å‘Š.

        Returns:
            distance_matrix: (24, 24) cosine distance
            closest_pairs: æœ€ã‚‚è¿‘ã„ 5 ãƒšã‚¢
            avg_separation: å¹³å‡åˆ†é›¢åº¦
            min_separation: æœ€å°åˆ†é›¢åº¦
        """
        self._ensure_initialized()
        proto = self._proto_tensor
        if hasattr(proto, 'cpu'):
            proto = proto.cpu().numpy()

        # L2 æ­£è¦åŒ–
        norms = np.linalg.norm(proto, axis=1, keepdims=True)
        norms[norms == 0] = 1
        normed = proto / norms

        # Cosine distance matrix: 1 - cosine_sim
        sim_matrix = normed @ normed.T
        dist_matrix = 1.0 - sim_matrix

        # ä¸Šä¸‰è§’ã® distance ã‚’åé›† (è‡ªåˆ†è‡ªèº«ã‚’é™¤ã)
        pairs = []
        for i in range(24):
            for j in range(i + 1, 24):
                pairs.append((THEOREM_KEYS[i], THEOREM_KEYS[j], float(dist_matrix[i, j])))

        pairs.sort(key=lambda x: x[2])
        dists = [p[2] for p in pairs]

        return {
            "distance_matrix": dist_matrix,
            "closest_pairs": pairs[:5],
            "farthest_pairs": pairs[-5:],
            "avg_separation": float(np.mean(dists)),
            "min_separation": float(min(dists)) if dists else 0,
            "max_separation": float(max(dists)) if dists else 0,
        }

    # --- 1d. Multiview Suggest (Q4) ---

    # PURPOSE: Q4 â€” WF å…¨æ–‡ã‚’åŠ å‘³ã—ãŸ multiview prototype ã§ suggest
    def suggest_multiview(self, user_input: str, top_k: int = 5) -> list[TheoremResult]:
        """Q4: Definition + WF description ã® multiview prototype ã§ suggest.

        é€šå¸¸ã® suggest() ãŒ definition ã®ã¿ã‚’è¦‹ã‚‹ã®ã«å¯¾ã—ã€
        ã“ã¡ã‚‰ã¯ WF ã® description ã‚‚åŠ å‘³ã—ãŸåºƒã„è¦–é‡ã§åˆ¤å®šã™ã‚‹ã€‚
        """
        self._ensure_initialized()
        if self._multiview_proto is None:
            return self.suggest(user_input, top_k=top_k)  # fallback

        input_emb = self._embedder.embed_batch([user_input])
        input_vec = np.array(input_emb[0], dtype=np.float32)

        # multiview proto ã¨ã® cosine similarity
        mv_proto = self._multiview_proto
        if hasattr(mv_proto, 'cpu'):
            mv_proto = mv_proto.cpu().numpy()

        proto_norms = np.linalg.norm(mv_proto, axis=1)
        input_norm = np.linalg.norm(input_vec)
        if input_norm == 0:
            input_norm = 1
        proto_norms[proto_norms == 0] = 1
        sims = (mv_proto @ input_vec) / (proto_norms * input_norm)

        results = []
        indexed_sims = [(THEOREM_KEYS[i], float(sims[i])) for i in range(24)]
        for theorem, sim in sorted(indexed_sims, key=lambda x: x[1], reverse=True)[:top_k]:
            defn = THEOREM_DEFINITIONS[theorem]
            results.append(TheoremResult(
                theorem=theorem,
                name=defn["name"],
                series=defn["series"],
                similarity=sim,
                command=defn["command"],
            ))
        return results

    # --- 1e. Inhibition Query (Q7) ---

    # PURPOSE: Q7 â€” ã‚ã‚‹å®šç†ã®â€œæŠ‘åˆ¶å¯¾è±¡â€ã‚’è¿”ã™
    def get_inhibited(self, theorem: str, threshold: float = 0.5) -> list[tuple[str, float]]:
        """Q7: æŒ‡å®šå®šç†ãŒæŠ‘åˆ¶ã™ã‚‹å®šç†ã‚’è¿”ã™.

        æŠ‘åˆ¶ = cosine distance > threshold ã®å®šç†ã€‚
        ã€Œã“ã®å®šç†ãŒæ´»æ€§åŒ–ã™ã‚‹ã¨ãã€ã©ã®å®šç†ãŒæŠ¼ã•ã‚Œã‚‹ã‹ã€ã€‚

        Args:
            theorem: e.g. "O1"
            threshold: inhibition threshold (default 0.5)

        Returns:
            [(theorem, inhibition_strength), ...] sorted by strength
        """
        self._ensure_initialized()
        if self._inhibition_matrix is None or theorem not in THEOREM_KEYS:
            return []

        idx = THEOREM_KEYS.index(theorem)
        inhib = self._inhibition_matrix
        if hasattr(inhib, 'cpu'):
            inhib = inhib.cpu().numpy()

        row = inhib[idx]
        pairs = []
        for i, strength in enumerate(row):
            if i != idx and strength > threshold:
                pairs.append((THEOREM_KEYS[i], float(strength)))
        return sorted(pairs, key=lambda x: x[1], reverse=True)

    # --- 1f. Keyword Decomposition (Q6) ---

    # PURPOSE: Q6 â€” å…¥åŠ›ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†è§£ã—ã€å„è¦ç´ ã§ suggest
    def suggest_decomposed(self, user_input: str, top_k: int = 3) -> dict:
        """Q6: å…¥åŠ›ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†è§£ã—ã€å„è¦ç´ ã§åˆ¥ã€…ã« suggest.

        LLM-free: å˜ç´”ãªãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²ã§ã€Œä½•ã‚’ã€ã€Œã©ã†ã€ã€Œãªãœã€ã‚’æŠ½å‡ºã€‚
        å®Œå…¨ãªåˆ†è§£ã¯ã›ãšã€ã€ŒN-gram çª“ã€ã§å…¥åŠ›ã®éƒ¨åˆ†ã”ã¨ã« Attractor ã‚’é€šã™ã€‚

        Returns:
            {
                "full": [TheoremResult, ...],  # å…¨æ–‡ã§ã® suggest
                "segments": [
                    {"text": str, "theorems": [TheoremResult, ...]},
                    ...
                ],
                "divergence": float,  # segment é–“ã®ä¸ä¸€è‡´åº¦
            }
        """
        self._ensure_initialized()

        # å…¨æ–‡ suggest
        full_results = self.suggest(user_input, top_k=top_k)

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²: å¥ç‚¹ã€ã€Œã€ã€ã€Œã€‚ã€ã€Œã€ã€ã€Œã€‚ã€ ã§åˆ†å‰²
        import re
        segments_text = re.split(r'[ã€ã€‚,.ã€€]+', user_input)
        segments_text = [s.strip() for s in segments_text if len(s.strip()) > 3]

        if len(segments_text) <= 1:
            # åˆ†è§£ã§ããªã„ (1 segment)
            return {
                "full": full_results,
                "segments": [{"text": user_input, "theorems": full_results}],
                "divergence": 0.0,
            }

        segments = []
        all_top1 = set()
        for seg in segments_text:
            seg_results = self.suggest(seg, top_k=top_k)
            segments.append({"text": seg, "theorems": seg_results})
            if seg_results:
                all_top1.add(seg_results[0].theorem)

        # divergence: ä½•å€‹ã®ç•°ãªã‚‹ top-1 theorem ãŒã‚ã‚‹ã‹
        # 1 = å…¨ segment ãŒåŒã˜ theorem, n/n = å…¨ segment ãŒç•°ãªã‚‹ theorem
        n_seg = len(segments)
        divergence = (len(all_top1) - 1) / max(n_seg - 1, 1) if n_seg > 1 else 0.0

        return {
            "full": full_results,
            "segments": segments,
            "divergence": round(divergence, 3),
        }

    # --- 2. X-series Flow Simulation ---

    # PURPOSE: å…¥åŠ›ã®åˆæœŸ activation ã‚’ X-series é·ç§»è¡Œåˆ—ã§ä¼æ’­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    def simulate_flow(
        self,
        user_input: str,
        steps: int = 10,
        convergence_threshold: float = 0.001,
    ) -> FlowResult:
        """å…¥åŠ›ã®åˆæœŸ activation ã‚’ X-series é·ç§»è¡Œåˆ—ã§ä¼æ’­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³."""
        self._ensure_initialized()

        # åˆæœŸ activation = cosine similarity
        sims = self._compute_similarities(user_input)
        initial = np.array([s for _, s in sorted(sims, key=lambda x: THEOREM_KEYS.index(x[0]))],
                           dtype=np.float32)

        # Softmax ã§ç¢ºç‡åˆ†å¸ƒåŒ–
        initial = self._softmax(initial, temperature=0.5)

        if TORCH_AVAILABLE and self._device is not None and self._device.type == "cuda":
            states = self._simulate_gpu(initial, steps, convergence_threshold)
        else:
            states = self._simulate_cpu(initial, steps, convergence_threshold)

        # åæŸåˆ¤å®š
        converged_at = -1
        for i in range(1, len(states)):
            diff = np.abs(states[i].activation - states[i-1].activation).max()
            if diff < convergence_threshold:
                converged_at = i
                break

        final_tops = states[-1].top_theorems

        return FlowResult(
            initial_similarities=sorted(sims, key=lambda x: x[1], reverse=True),
            states=states,
            converged_at=converged_at,
            final_theorems=final_tops,
        )

    def _simulate_gpu(self, initial: np.ndarray, steps: int, threshold: float) -> list[FlowState]:
        """GPU è¡Œåˆ—ç©ã§ãƒ•ãƒ­ãƒ¼ä¼æ’­."""
        from mekhane.fep.gpu import to_tensor
        state = to_tensor(initial, self._device)
        T = self._transition_matrix
        states = [self._make_flow_state(0, initial)]

        for step in range(1, steps + 1):
            state = state @ T
            # Re-normalize
            state = state / state.sum()
            state_np = state.cpu().numpy()
            states.append(self._make_flow_state(step, state_np))

            # Early convergence check
            if step > 1:
                diff = np.abs(state_np - states[-2].activation).max()
                if diff < threshold:
                    break

        return states

    def _simulate_cpu(self, initial: np.ndarray, steps: int, threshold: float) -> list[FlowState]:
        """CPU è¡Œåˆ—ç©ã§ãƒ•ãƒ­ãƒ¼ä¼æ’­."""
        state = initial.copy()
        T = self._transition_matrix if isinstance(self._transition_matrix, np.ndarray) \
            else self._transition_matrix.cpu().numpy()
        states = [self._make_flow_state(0, state)]

        for step in range(1, steps + 1):
            state = state @ T
            state = state / state.sum()
            states.append(self._make_flow_state(step, state.copy()))

            if step > 1:
                diff = np.abs(state - states[-2].activation).max()
                if diff < threshold:
                    break

        return states

    # --- 3. Monte Carlo Basin Detection ---

    # PURPOSE: ãƒ©ãƒ³ãƒ€ãƒ  embedding ã§ãƒãƒƒãƒ basin detection â€” GPU ã®çœŸã®å±…å ´æ‰€
    def detect_basins(self, n_samples: int = 10000) -> BasinResult:
        """ãƒ©ãƒ³ãƒ€ãƒ  embedding ã§ãƒãƒƒãƒ basin detection â€” GPU ã®çœŸã®å±…å ´æ‰€.

        å„ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«ã®æœ€ã‚‚è¿‘ã„å®šç† (argmax of cosine similarity) ã‚’è¨ˆç®—ã€‚
        flow ã¯é©ç”¨ã—ãªã„: ã“ã‚Œã¯ semantic space ä¸Šã®ã€Œå½±éŸ¿åœã€ã‚’æ¸¬å®šã™ã‚‹ã€‚
        """
        self._ensure_initialized()
        t0 = time.time()

        if TORCH_AVAILABLE and self._device is not None and self._device.type == "cuda":
            result = self._detect_basins_gpu(n_samples)
        else:
            result = self._detect_basins_cpu(n_samples)

        result.elapsed = time.time() - t0
        return result

    def _detect_basins_gpu(self, n_samples: int) -> BasinResult:
        """GPU ãƒãƒƒãƒ Monte Carlo: (N, D) @ (D, 24) â†’ argmax."""
        import torch
        from mekhane.fep.gpu import batch_cosine_similarity

        D = self._proto_tensor.shape[1]

        # ãƒ©ãƒ³ãƒ€ãƒ  embedding ç”Ÿæˆ (unit sphere ä¸Š)
        random_vecs = torch.randn(n_samples, D, device=self._device, dtype=torch.float32)
        random_vecs = torch.nn.functional.normalize(random_vecs, p=2, dim=-1)

        # ãƒãƒƒãƒ cosine similarity: (N, D) @ (D, 24) â†’ (N, 24)
        sims = batch_cosine_similarity(random_vecs, self._proto_tensor)

        # Argmax: å„ã‚µãƒ³ãƒ—ãƒ«ã®æœ€è¿‘æ¥å®šç†
        basin_indices = sims.argmax(dim=-1).cpu().numpy()

        basin_sizes = {}
        for idx in basin_indices:
            theorem = THEOREM_KEYS[idx]
            basin_sizes[theorem] = basin_sizes.get(theorem, 0) + 1

        basin_fractions = {k: v / n_samples for k, v in basin_sizes.items()}

        return BasinResult(
            n_samples=n_samples,
            basin_sizes=basin_sizes,
            basin_fractions=basin_fractions,
            elapsed=0,
        )

    def _detect_basins_cpu(self, n_samples: int) -> BasinResult:
        """CPU fallback."""
        proto = self._proto_tensor if isinstance(self._proto_tensor, np.ndarray) \
            else self._proto_tensor.cpu().numpy()
        D = proto.shape[1]

        random_vecs = np.random.randn(n_samples, D).astype(np.float32)
        norms = np.linalg.norm(random_vecs, axis=1, keepdims=True)
        random_vecs = random_vecs / norms

        proto_norm = proto / np.linalg.norm(proto, axis=1, keepdims=True)
        sims = random_vecs @ proto_norm.T  # (N, 24)

        basin_indices = sims.argmax(axis=1)

        basin_sizes = {}
        for idx in basin_indices:
            theorem = THEOREM_KEYS[idx]
            basin_sizes[theorem] = basin_sizes.get(theorem, 0) + 1

        basin_fractions = {k: v / n_samples for k, v in basin_sizes.items()}

        return BasinResult(
            n_samples=n_samples,
            basin_sizes=basin_sizes,
            basin_fractions=basin_fractions,
            elapsed=0,
        )

    # --- Internal ---

    def _compute_similarities(self, user_input: str) -> list[tuple[str, float]]:
        """å…¨ 24 å®šç†ã® similarity ã‚’è¨ˆç®—."""
        input_emb = np.array(self._embedder.embed(user_input), dtype=np.float32)

        if TORCH_AVAILABLE and self._device is not None and self._device.type == "cuda":
            from mekhane.fep.gpu import to_tensor, batch_cosine_similarity
            query = to_tensor(input_emb, self._device)
            sims = batch_cosine_similarity(query, self._proto_tensor)
            sims_np = sims.cpu().numpy()
            return [(k, float(sims_np[i])) for i, k in enumerate(THEOREM_KEYS)]
        else:
            proto = self._proto_tensor if isinstance(self._proto_tensor, np.ndarray) \
                else self._proto_tensor.cpu().numpy()
            proto_norm = proto / np.linalg.norm(proto, axis=1, keepdims=True)
            input_norm = input_emb / np.linalg.norm(input_emb)
            sims = input_norm @ proto_norm.T
            return [(k, float(sims[i])) for i, k in enumerate(THEOREM_KEYS)]

    @staticmethod
    def _softmax(x: np.ndarray, temperature: float = 1.0) -> np.ndarray:
        e = np.exp((x - x.max()) / temperature)
        return e / e.sum()

    @staticmethod
    def _softmax_batch(x: np.ndarray, temperature: float = 1.0) -> np.ndarray:
        e = np.exp((x - x.max(axis=1, keepdims=True)) / temperature)
        return e / e.sum(axis=1, keepdims=True)

    @staticmethod
    def _make_flow_state(step: int, activation: np.ndarray) -> FlowState:
        top_indices = np.argsort(activation)[::-1][:5]
        top_theorems = [(THEOREM_KEYS[i], float(activation[i])) for i in top_indices]
        return FlowState(step=step, activation=activation.copy(), top_theorems=top_theorems)


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

# PURPOSE: CLI: python -m mekhane.fep.theorem_attractor "å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ"
def main() -> None:
    """CLI: python -m mekhane.fep.theorem_attractor \"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ\" """
    import sys

    if len(sys.argv) < 2:
        print("Usage: python -m mekhane.fep.theorem_attractor <input_text>")
        print("       python -m mekhane.fep.theorem_attractor --basins [N]")
        sys.exit(1)

    if sys.argv[1] == "--basins":
        n = int(sys.argv[2]) if len(sys.argv) > 2 else 10000
        ta = TheoremAttractor()
        print(f"\nğŸ² Basin Detection (n={n:,})...")
        result = ta.detect_basins(n_samples=n)
        print(f"\n{'='*60}")
        print(f"Basin Map ({result.elapsed:.2f}s)")
        print(f"{'='*60}")
        for theorem in THEOREM_KEYS:
            frac = result.basin_fractions.get(theorem, 0)
            bar = "â–ˆ" * int(frac * 100)
            name = THEOREM_DEFINITIONS[theorem]["name"]
            print(f"  {theorem} {name:20s} {frac:6.1%} {bar}")
        return

    user_input = " ".join(sys.argv[1:])
    ta = TheoremAttractor()

    print(f"\nå…¥åŠ›: {user_input}")
    print("=" * 60)

    # 1. Theorem-level suggest
    results = ta.suggest(user_input, top_k=24)
    print("\nğŸ“Š å…¨ 24 å®šç†ã®å¼•åŠ›ãƒãƒƒãƒ—:")
    for r in results:
        bar = "â–ˆ" * int(r.similarity * 40)
        print(f"  {r.theorem} {r.name:20s} {r.similarity:.3f} {bar}")

    # 2. Flow simulation
    print("\nğŸŒŠ X-series Flow Simulation (10 steps):")
    flow = ta.simulate_flow(user_input, steps=10)
    for state in flow.states:
        tops = ", ".join(f"{t}={v:.3f}" for t, v in state.top_theorems[:3])
        print(f"  Step {state.step:2d}: {tops}")

    if flow.converged_at >= 0:
        print(f"  âœ… Converged at step {flow.converged_at}")
    else:
        print("  â³ Not converged in 10 steps")

    print(f"\nğŸ¯ Final: {' + '.join(t for t, _ in flow.final_theorems[:3])}")


if __name__ == "__main__":
    main()
