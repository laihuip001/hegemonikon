#!/usr/bin/env python3
# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- mekhane/mcp/ å‡ºå¼µ HGK MCP Gateway
"""
å‡ºå¼µ HGK MCP Gateway â€” ãƒ¢ãƒã‚¤ãƒ«ã‹ã‚‰ã® HGK ã‚¢ã‚¯ã‚»ã‚¹

FastMCP + Streamable HTTP ã§ã€Claude/ChatGPT ã®ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªã‹ã‚‰
MCP çµŒç”±ã§ HGK ã®èªçŸ¥æ©Ÿèƒ½ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãƒªãƒ¢ãƒ¼ãƒˆã‚µãƒ¼ãƒãƒ¼ã€‚

Usage:
    # ãƒ­ãƒ¼ã‚«ãƒ«èµ·å‹• (é–‹ç™º)
    python -m mekhane.mcp.hgk_gateway

    # Tailscale Funnel ã§å…¬é–‹
    tailscale funnel 8765
    python -m mekhane.mcp.hgk_gateway

Architecture:
    [ã‚¹ãƒãƒ› Claude/ChatGPT] â†’ MCP (Streamable HTTP) â†’ [ã“ã®ã‚µãƒ¼ãƒãƒ¼] â†’ [HGK ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«]
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any

# Project root
PROJECT_ROOT = Path(__file__).resolve().parents[2]  # hegemonikon/
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from mcp.server.fastmcp import FastMCP
from mcp.server.transport_security import TransportSecuritySettings
from mcp.server.auth.provider import (
    OAuthAuthorizationServerProvider,
    AuthorizationParams,
    AuthorizationCode,
    RefreshToken,
    AccessToken,
    construct_redirect_uri,
)
from mcp.shared.auth import OAuthClientInformationFull, OAuthToken

# =============================================================================
# Configuration
# =============================================================================

GATEWAY_HOST = os.getenv("HGK_GATEWAY_HOST", "127.0.0.1")
GATEWAY_PORT = int(os.getenv("HGK_GATEWAY_PORT", "8765"))

# Bearer Token for OAuth access token (generated once, used as the access token)
GATEWAY_TOKEN = os.getenv("HGK_GATEWAY_TOKEN", "")

# [C-1] Fail-safe: TOKEN æœªè¨­å®šæ™‚ã¯ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ‹’å¦
if not GATEWAY_TOKEN:
    print("âŒ FATAL: HGK_GATEWAY_TOKEN is not set. Refusing to start.", file=sys.stderr)
    print("   Set HGK_GATEWAY_TOKEN in .env or environment.", file=sys.stderr)
    sys.exit(1)

# [C-2] è¨±å¯ã•ã‚ŒãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆID (åå‰ä»˜ã) + è¨±å¯ã•ã‚ŒãŸ redirect_uri ãƒ‰ãƒ¡ã‚¤ãƒ³
# claude.ai ã¯æ¯ã‚»ãƒƒã‚·ãƒ§ãƒ³æ–°ã—ã„ UUID client_id ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€
# ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ã§è¨±å¯ã™ã‚‹ (redirect_uri ã«å«ã¾ã‚Œã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã§åˆ¤å®š)
ALLOWED_CLIENT_IDS: set[str] = {
    "claude.ai",
    "chatgpt.com",
    "hgk-mobile",
}
ALLOWED_REDIRECT_DOMAINS: set[str] = {
    "claude.ai",
    "chatgpt.com",
}

# Allowed hosts for DNS rebinding protection
_default_hosts = (
    "localhost,localhost:8765,"
    "127.0.0.1,127.0.0.1:8765,"
    "hegemonikon.tail3b6058.ts.net"
)
ALLOWED_HOSTS = os.getenv("HGK_GATEWAY_ALLOWED_HOSTS", _default_hosts).split(",")


# =============================================================================
# [L2] WBC Security Event Logger â€” Sympatheia çµ±åˆ
# =============================================================================

# Mneme ãƒ‘ã‚¹ï¼ˆSympatheia ã¨å…±æœ‰ï¼‰
_MNEME_DIR = Path(os.getenv("HGK_MNEME", str(Path.home() / "oikos/mneme/.hegemonikon")))


# PURPOSE: [L2-auto] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆã‚’ wbc_state.json ã«æ›¸ãè¾¼ã‚€ã€‚
def _wbc_log_security_event(
    event_type: str,
    severity: str,
    details: str,
    source: str = "hgk_gateway",
) -> None:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆã‚’ wbc_state.json ã«æ›¸ãè¾¼ã‚€ã€‚

    Sympatheia WBC ã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚¢ãƒ©ãƒ¼ãƒˆã‚’è¿½åŠ ã—ã€
    /boot æ™‚ã® sympatheia_status ã§æ¤œçŸ¥ã•ã‚Œã‚‹ã€‚
    """
    import json
    from datetime import datetime, timezone

    wbc_file = _MNEME_DIR / "wbc_state.json"
    try:
        _MNEME_DIR.mkdir(parents=True, exist_ok=True)
        if wbc_file.exists():
            state = json.loads(wbc_file.read_text("utf-8"))
        else:
            state = {"alerts": [], "totalAlerts": 0}

        alert = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "source": source,
            "severity": severity,
            "eventType": event_type,
            "details": details,
            "threatScore": 5 if severity == "medium" else (10 if severity == "high" else 2),
        }
        state["alerts"].append(alert)
        state["totalAlerts"] = state.get("totalAlerts", 0) + 1

        # ç›´è¿‘100ä»¶ã®ã¿ä¿æŒ
        state["alerts"] = state["alerts"][-100:]

        wbc_file.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")
    except Exception as e:
        print(f"âš ï¸ WBC log failed: {e}", file=sys.stderr)


# =============================================================================
# OAuth 2.1 Provider (auto-approve, single-user)
# =============================================================================

# PURPOSE: ã®çµ±ä¸€çš„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®Ÿç¾ã™ã‚‹
class HGKOAuthProvider(OAuthAuthorizationServerProvider[AuthorizationCode, RefreshToken, AccessToken]):
    """
    æœ€å° OAuth 2.1 ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã€‚
    - claude.ai Connector ç”¨ã® /authorize â†’ /token ãƒ•ãƒ­ãƒ¼ã‚’å‡¦ç†
    - èªè¨¼ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•æ‰¿èª (å˜ä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€GATEWAY_TOKEN ã§ä¿è­·)
    - ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
    """

    # PURPOSE: [L2-auto] åˆæœŸåŒ–: init__
    def __init__(self, access_token: str):
        self._access_token = access_token
        self._clients: dict[str, OAuthClientInformationFull] = {}
        self._auth_codes: dict[str, AuthorizationCode] = {}
        self._refresh_tokens: dict[str, RefreshToken] = {}

    # PURPOSE: client ã‚’å–å¾—ã™ã‚‹
    async def get_client(self, client_id: str) -> OAuthClientInformationFull | None:
        client = self._clients.get(client_id)
        if client is None:
            # [C-2] Check: named whitelist OR UUID format (claude.ai dynamic IDs)
            import re
            is_uuid = bool(re.match(
                r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$',
                client_id
            ))
            if client_id not in ALLOWED_CLIENT_IDS and not is_uuid:
                print(f"âš ï¸ Rejected unknown client: {client_id[:32]}", file=sys.stderr)
                _wbc_log_security_event(
                    event_type="client_rejected",
                    severity="medium",
                    details=f"Unknown client_id rejected: {client_id[:32]}",
                )
                return None
            # Auto-register: whitelisted names or UUID clients (claude.ai dynamic)
            from pydantic import AnyHttpUrl
            client = OAuthClientInformationFull(
                client_id=client_id,
                client_secret=None,
                redirect_uris=[AnyHttpUrl("https://claude.ai/api/mcp/auth_callback")],
                client_name=f"auto-{client_id[:16]}",
                grant_types=["authorization_code", "refresh_token"],
                response_types=["code"],
                token_endpoint_auth_method="none",
            )
            self._clients[client_id] = client
        return client

    # PURPOSE: client ã‚’ç™»éŒ²ã™ã‚‹
    async def register_client(self, client_info: OAuthClientInformationFull) -> None:
        self._clients[client_info.client_id] = client_info

    # PURPOSE: hgk_gateway ã® authorize å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    async def authorize(
        self, client: OAuthClientInformationFull, params: AuthorizationParams
    ) -> str:
        """Auto-approve: èªè¨¼ã‚³ãƒ¼ãƒ‰ã‚’å³ç™ºè¡Œã— redirect_uri ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã€‚"""
        import secrets
        # Dynamically add redirect_uri to client's registered URIs
        if client.redirect_uris is None:
            client.redirect_uris = [params.redirect_uri]
        elif params.redirect_uri not in client.redirect_uris:
            client.redirect_uris.append(params.redirect_uri)
        code = secrets.token_urlsafe(32)
        self._auth_codes[code] = AuthorizationCode(
            code=code,
            scopes=params.scopes or [],
            expires_at=time.time() + 600,  # 10 min
            client_id=client.client_id,
            code_challenge=params.code_challenge,
            redirect_uri=params.redirect_uri,
            redirect_uri_provided_explicitly=params.redirect_uri_provided_explicitly,
        )
        return construct_redirect_uri(
            str(params.redirect_uri),
            code=code,
            state=params.state,
        )

    # PURPOSE: authorization code ã‚’èª­ã¿è¾¼ã‚€
    async def load_authorization_code(
        self, client: OAuthClientInformationFull, authorization_code: str
    ) -> AuthorizationCode | None:
        ac = self._auth_codes.get(authorization_code)
        if ac and ac.client_id == client.client_id and ac.expires_at > time.time():
            return ac
        return None

    # PURPOSE: hgk_gateway ã® exchange authorization code å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    async def exchange_authorization_code(
        self, client: OAuthClientInformationFull, authorization_code: AuthorizationCode
    ) -> OAuthToken:
        """èªè¨¼ã‚³ãƒ¼ãƒ‰ â†’ ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³äº¤æ›ã€‚å›ºå®šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿”ã™ã€‚"""
        self._auth_codes.pop(authorization_code.code, None)
        import secrets
        refresh = secrets.token_urlsafe(32)
        self._refresh_tokens[refresh] = RefreshToken(
            token=refresh,
            client_id=client.client_id,
            scopes=authorization_code.scopes,
        )
        return OAuthToken(
            access_token=self._access_token,
            token_type="bearer",
            expires_in=86400,  # [C-4] 24 hours (was 1 year)
            refresh_token=refresh,
            scope=" ".join(authorization_code.scopes) if authorization_code.scopes else None,
        )

    # PURPOSE: refresh token ã‚’èª­ã¿è¾¼ã‚€
    async def load_refresh_token(
        self, client: OAuthClientInformationFull, refresh_token: str
    ) -> RefreshToken | None:
        rt = self._refresh_tokens.get(refresh_token)
        if rt and rt.client_id == client.client_id:
            return rt
        return None

    # PURPOSE: hgk_gateway ã® exchange refresh token å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    async def exchange_refresh_token(
        self,
        client: OAuthClientInformationFull,
        refresh_token: RefreshToken,
        scopes: list[str],
    ) -> OAuthToken:
        import secrets
        new_refresh = secrets.token_urlsafe(32)
        self._refresh_tokens.pop(refresh_token.token, None)
        self._refresh_tokens[new_refresh] = RefreshToken(
            token=new_refresh,
            client_id=client.client_id,
            scopes=scopes or refresh_token.scopes,
        )
        return OAuthToken(
            access_token=self._access_token,
            token_type="bearer",
            expires_in=86400,  # [C-4] 24 hours
            refresh_token=new_refresh,
            scope=" ".join(scopes) if scopes else None,
        )

    # PURPOSE: access token ã‚’èª­ã¿è¾¼ã‚€
    async def load_access_token(self, token: str) -> AccessToken | None:
        if token == self._access_token:
            return AccessToken(
                token=token,
                client_id="hgk",
                scopes=[],
            )
        # [L2] Invalid token â†’ WBC alert
        _wbc_log_security_event(
            event_type="invalid_token",
            severity="high",
            details=f"Invalid access token attempt (prefix: {token[:8]}...)",
        )
        return None

    # PURPOSE: hgk_gateway ã® revoke token å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    async def revoke_token(self, token: AccessToken | RefreshToken) -> None:
        if isinstance(token, RefreshToken):
            self._refresh_tokens.pop(token.token, None)


# =============================================================================
# Gateway Server
# =============================================================================

from mcp.server.auth.settings import AuthSettings, ClientRegistrationOptions

_GATEWAY_URL = "https://hegemonikon.tail3b6058.ts.net"

_oauth_provider = HGKOAuthProvider(GATEWAY_TOKEN) if GATEWAY_TOKEN else None
_auth_settings = AuthSettings(
    issuer_url=_GATEWAY_URL,
    resource_server_url=_GATEWAY_URL,
    client_registration_options=ClientRegistrationOptions(enabled=True),
) if GATEWAY_TOKEN else None

mcp = FastMCP(
    "hgk-gateway",
    host=GATEWAY_HOST,
    port=GATEWAY_PORT,
    auth_server_provider=_oauth_provider,
    auth=_auth_settings,
    transport_security=TransportSecuritySettings(
        enable_dns_rebinding_protection=True,
        allowed_hosts=ALLOWED_HOSTS,
    ),
    instructions=(
        "HegemonikÃ³n å‡ºå¼µ MCP Gatewayã€‚"
        "ãƒ¢ãƒã‚¤ãƒ«ã‹ã‚‰ HGK ã®èªçŸ¥æ©Ÿèƒ½ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã€‚"
        "/sop èª¿æŸ»ä¾é ¼æ›¸ã®ç”Ÿæˆã€KI/GnÅsis æ¤œç´¢ã€"
        "CCL ãƒ‘ãƒ¼ã‚¹ã€Doxa/Handoff å‚ç…§ã€ã‚¢ã‚¤ãƒ‡ã‚¢ãƒ¡ãƒ¢ä¿å­˜ã€"
        "Digestor (æ¶ˆåŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œãƒ»å€™è£œä¸€è¦§ãƒ»æ¶ˆåŒ–æ¸ˆãƒãƒ¼ã‚¯ãƒ»ãƒˆãƒ”ãƒƒã‚¯ç®¡ç†) ãŒå¯èƒ½ã€‚"
    ),
)

# Paths
MNEME_DIR = PROJECT_ROOT.parent / "mneme" / ".hegemonikon"
SESSIONS_DIR = MNEME_DIR / "sessions"
DOXA_DIR = MNEME_DIR / "doxa"
SOP_OUTPUT_DIR = MNEME_DIR / "workflows"
IDEA_DIR = MNEME_DIR / "ideas"


# =============================================================================
# P1: /sop èª¿æŸ»ä¾é ¼æ›¸ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
# =============================================================================

# PURPOSE: hgk_gateway ã® hgk sop generate å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
@mcp.tool()
def hgk_sop_generate(
    topic: str,
    decision: str = "",
    hypothesis: str = "",
) -> str:
    """
    /sop èª¿æŸ»ä¾é ¼æ›¸ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚

    Gemini Deep Research ã‚„ Perplexity ã«ã‚³ãƒ”ãƒšã—ã¦ä½¿ã†ã€‚
    HegemonikÃ³n /sop ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ¢ãƒã‚¤ãƒ«ç‰ˆã€‚

    Args:
        topic: èª¿æŸ»å¯¾è±¡ã®ãƒ†ãƒ¼ãƒ (ä¾‹: "FEP ã¨ Active Inference ã®æœ€æ–°å‹•å‘")
        decision: ã“ã®èª¿æŸ»ã®çµæœã€ä½•ã‚’æ±ºå®šã™ã‚‹ã‹
        hypothesis: äº‹å‰ä»®èª¬ (ã‚ã‚Œã°)
    """
    now = datetime.now().strftime("%Y-%m-%d")

    template = f"""# èª¿æŸ»ä¾é ¼æ›¸ï¼ˆæ·±æ˜ã‚Šç‰ˆï¼‰

> ãƒ†ãƒ¼ãƒ: {topic}
> ç”Ÿæˆæ—¥: {now}
> ç”Ÿæˆå…ƒ: HGK /sop (å‡ºå¼µç‰ˆ)

---

## å‡ºåŠ›å½¢å¼

ä»¥ä¸‹ã®4åˆ—ãƒ†ãƒ¼ãƒ–ãƒ«ã§æ§‹é€ åŒ–ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š

| é …ç›® | å€¤ | æ ¹æ‹ ï¼ˆå‡ºå…¸ï¼‰ | URL |
|:-----|:---|:-----------|:----|

---

## ã‚¿ã‚¹ã‚¯å®šç¾©

{topic}ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®è«–ç‚¹ã‚’**ç¶²ç¾…çš„ã‹ã¤æœ€æ–°ã®æƒ…å ±**ã«åŸºã¥ã„ã¦èª¿æŸ»ã—ã¦ãã ã•ã„ã€‚

## æ™‚é–“åˆ¶ç´„

- **éå»6ãƒ¶æœˆã®æƒ…å ±ã‚’å„ªå…ˆ**
- 2025å¹´ä»¥é™ã®è«–æ–‡ãƒ»è¨˜äº‹ã‚’é‡è¦–

## æ±ºå®šäº‹é …

{decision if decision else "ï¼ˆèª¿æŸ»çµæœã«åŸºã¥ã„ã¦æ±ºå®šã™ã‚‹ï¼‰"}

## ä»®èª¬

{hypothesis if hypothesis else "ï¼ˆä»®èª¬ãªã— â€” æ¢ç´¢çš„èª¿æŸ»ï¼‰"}

---

## è«–ç‚¹ï¼ˆå¿…é ˆé …ç›®ï¼‰

A. {topic}ã®ç¾çŠ¶
- A1: æœ€æ–°ã®å®šç¾©ãƒ»åˆ†é¡ã¯ã©ã†ãªã£ã¦ã„ã‚‹ã‹ï¼Ÿ
- A2: ä¸»è¦ãªç ”ç©¶ã‚°ãƒ«ãƒ¼ãƒ—ãƒ»å®Ÿè£…ã¯ï¼Ÿ
- A3: 2025å¹´ä»¥é™ã®é‡è¦ãªå¤‰åŒ–ãƒ»ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã¯ï¼Ÿ

B. å®Ÿè·µãƒ»å¿œç”¨
- B1: ç¾æ™‚ç‚¹ã§æœ€ã‚‚æœ‰åŠ¹ãªæ‰‹æ³•ãƒ»ãƒ„ãƒ¼ãƒ«ã¯ï¼Ÿ
- B2: æˆåŠŸäº‹ä¾‹ã¨å¤±æ•—äº‹ä¾‹ã¯ï¼Ÿ
- B3: ã‚³ã‚¹ãƒˆãƒ»å®Ÿè£…ã®ç¾å®Ÿçš„ãªåˆ¶ç´„ã¯ï¼Ÿ

C. å°†æ¥å±•æœ›
- C1: ä»Šå¾Œ6-12ãƒ¶æœˆã§äºˆæƒ³ã•ã‚Œã‚‹å¤‰åŒ–ã¯ï¼Ÿ
- C2: ãƒªã‚¹ã‚¯ã‚„æ³¨æ„ã™ã¹ãç‚¹ã¯ï¼Ÿ

---

> ã“ã®èª¿æŸ»ä¾é ¼æ›¸ã¯ HegemonikÃ³n /sop ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ (å‡ºå¼µç‰ˆ) ã§ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚
> Gemini Deep Research ã¾ãŸã¯ Perplexity ã«ã‚³ãƒ”ãƒšã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
"""

    # Save to file
    SOP_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    safe_topic = topic[:30].replace("/", "_").replace(" ", "_")
    output_path = SOP_OUTPUT_DIR / f"sop_{safe_topic}_{now}.md"
    output_path.write_text(template, encoding="utf-8")

    return f"## âœ… èª¿æŸ»ä¾é ¼æ›¸ã‚’ç”Ÿæˆã—ã¾ã—ãŸ\n\nä¿å­˜å…ˆ: `{output_path}`\n\n---\n\n{template}"


# =============================================================================
# P1: KI / GnÅsis æ¤œç´¢
# =============================================================================

# PURPOSE: hgk_gateway ã® hgk search å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
@mcp.tool()
def hgk_search(query: str, max_results: int = 5, mode: str = "hybrid") -> str:
    """
    HGK ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ (KI / GnÅsis / Sophia) ã‚’æ¤œç´¢ã™ã‚‹ã€‚

    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª (ä¾‹: "FEP ç²¾åº¦åŠ é‡", "èªçŸ¥ãƒã‚¤ã‚¢ã‚¹")
        max_results: æœ€å¤§çµæœæ•°
        mode: æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ â€” "hybrid" (ãƒ™ã‚¯ãƒˆãƒ«+ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰), "vector" (ãƒ™ã‚¯ãƒˆãƒ«ã®ã¿), "keyword" (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿)
    """
    results = []

    # --- ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ (GnosisIndex) ---
    if mode in ("hybrid", "vector"):
        try:
            from mekhane.anamnesis.index import GnosisIndex

            idx = GnosisIndex()
            vector_results = idx.search(query, k=max_results)
            for r in vector_results:
                title = r.get("title", "ä¸æ˜")
                authors = r.get("authors", "")
                abstract = r.get("abstract", "")[:200]
                source = r.get("source", "")
                score = r.get("_distance", None)
                score_str = f" (score: {score:.3f})" if score is not None else ""
                results.append(
                    f"ğŸ”¬ **{title}**{score_str}\n"
                    f"   è‘—è€…: {authors[:80]}\n"
                    f"   {abstract}..."
                )
        except ImportError:
            results.append("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (lancedb/sentence-transformers)")
        except Exception as e:
            results.append(f"âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

    # --- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ ---
    if mode in ("hybrid", "keyword"):
        # 1. KI (Knowledge Items) â€” ãƒ•ã‚¡ã‚¤ãƒ«åæ¤œç´¢
        ki_base = Path.home() / ".gemini" / "antigravity" / "knowledge"
        if ki_base.exists():
            ki_dirs = sorted(ki_base.iterdir())
            query_lower = query.lower()
            for ki_dir in ki_dirs:
                if ki_dir.is_dir():
                    metadata_path = ki_dir / "metadata.json"
                    if metadata_path.exists():
                        try:
                            meta = json.loads(metadata_path.read_text(encoding="utf-8"))
                            summary = meta.get("summary", "")
                            title = meta.get("title", ki_dir.name)
                            if query_lower in title.lower() or query_lower in summary.lower():
                                results.append(f"ğŸ“š **KI: {title}**\n   {summary[:150]}...")
                        except Exception:
                            pass

        # 2. Doxa (ä¿¡å¿µ)
        if DOXA_DIR.exists():
            for doxa_file in sorted(DOXA_DIR.glob("*.json")):
                try:
                    doxa = json.loads(doxa_file.read_text(encoding="utf-8"))
                    content = json.dumps(doxa, ensure_ascii=False)
                    if query.lower() in content.lower():
                        results.append(f"ğŸ’¡ **Doxa: {doxa_file.stem}**\n   {content[:150]}...")
                except Exception:
                    pass

        # 3. Handoff â€” æœ€æ–°3ä»¶ã‚’æ¤œç´¢
        if SESSIONS_DIR.exists():
            handoffs = sorted(SESSIONS_DIR.glob("handoff_*.md"), reverse=True)[:3]
            for hf in handoffs:
                try:
                    content = hf.read_text(encoding="utf-8")
                    if query.lower() in content.lower():
                        lines = content.split("\n")
                        matches = [l.strip() for l in lines if query.lower() in l.lower()][:3]
                        match_text = " / ".join(matches) if matches else "(ãƒãƒƒãƒç®‡æ‰€çœç•¥)"
                        results.append(f"ğŸ“‹ **Handoff: {hf.stem}**\n   {match_text[:150]}")
                except Exception:
                    pass

    if not results:
        return f"ğŸ” `{query}` ã«ä¸€è‡´ã™ã‚‹çµæœã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    mode_label = {"hybrid": "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰", "vector": "ãƒ™ã‚¯ãƒˆãƒ«", "keyword": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"}.get(mode, mode)
    header = f"## ğŸ” HGK æ¤œç´¢çµæœ: `{query}` ({mode_label})\n\n**{len(results)} ä»¶**\n\n"
    return header + "\n\n".join(results[:max_results])


# =============================================================================
# P2: CCL Dispatch
# =============================================================================

# PURPOSE: hgk_gateway ã® hgk ccl dispatch å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
@mcp.tool()
def hgk_ccl_dispatch(ccl: str) -> str:
    """
    CCL (Cognitive Control Language) å¼ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€æ§‹é€ ã‚’è§£æã™ã‚‹ã€‚

    Args:
        ccl: CCL å¼ (ä¾‹: "/noe+", "/dia+~*/noe", "/sop")
    """
    try:
        from hermeneus.src.dispatch import dispatch

        result = dispatch(ccl)

        if not result["success"]:
            return f"## âŒ CCL ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼\n\n**CCL**: `{ccl}`\n**ã‚¨ãƒ©ãƒ¼**: {result['error']}"

        return f"""## âœ… CCL ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒçµæœ

**CCL**: `{ccl}`

### AST æ§‹é€ 
```
{result['tree']}
```

### é–¢é€£ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
{', '.join(f'`{wf}`' for wf in result['workflows'])}

### å®Ÿè¡Œè¨ˆç”»
{result['plan_template']}"""
    except Exception as e:
        return f"## âŒ ã‚¨ãƒ©ãƒ¼\n\n`{e}`"


# =============================================================================
# P2: Doxa èª­ã¿å–ã‚Š
# =============================================================================

# PURPOSE: hgk_gateway ã® hgk doxa read å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
@mcp.tool()
def hgk_doxa_read() -> str:
    """
    Doxa (ä¿¡å¿µã‚¹ãƒˆã‚¢) ã®å†…å®¹ã‚’ä¸€è¦§è¡¨ç¤ºã™ã‚‹ã€‚
    HGK ã§è“„ç©ã•ã‚ŒãŸæ³•å‰‡ãƒ»æ•™è¨“ãƒ»ä¿¡å¿µã‚’å‚ç…§ã™ã‚‹ã€‚
    """
    if not DOXA_DIR.exists():
        return "## âš ï¸ Doxa ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"

    doxa_files = sorted(DOXA_DIR.glob("*.json"))
    if not doxa_files:
        return "## ğŸ“­ Doxa ã¯ç©ºã§ã™"

    lines = ["## ğŸ’¡ Doxa (ä¿¡å¿µã‚¹ãƒˆã‚¢)\n"]
    for df in doxa_files:
        try:
            data = json.loads(df.read_text(encoding="utf-8"))
            if isinstance(data, list):
                for item in data:
                    strength = item.get("strength", "?")
                    text = item.get("text", item.get("law", str(item)))
                    lines.append(f"- **[{strength}]** {text}")
            elif isinstance(data, dict):
                for key, value in data.items():
                    lines.append(f"- **{key}**: {value}")
        except Exception:
            lines.append(f"- âš ï¸ {df.name}: èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼")

    return "\n".join(lines)


# =============================================================================
# P3: Handoff å‚ç…§
# =============================================================================

# PURPOSE: hgk_gateway ã® hgk handoff read å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
@mcp.tool()
def hgk_handoff_read(count: int = 1) -> str:
    """
    æœ€æ–°ã® Handoff (ã‚»ãƒƒã‚·ãƒ§ãƒ³å¼•ãç¶™ãæ›¸) ã‚’èª­ã‚€ã€‚
    å‰å›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ä½•ã‚’ã—ãŸã‹ã€æ¬¡ã«ä½•ã‚’ã™ã¹ãã‹ã‚’ç¢ºèªã™ã‚‹ã€‚

    Args:
        count: èª­ã‚€ Handoff ã®æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1)
    """
    if not SESSIONS_DIR.exists():
        return "## âš ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"

    handoffs = sorted(SESSIONS_DIR.glob("handoff_*.md"), reverse=True)
    if not handoffs:
        return "## ğŸ“­ Handoff ãŒã‚ã‚Šã¾ã›ã‚“"

    lines = [f"## ğŸ“‹ æœ€æ–° Handoff ({min(count, len(handoffs))}/{len(handoffs)} ä»¶)\n"]

    for hf in handoffs[:count]:
        try:
            content = hf.read_text(encoding="utf-8")
            # First 50 lines
            summary = "\n".join(content.split("\n")[:50])
            lines.append(f"### {hf.stem}\n\n{summary}\n\n---")
        except Exception:
            lines.append(f"### {hf.stem}\n\nâš ï¸ èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼")

    return "\n".join(lines)


# =============================================================================
# P3: ã‚¢ã‚¤ãƒ‡ã‚¢ãƒ¡ãƒ¢ä¿å­˜
# =============================================================================

# PURPOSE: hgk_gateway ã® hgk idea capture å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
@mcp.tool()
def hgk_idea_capture(idea: str, tags: str = "") -> str:
    """
    ã‚¢ã‚¤ãƒ‡ã‚¢ãƒ¡ãƒ¢ã‚’ä¿å­˜ã™ã‚‹ã€‚å¤–å‡ºå…ˆã§ã®é–ƒãã‚’é€ƒã•ãªã„ã€‚
    æ¬¡å› /boot ã§è‡ªå‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ã€‚

    Args:
        idea: ã‚¢ã‚¤ãƒ‡ã‚¢ã®å†…å®¹ (æœ€å¤§10,000æ–‡å­—)
        tags: ã‚¿ã‚° (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ä¾‹: "FEP, è¨­è¨ˆ, å®Ÿé¨“")
    """
    # [C-3] Content size limit
    MAX_IDEA_SIZE = 10_000
    if len(idea) > MAX_IDEA_SIZE:
        return f"âŒ ã‚¨ãƒ©ãƒ¼: ã‚¢ã‚¤ãƒ‡ã‚¢ãŒé•·ã™ãã¾ã™ ({len(idea)} æ–‡å­—)ã€‚ä¸Šé™ã¯ {MAX_IDEA_SIZE} æ–‡å­—ã§ã™ã€‚"
    IDEA_DIR.mkdir(parents=True, exist_ok=True)

    now = datetime.now()
    filename = f"idea_{now.strftime('%Y%m%d_%H%M%S')}.md"
    filepath = IDEA_DIR / filename

    content = f"""# ğŸ’¡ ã‚¢ã‚¤ãƒ‡ã‚¢ãƒ¡ãƒ¢

> **æ—¥æ™‚**: {now.strftime('%Y-%m-%d %H:%M:%S')}
> **ã‚¿ã‚°**: {tags if tags else 'æœªåˆ†é¡'}
> **ã‚½ãƒ¼ã‚¹**: HGK å‡ºå¼µç‰ˆ (ãƒ¢ãƒã‚¤ãƒ«)

---

{idea}

---

*Captured via HGK Gateway*
"""
    filepath.write_text(content, encoding="utf-8")

    return f"## âœ… ã‚¢ã‚¤ãƒ‡ã‚¢ä¿å­˜å®Œäº†\n\nä¿å­˜å…ˆ: `{filepath}`\nã‚¿ã‚°: {tags if tags else 'æœªåˆ†é¡'}\n\næ¬¡å› `/boot` ã§è‡ªå‹•çš„ã«ç¢ºèªã•ã‚Œã¾ã™ã€‚"


# =============================================================================
# HGK Status (ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯)
# =============================================================================

# PURPOSE: hgk_gateway ã® hgk status å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
@mcp.tool()
def hgk_status() -> str:
    """
    HGK ã‚·ã‚¹ãƒ†ãƒ ã®æ¦‚è¦ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    ãƒ¢ãƒã‚¤ãƒ«ã‹ã‚‰ç¾åœ¨ã®çŠ¶æ…‹ã‚’ç¢ºèªã™ã‚‹ã€‚
    """
    status_items = []

    # Handoff count
    handoff_count = len(list(SESSIONS_DIR.glob("handoff_*.md"))) if SESSIONS_DIR.exists() else 0
    status_items.append(f"ğŸ“‹ Handoff: {handoff_count} ä»¶")

    # KI count
    ki_base = Path.home() / ".gemini" / "antigravity" / "knowledge"
    ki_count = len([d for d in ki_base.iterdir() if d.is_dir()]) if ki_base.exists() else 0
    status_items.append(f"ğŸ“š KI: {ki_count} ä»¶")

    # Doxa count
    doxa_count = len(list(DOXA_DIR.glob("*.json"))) if DOXA_DIR.exists() else 0
    status_items.append(f"ğŸ’¡ Doxa: {doxa_count} ä»¶")

    # Ideas count
    idea_count = len(list(IDEA_DIR.glob("*.md"))) if IDEA_DIR.exists() else 0
    status_items.append(f"ğŸŒŸ Ideas: {idea_count} ä»¶")

    # Latest handoff
    if SESSIONS_DIR.exists():
        handoffs = sorted(SESSIONS_DIR.glob("handoff_*.md"), reverse=True)
        if handoffs:
            status_items.append(f"ğŸ“… æœ€æ–° Handoff: `{handoffs[0].name}`")

    # Digestor status
    incoming_count = len(list(INCOMING_DIR.glob("eat_*.md"))) if INCOMING_DIR.exists() else 0
    processed_count = len(list(PROCESSED_DIR.glob("eat_*.md"))) if PROCESSED_DIR.exists() else 0
    status_items.append(f"\n### Digestor")
    status_items.append(f"ğŸ“¥ incoming: {incoming_count} ä»¶")
    status_items.append(f"ğŸ“¦ processed: {processed_count} ä»¶")

    try:
        from mekhane.ergasterion.digestor.state import get_status_summary
        status_items.append(get_status_summary())
    except Exception:
        status_items.append("ğŸ”„ Digestor: çŠ¶æ…‹ä¸æ˜")

    # Scheduler PID check
    pid_file = Path.home() / ".hegemonikon" / "digestor" / "scheduler.pid"
    if pid_file.exists():
        try:
            pid = int(pid_file.read_text().strip())
            os.kill(pid, 0)  # Check if process exists
            status_items.append("âš¡ Scheduler: ç¨¼åƒä¸­")
        except (ProcessLookupError, ValueError):
            status_items.append("ğŸ’¤ Scheduler: åœæ­¢ä¸­ (PID stale)")
    else:
        status_items.append("ğŸ’¤ Scheduler: åœæ­¢ä¸­")

    return f"## ğŸ  HGK ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹\n\n" + "\n".join(status_items)


# =============================================================================
# CCL Execute (CCL å¼ã®å®Ÿè¡Œ)
# =============================================================================

# PURPOSE: CCL å¼ã‚’ HermÄ“neus çµŒç”±ã§å®Ÿè¡Œã—ã€çµæœã‚’è¿”ã™
@mcp.tool()
def hgk_ccl_execute(ccl: str, context: str = "") -> str:
    """
    CCL å¼ã‚’å®Ÿè¡Œã—ã€çµæœã‚’è¿”ã™ã€‚
    dispatch (æ§‹æ–‡è§£æã®ã¿) ã¨ã¯ç•°ãªã‚Šã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿéš›ã«å®Ÿè¡Œã™ã‚‹ã€‚

    Args:
        ccl: CCL å¼ (ä¾‹: "/noe+", "/dia+~*/noe")ã€‚æœ€å¤§ 500 æ–‡å­—ã€‚
        context: å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (åˆ†æå¯¾è±¡ãªã©)ã€‚æœ€å¤§ 2000 æ–‡å­—ã€‚
    """
    # Input validation
    if len(ccl) > 500:
        return "âŒ CCL å¼ãŒé•·ã™ãã¾ã™ (æœ€å¤§ 500 æ–‡å­—)"
    if len(context) > 2000:
        return "âŒ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã¾ã™ (æœ€å¤§ 2000 æ–‡å­—)"

    try:
        from hermeneus.src.macro_executor import execute_and_explain
        result = execute_and_explain(ccl, context)
        # W12 Token Explosion å¯¾ç­–: å‡ºåŠ›ã‚’æœ€å¤§ 5000 æ–‡å­—ã«åˆ¶é™
        if len(result) > 5000:
            result = result[:5000] + "\n\n... (å‡ºåŠ›ãŒ 5000 æ–‡å­—ã‚’è¶…ãˆãŸãŸã‚åˆ‡ã‚Šè©°ã‚ã¾ã—ãŸ)"
        return result
    except ImportError:
        return "âŒ HermÄ“neus ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ (import ã‚¨ãƒ©ãƒ¼)"
    except Exception as e:
        return f"âŒ CCL å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}"


# =============================================================================
# Paper Search (è«–æ–‡æ¤œç´¢)
# =============================================================================

# PURPOSE: Semantic Scholar çµŒç”±ã§å­¦è¡“è«–æ–‡ã‚’æ¤œç´¢ã™ã‚‹
@mcp.tool()
def hgk_paper_search(query: str, limit: int = 5) -> str:
    """
    å­¦è¡“è«–æ–‡ã‚’æ¤œç´¢ã™ã‚‹ (Semantic Scholar çµŒç”±)ã€‚
    GnÅsis çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æ‹¡å……ã‚„èª¿æŸ»ä¾é ¼ã«ä½¿ç”¨ã€‚

    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª (ä¾‹: "active inference free energy")ã€‚æœ€å¤§ 200 æ–‡å­—ã€‚
        limit: æœ€å¤§çµæœæ•° (1-20ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 5)ã€‚
    """
    # Input validation
    if len(query) > 200:
        return "âŒ ã‚¯ã‚¨ãƒªãŒé•·ã™ãã¾ã™ (æœ€å¤§ 200 æ–‡å­—)"
    limit = max(1, min(20, limit))

    try:
        import signal

        # Anarkhia å¯¾ç­–: 30 ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        # PURPOSE: [L2-auto] å†…éƒ¨å‡¦ç†: timeout_handler
        def _timeout_handler(signum, frame):
            raise TimeoutError("API ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (30ç§’)")

        old_handler = signal.signal(signal.SIGALRM, _timeout_handler)
        signal.alarm(30)

        try:
            from mekhane.pks.semantic_scholar import SemanticScholarClient
            client = SemanticScholarClient()
            results = client.search(query, limit=limit)
        finally:
            signal.alarm(0)
            signal.signal(signal.SIGALRM, old_handler)

        if not results:
            return f"ğŸ” '{query}' ã®æ¤œç´¢çµæœ: 0 ä»¶"

        lines = [f"## ğŸ” è«–æ–‡æ¤œç´¢: '{query}' ({len(results)} ä»¶)\n"]
        for i, paper in enumerate(results, 1):
            # Paper ã¯ dataclass â€” å±æ€§ã‚¢ã‚¯ã‚»ã‚¹ã‚’ä½¿ç”¨ (.get() ã¯ä½¿ãˆãªã„)
            title = getattr(paper, "title", "ä¸æ˜")
            year = getattr(paper, "year", None) or "?"
            citations = getattr(paper, "citation_count", 0)
            paper_authors = getattr(paper, "authors", []) or []
            authors = ", ".join(
                a if isinstance(a, str) else str(a) for a in paper_authors[:3]
            )
            if len(paper_authors) > 3:
                authors += " et al."
            lines.append(f"### {i}. {title} ({year})")
            lines.append(f"- **è‘—è€…**: {authors}")
            lines.append(f"- **è¢«å¼•ç”¨æ•°**: {citations}")
            abstract = getattr(paper, "abstract", "") or ""
            if abstract:
                # Abstract ã‚’ 200 æ–‡å­—ã«åˆ¶é™
                if len(abstract) > 200:
                    abstract = abstract[:200] + "..."
                lines.append(f"- **è¦æ—¨**: {abstract}")
            lines.append("")

        return "\n".join(lines)
    except TimeoutError as e:
        return f"â±ï¸ {e}"
    except ImportError:
        return "âŒ SemanticScholarClient ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ (import ã‚¨ãƒ©ãƒ¼)"
    except Exception as e:
        return f"âŒ è«–æ–‡æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}"


# =============================================================================
# Digestor: Incoming Check (æ¶ˆåŒ–å€™è£œä¸€è¦§)
# =============================================================================

# PURPOSE: incoming/ ã®æ¶ˆåŒ–å€™è£œã‚’ç¢ºèªã™ã‚‹
INCOMING_DIR = MNEME_DIR / "incoming"
PROCESSED_DIR = MNEME_DIR / "processed"

# PURPOSE: [L2-auto] incoming/ ã®æœªæ¶ˆåŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã™ã‚‹ã€‚

@mcp.tool()
def hgk_digest_check() -> str:
    """
    incoming/ ã®æœªæ¶ˆåŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã™ã‚‹ã€‚
    æ¶ˆåŒ–å¾…ã¡ã®è«–æ–‡å€™è£œä¸€è¦§ã‚’è¿”ã™ã€‚
    """
    if not INCOMING_DIR.exists():
        return "## âš ï¸ incoming/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"

    files = sorted(INCOMING_DIR.glob("eat_*.md"))
    if not files:
        return "## ğŸ“­ æ¶ˆåŒ–å¾…ã¡ã®å€™è£œã¯ã‚ã‚Šã¾ã›ã‚“ (0 ä»¶)"

    lines = [f"## ğŸ“¥ æ¶ˆåŒ–å¾…ã¡å€™è£œ: {len(files)} ä»¶\n"]

    for i, f in enumerate(files, 1):
        try:
            content = f.read_text(encoding="utf-8")
            title = "(ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜)"
            score = ""
            topics_str = ""

            in_frontmatter = False
            for line in content.split("\n"):
                if line.strip() == "---":
                    if in_frontmatter:
                        break
                    in_frontmatter = True
                    continue
                if in_frontmatter:
                    if line.startswith("title:"):
                        title = line.split(":", 1)[1].strip().strip("\"'")
                    elif line.startswith("score:"):
                        score = line.split(":", 1)[1].strip()
                    elif line.startswith("topics:"):
                        topics_str = line.split(":", 1)[1].strip()

            lines.append(f"### {i}. {title}")
            if score:
                lines.append(f"- **Score**: {score}")
            if topics_str:
                lines.append(f"- **Topics**: {topics_str}")
            lines.append(f"- **File**: `{f.name}`\n")
        except Exception as e:
            lines.append(f"### {i}. {f.name} (èª­å–ã‚¨ãƒ©ãƒ¼: {e})\n")

    # processed ä»¶æ•°ã‚‚è¡¨ç¤º
    processed_count = len(list(PROCESSED_DIR.glob("eat_*.md"))) if PROCESSED_DIR.exists() else 0
    lines.append(f"---\nğŸ“¦ processed/: {processed_count} ä»¶ æ¶ˆåŒ–æ¸ˆ")

    return "\n".join(lines)


# =============================================================================
# Digestor: Mark Processed (æ¶ˆåŒ–å®Œäº†ãƒãƒ¼ã‚¯)
# =============================================================================

# PURPOSE: æ¶ˆåŒ–å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ processed/ ã«ç§»å‹•ã™ã‚‹
@mcp.tool()
def hgk_digest_mark(filenames: str = "") -> str:
    """
    æ¶ˆåŒ–å®Œäº†ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ incoming/ â†’ processed/ ã«ç§»å‹•ã™ã‚‹ã€‚

    Args:
        filenames: ç§»å‹•ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)ã€‚ç©ºã®å ´åˆã¯å…¨ eat_*.md ã‚’ç§»å‹•ã€‚
    """
    try:
        from mekhane.ergasterion.digestor.pipeline import mark_as_processed

        file_list = [f.strip() for f in filenames.split(",") if f.strip()] if filenames else None
        result = mark_as_processed(filenames=file_list)

        lines = [f"## âœ… processed/ ç§»å‹•çµæœ\n"]
        lines.append(f"**ç§»å‹•æˆåŠŸ**: {result['count']} ä»¶\n")

        for f in result["moved"]:
            lines.append(f"- âœ… `{f}`")
        for e in result["errors"]:
            lines.append(f"- âŒ `{e['file']}`: {e['error']}")

        return "\n".join(lines)
    except ImportError:
        return "âŒ DigestorPipeline ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
    except Exception as e:
        return f"âŒ ã‚¨ãƒ©ãƒ¼: {e}"


# =============================================================================
# Digestor: List Candidates (å€™è£œè©•ä¾¡)
# =============================================================================

# PURPOSE: Digestor selector ã§å€™è£œã‚’è©•ä¾¡ã™ã‚‹
@mcp.tool()
def hgk_digest_list(
    topics: str = "",
    max_candidates: int = 10,
) -> str:
    """
    Digestor ã® selector ã§è«–æ–‡å€™è£œã‚’è©•ä¾¡ã™ã‚‹ (dry-run)ã€‚
    incoming/ ã«ã¯æ›¸ãè¾¼ã¾ãšã€è©•ä¾¡çµæœã®ã¿è¿”ã™ã€‚

    Args:
        topics: å¯¾è±¡ãƒˆãƒ”ãƒƒã‚¯ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)ã€‚æœ€å¤§ 500 æ–‡å­—ã€‚ç©º=å…¨ãƒˆãƒ”ãƒƒã‚¯ã€‚
        max_candidates: æœ€å¤§å€™è£œæ•° (1-20ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 10)ã€‚
    """
    if len(topics) > 500:
        return "âŒ ãƒˆãƒ”ãƒƒã‚¯ãŒé•·ã™ãã¾ã™ (æœ€å¤§ 500 æ–‡å­—)"
    max_candidates = max(1, min(20, max_candidates))

    try:
        from mekhane.ergasterion.digestor.pipeline import DigestorPipeline

        topic_list = [t.strip() for t in topics.split(",") if t.strip()] if topics else None
        pipeline = DigestorPipeline()
        result = pipeline.run(
            topics=topic_list,
            max_papers=30,
            max_candidates=max_candidates,
            dry_run=True,
        )

        lines = [f"## ğŸ” æ¶ˆåŒ–å€™è£œãƒªã‚¹ãƒˆ (dry-run)\n"]
        lines.append(f"- **å–å¾—è«–æ–‡æ•°**: {result.total_papers}")
        lines.append(f"- **é¸å®šå€™è£œæ•°**: {result.candidates_selected}\n")

        for i, c in enumerate(result.candidates[:max_candidates], 1):
            lines.append(f"### {i}. [{c.score:.2f}] {c.paper.title[:80]}")
            if hasattr(c.paper, 'authors') and c.paper.authors:
                authors = ", ".join(c.paper.authors[:3])
                lines.append(f"- **è‘—è€…**: {authors}")
            lines.append("")

        return "\n".join(lines)
    except ImportError:
        return "âŒ DigestorPipeline ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
    except Exception as e:
        return f"âŒ å€™è£œãƒªã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}"


# =============================================================================
# Digestor: Topics (ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§)
# =============================================================================

# PURPOSE: æ¶ˆåŒ–å¯¾è±¡ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã‚’è¡¨ç¤ºã™ã‚‹
@mcp.tool()
def hgk_digest_topics() -> str:
    """
    æ¶ˆåŒ–å¯¾è±¡ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    topics.yaml ã«å®šç¾©ã•ã‚ŒãŸãƒ†ãƒ¼ãƒã¨è¨­å®šã‚’è¿”ã™ã€‚
    """
    try:
        import yaml

        topics_file = PROJECT_ROOT / "mekhane" / "ergasterion" / "digestor" / "topics.yaml"
        if not topics_file.exists():
            return "## âš ï¸ topics.yaml ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"

        data = yaml.safe_load(topics_file.read_text(encoding="utf-8"))
        settings = data.get("settings", {})
        topics_list = data.get("topics", [])

        lines = [f"## ğŸ“‹ æ¶ˆåŒ–å¯¾è±¡ãƒˆãƒ”ãƒƒã‚¯ ({len(topics_list)} ãƒ†ãƒ¼ãƒ)\n"]
        lines.append(f"- **æœ€å¤§å€™è£œæ•°**: {settings.get('max_candidates', '?')}")
        lines.append(f"- **æœ€å°ã‚¹ã‚³ã‚¢**: {settings.get('min_score', '?')}")
        lines.append(f"- **ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰**: {settings.get('match_mode', '?')}\n")

        for t in topics_list:
            tid = t.get("id", "?")
            desc = t.get("description", "")
            digest_to = ", ".join(t.get("digest_to", []))
            lines.append(f"### `{tid}`")
            lines.append(f"- {desc}")
            lines.append(f"- â†’ {digest_to}\n")

        return "\n".join(lines)
    except ImportError:
        return "âŒ PyYAML ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
    except Exception as e:
        return f"âŒ ãƒˆãƒ”ãƒƒã‚¯èª­å–ã‚¨ãƒ©ãƒ¼: {e}"


# =============================================================================
# Digest Run (æ¶ˆåŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)
# =============================================================================

# PURPOSE: Digestor ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã€æ¶ˆåŒ–å€™è£œã‚’ç”Ÿæˆã™ã‚‹
@mcp.tool()
def hgk_digest_run(
    topics: str = "",
    max_papers: int = 20,
    dry_run: bool = True,
) -> str:
    """
    Digestor ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ dry_run (ãƒ¬ãƒãƒ¼ãƒˆã®ã¿)ã€‚dry_run=False ã§ .md ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã€‚

    Args:
        topics: å¯¾è±¡ãƒˆãƒ”ãƒƒã‚¯ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)ã€‚æœ€å¤§ 500 æ–‡å­—ã€‚ç©ºã®å ´åˆã¯å…¨ãƒˆãƒ”ãƒƒã‚¯ã€‚
        max_papers: å–å¾—ã™ã‚‹æœ€å¤§è«–æ–‡æ•° (1-50ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 20)ã€‚
        dry_run: True=ãƒ¬ãƒãƒ¼ãƒˆã®ã¿ã€False=.md ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ (incoming/ ã«å‡ºåŠ›)ã€‚
    """
    # Input validation
    if len(topics) > 500:
        return "âŒ ãƒˆãƒ”ãƒƒã‚¯ãŒé•·ã™ãã¾ã™ (æœ€å¤§ 500 æ–‡å­—)"
    max_papers = max(1, min(50, max_papers))

    try:
        from mekhane.ergasterion.digestor.pipeline import DigestorPipeline

        topic_list = [t.strip() for t in topics.split(",") if t.strip()] if topics else None
        pipeline = DigestorPipeline()
        report = pipeline.run(
            topics=topic_list,
            max_papers=max_papers,
            dry_run=dry_run,
        )

        mode_label = "ğŸ§ª DRY RUN" if dry_run else "ğŸš€ LIVE"
        result = f"## {mode_label} æ¶ˆåŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œçµæœ\n\n"

        if isinstance(report, dict):
            result += f"- **å–å¾—è«–æ–‡æ•°**: {report.get('fetched', 0)}\n"
            result += f"- **å€™è£œæ•°**: {report.get('candidates', 0)}\n"
            result += f"- **é‡è¤‡æ’é™¤**: {report.get('deduplicated', 0)}\n"
            if not dry_run:
                result += f"- **ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«**: {report.get('generated_files', 0)} ä»¶\n"
        elif isinstance(report, str):
            # Report ãŒæ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™ (5000 æ–‡å­—åˆ¶é™)
            if len(report) > 5000:
                report = report[:5000] + "\n\n... (å‡ºåŠ›åˆ‡ã‚Šè©°ã‚)"
            result += report
        else:
            result += str(report)

        return result
    except ImportError:
        return "âŒ DigestorPipeline ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ (import ã‚¨ãƒ©ãƒ¼)"
    except Exception as e:
        return f"âŒ æ¶ˆåŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}"


# =============================================================================
# OchÄ“ma: LLM å‘¼å‡ºã— (Antigravity LS çµŒç”±)
# =============================================================================

# Rate limiter: 5 req/min
_ask_timestamps: list[float] = []
_ASK_RATE_LIMIT = 5
_ASK_RATE_WINDOW = 60  # seconds
# PURPOSE: [L2-auto] ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒã‚§ãƒƒã‚¯ã€‚True = è¨±å¯ã€False = æ‹’å¦ã€‚


def _check_rate_limit() -> bool:
    """ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒã‚§ãƒƒã‚¯ã€‚True = è¨±å¯ã€False = æ‹’å¦ã€‚"""
    now = time.time()
    _ask_timestamps[:] = [t for t in _ask_timestamps if now - t < _ASK_RATE_WINDOW]
    if len(_ask_timestamps) >= _ASK_RATE_LIMIT:
        return False
    _ask_timestamps.append(now)
    return True

# PURPOSE: IDE ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—ã™ã‚‹
@mcp.tool()
def hgk_sessions() -> str:
    """
    IDE ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ (cascade) ä¸€è¦§ã‚’å–å¾—ã™ã‚‹ã€‚

    å„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã¯ cascade_id, ã‚¹ãƒ†ãƒƒãƒ—æ•°, ã‚µãƒãƒª, æœ€çµ‚æ›´æ–°æ—¥æ™‚ãŒå«ã¾ã‚Œã‚‹ã€‚
    hgk_session_read ã‚„ hgk_ask (cascade_id æŒ‡å®š) ã¨çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ã™ã‚‹ã€‚
    """
    try:
        from mekhane.ochema.antigravity_client import AntigravityClient

        client = AntigravityClient()
        data = client.session_info()

        sessions = data.get("sessions", [])
        if not sessions:
            return "ğŸ“­ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“"

        lines = [f"## ğŸ“‹ IDE ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§ ({data.get('total', 0)} ä»¶)\n"]
        for s in sessions:
            status_icon = "ğŸŸ¢" if s.get("status") == "active" else "âšª"
            summary = s.get("summary", "")[:80] or "(ã‚µãƒãƒªãªã—)"
            lines.append(
                f"- {status_icon} `{s['cascade_id'][:12]}...` "
                f"| {s.get('step_count', 0)} steps "
                f"| {summary}"
            )
        return "\n".join(lines)
    except RuntimeError as e:
        return f"âŒ LS æœªæ¤œå‡º: {e}"
    except Exception as e:
        return f"âŒ ã‚¨ãƒ©ãƒ¼: {e}"


# PURPOSE: IDE ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¼šè©±å†…å®¹ã‚’èª­ã¿å–ã‚‹
@mcp.tool()
def hgk_session_read(
    cascade_id: str,
    max_turns: int = 10,
    full: bool = False,
) -> str:
    """
    IDE ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¼šè©±å†…å®¹ã‚’èª­ã¿å–ã‚‹ã€‚

    user/assistant/tool ã®å…¨ã‚¿ãƒ¼ãƒ³ã‚’æ™‚ç³»åˆ—ã§è¿”ã™ã€‚
    claude.ai â†” IDE ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³åŒæœŸã«ä½¿ç”¨ã™ã‚‹ã€‚

    Args:
        cascade_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã® cascade_id (hgk_sessions ã§å–å¾—)
        max_turns: è¿”ã™æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10)
        full: True â†’ ãƒ•ãƒ«å–å¾— (ä¸Šé™ 30000 æ–‡å­—)
    """
    if not cascade_id or not cascade_id.strip():
        return "âŒ cascade_id ãŒç©ºã§ã™"

    try:
        from mekhane.ochema.antigravity_client import AntigravityClient

        client = AntigravityClient()
        data = client.session_read(
            cascade_id.strip(),
            max_turns=max(1, min(50, max_turns)),
            full=full,
        )

        if "error" in data:
            return f"âŒ {data['error']}"

        conversation = data.get("conversation", [])
        if not conversation:
            return f"ğŸ“­ ã‚»ãƒƒã‚·ãƒ§ãƒ³ `{cascade_id[:12]}...` ã«ä¼šè©±ãŒã‚ã‚Šã¾ã›ã‚“"

        lines = [
            f"## ğŸ’¬ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¼šè©±ãƒ­ã‚°\n",
            f"**Cascade**: `{data['cascade_id']}`",
            f"**Summary**: {data.get('summary', 'N/A')}",
            f"**Total Steps**: {data.get('total_steps', 0)} | "
            f"**Turns shown**: {len(conversation)}\n",
            "---\n",
        ]

        for turn in conversation:
            role = turn.get("role", "")
            if role == "user":
                content = turn.get("content", "")
                trunc = " âœ‚ï¸" if turn.get("truncated") else ""
                lines.append(f"### ğŸ‘¤ User{trunc}\n{content}\n")
            elif role == "assistant":
                content = turn.get("content", "")
                model = turn.get("model", "")
                trunc = " âœ‚ï¸" if turn.get("truncated") else ""
                model_label = f" ({model})" if model else ""
                lines.append(f"### ğŸ¤– Assistant{model_label}{trunc}\n{content}\n")
            elif role == "tool":
                tool_name = turn.get("tool", "unknown")
                lines.append(f"- ğŸ”§ `{tool_name}`\n")

        result = "\n".join(lines)

        # ã‚µã‚¤ã‚ºåˆ¶å¾¡
        max_size = 30000 if full else 15000
        if len(result) > max_size:
            result = result[:max_size] + f"\n\n... (å‡ºåŠ›ãŒ {max_size} æ–‡å­—ã‚’è¶…ãˆãŸãŸã‚åˆ‡ã‚Šè©°ã‚)"

        return result
    except RuntimeError as e:
        return f"âŒ LS æœªæ¤œå‡º: {e}"
    except Exception as e:
        return f"âŒ ã‚¨ãƒ©ãƒ¼: {e}"


# PURPOSE: LLM ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚Šå¿œç­”ã‚’å–å¾—ã™ã‚‹ (Antigravity LS çµŒç”±)
@mcp.tool()
def hgk_ask(
    message: str,
    model: str = "MODEL_CLAUDE_4_5_SONNET_THINKING",
    timeout: int = 120,
    cascade_id: str = "",
) -> str:
    """
    LLM ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚Šå¿œç­”ã‚’å–å¾—ã™ã‚‹ (Antigravity LS çµŒç”±)ã€‚

    ã‚³ã‚¹ãƒˆ0ã€API key ä¸è¦ã€‚IDE ã® Language Server ã‚’çµŒç”±ã—ã¦
    Claude, Gemini, GPT ç­‰ã‚’å‘¼ã³å‡ºã™ã€‚

    cascade_id ã‚’æŒ‡å®šã™ã‚‹ã¨ã€æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æ–‡è„ˆã‚’å¼•ãç¶™ã„ã§è³ªå•ã§ãã‚‹ã€‚
    çœç•¥æ™‚ã¯æ–°è¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ã€‚

    Args:
        message: LLM ã«é€ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ (æœ€å¤§ 5000 æ–‡å­—)
        model: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ« (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Claude Sonnet)
        timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•° (æœ€å¤§ 300)
        cascade_id: æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³ã® cascade_id (çœç•¥æ™‚ã¯æ–°è¦)
    """
    # [C-3] Input validation
    if not message or not message.strip():
        return "âŒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã§ã™"
    if len(message) > 5000:
        return f"âŒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé•·ã™ãã¾ã™ ({len(message)} æ–‡å­—ã€ä¸Šé™ 5000)"
    timeout = max(10, min(300, timeout))

    # Rate limit
    if not _check_rate_limit():
        return "âš ï¸ ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆè¶…é (5 å›/åˆ†)ã€‚å°‘ã—å¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"

    try:
        from mekhane.ochema.antigravity_client import AntigravityClient

        client = AntigravityClient()

        if cascade_id and cascade_id.strip():
            # æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ 
            cid = cascade_id.strip()
            client._send_message(cid, message, model)
            response = client._poll_response(cid, float(timeout))
        else:
            # æ–°è¦ã‚»ãƒƒã‚·ãƒ§ãƒ³
            response = client.ask(message, model=model, timeout=float(timeout))

        result = f"## ğŸ¤– LLM å¿œç­”\n\n**ãƒ¢ãƒ‡ãƒ«**: `{response.model}`\n\n---\n\n{response.text}"

        if response.thinking:
            result += f"\n\n---\n\n<details><summary>ğŸ’­ æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹</summary>\n\n{response.thinking[:2000]}\n\n</details>"

        # W12 Token Explosion å¯¾ç­–
        if len(result) > 8000:
            result = result[:8000] + "\n\n... (å‡ºåŠ›ãŒ 8000 æ–‡å­—ã‚’è¶…ãˆãŸãŸã‚åˆ‡ã‚Šè©°ã‚ã¾ã—ãŸ)"

        return result
    except RuntimeError as e:
        return f"âŒ LS æœªæ¤œå‡º: {e}\n\n> Antigravity IDE ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„"
    except Exception as e:
        return f"âŒ LLM ã‚¨ãƒ©ãƒ¼: {e}"


# PURPOSE: åˆ©ç”¨å¯èƒ½ãª LLM ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã™ã‚‹
@mcp.tool()
def hgk_models() -> str:
    """
    åˆ©ç”¨å¯èƒ½ãª LLM ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã™ã‚‹ã€‚
    Antigravity LS ãŒæä¾›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¨ã‚¯ã‚©ãƒ¼ã‚¿æ®‹é‡ã‚’ç¢ºèªã§ãã‚‹ã€‚
    """
    try:
        from mekhane.ochema.antigravity_client import AntigravityClient

        client = AntigravityClient()
        models = client.list_models()

        if not models:
            return "ğŸ“­ ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"

        lines = ["## ğŸ¤– åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«\n"]
        lines.append("| ãƒ¢ãƒ‡ãƒ« | ãƒ©ãƒ™ãƒ« | æ®‹é‡ |")
        lines.append("|:-------|:-------|-----:|")
        for m in models:
            remaining = m.get("remaining", 0)
            icon = "ğŸŸ¢" if remaining > 50 else "ğŸŸ¡" if remaining > 10 else "ğŸ”´"
            lines.append(
                f"| `{m.get('name', 'unknown')}` "
                f"| {m.get('label', '')} "
                f"| {icon} {remaining}% |"
            )
        return "\n".join(lines)
    except RuntimeError as e:
        return f"âŒ LS æœªæ¤œå‡º: {e}"
    except Exception as e:
        return f"âŒ ã‚¨ãƒ©ãƒ¼: {e}"


# PURPOSE: Antigravity LS ã®æ¥ç¶šçŠ¶æ³ã‚’ç¢ºèªã™ã‚‹
@mcp.tool()
def hgk_ls_status() -> str:
    """
    Antigravity LS ã®æ¥ç¶šçŠ¶æ³ã‚’ç¢ºèªã™ã‚‹ã€‚
    LS ãŒç¨¼åƒã—ã¦ã„ã‚‹ã‹ã€PID, ãƒãƒ¼ãƒˆ, ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    """
    try:
        from mekhane.ochema.antigravity_client import AntigravityClient

        client = AntigravityClient()
        status = client.get_status()

        return f"""## ğŸ”Œ Language Server ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹

**çŠ¶æ…‹**: âœ… æ¥ç¶šæ¸ˆã¿
**PID**: {client.pid}
**Port**: {client.port}

---

{status}"""
    except RuntimeError as e:
        return f"## ğŸ”Œ Language Server ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹\n\n**çŠ¶æ…‹**: âŒ æœªæ¤œå‡º\n**ã‚¨ãƒ©ãƒ¼**: {e}"
    except Exception as e:
        return f"âŒ ã‚¨ãƒ©ãƒ¼: {e}"


# =============================================================================
# Sympatheia: ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§
# =============================================================================

# PURPOSE: HGK ã‚·ã‚¹ãƒ†ãƒ ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ (Sympatheia èª­å–ã‚Š)
@mcp.tool()
def hgk_health() -> str:
    """
    HGK ã‚·ã‚¹ãƒ†ãƒ ã®è©³ç´°ãªå¥å…¨æ€§ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    Heartbeat, WBC ã‚¢ãƒ©ãƒ¼ãƒˆ, Health ã‚¹ã‚³ã‚¢ã‚’ç¢ºèªã€‚
    """
    lines = ["## ğŸ©º HGK Health Report\n"]

    # 1. Heartbeat
    hb_file = _MNEME_DIR / "heartbeat.json"
    if hb_file.exists():
        try:
            hb = json.loads(hb_file.read_text("utf-8"))
            beats = hb.get("totalBeats", 0)
            last = hb.get("lastBeat", "ä¸æ˜")
            lines.append(f"### ğŸ’“ Heartbeat\n- **ç·æ‹å‹•æ•°**: {beats}\n- **æœ€çµ‚æ‹å‹•**: {last}\n")
        except Exception:
            lines.append("### ğŸ’“ Heartbeat\n- âš ï¸ èª­å–ã‚Šã‚¨ãƒ©ãƒ¼\n")
    else:
        lines.append("### ğŸ’“ Heartbeat\n- æœªæ¤œå‡º\n")

    # 2. WBC Alerts
    wbc_file = _MNEME_DIR / "wbc_state.json"
    if wbc_file.exists():
        try:
            wbc = json.loads(wbc_file.read_text("utf-8"))
            total = wbc.get("totalAlerts", 0)
            alerts = wbc.get("alerts", [])
            recent = alerts[-5:] if alerts else []

            lines.append(f"### ğŸ›¡ï¸ WBC Alerts\n- **ç·ã‚¢ãƒ©ãƒ¼ãƒˆæ•°**: {total}\n")
            if recent:
                lines.append("**ç›´è¿‘5ä»¶:**\n")
                for a in reversed(recent):
                    sev = a.get("severity", "?")
                    ts = a.get("timestamp", "?")[:19]
                    details = a.get("details", "")[:80]
                    icon = "ğŸ”´" if sev == "high" else ("ğŸŸ¡" if sev == "medium" else "ğŸŸ¢")
                    lines.append(f"- {icon} [{sev}] {ts} â€” {details}")
                lines.append("")
            else:
                lines.append("- âœ… ã‚¢ãƒ©ãƒ¼ãƒˆãªã—\n")
        except Exception:
            lines.append("### ğŸ›¡ï¸ WBC\n- âš ï¸ èª­å–ã‚Šã‚¨ãƒ©ãƒ¼\n")
    else:
        lines.append("### ğŸ›¡ï¸ WBC\n- æœªæ¤œå‡º\n")

    # 3. Health Metrics (latest entry)
    health_file = _MNEME_DIR / "health_metrics.jsonl"
    if health_file.exists():
        try:
            last_line = ""
            with open(health_file, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        last_line = line
            if last_line:
                metric = json.loads(last_line)
                score = metric.get("score", "?")
                lines.append(f"### ğŸ“Š Health Score\n- **æœ€æ–°ã‚¹ã‚³ã‚¢**: {score}\n")
        except Exception:
            lines.append("### ğŸ“Š Health Score\n- âš ï¸ èª­å–ã‚Šã‚¨ãƒ©ãƒ¼\n")

    # 4. Git Status
    git_file = _MNEME_DIR / "git_sentinel_state.json"
    if git_file.exists():
        try:
            git = json.loads(git_file.read_text("utf-8"))
            dirty = git.get("isDirty", False)
            modified = len(git.get("modifiedFiles", []))
            icon = "ğŸŸ¡" if dirty else "ğŸŸ¢"
            lines.append(f"### {icon} Git\n- **Dirty**: {dirty}\n- **å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«**: {modified}\n")
        except Exception:
            pass

    return "\n".join(lines)


# PURPOSE: æœªèª­é€šçŸ¥ã®ç¢ºèª (Sympatheia notifications)
@mcp.tool()
def hgk_notifications(limit: int = 10) -> str:
    """
    æœªèª­é€šçŸ¥ã‚’ç¢ºèªã™ã‚‹ã€‚
    HGK ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®é€šçŸ¥ (INFO/HIGH/CRITICAL) ã‚’è¡¨ç¤ºã€‚

    Args:
        limit: è¡¨ç¤ºä»¶æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10)
    """
    notif_file = _MNEME_DIR / "notifications.jsonl"
    if not notif_file.exists():
        return "## ğŸ”” é€šçŸ¥\n\nğŸ“­ é€šçŸ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"

    try:
        notifications = []
        with open(notif_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        notifications.append(json.loads(line))
                    except json.JSONDecodeError:
                        pass

        if not notifications:
            return "## ğŸ”” é€šçŸ¥\n\nâœ… é€šçŸ¥ã¯ã‚ã‚Šã¾ã›ã‚“"

        limit = max(1, min(50, limit))
        recent = notifications[-limit:]

        lines = [f"## ğŸ”” é€šçŸ¥ ({len(recent)}/{len(notifications)} ä»¶)\n"]

        for n in reversed(recent):
            level = n.get("level", n.get("notification_level", "INFO"))
            title = n.get("title", "ç„¡é¡Œ")
            body = n.get("body", "")[:100]
            ts = n.get("timestamp", "?")[:19]

            icon = {"CRITICAL": "ğŸ”´", "HIGH": "ğŸŸ ", "INFO": "ğŸ”µ"}.get(level, "âšª")
            lines.append(f"- {icon} **[{level}]** {title}")
            if body:
                lines.append(f"  {body}")
            lines.append(f"  *{ts}*")
            lines.append("")

        return "\n".join(lines)
    except Exception as e:
        return f"âŒ é€šçŸ¥èª­å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}"


# =============================================================================
# Entry Point
# =============================================================================

if __name__ == "__main__":
    # C-1 fail-safe ensures GATEWAY_TOKEN is always set at this point
    print("ğŸ”’ OAuth 2.1 authentication ENABLED")
    print(f"ğŸš€ HGK Gateway starting on {GATEWAY_HOST}:{GATEWAY_PORT}")
    mcp.run(transport="streamable-http")

