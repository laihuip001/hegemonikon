# PROOF: [L2/„Ç§„É≥„Éï„É©] <- mekhane/mcp/ Ochƒìma MCP Server
#!/usr/bin/env python3
"""
Ochƒìma MCP Server ‚Äî Antigravity Language Server Bridge

Antigravity IDE „ÅÆ Language Server „Çí‰ªã„Åó„Å¶ LLM „ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê„ÇíË°å„ÅÜ
MCP „Çµ„Éº„Éê„ÄÇ4-Step API „Éï„É≠„Éº (StartCascade ‚Üí SendMessage ‚Üí
GetTrajectories ‚Üí GetSteps) „ÇíMCP tool „Å®„Åó„Å¶ÂÖ¨Èñã„Åô„Çã„ÄÇ

CRITICAL: This file follows MCP stdio protocol rules:
- stdout: JSON-RPC messages ONLY
- stderr: All logging and debug output

WARNING: ToS „Ç∞„É¨„Éº„Çæ„Éº„É≥„ÄÇÂÆüÈ®ìÁî®ÈÄîÈôêÂÆö„ÄÇÂÖ¨ÈñãÁ¶ÅÊ≠¢„ÄÇ
"""

import sys
import os

# ============ CRITICAL: Platform-specific asyncio setup ============
if sys.platform == "win32":
    import asyncio
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# ============ CRITICAL: Redirect ALL stdout to stderr ============
import io

_original_stdout = sys.stdout
_stderr_wrapper = sys.stderr


# Debug logging to stderr
# PURPOSE: log ‚Äî MCP „Çµ„Éº„Éì„Çπ„ÅÆÂá¶ÁêÜ
def log(msg):
    print(f"[ochema-mcp] {msg}", file=sys.stderr, flush=True)


log("Starting Ochƒìma MCP Server...")
log(f"Python: {sys.executable}")

# ============ Import path setup ============
from pathlib import Path

# mekhane/mcp/ ‚Üí mekhane/ ‚Üí hegemonikon/ (project root)
_mekhane_dir = Path(__file__).parent.parent
_project_root = _mekhane_dir.parent
for _p in [str(_project_root), str(_mekhane_dir)]:
    if _p not in sys.path:
        sys.path.insert(0, _p)
log(f"Added to path: {_project_root}")


# ============ Suppress stdout during imports ============
# PURPOSE: „ÇØ„É©„Çπ: StdoutSuppressor
class StdoutSuppressor:
    def __init__(self):
        self._null = io.StringIO()
        self._old_stdout = None

    # PURPOSE: [L2-auto] ÂÜÖÈÉ®Âá¶ÁêÜ: enter__
    def __enter__(self):
        self._old_stdout = sys.stdout
        sys.stdout = self._null
        return self

    # PURPOSE: [L2-auto] ÂÜÖÈÉ®Âá¶ÁêÜ: exit__
    def __exit__(self, *args):
        sys.stdout = self._old_stdout
        captured = self._null.getvalue()
        if captured.strip():
            log(f"Suppressed stdout: {captured[:100]}...")


# Import MCP SDK
try:
    from mcp.server import Server
    from mcp.server.stdio import stdio_server
    from mcp.types import Tool, TextContent

    log("MCP imports successful")
except Exception as e:
    log(f"MCP import error: {e}")
    sys.exit(1)

# Initialize MCP server
server = Server(
    name="ochema",
    version="0.1.0",
    instructions=(
        "Ochƒìma ‚Äî Antigravity Language Server bridge. "
        "Send prompts to LLM (Claude/Gemini) via local Language Server. "
        "Also provides status and model listing."
    ),
)
log("Server initialized")

# Lazy-loaded client instance
_client = None


# PURPOSE: get_client ‚Äî AntigravityClient „ÅÆ„Ç∑„É≥„Ç∞„É´„Éà„É≥ÂèñÂæó
def get_client():
    """AntigravityClient „Çí„Ç∑„É≥„Ç∞„É´„Éà„É≥„ÅßÂèñÂæó„ÄÇ"""
    global _client
    if _client is None:
        try:
            with StdoutSuppressor():
                from mekhane.ochema.antigravity_client import AntigravityClient
            _client = AntigravityClient()
            log(f"Client initialized: PID={_client.pid} Port={_client.port}")
        except Exception as e:
            log(f"Client init error: {e}")
            raise
    return _client

# PURPOSE: [L2-auto] List available tools.

@server.list_tools()
async def list_tools():
    """List available tools."""
    log("list_tools called")
    return [
        Tool(
            name="ask",
            description=(
                "Send a prompt to an LLM via Antigravity Language Server. "
                "Returns the LLM response text, thinking process, and model info. "
                "Available models: Claude Sonnet 4.5 Thinking (default), "
                "Claude Opus 4.6 Thinking, Gemini 2.5 Pro, Gemini 2.5 Flash, GPT-4.1."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "message": {
                        "type": "string",
                        "description": "The prompt to send to the LLM",
                    },
                    "model": {
                        "type": "string",
                        "description": (
                            "Model enum string. Defaults to MODEL_CLAUDE_4_5_SONNET_THINKING. "
                            "Options: MODEL_CLAUDE_4_5_SONNET_THINKING, "
                            "MODEL_PLACEHOLDER_M26 (Opus 4.6), "
                            "MODEL_GEMINI_2_5_PRO, MODEL_GEMINI_2_5_FLASH, MODEL_GPT_4_1"
                        ),
                        "default": "MODEL_CLAUDE_4_5_SONNET_THINKING",
                    },
                    "timeout": {
                        "type": "number",
                        "description": "Max wait seconds for LLM response (default: 120)",
                        "default": 120,
                    },
                },
                "required": ["message"],
            },
        ),
        Tool(
            name="status",
            description="Get Language Server connection status and quota info.",
            inputSchema={"type": "object", "properties": {}},
        ),
        Tool(
            name="models",
            description="List available LLM models with remaining quota percentages.",
            inputSchema={"type": "object", "properties": {}},
        ),
        Tool(
            name="session_info",
            description=(
                "Get current session info: step count, Context Rot risk score, "
                "and session metadata. Use to monitor context health."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "conversation_id": {
                        "type": "string",
                        "description": "Optional: specific conversation ID to inspect. If omitted, shows all active sessions.",
                    },
                },
            },
        ),
    ]

# PURPOSE: [L2-auto] Handle tool calls.

@server.call_tool(validate_input=True)
async def call_tool(name: str, arguments: dict):
    """Handle tool calls."""
    log(f"call_tool: {name} with {arguments}")

    if name == "ask":
        message = arguments.get("message", "")
        model = arguments.get("model", "MODEL_CLAUDE_4_5_SONNET_THINKING")
        timeout = arguments.get("timeout", 120)

        if not message:
            return [TextContent(type="text", text="Error: message is required")]

        try:
            client = get_client()
            log(f"Sending to {model}: {message[:50]}...")

            response = client.ask(message, model=model, timeout=timeout)

            output_lines = [
                "# Ochƒìma LLM Response\n",
                f"**Model**: {response.model}",
                f"**Cascade ID**: {response.cascade_id[:16]}...",
                "",
                "## Response\n",
                response.text,
            ]

            if response.thinking:
                output_lines.extend([
                    "",
                    "## Thinking\n",
                    response.thinking[:1000],
                ])

            if response.token_usage:
                output_lines.extend([
                    "",
                    "## Token Usage\n",
                    f"```json\n{response.token_usage}\n```",
                ])

            log(f"Response received: {len(response.text)} chars")
            return [TextContent(type="text", text="\n".join(output_lines))]

        except TimeoutError as e:
            log(f"Timeout: {e}")
            return [TextContent(type="text", text=f"Error: LLM response timed out ({timeout}s)")]
        except Exception as e:
            log(f"Ask error: {e}")
            return [TextContent(type="text", text=f"Error: {str(e)}")]

    elif name == "status":
        try:
            client = get_client()
            status = client.get_status()

            user_status = status.get("userStatus", {})
            output_lines = [
                "# Ochƒìma Status\n",
                f"- **PID**: {client.pid}",
                f"- **Port**: {client.port}",
                f"- **Workspace**: {client.workspace}",
                f"- **Name**: {user_status.get('name', 'N/A')}",
            ]

            log("Status returned")
            return [TextContent(type="text", text="\n".join(output_lines))]

        except Exception as e:
            log(f"Status error: {e}")
            return [TextContent(type="text", text=f"Error: {str(e)}")]

    elif name == "models":
        try:
            client = get_client()
            models = client.list_models()

            output_lines = ["# Available Models\n"]
            output_lines.append("| Model | Label | Remaining |")
            output_lines.append("|:------|:------|----------:|")
            for m in models:
                output_lines.append(
                    f"| `{m['name']}` | {m['label']} | {m['remaining']}% |"
                )

            log(f"Models listed: {len(models)}")
            return [TextContent(type="text", text="\n".join(output_lines))]

        except Exception as e:
            log(f"Models error: {e}")
            return [TextContent(type="text", text=f"Error: {str(e)}")]

    elif name == "session_info":
        try:
            client = get_client()
            conv_id = arguments.get("conversation_id")

            # Fetch all trajectories
            all_data = client._rpc(
                "exa.language_server_pb.LanguageServerService/GetAllCascadeTrajectories", {}
            )
            summaries = all_data.get("trajectorySummaries", {})

            if conv_id and conv_id in summaries:
                # Show specific session
                info = summaries[conv_id]
                step_count = info.get("stepCount", 0)
                status = info.get("status", "unknown")
                rot_level = (
                    "üî¥ CRITICAL" if step_count > 50
                    else "üü° WARNING" if step_count > 30
                    else "üü¢ HEALTHY"
                )
                output_lines = [
                    "# Session Info\n",
                    f"- **ID**: `{conv_id[:16]}...`",
                    f"- **Summary**: {info.get('summary', 'N/A')}",
                    f"- **Steps**: {step_count}",
                    f"- **Status**: {status}",
                    f"- **Context Rot**: {rot_level}",
                ]
            else:
                # Show overview of all sessions sorted by step count
                sessions = []
                for cid, info in summaries.items():
                    sessions.append({
                        "id": cid[:12],
                        "summary": info.get("summary", "")[:40],
                        "steps": info.get("stepCount", 0),
                        "status": info.get("status", "unknown"),
                    })
                sessions.sort(key=lambda x: x["steps"], reverse=True)

                output_lines = [
                    f"# Session Overview ({len(sessions)} total)\n",
                    "| ID | Summary | Steps | Rot Risk |",
                    "|:---|:--------|------:|:--------|",
                ]
                for s in sessions[:15]:
                    rot = (
                        "üî¥" if s["steps"] > 50
                        else "üü°" if s["steps"] > 30
                        else "üü¢"
                    )
                    output_lines.append(
                        f"| `{s['id']}` | {s['summary']} | {s['steps']} | {rot} |"
                    )

            log(f"Session info returned: {len(summaries)} sessions")
            return [TextContent(type="text", text="\n".join(output_lines))]

        except Exception as e:
            log(f"Session info error: {e}")
            return [TextContent(type="text", text=f"Error: {str(e)}")]

    else:
        return [TextContent(type="text", text=f"Unknown tool: {name}")]


# PURPOSE: ochema_mcp_server „ÅÆ main Âá¶ÁêÜ„ÇíÂÆüË°å„Åô„Çã
async def main():
    """Run the MCP server."""
    log("Starting stdio server...")
    try:
        async with stdio_server() as streams:
            log("stdio_server connected")
            await server.run(
                streams[0],
                streams[1],
                server.create_initialization_options(),
            )
    except Exception as e:
        log(f"Server error: {e}")
        raise


if __name__ == "__main__":
    import asyncio

    log("Running main...")
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        log("Server stopped by user")
    except Exception as e:
        log(f"Fatal error: {e}")
        sys.exit(1)
