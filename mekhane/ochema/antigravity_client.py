# PURPOSE: OchÄ“ma â€” Antigravity Language Server ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# REASON: Ultra ãƒ—ãƒ©ãƒ³ã® LLM + ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç† + Quota ç›£è¦–ã‚’ HGK ã‹ã‚‰åˆ©ç”¨ã™ã‚‹æ©‹æ¸¡ã—
"""OchÄ“ma (á½„Ï‡Î·Î¼Î±, ä¹—ã‚Šç‰©) â€” Antigravity Language Server Client.

Local Language Server ã® ConnectRPC JSON ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä»‹ã—ã¦
LLM ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’è¡Œã†éå…¬å¼ Python ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã€‚

4-Step API Flow:
    1. StartCascade          â†’ cascade_id å–å¾—
    2. SendUserCascadeMessage â†’ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡
    3. GetAllCascadeTrajectories â†’ trajectory_id å–å¾—
    4. GetCascadeTrajectorySteps â†’ LLM å¿œç­”å–å¾— (ãƒãƒ¼ãƒªãƒ³ã‚°)

WARNING: ToS ã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³ã€‚å®Ÿé¨“ç”¨é€”é™å®šã€‚å…¬é–‹ç¦æ­¢ã€‚
"""

from __future__ import annotations

import json
import re
import ssl
import subprocess
import os
import time
import urllib.request
import uuid
from dataclasses import dataclass, field
from typing import Optional


# --- Data Classes ---

@dataclass
# PURPOSE: [L2-auto] LLM ã‹ã‚‰ã®å¿œç­”ã‚’ä¿æŒã™ã‚‹ã€‚
class LLMResponse:
    """LLM ã‹ã‚‰ã®å¿œç­”ã‚’ä¿æŒã™ã‚‹ã€‚"""
    text: str = ""
    thinking: str = ""
    model: str = ""
    token_usage: dict = field(default_factory=dict)
    cascade_id: str = ""
    trajectory_id: str = ""
    raw_steps: list = field(default_factory=list)


@dataclass
# PURPOSE: [L2-auto] Language Server ã®æ¥ç¶šæƒ…å ±ã€‚
class LSInfo:
    """Language Server ã®æ¥ç¶šæƒ…å ±ã€‚"""
    pid: int = 0
    csrf: str = ""
    port: int = 0
    workspace: str = ""
    all_ports: list = field(default_factory=list)


# --- Constants ---

DEFAULT_MODEL = "MODEL_CLAUDE_4_5_SONNET_THINKING"
DEFAULT_TIMEOUT = 120  # seconds
POLL_INTERVAL = 1.0  # seconds
USER_AGENT = "ochema/0.1"

# RPC endpoints
RPC_BASE = "exa.language_server_pb.LanguageServerService"
RPC_GET_STATUS = f"{RPC_BASE}/GetUserStatus"
RPC_START_CASCADE = f"{RPC_BASE}/StartCascade"
RPC_SEND_MESSAGE = f"{RPC_BASE}/SendUserCascadeMessage"
RPC_GET_TRAJECTORIES = f"{RPC_BASE}/GetAllCascadeTrajectories"
RPC_GET_STEPS = f"{RPC_BASE}/GetCascadeTrajectorySteps"
RPC_MODEL_CONFIG = f"{RPC_BASE}/GetCascadeModelConfigData"
RPC_EXPERIMENT_STATUS = f"{RPC_BASE}/GetStaticExperimentStatus"
RPC_USER_MEMORIES = f"{RPC_BASE}/GetUserMemories"

# Episode memory
BRAIN_DIR = os.path.expanduser("~/.gemini/antigravity/brain")


# --- Client ---

# PURPOSE: [L2-auto] Antigravity Language Server ã®éå…¬å¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã€‚
class AntigravityClient:
    """Antigravity Language Server ã®éå…¬å¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã€‚

    Usage:
        client = AntigravityClient()
        response = client.ask("Say hello world")
        print(response.text)
    """

    # PURPOSE: [L2-auto] LS ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦æ¥ç¶šã™ã‚‹ã€‚
    def __init__(self, workspace: str = "hegemonikon"):
        """LS ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦æ¥ç¶šã™ã‚‹ã€‚

        Args:
            workspace: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹å (ps aux ã®ãƒ•ã‚£ãƒ«ã‚¿ã«ä½¿ç”¨)
        """
        self.workspace = workspace
        self._ssl_ctx = self._make_ssl_context()
        self.ls = self._detect_ls()

    @property
    # PURPOSE: [L2-auto] é–¢æ•°: pid
    def pid(self) -> int:
        return self.ls.pid

    @property
    # PURPOSE: [L2-auto] é–¢æ•°: port
    def port(self) -> int:
        return self.ls.port

    @property
    # PURPOSE: [L2-auto] é–¢æ•°: csrf
    def csrf(self) -> str:
        return self.ls.csrf

    # --- Public API ---

    # PURPOSE: [L2-auto] LLM ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚Šã€å¿œç­”ã‚’å–å¾—ã™ã‚‹ã€‚
    def ask(
        self,
        message: str,
        model: str = DEFAULT_MODEL,
        timeout: float = DEFAULT_TIMEOUT,
    ) -> LLMResponse:
        """LLM ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚Šã€å¿œç­”ã‚’å–å¾—ã™ã‚‹ã€‚

        4-Step ãƒ•ãƒ­ãƒ¼ã‚’å†…éƒ¨ã§å®Ÿè¡Œ:
        1. StartCascade â†’ cascade_id
        2. SendUserCascadeMessage â†’ {}
        3. GetAllCascadeTrajectories â†’ trajectory_id
        4. GetCascadeTrajectorySteps â†’ response (polling)

        Args:
            message: LLM ã«é€ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            model: ãƒ¢ãƒ‡ãƒ«å (enum string)
            timeout: æœ€å¤§å¾…æ©Ÿç§’æ•°

        Returns:
            LLMResponse with text, thinking, model, token_usage
        """
        # Step 1: Start Cascade
        cascade_id = self._start_cascade()

        # Step 2: Send Message
        self._send_message(cascade_id, message, model)

        # Step 3-4: Poll for response
        return self._poll_response(cascade_id, timeout)

    # PURPOSE: [L2-auto] LS ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—ã™ã‚‹ã€‚æ¥ç¶šç¢ºèªã«ã‚‚ä½¿ç”¨ã€‚
    def get_status(self) -> dict:
        """LS ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—ã™ã‚‹ã€‚æ¥ç¶šç¢ºèªã«ã‚‚ä½¿ç”¨ã€‚"""
        return self._rpc(RPC_GET_STATUS, {
            "metadata": {
                "ideName": "antigravity",
                "extensionName": "antigravity",
                "locale": "en",
            }
        })

    # PURPOSE: [L2-auto] åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã™ã‚‹ã€‚
    def list_models(self) -> list[dict]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã™ã‚‹ã€‚"""
        status = self.get_status()
        configs = (
            status.get("userStatus", {})
            .get("cascadeModelConfigData", {})
            .get("clientModelConfigs", [])
        )
        return [
            {
                "name": c.get("modelOrAlias", {}).get("model", ""),
                "label": c.get("label", ""),
                "remaining": round(
                    c.get("quotaInfo", {}).get("remainingFraction", 0) * 100
                ),
            }
            for c in configs
            if c.get("quotaInfo")
        ]

    # PURPOSE: [L2-auto] å…¨ãƒ¢ãƒ‡ãƒ«ã® Quota æ®‹é‡ã¨è¨­å®šã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å–å¾—ã™ã‚‹ã€‚
    def quota_status(self) -> dict:
        """å…¨ãƒ¢ãƒ‡ãƒ«ã® Quota æ®‹é‡ã¨è¨­å®šã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å–å¾—ã™ã‚‹ã€‚

        Returns:
            dict with models (list), experiments (list), total_models (int)
        """
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šã¨ Quota
        config = self._rpc(RPC_MODEL_CONFIG, {})
        models = []
        for c in config.get("clientModelConfigs", []):
            quota = c.get("quotaInfo", {})
            models.append({
                "label": c.get("label", ""),
                "model": c.get("modelOrAlias", {}).get("model", ""),
                "remaining_pct": round(
                    quota.get("remainingFraction", 0) * 100
                ),
                "reset_time": quota.get("resetTime", ""),
                "images": c.get("supportsImages", False),
                "recommended": c.get("isRecommended", False),
            })

        # Experiment flags (context/memory é–¢é€£ã®ã¿)
        exp_data = self._rpc(RPC_EXPERIMENT_STATUS, {})
        context_keys = {
            "CASCADE_USE_EXPERIMENT_CHECKPOINTER",
            "CASCADE_GLOBAL_CONFIG_OVERRIDE",
            "CASCADE_USER_MEMORIES_IN_SYS_PROMPT",
            "CASCADE_ENABLE_AUTOMATED_MEMORIES",
            "CHAT_TOKENS_SOFT_LIMIT",
            "CHAT_COMPLETION_TOKENS_SOFT_LIMIT",
            "CASCADE_MEMORY_CONFIG_OVERRIDE",
            "MAX_PAST_TRAJECTORY_TOKENS_FOR_RETRIEVAL",
            "CUMULATIVE_PROMPT_CASCADE_CONFIG",
            "CORTEX_CONFIG",
        }
        experiments = [
            {
                "key": s.get("experimentKey", ""),
                "enabled": s.get("enabled", False),
            }
            for s in exp_data.get("status", [])
            if s.get("experimentKey", "") in context_keys
        ]

        return {
            "models": models,
            "experiments": experiments,
            "total_models": len(models),
        }

    # PURPOSE: [L2-auto] ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã€‚
    def session_info(self, cascade_id: Optional[str] = None) -> dict:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã€‚

        Args:
            cascade_id: ç‰¹å®šã‚»ãƒƒã‚·ãƒ§ãƒ³ã® ID (çœç•¥æ™‚ã¯å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§)

        Returns:
            dict with sessions (list) or session detail
        """
        data = self._rpc(RPC_GET_TRAJECTORIES, {})
        summaries = data.get("trajectorySummaries", {})

        sessions = []
        for cid, info in summaries.items():
            session = {
                "cascade_id": cid,
                "trajectory_id": info.get("trajectoryId", ""),
                "step_count": info.get("stepCount", 0),
                "summary": info.get("summary", ""),
                "status": info.get("status", ""),
                "created": info.get("createdTime", ""),
                "modified": info.get("lastModifiedTime", ""),
                "last_input_time": info.get("lastUserInputTime", ""),
            }
            sessions.append(session)

        if cascade_id:
            # ç‰¹å®šã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è©³ç´°
            for s in sessions:
                if s["cascade_id"] == cascade_id:
                    # ã‚¹ãƒ†ãƒƒãƒ—è©³ç´°ã‚‚å–å¾—
                    steps_data = self._rpc(RPC_GET_STEPS, {
                        "cascadeId": cascade_id,
                        "trajectoryId": s["trajectory_id"],
                    })
                    step_types: dict[str, int] = {}
                    for step in steps_data.get("steps", []):
                        st = step.get("type", "UNKNOWN")
                        step_types[st] = step_types.get(st, 0) + 1
                    s["step_types"] = step_types
                    return s
            return {"error": f"cascade_id {cascade_id} not found"}

        # æœ€æ–°é †ã«ã‚½ãƒ¼ãƒˆ
        sessions.sort(key=lambda x: x.get("modified", ""), reverse=True)
        return {
            "total": len(sessions),
            "sessions": sessions[:20],  # æœ€æ–°20ä»¶
        }

    # PURPOSE: [L2-auto] ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¼šè©±å†…å®¹ã‚’èª­ã¿å–ã‚‹ã€‚
    def session_read(
        self,
        cascade_id: str,
        max_turns: int = 10,
        full: bool = False,
    ) -> dict:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¼šè©±å†…å®¹ã‚’èª­ã¿å–ã‚‹ã€‚

        ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç¨®åˆ¥ã”ã¨ã«ãƒ‘ãƒ¼ã‚¹ã—ã€æ™‚ç³»åˆ—ã®ä¼šè©±ãƒ­ã‚°ã¨ã—ã¦è¿”ã™ã€‚

        Args:
            cascade_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã® cascade_id
            max_turns: è¿”ã™æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10)
            full: True â†’ ãƒ•ãƒ«å–å¾— (ä¸Šé™ 30000 æ–‡å­—)

        Returns:
            dict with conversation (list of turns), metadata
        """
        # trajectory_id ã‚’å–å¾—
        info = self.session_info(cascade_id)
        if "error" in info:
            return info
        trajectory_id = info.get("trajectory_id", "")
        if not trajectory_id:
            return {"error": f"No trajectory for cascade {cascade_id}"}

        # å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’å–å¾—
        steps_data = self._rpc(RPC_GET_STEPS, {
            "cascadeId": cascade_id,
            "trajectoryId": trajectory_id,
        })
        steps = steps_data.get("steps", [])

        # ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä¼šè©±ã‚¿ãƒ¼ãƒ³ã«ãƒ‘ãƒ¼ã‚¹
        conversation: list[dict] = []
        max_content = 30000 if full else 2000

        for step in steps:
            step_type = step.get("type", "")
            status = step.get("status", "")

            if step_type == "CORTEX_STEP_TYPE_USER_REQUEST":
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
                items = step.get("userRequest", {}).get("items", [])
                text = ""
                for item in items:
                    if "text" in item:
                        text += item["text"]
                if text:
                    conversation.append({
                        "role": "user",
                        "content": text[:max_content],
                        "truncated": len(text) > max_content,
                    })

            elif step_type == "CORTEX_STEP_TYPE_PLANNER_RESPONSE":
                # Claude å¿œç­”
                pr = step.get("plannerResponse", {})
                text = pr.get("response", "")
                model = pr.get("generatorModel", "")
                if text:
                    conversation.append({
                        "role": "assistant",
                        "content": text[:max_content],
                        "model": model,
                        "truncated": len(text) > max_content,
                    })

            elif step_type == "CORTEX_STEP_TYPE_MCP_TOOL":
                # ãƒ„ãƒ¼ãƒ«å‘¼å‡ºã— (ã‚µãƒãƒªã®ã¿)
                tool_info = step.get("mcpToolCall", step.get("toolCall", {}))
                tool_name = tool_info.get("toolName", tool_info.get("name", "unknown"))
                tool_status = "done" if status == "CORTEX_STEP_STATUS_DONE" else status
                conversation.append({
                    "role": "tool",
                    "tool": tool_name,
                    "status": tool_status,
                })

            elif step_type == "CORTEX_STEP_TYPE_TOOL_CALL":
                # å†…éƒ¨ãƒ„ãƒ¼ãƒ«å‘¼å‡ºã—
                tool_call = step.get("toolCall", {})
                tool_name = tool_call.get("name", "unknown")
                conversation.append({
                    "role": "tool",
                    "tool": tool_name,
                    "status": "done" if status == "CORTEX_STEP_STATUS_DONE" else status,
                })

        # æœ€æ–° N ã‚¿ãƒ¼ãƒ³
        if not full and len(conversation) > max_turns * 3:
            # user+assistant+tool ã§ç´„3ã‚¨ãƒ³ãƒˆãƒª/ã‚¿ãƒ¼ãƒ³
            conversation = conversation[-(max_turns * 3):]

        return {
            "cascade_id": cascade_id,
            "trajectory_id": trajectory_id,
            "total_steps": len(steps),
            "total_turns": len(conversation),
            "summary": info.get("summary", ""),
            "conversation": conversation,
        }

    # PURPOSE: [L2-auto] éå»ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ (.system_generated/steps/) ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã€‚
    def session_episodes(self, brain_id: Optional[str] = None) -> dict:
        """éå»ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ (.system_generated/steps/) ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã€‚

        Args:
            brain_id: ç‰¹å®š brain ã® ID (çœç•¥æ™‚ã¯å…¨ brain ä¸€è¦§)

        Returns:
            dict with episodes summary or specific episode contents
        """
        if not os.path.isdir(BRAIN_DIR):
            return {"error": f"Brain directory not found: {BRAIN_DIR}"}

        if brain_id:
            # ç‰¹å®š brain ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å–å¾—
            brain_path = os.path.join(BRAIN_DIR, brain_id, ".system_generated", "steps")
            if not os.path.isdir(brain_path):
                return {"error": f"No episodes for brain {brain_id}"}

            episodes = []
            for step_dir in sorted(os.listdir(brain_path)):
                output_file = os.path.join(brain_path, step_dir, "output.txt")
                if os.path.isfile(output_file):
                    size = os.path.getsize(output_file)
                    # å…ˆé ­200æ–‡å­—ã ã‘èª­ã‚€
                    with open(output_file, "r", errors="replace") as f:
                        preview = f.read(200)
                    episodes.append({
                        "step": int(step_dir) if step_dir.isdigit() else step_dir,
                        "size_bytes": size,
                        "preview": preview,
                    })
            return {
                "brain_id": brain_id,
                "total_episodes": len(episodes),
                "episodes": episodes,
            }

        # å…¨ brain ä¸€è¦§
        brains = []
        for entry in os.listdir(BRAIN_DIR):
            sys_gen = os.path.join(BRAIN_DIR, entry, ".system_generated", "steps")
            if os.path.isdir(sys_gen):
                count = len([
                    d for d in os.listdir(sys_gen)
                    if os.path.isfile(os.path.join(sys_gen, d, "output.txt"))
                ])
                if count > 0:
                    # brain ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆåã‚’å–å¾—
                    task_file = os.path.join(BRAIN_DIR, entry, "task.md")
                    title = ""
                    if os.path.isfile(task_file):
                        with open(task_file, "r", errors="replace") as f:
                            for line in f:
                                if line.startswith("# "):
                                    title = line[2:].strip()
                                    break
                    brains.append({
                        "brain_id": entry,
                        "episode_count": count,
                        "title": title,
                    })
        brains.sort(key=lambda x: x["episode_count"], reverse=True)
        return {
            "total_brains": len(brains),
            "brains": brains,
        }

    # --- Proposal A: Context Rot Detection ---

    # PURPOSE: [L2-auto] ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¥å…¨æ€§ã‚’è©•ä¾¡ã™ã‚‹ã€‚
    def context_health(self, cascade_id: Optional[str] = None) -> dict:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¥å…¨æ€§ã‚’è©•ä¾¡ã™ã‚‹ã€‚

        tool-mastery.md Â§5.5 ã® N chat messages é–¾å€¤ã«åŸºã¥ã:
            â‰¤30: ğŸŸ¢ HEALTHY
            31-50: ğŸŸ¡ WARNING
            >50: ğŸ”´ DANGER

        Args:
            cascade_id: ç‰¹å®šã‚»ãƒƒã‚·ãƒ§ãƒ³ (çœç•¥æ™‚ã¯æœ€æ–°ã® RUNNING ã‚»ãƒƒã‚·ãƒ§ãƒ³)

        Returns:
            dict with level, message, step_count, recommendation
        """
        sessions = self.session_info()
        if "error" in sessions:
            return sessions

        target = None
        if cascade_id:
            for s in sessions.get("sessions", []):
                if s["cascade_id"] == cascade_id:
                    target = s
                    break
        else:
            # æœ€æ–°ã® RUNNING ã‚»ãƒƒã‚·ãƒ§ãƒ³
            for s in sessions.get("sessions", []):
                if "RUNNING" in s.get("status", ""):
                    target = s
                    break
            # ãªã‘ã‚Œã°æœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³
            if not target and sessions.get("sessions"):
                target = sessions["sessions"][0]

        if not target:
            return {"level": "unknown", "message": "No sessions found"}

        step_count = target.get("step_count", 0)

        if step_count <= 30:
            level = "healthy"
            icon = "ğŸŸ¢"
            message = "Context is healthy"
            recommendation = None
        elif step_count <= 50:
            level = "warning"
            icon = "ğŸŸ¡"
            message = "Context pressure rising"
            recommendation = "Consider /bye soon"
        else:
            level = "danger"
            icon = "ğŸ”´"
            message = "Context Rot risk HIGH"
            recommendation = "/bye recommended â€” context degradation likely"

        # Quota ã‚‚çµ±åˆ
        try:
            quota = self.quota_status()
            low_quota_models = [
                m["label"] for m in quota.get("models", [])
                if m["remaining_pct"] < 20
            ]
        except Exception:
            low_quota_models = []

        return {
            "level": level,
            "icon": icon,
            "message": message,
            "step_count": step_count,
            "cascade_id": target.get("cascade_id", ""),
            "summary": target.get("summary", ""),
            "recommendation": recommendation,
            "low_quota_models": low_quota_models,
        }

    # --- Proposal C: Multi-Model Orchestration ---

    # Model routing table: task keywords â†’ preferred model
    _MODEL_ROUTES = {
        # Claude Thinking â€” deep analysis, security, architecture
        "MODEL_CLAUDE_4_5_SONNET_THINKING": [
            "security", "audit", "architecture", "design", "review",
            "analyze", "explain", "why", "philosophy", "proof",
        ],
        # Gemini Flash â€” speed, simple tasks
        "MODEL_PLACEHOLDER_M18": [
            "translate", "format", "list", "simple", "quick",
            "calculate", "convert", "summarize",
        ],
        # Gemini Pro â€” general purpose, multimodal
        "MODEL_PLACEHOLDER_M8": [
            "image", "video", "multimodal", "diagram", "chart",
        ],
    }

    # Fallback chain
    _MODEL_FALLBACK = {
        "MODEL_CLAUDE_4_5_SONNET_THINKING": "MODEL_PLACEHOLDER_M26",
        "MODEL_PLACEHOLDER_M26": "MODEL_CLAUDE_4_5_SONNET",
        "MODEL_CLAUDE_4_5_SONNET": "MODEL_PLACEHOLDER_M18",
        "MODEL_PLACEHOLDER_M8": "MODEL_PLACEHOLDER_M18",
        "MODEL_PLACEHOLDER_M18": "MODEL_CLAUDE_4_5_SONNET",
    }

    # PURPOSE: [L2-auto] ã‚¿ã‚¹ã‚¯å†…å®¹ã«å¿œã˜ã¦æœ€é©ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•é¸æŠã—ã€LLM ã«å•ã„åˆã‚ã›ã‚‹ã€‚
    def smart_ask(
        self,
        message: str,
        timeout: float = DEFAULT_TIMEOUT,
    ) -> LLMResponse:
        """ã‚¿ã‚¹ã‚¯å†…å®¹ã«å¿œã˜ã¦æœ€é©ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•é¸æŠã—ã€LLM ã«å•ã„åˆã‚ã›ã‚‹ã€‚

        T2 Krisis priority rules ã‚’å†…éƒ¨ã§å†ç¾:
            1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã§ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
            2. Quota æ®‹é‡ãƒã‚§ãƒƒã‚¯ (20%æœªæº€ãªã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
            3. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ Claude Sonnet 4.5 Thinking

        Args:
            message: LLM ã«é€ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            timeout: æœ€å¤§å¾…æ©Ÿç§’æ•°

        Returns:
            LLMResponse (model ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å)
        """
        selected = self._select_model(message)
        return self.ask(message, model=selected, timeout=timeout)

    # PURPOSE: [L2-auto] ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹ã¨ Quota ã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ã€‚
    def _select_model(self, message: str) -> str:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹ã¨ Quota ã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ã€‚"""
        msg_lower = message.lower()

        # Step 1: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ
        best_model = DEFAULT_MODEL
        best_score = 0
        for model, keywords in self._MODEL_ROUTES.items():
            score = sum(1 for kw in keywords if kw in msg_lower)
            if score > best_score:
                best_score = score
                best_model = model

        # Step 2: Quota ãƒã‚§ãƒƒã‚¯ â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        try:
            quota = self.quota_status()
            model_quota = {
                m["model"]: m["remaining_pct"]
                for m in quota.get("models", [])
            }

            current = best_model
            attempts = 0
            while attempts < 3:
                remaining = model_quota.get(current, 100)
                if remaining >= 20:
                    return current
                # Quota ä¸è¶³ â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                fallback = self._MODEL_FALLBACK.get(current)
                if not fallback:
                    break
                current = fallback
                attempts += 1
        except Exception:
            pass

        return best_model

    # --- Proposal D: Session Archive ---

    # PURPOSE: [L2-auto] ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ Markdown ã§ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã™ã‚‹ã€‚
    def archive_sessions(
        self,
        output_dir: Optional[str] = None,
        max_sessions: int = 5,
        since: Optional[str] = None,
    ) -> dict:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ Markdown ã§ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã™ã‚‹ã€‚

        Args:
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ~/oikos/mneme/.ochema/sessions/)
            max_sessions: æœ€å¤§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ•°
            since: ã“ã®æ—¥æ™‚ä»¥é™ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ (ISO format)

        Returns:
            dict with exported (list of paths), skipped (int)
        """
        if output_dir is None:
            output_dir = os.path.expanduser(
                "~/oikos/mneme/.ochema/sessions"
            )
        os.makedirs(output_dir, exist_ok=True)

        sessions = self.session_info()
        if "error" in sessions:
            return sessions

        exported: list[str] = []
        skipped = 0

        for s in sessions.get("sessions", [])[:max_sessions]:
            cid = s["cascade_id"]
            modified = s.get("modified", "")

            # since ãƒ•ã‚£ãƒ«ã‚¿
            if since and modified < since:
                skipped += 1
                continue

            # æ—¢ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ¸ˆã¿ã‹ç¢ºèª
            filename = f"session_{cid[:12]}_{modified[:10]}.md"
            filepath = os.path.join(output_dir, filename)
            if os.path.exists(filepath):
                skipped += 1
                continue

            # ä¼šè©±ã‚’å–å¾—
            try:
                conv = self.session_read(cid, max_turns=50, full=True)
            except Exception:
                skipped += 1
                continue

            # Markdown ç”Ÿæˆ
            lines = [
                f"# Session {cid[:12]}",
                f"",
                f"- **Cascade ID**: `{cid}`",
                f"- **Modified**: {modified}",
                f"- **Steps**: {conv.get('total_steps', 0)}",
                f"- **Summary**: {conv.get('summary', '(none)')}",
                f"",
                f"---",
                f"",
            ]

            for turn in conv.get("conversation", []):
                role = turn.get("role", "")
                if role == "user":
                    lines.append(f"## ğŸ‘¤ User\n")
                    lines.append(turn.get("content", ""))
                    lines.append("")
                elif role == "assistant":
                    model = turn.get("model", "")
                    lines.append(f"## ğŸ¤– Assistant ({model})\n")
                    lines.append(turn.get("content", ""))
                    lines.append("")
                elif role == "tool":
                    tool = turn.get("tool", "")
                    lines.append(f"- ğŸ”§ `{tool}` ({turn.get('status', '')})")

            # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãå‡ºã—
            with open(filepath, "w", encoding="utf-8") as f:
                f.write("\n".join(lines))
            exported.append(filepath)

        return {
            "exported": exported,
            "skipped": skipped,
            "output_dir": output_dir,
        }

    # --- Internal: LS Detection ---

    # PURPOSE: [L2-auto] Language Server ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ¤œå‡ºã—ã€æ¥ç¶šæƒ…å ±ã‚’è¿”ã™ã€‚
    def _detect_ls(self) -> LSInfo:
        """Language Server ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ¤œå‡ºã—ã€æ¥ç¶šæƒ…å ±ã‚’è¿”ã™ã€‚

        agq-check.sh L80-117 ã® Python ç§»æ¤ã€‚
        """
        info = LSInfo(workspace=self.workspace)

        # Step 1: ps aux ã‹ã‚‰ LS ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ¤œå‡º
        try:
            ps_out = subprocess.check_output(
                ["ps", "aux"], text=True, stderr=subprocess.DEVNULL
            )
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"ps aux failed: {e}") from e

        proc_line = ""
        # Antigravity ã¯ãƒ•ã‚©ãƒ«ãƒ€åã®ãƒã‚¤ãƒ•ãƒ³ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«å¤‰æ›ã™ã‚‹
        # (e.g., synteleia-sandbox â†’ synteleia_sandbox)
        ws_normalized = self.workspace.replace("-", "_")
        for line in ps_out.splitlines():
            if "language_server_linux" in line:
                line_normalized = line.replace("-", "_")
                if ws_normalized in line_normalized:
                    if "grep" not in line:
                        proc_line = line
                        break

        if not proc_line:
            raise RuntimeError(
                f"Language Server not found (workspace: {self.workspace})"
            )

        # PID å–å¾—
        parts = proc_line.split()
        info.pid = int(parts[1])

        # CSRF ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—
        csrf_match = re.search(r"csrf_token\s+(\S+)", proc_line)
        if not csrf_match:
            raise RuntimeError("CSRF token not found in process cmdline")
        info.csrf = csrf_match.group(1)

        # Step 2: ss ã‹ã‚‰ãƒªã‚¹ãƒ‹ãƒ³ã‚°ãƒãƒ¼ãƒˆã‚’å–å¾—
        try:
            ss_out = subprocess.check_output(
                ["ss", "-tlnp"], text=True, stderr=subprocess.DEVNULL
            )
        except subprocess.CalledProcessError:
            ss_out = ""

        ports = set()
        for line in ss_out.splitlines():
            if f"pid={info.pid}" in line:
                port_match = re.search(r"127\.0\.0\.1:(\d+)", line)
                if port_match:
                    ports.add(int(port_match.group(1)))

        if not ports:
            raise RuntimeError(f"No listening ports for PID {info.pid}")

        info.all_ports = sorted(ports)

        # Step 3: å…¨ãƒãƒ¼ãƒˆè©¦è¡Œ â†’ GetUserStatus æˆåŠŸã§ç¢ºå®š
        for port in info.all_ports:
            try:
                result = self._raw_rpc(port, info.csrf, RPC_GET_STATUS, {
                    "metadata": {
                        "ideName": "antigravity",
                        "extensionName": "antigravity",
                        "locale": "en",
                    }
                })
                if "userStatus" in result:
                    info.port = port
                    return info
            except Exception:
                continue

        raise RuntimeError(
            f"All ports failed ({info.all_ports}) for GetUserStatus"
        )

    # --- Internal: 4-Step Flow ---

    # CortexTrajectorySource enum (from extension.js proto3 å®šç¾©):
    #   0=UNSPECIFIED, 1=CASCADE_CLIENT, 2=EXPLAIN_PROBLEM,
    #   12=INTERACTIVE_CASCADE (IDE æ¨™æº–), 15=SDK
    SOURCE_INTERACTIVE_CASCADE = 12

    # PURPOSE: [L2-auto] Step 1: StartCascade â†’ cascade_id ã‚’å–å¾—ã€‚
    def _start_cascade(self) -> str:
        """Step 1: StartCascade â†’ cascade_id ã‚’å–å¾—ã€‚"""
        result = self._rpc(RPC_START_CASCADE, {
            "source": self.SOURCE_INTERACTIVE_CASCADE,
        })
        cascade_id = result.get("cascadeId", "")
        if not cascade_id:
            raise RuntimeError(f"StartCascade returned no cascadeId: {result}")
        return cascade_id

    # PURPOSE: [L2-auto] Step 2: SendUserCascadeMessage â†’ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ã€‚
    def _send_message(self, cascade_id: str, message: str, model: str) -> None:
        """Step 2: SendUserCascadeMessage â†’ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ã€‚

        walkthrough.md ã®å®Ÿè¨¼æ¸ˆã¿ curl æ§‹é€ ã«æº–æ‹ :
        - items: ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã«ç›´æ¥é…ç½®
        - cascadeConfig.plannerConfig: conversational + planModel
        """
        self._rpc(RPC_SEND_MESSAGE, {
            "cascadeId": cascade_id,
            "items": [{"text": message}],
            "cascadeConfig": {
                "plannerConfig": {
                    "conversational": {},
                    "planModel": model,
                },
            },
        })

    # PURPOSE: [L2-auto] Step 3-4: ãƒãƒ¼ãƒªãƒ³ã‚°ã§ LLM å¿œç­”ã‚’å–å¾—ã€‚
    def _poll_response(
        self, cascade_id: str, timeout: float
    ) -> LLMResponse:
        """Step 3-4: ãƒãƒ¼ãƒªãƒ³ã‚°ã§ LLM å¿œç­”ã‚’å–å¾—ã€‚

        walkthrough.md ã®æ§‹é€ :
        - GetAllCascadeTrajectories: trajectorySummaries[cascadeId].trajectoryId
        - GetCascadeTrajectorySteps: cascadeId + trajectoryId ãŒå¿…é ˆ
        """
        start_time = time.time()
        trajectory_id = ""

        while time.time() - start_time < timeout:
            # Step 3: trajectory_id ã‚’å–å¾—
            if not trajectory_id:
                try:
                    trajs = self._rpc(RPC_GET_TRAJECTORIES, {})
                    # Response: {"trajectorySummaries": {"<cascadeId>": {...}}}
                    summaries = trajs.get("trajectorySummaries", {})
                    cascade_summary = summaries.get(cascade_id, {})
                    if cascade_summary:
                        trajectory_id = cascade_summary.get(
                            "trajectoryId", ""
                        )
                except Exception:
                    pass

            # Step 4: trajectory ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å–å¾—
            if trajectory_id:
                try:
                    steps_result = self._rpc(RPC_GET_STEPS, {
                        "cascadeId": cascade_id,
                        "trajectoryId": trajectory_id,
                    })
                    steps = steps_result.get("steps", [])
                    turn_state = steps_result.get("turnState", "")

                    # PLANNER_RESPONSE ã‚¹ãƒ†ãƒƒãƒ—ãŒå­˜åœ¨ã—ã€çŠ¶æ…‹ãŒå®Œäº†ã‚’ç¤ºã™
                    has_response = any(
                        s.get("type") == "CORTEX_STEP_TYPE_PLANNER_RESPONSE"
                        and s.get("status") == "CORTEX_STEP_STATUS_DONE"
                        for s in steps
                    )
                    # turnState: "" (ç©º) or "TURN_STATE_WAITING_FOR_USER"
                    is_done = turn_state in (
                        "", "TURN_STATE_WAITING_FOR_USER",
                    )
                    if has_response and is_done:
                        return self._parse_steps(
                            steps, cascade_id, trajectory_id
                        )
                except Exception:
                    pass

            time.sleep(POLL_INTERVAL)

        raise TimeoutError(
            f"LLM response timed out after {timeout}s "
            f"(cascade_id={cascade_id})"
        )

    # PURPOSE: [L2-auto] ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰ LLM å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ã€‚
    def _parse_steps(
        self,
        steps: list,
        cascade_id: str,
        trajectory_id: str,
    ) -> LLMResponse:
        """ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰ LLM å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ã€‚

        walkthrough.md ã®æ§‹é€ :
        - type: CORTEX_STEP_TYPE_PLANNER_RESPONSE
        - plannerResponse: {response, thinking, generatorModel, ...}
        """
        response = LLMResponse(
            cascade_id=cascade_id,
            trajectory_id=trajectory_id,
            raw_steps=steps,
        )

        for step in steps:
            step_type = step.get("type", "")
            if step_type == "CORTEX_STEP_TYPE_PLANNER_RESPONSE":
                pr = step.get("plannerResponse", {})
                if "response" in pr:
                    response.text += pr["response"]
                if "thinking" in pr:
                    response.thinking += pr["thinking"]
                if "generatorModel" in pr:
                    response.model = pr["generatorModel"]
                if "tokenUsage" in pr:
                    response.token_usage = pr["tokenUsage"]

        return response

    # --- Internal: HTTP/RPC ---

    # PURPOSE: [L2-auto] ConnectRPC JSON ã§ RPC ã‚’å‘¼ã³å‡ºã™ã€‚
    def _rpc(self, endpoint: str, payload: dict) -> dict:
        """ConnectRPC JSON ã§ RPC ã‚’å‘¼ã³å‡ºã™ã€‚"""
        return self._raw_rpc(self.ls.port, self.ls.csrf, endpoint, payload)

    # PURPOSE: [L2-auto] ä½ãƒ¬ãƒ™ãƒ« RPC å‘¼ã³å‡ºã—ã€‚
    def _raw_rpc(
        self, port: int, csrf: str, endpoint: str, payload: dict
    ) -> dict:
        """ä½ãƒ¬ãƒ™ãƒ« RPC å‘¼ã³å‡ºã—ã€‚"""
        url = f"https://127.0.0.1:{port}/{endpoint}"
        data = json.dumps(payload).encode("utf-8")

        req = urllib.request.Request(
            url,
            data=data,
            method="POST",
            headers={
                "Content-Type": "application/json",
                "X-Codeium-Csrf-Token": csrf,
                "Connect-Protocol-Version": "1",
                "User-Agent": USER_AGENT,
            },
        )

        with urllib.request.urlopen(req, context=self._ssl_ctx, timeout=10) as resp:
            body = resp.read().decode("utf-8")
            if not body:
                return {}
            return json.loads(body)

    # --- Internal: Utilities ---

    @staticmethod
    # PURPOSE: [L2-auto] è‡ªå·±ç½²åè¨¼æ˜æ›¸ã‚’è¨±å¯ã™ã‚‹ SSL ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‚
    def _make_ssl_context() -> ssl.SSLContext:
        """è‡ªå·±ç½²åè¨¼æ˜æ›¸ã‚’è¨±å¯ã™ã‚‹ SSL ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‚"""
        ctx = ssl.create_default_context()
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        return ctx

    @staticmethod
    # PURPOSE: [L2-auto] ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å–å¾—ã€‚
    def _get_user() -> str:
        """ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å–å¾—ã€‚"""
        import os
        return os.environ.get("USER", "makaron8426")
