#!/usr/bin/env python3
# PROOF: [L2/Mekhane] <- ochema/ A0â†’Implementationâ†’hgk_dev
# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- mekhane/ochema/ A0â†’Cortex ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ HGK é–‹ç™º CLI
# PURPOSE: IDE ãªã—ã§ Cortex API çµŒç”±ã® HGK é–‹ç™ºã‚’å¯èƒ½ã«ã™ã‚‹å¯¾è©±åž‹ CLI

"""
hgk-dev â€” Cortex API ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ HGK é–‹ç™º CLI

IDE (Antigravity) ã‚’èµ·å‹•ã›ãšã«ã€Cortex API çµŒç”±ã§ AI ã¨å¯¾è©±ã—ãªãŒã‚‰
HGK é–‹ç™ºã‚’è¡Œã†ãŸã‚ã® CLI ãƒ„ãƒ¼ãƒ«ã€‚

Architecture:
    User (terminal) â†’ hgk-dev CLI â†’ CortexClient
        â†’ Gemini: ask_with_tools() (generateContent + Function Calling)
        â†’ Claude: chat() (generateChat) + text-based tool use
    Both routes support local file read/write/search/command execution.

Available models (ALL free via AI Ultra):
    Gemini: gemini-3-pro-preview, gemini-2.5-pro, gemini-2.5-flash
    Claude: claude-sonnet-4-5, claude-opus-4-6

Usage:
    python -m mekhane.ochema.hgk_dev
    python -m mekhane.ochema.hgk_dev --model claude-sonnet-4-5
    python -m mekhane.ochema.hgk_dev --model gemini-3-pro-preview --budget 8192
"""

from __future__ import annotations

import argparse
import json
import os
import readline
import sys
import time
from pathlib import Path
from typing import Any

# --- Constants ---

HISTFILE = Path.home() / ".hgk_dev_history"
MAX_HISTORY = 500

# Claude models use generateChat + text-based tool use
CLAUDE_MODELS = {"claude-sonnet-4-5", "claude-opus-4-6"}

# Gemini models use generateContent + native Function Calling
GEMINI_MODELS = {
    "gemini-3-pro-preview", "gemini-3-flash-preview",
    "gemini-2.5-pro", "gemini-2.5-flash",
    "gemini-2.0-flash",
}

ALL_MODELS = CLAUDE_MODELS | GEMINI_MODELS

DEFAULT_MODEL = "gemini-3-pro-preview"

# --- Colors ---

class C:
    """ANSI color codes."""
    RESET = "\033[0m"
    BOLD = "\033[1m"
    DIM = "\033[2m"
    RED = "\033[31m"
    GREEN = "\033[32m"
    YELLOW = "\033[33m"
    BLUE = "\033[34m"
    MAGENTA = "\033[35m"
    CYAN = "\033[36m"


# --- System Prompts ---

HGK_DEV_SYSTEM_PROMPT = """\
ã‚ãªãŸã¯ HegemonikÃ³n èªçŸ¥ãƒã‚¤ãƒ‘ãƒ¼ãƒã‚¤ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®é–‹ç™ºã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿æ›¸ãã€æ¤œç´¢ã€ã‚³ãƒžãƒ³ãƒ‰å®Ÿè¡ŒãŒå¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’æŒã£ã¦ã„ã¾ã™ã€‚

## ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
- ~/oikos/hegemonikon â€” ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
- ~/oikos/mneme â€” è¨˜æ†¶ãƒ»ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 
- kernel/: å…¬ç†ä½“ç³» (SACRED â€” å¤‰æ›´ç¦æ­¢)
- mekhane/: å®Ÿè£… (ochema, periskope, mcp, basanos, etc.)
- synergeia/: çµ±åˆãƒ¬ã‚¤ãƒ¤ãƒ¼
- .agent/: ãƒ«ãƒ¼ãƒ«ã€ã‚¹ã‚­ãƒ«ã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

## ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
- ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´å‰ã«å¿…ãš read_file ã§ç¾çŠ¶ã‚’ç¢ºèªã™ã‚‹
- ç ´å£Šçš„æ“ä½œå‰ã«è¨ˆç”»ã‚’èª¬æ˜Žã™ã‚‹
- ä¸ç¢ºã‹ãªå ´åˆã¯ [æŽ¨å®š] [ä»®èª¬] ãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ã‚‹
- å…¨å‡ºåŠ›ã¯æ—¥æœ¬èªž (ã‚³ãƒ¼ãƒ‰ã‚³ãƒ¡ãƒ³ãƒˆã¯è‹±èªžå¯)
- kernel/ ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ã—ã¦ã¯ãªã‚‰ãªã„
- rm -rf, git push --force ã¯å®Ÿè¡Œç¦æ­¢
- ãƒ†ã‚¹ãƒˆ: cd ~/oikos/hegemonikon && .venv/bin/python -m pytest
"""


def _is_claude(model: str) -> bool:
    """Check if model is a Claude model."""
    return model in CLAUDE_MODELS or model.startswith("claude")


# --- Gemini Agent (native Function Calling) ---

def _run_gemini_turn(
    client: Any,
    message: str,
    *,
    model: str,
    system_instruction: str,
    thinking_budget: int | None,
    max_iterations: int,
    max_tokens: int,
    timeout: float,
) -> str:
    """Run a single turn with Gemini's native tool use."""
    response = client.ask_with_tools(
        message=message,
        model=model,
        system_instruction=system_instruction,
        thinking_budget=thinking_budget,
        max_iterations=max_iterations,
        max_tokens=max_tokens,
        timeout=timeout,
    )
    return response.text


# --- Claude Agent (text-based tool use via generateChat) ---

def _run_claude_turn(
    client: Any,
    message: str,
    history: list[dict[str, Any]],
    *,
    model: str,
    system_instruction: str,
    max_iterations: int,
    timeout: float,
) -> tuple[str, list[dict[str, Any]]]:
    """Run a single turn with Claude's text-based tool use.

    Returns:
        (final_text, updated_history)
    """
    from .tools import (
        build_tool_descriptions,
        execute_tool,
        has_tool_calls,
        parse_tool_calls_from_text,
        strip_tool_calls,
    )

    # Prepend tool instructions to the user message on first turn
    if not history:
        tool_desc = build_tool_descriptions()
        prefixed = (
            f"{system_instruction}\n\n"
            f"## ãƒ„ãƒ¼ãƒ«\n"
            f"ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†ã¨ãã¯ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã§å¿œç­”ã—ã¦ãã ã•ã„:\n\n"
            f"```tool_call\n"
            f'{{"name": "tool_name", "args": {{"arg1": "value1"}}}}\n'
            f"```\n\n"
            f"åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:\n{tool_desc}\n\n"
            f"ãƒ‘ã‚¹ã¯ /home/makaron8426/oikos/ ä»¥ä¸‹ã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚\n\n"
            f"---\n\n{message}"
        )
    else:
        prefixed = message

    # Add user message to history
    history.append({"author": 1, "content": prefixed})

    for iteration in range(max_iterations):
        response = client.chat(
            message=prefixed if iteration == 0 else "",
            model=model,
            history=history if iteration > 0 else history[:-1],
            timeout=timeout,
        )

        text = response.text

        if not has_tool_calls(text):
            # No tool calls â€” final response
            history.append({"author": 2, "content": text})
            return text, history

        # Execute tool calls
        tool_calls = parse_tool_calls_from_text(text)
        results: list[str] = []

        for tc in tool_calls:
            name = tc["name"]
            args = tc["args"]
            print(f"  {C.DIM}ðŸ”§ {name}({json.dumps(args, ensure_ascii=False)[:80]}){C.RESET}")
            result = execute_tool(name, args)
            results.append(f"## {name} result:\n```json\n{json.dumps(result, ensure_ascii=False, indent=2)[:2000]}\n```")

        # Add model response + tool results to history
        history.append({"author": 2, "content": text})
        tool_result_text = "\n\n".join(results)
        history.append({"author": 1, "content": f"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæžœ:\n\n{tool_result_text}\n\nç¶šã‘ã¦ãã ã•ã„ã€‚"})
        prefixed = ""  # Next iteration uses history only

    # Max iterations
    narrative = strip_tool_calls(text) if text else ""
    history.append({"author": 2, "content": narrative + "\n\n(æœ€å¤§ãƒ„ãƒ¼ãƒ«ä½¿ç”¨å›žæ•°ã«é”ã—ã¾ã—ãŸ)"})
    return narrative + "\n\n(æœ€å¤§ãƒ„ãƒ¼ãƒ«ä½¿ç”¨å›žæ•°ã«é”ã—ã¾ã—ãŸ)", history


# --- REPL ---

def _print_banner(model: str, thinking_budget: int | None) -> None:
    """Print startup banner."""
    model_type = "Claude (generateChat)" if _is_claude(model) else "Gemini (generateContent)"
    budget_str = f" | budget={thinking_budget}" if thinking_budget is not None else ""

    print(f"""
{C.CYAN}{C.BOLD}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        hgk-dev â€” ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ HGK é–‹ç™º CLI       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{C.RESET}
  {C.GREEN}ãƒ¢ãƒ‡ãƒ«{C.RESET}: {model} ({model_type}{budget_str})
  {C.GREEN}ãƒ„ãƒ¼ãƒ«{C.RESET}: read_file, write_file, search_text, run_command, git_diff, git_log
  {C.GREEN}ã‚³ãƒžãƒ³ãƒ‰{C.RESET}: /quit /model /clear /help
  {C.DIM}Cortex API çµŒç”± â€” IDE ä¸è¦ â€” å…¨ãƒ¢ãƒ‡ãƒ«ç„¡æ–™ (AI Ultra){C.RESET}
""")


def _print_help() -> None:
    """Print help."""
    print(f"""
{C.BOLD}ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚³ãƒžãƒ³ãƒ‰:{C.RESET}
  /quit, /q       â€” çµ‚äº†
  /model <name>   â€” ãƒ¢ãƒ‡ãƒ«åˆ‡æ›¿ (e.g. /model claude-sonnet-4-5)
  /models         â€” åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
  /clear          â€” ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢
  /history        â€” ä¼šè©±å±¥æ­´è¡¨ç¤º
  /help, /h       â€” ã“ã®ãƒ˜ãƒ«ãƒ—
""")


def main() -> None:
    """Main REPL entry point."""
    parser = argparse.ArgumentParser(
        description="hgk-dev â€” Cortex API ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ HGK é–‹ç™º CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=(
            "ä¾‹:\n"
            "  python -m mekhane.ochema.hgk_dev\n"
            "  python -m mekhane.ochema.hgk_dev --model claude-sonnet-4-5\n"
            "  python -m mekhane.ochema.hgk_dev --model gemini-2.5-pro --budget 8192\n"
        ),
    )
    parser.add_argument("--model", "-m", default=DEFAULT_MODEL,
                       help=f"ãƒ¢ãƒ‡ãƒ« (default: {DEFAULT_MODEL})")
    parser.add_argument("--budget", "-b", type=int, default=None,
                       help="Thinking budget (Gemini ã®ã¿)")
    parser.add_argument("--max-iterations", "-i", type=int, default=10,
                       help="æœ€å¤§ãƒ„ãƒ¼ãƒ«ä½¿ç”¨å›žæ•°/ã‚¿ãƒ¼ãƒ³ (default: 10)")
    parser.add_argument("--max-tokens", "-t", type=int, default=8192,
                       help="æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ (default: 8192)")
    parser.add_argument("--timeout", type=float, default=120.0,
                       help="API ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’ (default: 120)")

    args = parser.parse_args()
    model = args.model
    thinking_budget = args.budget

    # Import client
    try:
        from .cortex_client import CortexClient
    except ImportError:
        # Direct execution
        sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
        from mekhane.ochema.cortex_client import CortexClient

    client = CortexClient()

    # Setup readline history
    try:
        readline.read_history_file(str(HISTFILE))
    except FileNotFoundError:
        pass
    readline.set_history_length(MAX_HISTORY)

    # State
    claude_history: list[dict[str, Any]] = []

    _print_banner(model, thinking_budget)

    try:
        while True:
            try:
                prompt = input(f"{C.BOLD}{C.BLUE}>>> {C.RESET}")
            except EOFError:
                print("\n")
                break

            prompt = prompt.strip()
            if not prompt:
                continue

            # --- Slash commands ---
            if prompt.startswith("/"):
                cmd_parts = prompt.split(maxsplit=1)
                cmd = cmd_parts[0].lower()

                if cmd in ("/quit", "/q"):
                    break
                elif cmd == "/model":
                    if len(cmd_parts) < 2:
                        print(f"  ç¾åœ¨: {C.GREEN}{model}{C.RESET}")
                        continue
                    new_model = cmd_parts[1].strip()
                    if new_model not in ALL_MODELS:
                        print(f"  {C.RED}ä¸æ˜Žãªãƒ¢ãƒ‡ãƒ«: {new_model}{C.RESET}")
                        print(f"  åˆ©ç”¨å¯èƒ½: {', '.join(sorted(ALL_MODELS))}")
                        continue
                    model = new_model
                    claude_history = []  # Reset history on model change
                    _print_banner(model, thinking_budget)
                elif cmd == "/models":
                    print(f"\n  {C.BOLD}Gemini (generateContent + Function Calling):{C.RESET}")
                    for m in sorted(GEMINI_MODELS):
                        marker = " â† ç¾åœ¨" if m == model else ""
                        print(f"    {C.GREEN}{m}{C.RESET}{marker}")
                    print(f"\n  {C.BOLD}Claude (generateChat + text-based tool use):{C.RESET}")
                    for m in sorted(CLAUDE_MODELS):
                        marker = " â† ç¾åœ¨" if m == model else ""
                        print(f"    {C.MAGENTA}{m}{C.RESET}{marker}")
                    print()
                elif cmd == "/clear":
                    claude_history = []
                    print(f"  {C.DIM}ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ{C.RESET}")
                elif cmd == "/history":
                    if _is_claude(model):
                        print(f"  Claude å±¥æ­´: {len(claude_history)} ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
                        for i, h in enumerate(claude_history[-6:]):
                            role = "User" if h["author"] == 1 else "AI"
                            print(f"    [{role}] {h['content'][:60]}...")
                    else:
                        print(f"  {C.DIM}Gemini ã¯ã‚¿ãƒ¼ãƒ³ã”ã¨ã«ãƒªã‚»ãƒƒãƒˆã•ã‚Œã¾ã™{C.RESET}")
                elif cmd in ("/help", "/h"):
                    _print_help()
                else:
                    print(f"  {C.RED}ä¸æ˜Žãªã‚³ãƒžãƒ³ãƒ‰: {cmd}{C.RESET} (/help ã§ä¸€è¦§)")
                continue

            # --- AI query ---
            start_time = time.monotonic()

            try:
                if _is_claude(model):
                    # Claude: generateChat + text-based tool use
                    text, claude_history = _run_claude_turn(
                        client, prompt, claude_history,
                        model=model,
                        system_instruction=HGK_DEV_SYSTEM_PROMPT,
                        max_iterations=args.max_iterations,
                        timeout=args.timeout,
                    )
                else:
                    # Gemini: generateContent + native Function Calling
                    text = _run_gemini_turn(
                        client, prompt,
                        model=model,
                        system_instruction=HGK_DEV_SYSTEM_PROMPT,
                        thinking_budget=thinking_budget,
                        max_iterations=args.max_iterations,
                        max_tokens=args.max_tokens,
                        timeout=args.timeout,
                    )

                elapsed = time.monotonic() - start_time
                print(f"\n{C.GREEN}{'â”€' * 60}{C.RESET}")
                print(text)
                print(f"{C.GREEN}{'â”€' * 60}{C.RESET}")
                print(f"  {C.DIM}{model} | {elapsed:.1f}s{C.RESET}\n")

            except KeyboardInterrupt:
                print(f"\n  {C.YELLOW}ä¸­æ–­ã—ã¾ã—ãŸ{C.RESET}")
            except Exception as e:
                print(f"\n  {C.RED}ã‚¨ãƒ©ãƒ¼: {e}{C.RESET}\n")

    finally:
        # Save readline history
        try:
            readline.write_history_file(str(HISTFILE))
        except OSError:
            pass
        print(f"\n{C.DIM}hgk-dev çµ‚äº†{C.RESET}")


if __name__ == "__main__":
    main()
