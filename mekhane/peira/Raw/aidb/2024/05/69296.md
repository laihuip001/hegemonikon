---
source_url: https://ai-data-base.com/archives/69296
captured_at: 2026-01-18T22:02:09.830552
title: "反復学習でCoTによる推論性能を向上させる手法 Metaとニューヨーク大学による研究"
publish_date: 2024.05.20
tags: ["サーベイ37", "ペルソナ・シミュレーション36", "オープンソース25", "エンタメ・アート23", "SE9", "コーディング56", "画像認識20", "音声8", "教育・キャリア9", "実証137", "画像生成9", "医療・ヘルスケア33", "ハルシネーション16", "ポジション8", "手法", "ロボット6", "製造・デザイン9", "ファインチューニング16", "ベンチマーク・リソース22", "分析54", "テクニカルレポート15", "安全性39", "マルチモーダル23", "金融・経済10", "セキュリティ16", "LLM", "政治・社会29", "手法426", "LLM659", "RAG50", "エージェント128", "プロンプト技術156"]
conversion_method: browser_subagent_v1_parallel
batch_id: 4
is_premium: unknown
---

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F69296&text=%E5%8F%8D%E5%BE%A9%E5%AD%A6%E7%BF%92%E3%81%A7CoT%E3%81%AB%E3%82%88%E3%82%8B%E6%8E%A8%E8%AB%96%E6%80%A7%E8%83%BD%E3%82%92%E5%90%91%E4%B8%8A%E3%81%95%E3%81%9B%E3%82%8B%E6%89%8B%E6%B3%95+Meta%E3%81%A8%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A8%E3%83%BC%E3%82%AF%E5%A4%A7%E5%AD%A6%E3%81%AB%E3%82%88%E3%82%8B%E7%A0%94%E7%A9%B6) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F69296&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F69296)

[手法](https://ai-data-base.com/archives/type-tag/method)

[LLM](https://ai-data-base.com/archives/tech-tag/llm)

LLMは論理的な推論をする能力が限られており、特に数学や科学の問題では精度が低いという課題があります。そこでMetaとニューヨーク大学の研究チームは、複数の推論ステップを繰り返し行うことでモデルの答えをより正確にする新しい方法を考案しました。結果として、他の従来モデルを上回る高い精度を達成しました。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69296-1024x576.jpg)

**参照論文情報**

  * タイトル：Iterative Reasoning Preference Optimization

  * 著者：Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, Jason Weston

  * 所属：FAIR at Meta, New York University




**本記事の関連研究** ：

  * [CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果](https://ai-data-base.com/archives/62364)

  * [GPT-4などのLLMに「自らの論理的な整合性をチェック」させるフレームワーク『LogiCoT』と実行プロンプト](https://ai-data-base.com/archives/55805)

  * [算術タスクでGPT-4を圧倒的に上回るコンパクトなモデル『MathGLM』登場。やはりステップ・バイ・ステップが重要](https://ai-data-base.com/archives/55122)

  * [Geminiの高い推論能力を活かして、過去最高水準のプログラミングAI『AlphaCode 2』も誕生したとの報告](https://ai-data-base.com/archives/60201)




## 背景

これまで、LLMの推論能力を向上させる手法の研究は、数多く行われてきました。その中でも、特に注目されてきたプロンプト・学習手法には、以下のようなものがあります。

  * Chain-of-Thought (CoT): LLMに一連の推論ステップを生成させることで、推論能力を向上させる手法

  * STaR (Self-Taught Reasoning) : CoTを生成し、正解を導く推論のみを用いて反復的にSFT (Supervised Fine-Tuning、教師付きファインチューニング)を行う手法

  * V-STaR : DPO（Direct Preference Optimization）で学習した検証モデルを用いてSFTの生成サンプルをフィルタリングする手法

  * Expert Iteration : 報酬モデルを仮定し、生成サンプルをフィルタリングしてSFTを反復的に行う手法




また、反復的なPreference Optimization（モデルの出力が人間の選好や価値観に合うように最適化するための手法）に関する研究についても盛んに行われ、以下のような手法が提案されてきました。

  * Iterative DPO : DPOを用いて選好ペアを最適化し、更新されたモデルで新たな選好ペアを生成する反復的な手法。

  * Self-Rewarding LLMs : LLM自身を報酬モデルとして用いたIterative DPO。

  * SPIN : 人間のラベルをWinner、前の反復の生成サンプルをLoserとするIterative DPOに類似した手法。




これらの既存の反復的な学習手法は、一般的な指示に対するチューニングでは良い性能を発揮します。しかし、この学習方法においては、LLMの推論LLMを向上できないというのが現状です。

そこで今回、LLMの推論性能を向上させるために、新たな学習方法「Iterative Reasoning Preference Optimization」が開発されました。この手法によって、LLMの推論タスクにおける性能を引き伸ばせるだけでなく、Human-in-the-Loop（機械学習モデルの学習プロセスに、人間を関与させる手法）や追加データを不要とするため、効率的に学習を進められます。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信



[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F69296&text=%E5%8F%8D%E5%BE%A9%E5%AD%A6%E7%BF%92%E3%81%A7CoT%E3%81%AB%E3%82%88%E3%82%8B%E6%8E%A8%E8%AB%96%E6%80%A7%E8%83%BD%E3%82%92%E5%90%91%E4%B8%8A%E3%81%95%E3%81%9B%E3%82%8B%E6%89%8B%E6%B3%95+Meta%E3%81%A8%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A8%E3%83%BC%E3%82%AF%E5%A4%A7%E5%AD%A6%E3%81%AB%E3%82%88%E3%82%8B%E7%A0%94%E7%A9%B6) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F69296&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F69296)
