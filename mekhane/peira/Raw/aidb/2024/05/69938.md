---
source_url: https://ai-data-base.com/archives/69938
captured_at: 2026-01-18T22:02:09.849331
title: "多くの「長いコンテキストを要するタスク」を、短いコンテキストウィンドウのLLMで解決する手法"
publish_date: 2024.05.29
tags: ["サーベイ37", "ペルソナ・シミュレーション36", "オープンソース25", "エンタメ・アート23", "SE9", "コーディング56", "画像認識20", "音声8", "教育・キャリア9", "実証137", "画像生成9", "医療・ヘルスケア33", "ハルシネーション16", "ポジション8", "手法", "ロボット6", "製造・デザイン9", "ファインチューニング16", "ベンチマーク・リソース22", "分析54", "テクニカルレポート15", "安全性39", "マルチモーダル23", "金融・経済10", "セキュリティ16", "プロンプト技術", "LLM", "政治・社会29", "手法426", "LLM659", "RAG50", "エージェント128", "プロンプト技術156"]
conversion_method: browser_subagent_v1_parallel
batch_id: 4
is_premium: unknown
---

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F69938&text=%E5%A4%9A%E3%81%8F%E3%81%AE%E3%80%8C%E9%95%B7%E3%81%84%E3%82%B3%E3%83%B3%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E3%82%92%E8%A6%81%E3%81%99%E3%82%8B%E3%82%BF%E3%82%B9%E3%82%AF%E3%80%8D%E3%82%92%E3%80%81%E7%9F%AD%E3%81%84%E3%82%B3%E3%83%B3%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%89%E3%82%A6%E3%81%AELLM%E3%81%A7%E8%A7%A3%E6%B1%BA%E3%81%99%E3%82%8B%E6%89%8B%E6%B3%95) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F69938&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F69938)

[手法](https://ai-data-base.com/archives/type-tag/method)

[LLM](https://ai-data-base.com/archives/tech-tag/llm)[プロンプト技術](https://ai-data-base.com/archives/tech-tag/prompt)

長いコンテキストのタスクに対し、短いプロンプトのみ処理できるモデルでも取り組める「LC-Boost」フレームワークが考案されました。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69938-1024x576.jpg)

**参照論文情報**

  * タイトル：Are Long-LLMs A Necessity For Long-Context Tasks?

  * 著者：Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Yujia Zhou, Xu Chen, Zhicheng Dou

  * 所属：Renmin University of China, Beijing Academy of Artificial Intelligence




**本記事の関連研究** ：

  * [LLMのプロンプトに数百から数千の例を含める超長尺のコンテキスト内学習（In-context learning）とファインチューニングの性能比較](https://ai-data-base.com/archives/68564)

  * [LLMにおける、長いコンテキストから欲しい情報を見つけ出す「needle-in-a-haystack（干し草の中の針）」テスト結果とプロンプト例](https://ai-data-base.com/archives/68016)

  * [GPT-4o、Gemini、Claude 3などにおける「長いプロンプトのマルチモーダルタスク」性能を測定した結果](https://ai-data-base.com/archives/69354)

  * [スタンフォード大学の研究者ら、GPT-4oとGemini1.5 Proで「マルチモーダルモデルにおける『Many-Shot』の効果」を検証](https://ai-data-base.com/archives/69211)




## 背景

最近、長文の質問応答や要約などのタスクにLLMが活用されるようになってきました。しかし、一部のモデルは長いプロンプトを処理できるようになっていますが、既存のLLMの多くは、限られた長さのコンテキストしか処理できないという制約があります。

一般的に、LLMのコンテキストウィンドウを拡張すれば長いコンテキストへの対応が可能になります。しかしモデルの学習や適用に膨大なコストがかかるだけでなく、短いコンテキストに対する汎用性が損なわれる恐れもあります。そこで、長いコンテキストを短いコンテキストに分解することで、効率的に長いコンテキストのタスクを解決できないかという発想が生まれました。

こうした背景から、短いコンテキストのみ処理するLLMを用いて長いコンテキストのタスクに取り組む新たな手法LC-Boost（Long-Context Bootstrapper）が考案されました。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信



[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F69938&text=%E5%A4%9A%E3%81%8F%E3%81%AE%E3%80%8C%E9%95%B7%E3%81%84%E3%82%B3%E3%83%B3%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E3%82%92%E8%A6%81%E3%81%99%E3%82%8B%E3%82%BF%E3%82%B9%E3%82%AF%E3%80%8D%E3%82%92%E3%80%81%E7%9F%AD%E3%81%84%E3%82%B3%E3%83%B3%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%89%E3%82%A6%E3%81%AELLM%E3%81%A7%E8%A7%A3%E6%B1%BA%E3%81%99%E3%82%8B%E6%89%8B%E6%B3%95) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F69938&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F69938)
