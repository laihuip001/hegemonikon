---
source_url: https://ai-data-base.com/archives/70358
captured_at: 2026-01-18T22:02:09.868621
title: "MMLUをアップデートしたベンチマーク『MMLU-Pro』Phi-3やLlama 3、Claude 3、GPT-4oなどの評価結果"
publish_date: 2024.06.05
tags: ["サーベイ37", "ペルソナ・シミュレーション36", "オープンソース25", "エンタメ・アート23", "SE9", "コーディング56", "画像認識20", "音声8", "教育・キャリア9", "実証137", "画像生成9", "医療・ヘルスケア33", "ハルシネーション16", "ポジション8", "ロボット6", "製造・デザイン9", "ファインチューニング16", "ベンチマーク・リソース22", "分析54", "テクニカルレポート15", "安全性39", "マルチモーダル23", "金融・経済10", "セキュリティ16", "LLM", "政治・社会29", "手法426", "LLM659", "RAG50", "ベンチマーク・リソース", "エージェント128", "プロンプト技術156"]
conversion_method: browser_subagent_v1_parallel
batch_id: 4
is_premium: unknown
---

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F70358&text=MMLU%E3%82%92%E3%82%A2%E3%83%83%E3%83%97%E3%83%87%E3%83%BC%E3%83%88%E3%81%97%E3%81%9F%E3%83%99%E3%83%B3%E3%83%81%E3%83%9E%E3%83%BC%E3%82%AF%E3%80%8EMMLU-Pro%E3%80%8FPhi-3%E3%82%84Llama+3%E3%80%81Claude+3%E3%80%81GPT-4o%E3%81%AA%E3%81%A9%E3%81%AE%E8%A9%95%E4%BE%A1%E7%B5%90%E6%9E%9C) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F70358&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F70358)

[ベンチマーク・リソース](https://ai-data-base.com/archives/type-tag/resource)

[LLM](https://ai-data-base.com/archives/tech-tag/llm)

LLMの性能が、様々な分野でどれほど進歩したかを測るための基準が大事になっています。今まではMMLU（Massive Multitask Language Understanding）が重要な基準でした。しかし、モデルが賢くなるにつれて、MMLUだけではモデルの性能の違いを見分けるのが難しくなってきました。

そこで、研究者らはMMLU-Proという新しいデータセットを開発しました。MMLUよりもさらに難しく、論理的な思考力を試す質問を多く含んでいます。また、簡単すぎる質問や間違った質問を取り除きました。研究者らは早速、本ベンチマークにおけるGPT-4oなどのLLMの成績をテストしました。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70358-1024x576.jpg)

**参照論文情報**

  * タイトル：MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark

  * 著者：Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen

  * 所属：University of Waterloo, University of Toronto, Carnegie Mellon University




**本記事の関連研究** ：

  * [AGIを見据えて専門家レベルの問題を集めたベンチマーク「MMMU」、GPT-4VやGemini Ultraでも正解率6割未満](https://ai-data-base.com/archives/61463)

  * [AGIへのロードマップ カーネギーメロン大学など複数機関からの研究グループが提唱](https://ai-data-base.com/archives/70005)

  * [Claude 3のベンチマーク評価結果 論文（テクニカルレポート）より](https://ai-data-base.com/archives/65693)

  * [日常能力を試すテスト『GAIA』正答率、人間92%に対してGPT-4は15% 一般的なニーズに応えるAI開発の指針に](https://ai-data-base.com/archives/59440)




## 背景

GPT-4、Claude、GeminiといったLLMが大きな進歩を遂げています。さまざまなタスクにおいて高い汎用性と専門性を示しており、専門家レベルの知能の実現に向けて大きく前進しています。

LLMの能力を評価するために、いくつかの代表的なベンチマークが使用されてきました。中でもMMLUは、幅広い分野をカバーし、質の高い問題を含んでいることから、LLMの評価に広く用いられてきました。

しかし、最新のLLMの急速な進歩によって、MMLUでの性能が頭打ちになってきています。2023年3月にGPT-4が86.4%の正解率を達成して以降、大きな進歩は見られていません。MMLUには以下のような問題点があると考えられています。

  1. 選択肢が4つしかないため、LLMが本当に理解していなくとも答えを導き出せてしまう可能性がある。

  2. STEM（科学・技術・工学・数学）分野の問題は知識重視で、複雑な推論を必要としない。

  3. 解答不可能な問題や誤ってアノテーションされた問題が含まれている。




こうした背景から、LLMの能力をより適切に評価するために、新たなベンチマークMMPU-Proが開発されました。以下のような特徴があります。

  1. 選択肢を10個に増やすことで、偶然正解する確率を下げ、難易度と頑健性を高めた。

  2. 大学レベルの難しい問題を増やし、複雑な推論を必要とする問題を多く含めた。

  3. 専門家によるレビューを2回行い、データセットのノイズを減らした。




実験では、最も性能が高いと考えられているモデルでも程よく苦戦しており、レベル設定の高さが示唆されています。下記は本実験で使用されたモデル一覧です。

**クローズドソースモデル**

GPT-4o  
GPT-4-Turbo  
Claude-3-Opus  
Claude-3-Sonnet  
Gemini-1.5-Pro  
Gemini-1.5-Flash  
Yi-Large

**オープンソースモデル**

Llama-3-70B-Instruct  
Llama-3-70B  
Llama-2-70B  
Llama-3-8B-Instruct  
Llama-3-8B  
Phi-3-medium-4k-instruct  
Phi-3-mini-4k-instruct  
DeepSeek-V2-Chat  
Qwen1.5-110B  
Qwen1.5-72B-Chat  
MAmmoTH2-8x7B-Plus  
Mixtral-8x7B-Instruct-v0.1  
Mixtral-8x7B-v0.1  
Mistral-7B-v0.1  
Gemma-7B  
Gemma-2B  
Yi-1.5-34B-Chat  
Yi-34B  
InternMath-20B-Plus  
InternMath-7B-Plus  
Staring-7B  
c4ai-command-r-v01  
OpenChat-3.5-8B  
Zephyr-7B-Beta  
Neo-7B-Instruct  
Llemma-7B

今回、上記モデルの結果がMMLUとMMLU-Pro両方でまとめられています。  
また、GPT-4oにおいてはエラーパターンも分析されています。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信



[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F70358&text=MMLU%E3%82%92%E3%82%A2%E3%83%83%E3%83%97%E3%83%87%E3%83%BC%E3%83%88%E3%81%97%E3%81%9F%E3%83%99%E3%83%B3%E3%83%81%E3%83%9E%E3%83%BC%E3%82%AF%E3%80%8EMMLU-Pro%E3%80%8FPhi-3%E3%82%84Llama+3%E3%80%81Claude+3%E3%80%81GPT-4o%E3%81%AA%E3%81%A9%E3%81%AE%E8%A9%95%E4%BE%A1%E7%B5%90%E6%9E%9C) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F70358&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F70358)
