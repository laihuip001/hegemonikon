---
source_url: https://ai-data-base.com/archives/74778
captured_at: 2026-01-18T23:31:20.382049
title: "モデルとデータの大規模化で変化するLLMのハルシネーション"
publish_date: 2024.08.25
tags: ["画像認識 20", "サーベイ 37", "安全性 39", "政治・社会 29", "分析 54", "オープンソース 25", "マルチモーダル 23", "SE 9", "テクニカルレポート 15", "手法", "手法 426", "ハルシネーション", "エージェント 128", "金融・経済 10", "RAG 50", "ベンチマーク・リソース 22", "ファインチューニング 16", "LLM", "ロボット 6", "プロンプト技術 156", "ハルシネーション 16", "LLM 660", "コーディング 56", "ペルソナ・シミュレーション 36", "ポジション 8", "セキュリティ 16", "実証 137", "エンタメ・アート 23", "製造・デザイン 9", "教育・キャリア 9", "画像生成 9", "音声 8", "医療・ヘルスケア 33"]
conversion_method: fast_collect_v1
batch_id: 1
---

# モデルとデータの大規模化で変化するLLMのハルシネーション

2024.08.262025.11.21

[深堀り解説](https://ai-data-base.com/archives/category/deep-dive)

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAQAAQMAAABF07nAAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAJZJREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAewEEHgABB9i6GAAAAABJRU5ErkJggg==) ![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74778_eye.jpeg)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F74778&text=%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A8%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%A4%A7%E8%A6%8F%E6%A8%A1%E5%8C%96%E3%81%A7%E5%A4%89%E5%8C%96%E3%81%99%E3%82%8BLLM%E3%81%AE%E3%83%8F%E3%83%AB%E3%82%B7%E3%83%8D%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F74778&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F74778)

[手法](https://ai-data-base.com/archives/type-tag/method)

[LLM](https://ai-data-base.com/archives/tech-tag/llm)[ハルシネーション](https://ai-data-base.com/archives/tech-tag/hallucination)

まるで人間のように文章を作ることができるLLMの能力が注目を集めています。しかし、LLMが時に事実と違うことを言ってしまうことがあり、これは「ハルシネーション」と呼ばれる問題となっています。

今回Googleの研究者らは、このハルシネーションが、LLMの大きさと、学習に使ったデータの量によってどう変わるのかを調べました。また、LLMが嘘をついているかどうかをどうやって見つけるか、という問題についても研究しています。

この記事では、研究で使われた方法や、わかったこと、そして将来どのように役立つのかについて詳しく説明します。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74778-1024x576.jpg)

**参照論文情報**

  * タイトル：Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability

  * 著者：Jiri Hron, Laura Culp, Gamaleldin Elsayed, Rosanne Liu, Ben Adlam, Maxwell Bileschi, Bernd Bohnet, JD Co-Reyes, Noah Fiedel, C. Daniel Freeman, Izzeddin Gur, Kathleen Kenealy, Jaehoon Lee, Peter J. Liu, Gaurav Mishra, Igor Mordatch, Azade Nova, Roman Novak, Aaron Parisi, Jeffrey Pennington, Alex Rizkowsky, Isabelle Simpson, Hanie Sedghi, Jascha Sohl-dickstein, Kevin Swersky, Sharad Vikram, Tris Warkentin, Lechao Xiao, Kelvin Xu, Jasper Snoek, Simon Kornblith

  * 所属：Google DeepMind




**本記事の関連研究**

  * [ファインチューニングがLLMの幻覚（ハルシネーション）に与える影響 Googleなどによる検証結果](https://ai-data-base.com/archives/69421)

  * [マルチモーダルLLMにおける幻覚（ハルシネーション）の原因と対策 クリエイティブでの活用も推奨 AWSなどが網羅的に調査](https://ai-data-base.com/archives/68720)

  * [LLMの内部状態を観察することで「出力がハルシネーションか否かを判別する」手法『LLMファクトスコープ』](https://ai-data-base.com/archives/61651)

  * [LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ](https://ai-data-base.com/archives/58767)




## 背景

LLMはどんどん賢くなっていますが、「ハルシネーション」の問題はまだあります。LLMが生成する”事実と異なる情報”は一見正しそうに見えるため、影響を考慮すると解決するのが望ましいとされています。

LLMの性能向上に関しては「スケーリング則」という経験則が知られており、データセットやモデルのサイズが大きくなるにつれてLLMの性能が向上するという法則です。つまり「大きくすればするほど良くなる」という考え方です。しかし、ハルシネーションがスケールにどう依存するかについては、まだ十分に理解されていません。

ハルシネーションにはさまざまな種類があり、皆が「これがハルシネーションだ」と言える定義はありません。そこで今回研究者らは「正しい答えが学習データにそのまま書いてある場合」だけを調べました。それがハルシネーションを最も正確に検出する手段だからです。

なお、普通の文章だとどんな知識が入っているかを正確に知るのは難しいという問題があります。そのため研究者らは、「ナレッジグラフ」というデータの形式を使用して実験を行いました。ナレッジグラフなら、どんな事実が入っているかを完全にコントロールでき、LLMが言ったことが本当にデータの中にあるかどうかを簡単に確認できます。そのためナレッジグラフを使ってLLMを訓練すれば、LLMが学習データをどれくらい間違って覚えているか、そしてLLMの大きさによってそれがどう変わるかを調べられます。

このようにして、LLMのハルシネーションと大きさの関係について今までよりもっとよく分かるようになると期待された研究が行われました。詳しい研究アプローチや実験結果は以下で紹介します。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信



[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

クリップする [](https://twitter.com/share?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F74778&text=%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A8%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%A4%A7%E8%A6%8F%E6%A8%A1%E5%8C%96%E3%81%A7%E5%A4%89%E5%8C%96%E3%81%99%E3%82%8BLLM%E3%81%AE%E3%83%8F%E3%83%AB%E3%82%B7%E3%83%8D%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3) [](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fai-data-base.com%2Farchives%2F74778&src=sdkpreparse) [](https://note.com/intent/post?url=https%3A%2F%2Fai-data-base.com%2Farchives%2F74778)
