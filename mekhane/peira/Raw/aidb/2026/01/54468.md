---
source_url: https://ai-data-base.com/archives/54468
captured_at: 2026-01-19T07:30:09.010962
title: "大規模言語モデルのセーフガードを故意に突破する「脱獄プロンプト」とは - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

GPT-4などLLMには安全面、倫理面から禁止されている使用方法があり、運営はセーフガードを実装して技術的に乱用を防いでいます。しかしセーフガードは万能ではありません。

本記事ではセーフガードを突破する悪意あるプロンプト手法脱獄プロンプトについての調査結果を紹介します。

**参照論文情報**

  * タイトル：”Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models

  * 著者：Xinyue Shen et al.

  * 所属：CISPA、NetApp

  * URL：<https://doi.org/10.48550/arXiv.2308.03825>

**関連研究**

  * [ダークウェブの深淵を照らす言語モデル「DarkBERT」登場](https://ai-data-base.com/archives/52629)

  * [Metaの研究者ら「GPT-4をきびしくサポートする」AIのShepherd（シェパード）開発](https://ai-data-base.com/archives/54440)

  * [カーネギーメロン大など、大規模言語モデルの脆弱性を突く攻撃手法が存在することを指摘](https://ai-data-base.com/archives/54302)

## 研究の背景

LLMには、以下のような潜在的なリスクが存在します。

  * 不適切な内容の生成

  * 個人情報の漏洩

  * 誤情報の拡散

このため、LLMの運営者は、これらのリスクを軽減するためのセーフガードを実装しています。

一方で、悪意のあるユーザーがセーフガードを故意に突破する「脱獄プロンプト」の存在が確認されています。DiscordやRedditなどのオンラインフォーラムでは、これらのプロンプトが共有され、セーフガードの限界が試されています。

この現象は、LLMの安全な開発と運用に対する新たな課題を提起しており、脱獄プロンプトの特性と構造を理解することが、今後のセーフガードの強化に不可欠です。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAwUAAAJ4AQMAAAAOYN2RAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAFJJREFUGBntwTEBAAAAwiD7p95vBmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAn8fAAAVlejJ0AAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2023/08/AIDB_54468_2.png)攻撃シナリオの例

