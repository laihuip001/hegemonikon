---
source_url: https://ai-data-base.com/archives/55232
captured_at: 2026-01-19T07:30:25.091136
title: "LLMに自身のハルシネーション（幻覚）を「自覚」させ、減らす方法 - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

LLMは非常に大規模なテキストデータから学習を行い、人間のようなテキストを生成することが可能です。しかし、時に「ハルシネーション（幻覚）」という問題が起こります。これはLLMが実在しない情報や参照を生成してしまう現象を指します。

Microsoftとスタンフォード大学の研究者チームは、このハルシネーション現象に焦点を当てた研究を行いました。そして、LLMが生成した参照（引用文献などの情報）が幻覚かどうかを判断する方法を探求しています。

**参照論文情報**

  * タイトル：Do Language Models Know When They’re Hallucinating References?

  * 著者：Ayush Agrawal, Mirac Suzgun, Lester Mackey, Adam Tauman Kalai

  * 所属：Microsoft Research India, Stanford University, Microsoft Research

  * URL：<https://doi.org/10.48550/arXiv.2305.18248>

目次

Toggle

  * ハルシネーションについて
    * 定義
    * その影響
    * 評価方法
  * 調査と検証
    * 質問テンプレートの利用
    * 詳細な質問を行う
    * 体系的な調査
  * 検証結果
    * 幻覚の削減方法
  * 幻覚を減らす工夫
  * PREMIUM

## ハルシネーションについて

### 定義

LLM（Large Language
Models）は、実在しない情報や参照を事実のように生成することがあります。これを「ハルシネーション（Hallucination：幻覚）」と呼びます。ハルシネーションは、言語モデルが訓練データに基づいて”いない”内容のテキストを生成する現象を指します。

### その影響

ハルシネーションは、言語モデルが文献レビューを生成する際や関連研究の探索と引用に利用される場合によくない影響を与えます。この問題は、特に「参照のハルシネーション（Hallucinating
References）」として知られ、研究が進められています。また、これが公衆の注意を引くようになり、特に医療分野での問題として指摘されています。ハルシネーションは、ユーザーがモデルをより信頼するようになるにつれて、問題が増大するとされています。

### 評価方法

ハルシネーションの評価は通常、言語モデルの訓練データへのアクセスを必要とします。しかし、今回紹介する研究では、ウェブ検索での完全一致を基準にしてハルシネーションを評価する方法を提案しています。

## 調査と検証

このセクションでは、研究者が行った調査プロセスについて詳しく説明します。

### 質問テンプレートの利用

研究初期段階で、研究者は3つの異なる質問テンプレートを用いてLLMに質問を行いました。研究者らはLLMがどのような回答を生成するのか、そしてどのような質問がハルシネーションを引き起こしやすいのかを理解しようとしました。

### 詳細な質問を行う

次に、研究者はLLMにさらに詳細な質問を行いました。このステップは、調査インタビューからインスパイアを受けたもので、一貫性を評価するために複数の質問を行う手法を採用しています。LLMがどのように回答を構築し、それが一貫しているかどうかを評価しました。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAEFAQMAAABHJgfUAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAADdJREFUGBntwTEBAAAAwiD7p14MH2AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPAWg4UAAUcnYvMAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2023/09/AIDB_55232_1-1024x261.jpg)

