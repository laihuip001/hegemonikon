---
source_url: https://ai-data-base.com/archives/61914
captured_at: 2026-01-19T07:31:52.312255
title: "1.1Bパラメータの小さなモデルを巨大データ（約3兆トークン）で訓練したモデル『TinyLlama』が、比較的優秀な性能を発揮 - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

小型の言語モデルを極めて大きいデータ量でトレーニングすると、類似サイズのモデルよりもシンプルに著しく優れた性能になったことが明らかにされました。

研究者らは1.1Bパラメータの「TinyLlama」を約3兆トークンで訓練して様々なタスク（常識推論や問題解決）で実験した結果を報告しています

本記事では研究の詳細を見ていきます。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2024/01/AIDB_61914_thum-1024x576.jpg)

