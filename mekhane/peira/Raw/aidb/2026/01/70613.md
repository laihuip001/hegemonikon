---
source_url: https://ai-data-base.com/archives/70613
captured_at: 2026-01-19T07:33:39.461675
title: "難しいベンチマークで高性能なLLMでも単純な問題で間違えてしまう現象について「不思議の国のアリス問題」とGPT-4o、Claude-3、Llama 3などで分析 - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

最先端のLLMであっても、人間には簡単に解けるシンプルな問題に対して誤った解答を出す現象が存在します。今回研究者らは、この現象に焦点を当てて実験で調べました。GPT-4oやClaude
3 opus、Llama 3などさまざまなモデルが対象となっています。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2024/06/AIDB_70613-1024x576.jpg)

**参照論文情報**

  * タイトル：Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models

  * 著者：Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, Jenia Jitsev

  * 所属：LAION, Juelich Supercomputing Center (JSC), Research Center Juelich (FZJ), School of Electrical and Electronic Engineering, University of Bristol, Open-Ψ (Open-Sci) Collective.

**本記事の関連研究** ：

  * [MMLUをアップデートしたベンチマーク『MMLU-Pro』Phi-3やLlama 3、Claude 3、GPT-4oなどの評価結果](https://ai-data-base.com/archives/70358)

  * [AGIを見据えて専門家レベルの問題を集めたベンチマーク「MMMU」、GPT-4VやGemini Ultraでも正解率6割未満](https://ai-data-base.com/archives/61463)

  * [Claude 3のベンチマーク評価結果 論文（テクニカルレポート）より](https://ai-data-base.com/archives/65693)

  * [大規模言語モデル（LLM）のこれまでとこれから④ -ベンチマーク別の優秀なモデルと将来展望編-](https://ai-data-base.com/archives/64487)

## 背景

LLMは膨大なテキストデータを学習することで様々なタスクに適用できる汎用的な言語能力を獲得できるとされ、大きな注目を集めています。

しかし、LLMの真の能力については慎重に検証する必要があるのも事実です。標準的なベンチマークテストでは高いスコアを出すLLMでも、実際には推論能力や知識の一般化に問題があるのではないかという指摘がなされています。LLMの能力を過大評価しないように気をつけなければいけません。

そこで今回研究者らは、LLMの基本的な推論能力を評価するために、非常にシンプルな常識推論問題を考案しました。これを不思議の国のアリス問題と命名しています。  
子供でも簡単に解けるほどシンプルなのに、LLMが意外と躓いてしまうというのが特徴です。なおLLMは正しい答えに辿り着けないだけでなく、間違った答えに過剰な自信を示したり、答えを正当化しようとする様子も見られました。

なお実験されたモデル一覧は以下のとおりです。

GPT-4o-2024-05-13  
GPT-4-turbo-2024-04-09  
GPT-4-0125-preview  
GPT-4-0613  
GPT-3.5-turbo-0125  
Claude-3-opus-20240229  
Claude-3-sonnet-20240229  
Claude-3-haiku-20240307  
Gemini 1.0 Pro  
gemma-7b-it  
gemma-2b-it  
Mistral-large-2402  
Mistral-medium-2312  
Mistral-small-2402  
open-mixtral-8x22b-instruct-v0.1  
open-mixtral-8x7b-instruct-v0.1  
open-mistral-7b-instruct-v0.2  
Command R+  
Dbrx Instruct  
Llama 2 70B Chat  
Llama 2 13B Chat  
Llama 2 7B Chat  
Llama 3 70B Chat  
Llama 3 8B Chat  
Qwen 1.5 1.8B – 72B Chat

以下で実験設計と主な結果について紹介します。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信

[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

