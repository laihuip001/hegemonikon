---
source_url: https://ai-data-base.com/archives/71857
captured_at: 2026-01-19T07:33:51.639324
title: "LLMはRAGコンテキストと事前知識のどちらに依存する？ - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

RAGによって情報が取得された時、モデルは内部の知識とどう折り合いをつけているのでしょうか？研究者らは特殊な方法によって、モデルが外部コンテキストと内部知識のどちらに依存しているかを詳しく調べました。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2024/06/AIDB_71857-1024x576.jpg)

**参照論文情報**

  * タイトル：From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries

  * 著者：Hitesh Wadhwa, Rahul Seetharaman, Somyaa Aggarwal, Reshmi Ghosh, Samyadeep Basu, Soundararajan Srinivasan, Wenlong Zhao, Shreyas Chaudhari, Ehsan Aghazadeh

  * 所属：University of Massachusetts, Amherst, Microsoft, University of Maryland, College Park

**本記事の関連研究** ：

  * [包括的なRAG評価ベンチマーク『CRAG』Metaなどが開発](https://ai-data-base.com/archives/70850)

  * [RAGの失敗パターン7選と教訓9箇条](https://ai-data-base.com/archives/69154)

  * [小さなRetrieverとLLMの組み合わせによる実用的なワークフロー生成システム またはRAGで幻覚を減らす手法](https://ai-data-base.com/archives/68219)

  * [RAGにおいてLLMが「役立たない情報を無視」できるようにする『RAFT』QAタスクで従来の手法を大幅に上回る結果を達成](https://ai-data-base.com/archives/66269)

## 背景

LLMの活用が広がる中で、外部知識を組み込んで推論能力を高める「検索拡張生成(RAG)」が注目を集めています。ユーザーの質問に対してより適切な回答を生成できるように、外部のデータソースから関連情報を検索し、その情報をモデルの入力に追加する手法です。

RAGは、LLMの幻覚（事実と異なる情報の生成）を軽減し、最新の情報や専門知識を取り入れる上で効果的だと考えられています。しかし、RAGが実際にどのようにLLMの推論プロセスに影響を与えているのかについては、まだ十分に理解されていません。

これまで、LLMの内部動作を解明しようとする研究は進められてきました。例えば、モデル内の特定の隠れ層が事実の予測にどのように寄与しているかを調べる「因果追跡」や、モデルのパラメータを直接編集して知識を更新する手法などが開発されてきました。

そんな中、今回研究者らは、LLMが事実に基づく質問に答える際に、モデルに組み込まれた知識とRAGによって提供される外部情報をどのように使い分けているのかを分析しました。

以下で詳しく紹介します。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信

[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

