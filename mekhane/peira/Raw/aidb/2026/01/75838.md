---
source_url: https://ai-data-base.com/archives/75838
captured_at: 2026-01-19T07:27:19.554235
title: "生成回数を増やすだけでLLMの性能が大幅に向上するシンプルな法則 実用上のポイント - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

この記事では、LLMの性能を向上させる新しい方法を提案している研究を紹介します。

従来のモデルサイズの拡大やデータの増加とは異なり、今回研究者らは「推論時の計算量を増やす」ことで性能を高める方法を探っています。「反復サンプリング」という手法を使い、複数回の推論を行って最適な解答を選び出すアプローチを取っています。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2025/09/AIDB_75838-1024x576.png)

**本記事の関連研究**

  * [小さなLLMを多数組み合わせることで、単一の巨大モデルに匹敵する可能性](https://ai-data-base.com/archives/64708)

  * [オープンソースモデルでも力を合わせればGPT-4oに匹敵することを示す「Mixture-of-Agents（MoA）」アーキテクチャ](https://ai-data-base.com/archives/71419)

## 背景

LLMの能力向上は、これまで主に学習時の計算リソースを増やすことで達成されてきました。モデルのサイズを大きくしたり、より大規模なデータセットで事前学習を行ったり、人間の嗜好を反映したラベルを用いて後学習を実施するなど、学習段階への投資が大きな成果を生んでいました。

しかし、推論時の計算リソースの活用については、これまであまり積極的な投資が行われていませんでした。多くの場合、ユーザーや開発者は1回のみの試行で問題解決を試みており、推論時の計算能力が十分に活用されていなかった可能性があります。

この状況に注目し、今回研究者たちは推論時の計算リソースを拡大する方法として「反復サンプリング」の可能性を探ることにしました。「反復サンプリング」は、深層学習の他の分野ですでに成功を収めている考え方です。たとえば、ゲームの分野では、推論時に多くの未来の状態を探索して最適な手を決定する手法が使われています。また、LLMと組み合わせたツリーベースの方法も、モデルの計画立案やさまざまなアプローチの探索能力を高めるのに効果的であることが示されています。

さらに、コーディングや数学的推論、パズル解決などの分野でも、反復サンプリングが効果的であることが先行研究で示されています。特にコーディングのタスクでは、最大で100万回のサンプリングまで性能が向上し続けることが報告されています。

こうした背景から、研究チームは反復サンプリングをさまざまなタスクやモデルに適用し、その効果を体系的に調べる必要性を感じました。中でもカバレッジ（任意のサンプルで問題を解決できる割合）とサンプル数の関係、異なるモデルやタスクでのスケーリングの一貫性、そして自動検証が難しいタスクでの課題など、広範な理解が重要だと考えました。

また、推論時のスケーリング法則が存在する可能性を探ることも目的の一つとなっています。学習のスケーリング法則が投資にヒントを与えたように、推論時のスケーリング法則が発見されれば次の戦略が考えやすくなります。

以下では、本研究の実験内容と結果を中心に詳しく紹介します。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信

[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

