---
source_url: https://ai-data-base.com/archives/80376
captured_at: 2026-01-19T07:35:08.349508
title: "LLMにおける事実性の評価＆向上に役立つデータセットの作り方 - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

本記事では、LLMが抱える「ハルシネーション」問題に対応するために開発された、事実性評価用データセット生成手法を紹介します。  
元テキストから抽出した”事実に関する主張”を微妙に書き換えて異なる情報を生成することで、LLMが正・誤を見分ける能力を評価できるようにするといった方法論です。  
研究者らは実際にこの方法論を用いてWikipediaデータをもとに新しいデータセットを構築し、本アイデアの有用性を実証しました。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2024/12/AIDB_80376-1024x576.jpg)

**発表者情報**

  * 研究者：Alessandro Scirè et al.

  * 研究機関：Babelscape, Sapienza University of Rome

**ハルシネーションの関連研究**

  * [モデルとデータの大規模化で変化するLLMのハルシネーション Google DeepMindの研究](https://ai-data-base.com/archives/74778)

  * [ファインチューニングがLLMの幻覚（ハルシネーション）に与える影響 Googleなどによる検証結果](https://ai-data-base.com/archives/69421)

  * [OpenAIが新しくLLMの事実性評価ベンチマーク『SimpleQA』をリリース 実用に役立つ知見も得られる](https://ai-data-base.com/archives/77893)

  * [LLMが生成した長いテキストにおける「事実性」を自動で評価するLLMエージェントフレームワーク『SAFE』Google DeepMindが開発](https://ai-data-base.com/archives/66502)

## 背景

LLMは、ハルシネーションと呼ばれる、事実に基づかない内容を生成してしまうという問題を抱えています。LLMはあたかも人間が書いたかのような流暢なテキストを生成するため、ユーザーがその内容を鵜呑みにしてしまう可能性があり、ハルシネーションは深刻な問題となっています。

これまでに行われてきた事実性評価研究では、生成されたテキストが、例えばニュース記事や書籍などのソース文書と一致しているかを検証していました。しかし、現実世界のように様々な種類のテキストが混在する状況には対応できていませんでした。また、検証に使うソース文書が常に存在するという前提にも無理があります。

有力なプロジェクトには[FEVER (Fact Extraction and VERification)
](https://fever.ai/)があり、開発者らはWikipediaから抽出した文章を少し改変してデータセットを作りました。185,445件もの”事実に関する主張”を、それが正しいか、間違っているか、情報不足かを判断するためのものです。しかし、FEVERは個別の事実の真偽を判定することに特化しており、現実世界のテキストのように、複数の事実が複雑に絡み合った文章全体を評価するには不向きでした。

さらに、FACTORやFELMといった「LLM正しいテキストと間違ったテキストを自動生成する」フレームワークも考案されてきました。事実性評価のためのデータを作るためです。しかし生成されるデータセットは規模が小さく、十分ではありませんでした。

そこで、研究者たちは、現実世界の複雑なテキストを評価できる、より大規模で汎用的な、新しい事実性評価手法の開発に挑むことになりました。

本研究で示されている方法論を応用すると、独自のチャットボットを作成する際に「LLMが正しい答えを述べられるかどうかをチェックする」作業に役立つかもしれません。

研究のポイント

  1. 元の文章から細かい主張を抜き出し、それをほんの少し書き換えて間違った情報も作り出す方法論を考案した

  2. Wikipediaをもとに「幅広いトピックをカバーしている正しい文と間違った文のペア」を何万組も用意した大規模なデータセットを開発し、誰でも使えるようにした

以下で詳細を紹介します。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信

[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

