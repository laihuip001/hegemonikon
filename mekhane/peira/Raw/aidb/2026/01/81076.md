---
source_url: https://ai-data-base.com/archives/81076
captured_at: 2026-01-19T07:35:15.709794
title: "18兆トークンで学習されたオープンソースLLM『Qwen2.5』シリーズの性能 - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

本記事では、新たに発表された大規模言語モデル（LLM）ファミリーQwen2.5シリーズを紹介します。LLMの開発競争が世界中で加速する中、モデルやデータの規模拡大だけでなく、効果的な学習手法の確立が求められてきました。研究者らは、18兆トークンにまで拡大した事前学習データと100万件以上のサンプルを用いた高度な教師あり学習、そして多段階強化学習を組み合わせることで、次世代のモデル開発に取り組みました。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2024/12/AIDB_81076-1024x576.jpg)

**発表者情報**

  * 研究機関：アリババQwenチーム

**オープンソースモデルの関連研究**

  * [オープンソースのコード生成LLMが商用LLMに追いつく Qwen2.5-Coderの能力値全容](https://ai-data-base.com/archives/78609)

  * [動画を理解する軽量なLLM『Apollo』、オープンソースで登場（商用利用も可能）](https://ai-data-base.com/archives/80871)

  * [OpenAIのo1モデルへの対抗馬 アリババが独自の推論モデル「Marco-o1」を開発 オープンソースで公開](https://ai-data-base.com/archives/79273)

## 背景

LLMの開発競争が世界中で加速する中、モデルの性能向上に向けた取り組みが続けられています。言語理解や生成、推論といった基本的な能力の向上には、モデルやデータの規模拡大だけでなく、効果的な学習手法の確立が不可欠とされてきました。

従来の手法では、事前学習後のモデルに対して教師ありファインチューニングを適用し、さらに人間からのフィードバックを活用した強化学習を組み合わせることで、モデルの性能向上が図られてきました。しかし、長文の処理や構造化データの分析、指示への正確な追従といった課題が残されていました。

また、モデルの推論時における処理能力の向上も重要な課題となっています。段階的な推論や熟考のプロセスを実現することで、より複雑な問題解決への対応が模索されてきました。

こうした技術的な課題に加えて、研究開発の成果を広く活用可能にするための取り組みも進められています。オープンソースのLLMを公開することで、研究者や開発者がより容易にアクセスでき、様々な分野でのイノベーション創出につながることが期待されています。

このような背景のもと、研究者らは事前学習データを7兆トークンから18兆トークンへと大幅に拡大し、さらに100万件以上のサンプルを用いた高度な教師ありファインチューニングと多段階強化学習を組み合わせることで、次世代のLLM「Qwen2.5」の開発に取り組みました。

以下で詳しく紹介します。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信

[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

