---
source_url: https://ai-data-base.com/archives/95674
captured_at: 2026-01-19T07:27:15.788096
title: "Copilotサービスを使う上で気を付けたいプロンプトインジェクションによる機密情報漏洩リスク - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

本記事では、Copilotを使う際に気をつけたいポイントとして、プロンプトインジェクションによる機密情報の漏洩リスクを検証した事例を紹介します。

プロンプトインジェクションとは、外部のメールや文書に紛れた“指示”が、LLMに拾われてしまうことで、意図しない動作を引き起こす現象です。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2025/09/AIDB_95674-1024x576.png)

**本記事の関連研究**

  * [主要LLM各社のプライバシーポリシー比較 ユーザーのデータはどう扱われるか](https://ai-data-base.com/archives/94909)

  * [実システムにLLMを組み込み運用する上でどの脅威を対処すべきか、どこに防御を置くかを決める](https://ai-data-base.com/archives/95450)

## 背景

LLMを日常的に業務の文書作成やデータの整理に加え、外部ツールとの連携まで視野に入れて使っている方も多いのではないでしょうか。

たとえば代表的なサービスであるMicrosoft
365は、すでにLLMが業務の中に組み込まれ実運用されています。導入企業は1万社を超えており、LLMアシストでメールや社内フォルダを扱うことで、日々の業務を効率化できている実感を持つ人々は既に沢山います。

こうした広がりの一方で、思わぬセキュリティの課題も見えてきました。なかでも指示の乗っ取りがリスクとして注目されており、業界の脅威リストでも最上位に挙げられています。

指示の乗っ取りは、専門用語では「プロンプトインジェクション」と呼ばれるものです。ユーザーが気づかないうちに、他人の指示が自分のLLMに入力されてしまい、その結果予期せぬ動作が引き起こされます。この用語が登場した当初は、「こうしたリスクも存在可能」という認識のみにとどまっていましたが、最近になって問題が現実的になってきました。

LLMを日々使う人や導入を検討している人にとって、リスクは他人事ではありません。本記事は、事例をもとにLLMシステムの脆弱性を分析し、防御対策も考察していきます。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

  * 全記事・論文コンテンツを無制限で閲覧可能
  * 平日毎日更新、専門家による最新リサーチを配信

[まずはアカウントを作成](/membership-join)

[ログイン](/membership-login)

[プレミアム会員について](/premium-visitor)

