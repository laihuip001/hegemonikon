---
source_url: https://ai-data-base.com/archives/96420
captured_at: 2026-01-19T07:26:47.565421
title: "考えるAIとどう組むかの実践録 - AIDB"
publish_date: 2026.01.19
tags: []
conversion_method: restoration_script
is_premium: unknown
---

本企画では、[AIDBのX](https://x.com/ai_database)で紹介されたいくつかの最新AI研究を、ダイジェスト形式でお届けします。

普段の有料会員向け記事では、技術的な切り口から研究を詳しく紹介していますが、この企画では科学的な知識として楽しめるよう、テーマの概要をわかりやすくお伝えします。

今週は、AIが創造を助ける場面と固めてしまう場面、結論を急がせず思考を深める設計、競技級の専門問題への強さと弱点、言葉に残る感情の軌跡、対話のトーンが体験を左右する現実、新語で振る舞いを制御する手法、そして問題文だけから研究案をひねり出す力までを追いました。人とAIの距離感を整え、成果と安心の両立を図るヒントが見えてきます。

研究に対する反応が気になる方は、ぜひAIDBのXアカウント
([@ai_database](https://x.com/ai_database))で紹介ポストもご覧ください。中には多くの引用やコメントが寄せられた話題もあります。

また、一部は[Posfie](https://posfie.com/@ai_database)にも掲載されており、読者のリアクションをまとめたページもあわせて公開しています。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAJAAQMAAAApW4aWAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF5JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgHsBIk8AAeBiYYYAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2025/10/AIDB_science-1024x576.png)

目次

Toggle

  * AIは創造力の味方にも足かせにもなる
  * 結論を急がないAIが自分の考えを深くする
  * 星の難問には強いが図形は苦手 新モデルは天文オリンピック級
  * 怒りは長く、喜びは短い ことばの感情をモデルは覚えている
  * 一言のトーンで乗り心地が変わる 自動運転タクシーの会話研究
  * 作った言葉で動きが変わる 機械だけの合言葉も見えてきた
  * 問題文だけで研究案を出す 既存と違う筋の良い解も
  * まとめ

## AIは創造力の味方にも足かせにもなる

LLMが人間の創造性にどう影響するかを調べたところ、簡単な課題であれば創造性を刺激し、難しい課題の場合は創造性を阻害する可能性が示唆されました。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAApgAAANrAQMAAADYjnqJAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAF1JREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgQfKwABWCuS5AAAAABJRU5ErkJggg==)![](https://ai-
data-base.com/wp-content/uploads/2025/10/image-15.png)

たとえば道具の新しい使い方を考えるような簡単な課題では、LLMと協力する方が人間同士より創造的なアイデアを出せました。

ところが、革新的なものを考案するような複雑な課題になると、逆にLLMの影響で人間の創造性が低下してしまう現象が観察されました。  
詳しく見てみると、LLMが一度に大量の情報を提示すると人間の脳の処理能力が追いつかずLLMの話すがままになってしまうから、のようでした。

そしてどちらの場合も、LLMと作業した人は自分の創造性を低く評価する傾向がありました。LLMが完ぺきで自分がちっぽけに見えてしまうためです。

研究者たちは対策として複雑な課題ではLLMに一度に少量のアイデアだけを出させるよう制約すると、創造性が回復しました。

[この話題へのみんなの反応を見る (Xに移動)](https://x.com/ai_database/status/1975489047360593964)

#### 参考文献

Inspiration booster or creative fixation? The dual mechanisms of LLMs in
shaping individual creativity in tasks of different complexity

<https://www.nature.com/articles/s41599-025-05867-9>

Xusen Cheng & Lulu Zhang

Renmin University of China

* * *

## 結論を急がないAIが自分の考えを深くする

成人は1日に大小含め推定35,000回の意思決定をしていると言われています。そして現代ではLLMに決断を相談する人が増えています。

研究者らが「（LLMの助けなしで）人々が自分の力でどう考えるか」を調査してみると、自分では十分に考えたつもりでも、実際は視野が狭く浅い思考になっている人が多いと示唆されました。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAu4AAAQAAQMAAACETU5BAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAHVJREFUGBntwTEBAAAAwiD7p95tB2AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAnAB8DwABpl1pRwAAAABJRU5ErkJggg==)![](https://ai-
data-base.com/wp-content/uploads/2025/10/image-16-750x1024.png)

しかし同時に今のLLMは急いで答えを出す傾向があり、ユーザーがこれに頼り切りになると自分で考える機会がおろそかになる恐れが指摘されています。

そこで今回研究者らは、「あなたの考えは偏っているようです。起こりうるリスクについても考えてみませんか」といった形で、LLMがユーザーの思考パターンを鏡のように映し出すフレームワークを開発。  
調べたところ、こうしたエージェントによる支援を望む声はかなり多かったそうです。

[この話題へのみんなの反応を見る (Xに移動)](https://x.com/ai_database/status/1975545695290728566)

#### 参考文献

Reflection Before Action: Designing a Framework for Quantifying Thought
Patterns for Increased Self-awareness in Personal Decision Making

<https://arxiv.org/abs/2510.04364>

Morita Tarvirdians, Senthil Chandrasegaran, Hayley Hung, Catholijn M. Jonker,
Catharine Oertel

TU Delft, Leiden University

* * *

## 星の難問には強いが図形は苦手 新モデルは天文オリンピック級

GPT-5やGemini 2.5 Proは国際天文学・宇宙物理学オリンピックで金メダルに到達したそうです。

なお、GPT-5に至っては複数の年度で最優秀の人間学生を上回る結果を出しています。  
試しに天文や宇宙物理について尋ねてみてはいかがでしょうか。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAApQAAAQAAQMAAABlJlsQAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAGpJREFUGBntwTEBAAAAwiD7p14LL2AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFwEUA8AAS7Iw8cAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2025/10/image-17-660x1024.png)

ただし、完璧というわけではありません。幾何学や空間的な推論を必要とする問題は苦手でした。  
また、単純な計算ミスは少ないものの、法則の理解が不完全だったり、概念を間違って適用したりすることが多いことも分かりました。

総合的には天文学の難問を解く能力があるものの、人間が得意な課題ではまだおよばない部分もあるということです。

なお、国際天文学・宇宙物理学オリンピックの例題としてはダークエネルギーの宇宙論的影響、銀河団からのX線放射分析などがあり、こうした問題はLLMは得意なようです。

[この話題へのみんなの反応を見る (Xに移動)](https://x.com/ai_database/status/1975866137146724830)

#### 参考文献

Large Language Models Achieve Gold Medal Performance at the International
Olympiad on Astronomy & Astrophysics (IOAA)

<https://arxiv.org/abs/2510.05016>

<https://github.com/OSU-NLP-Group/LLM-IOAA>

Lucas Carrit Delgado Pinheiro, Ziru Chen, Bruno Caixeta Piazza, Ness Shroff,
Yingbin Liang, Yuan-Sen Ting, Huan Sun

The Ohio State University, Universidade de São Paulo

* * *

## 怒りは長く、喜びは短い ことばの感情をモデルは覚えている

研究者らがLLMの内部を覗き見た結果、ユーザーがメッセージにおける初めの方で示した感情は、モデルの返答が進んでも内部では記憶されていることが判明しました。  
ただし、この持続時間は感情によって異なり、怒りや恐れは長く残る一方、喜びは素早く消えていきます。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAvoAAAQAAQMAAACqCG/TAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAHZJREFUGBntwQEBAAAAgqD+r3ZIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM4FhA8AAajeamAAAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2025/10/image-18-762x1024.png)

これはLLMが役立つアシスタントとして訓練された結果そのような挙動に反映されるのだと推測されています。

また、モデルは処理の比較的早い段階で「これは悲しい文章だ」と理解し、その後のプロセスでその理解を使って適切な返答を生成しているようです。

そして「感情的になってください」と指示すると、モデルは悲しみに共感的に反応するようになります。

これらの一連の結果は、LLMが本当に感情を「感じている」わけではないものの、感情を非常に体系的に処理していることを示唆しています。

[この話題へのみんなの反応を見る (Xに移動)](https://x.com/ai_database/status/1976267581913989153)

#### 参考文献

Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent,
Retain, and Express Emotion

<https://arxiv.org/abs/2510.04064>

Jingxiang Zhang, Lujia Zhong

University of Southern California

#### 関連記事

> [🔒 LLMに自分自身の内部動作を説明させる手法](https://ai-data-base.com/archives/63048)

* * *

## 一言のトーンで乗り心地が変わる 自動運転タクシーの会話研究

人間は会話全体の平均的な印象ではなく、嫌だった瞬間を記憶する傾向があるため、AIとの会話においても、少しでもネガティブな内容があると心の中で尾を引いてしまう可能性があると示唆されています。

今回、自動運転タクシーに対する人間の評価を調査したところ、AIの応答がポジティブな 調子であることが評価において極めて重要な要素でした。  
AI応答の明るさはサービス品質全体の評価における4割ほどを左右する要因だと推測されています。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAt4AAAQAAQMAAADsvsx5AAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAHNJREFUGBntwYEAAAAAw6D7U8/gBNUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALgCdA8AAWaFfI0AAAAASUVORK5CYII=)![](https://ai-
data-base.com/wp-content/uploads/2025/10/image-19-734x1024.png)

ハイテクなサービスであっても会話から得られる些細な印象でサービスの評価が大きく変わるのであれば、そうしたリスクに対処する必要があります。  
しかし、現在の技術ではAIが会話から人間の感じ方を完全に捉えるのはまだ難しいと考えられています。

会話機能つきのサービスに対するユーザー評価を分析する際には、この点を織り込むべきかもしれません。

なお会話の平均的な印象ではなく嫌な瞬間に引っ張られる心理現象は「ネガティビティ・バイアス」と呼ばれています。

[この話題へのみんなの反応を見る (Xに移動)](https://x.com/ai_database/status/1976637958670500161)

#### 参考文献

Sentiment Matters: An Analysis of 200 Human-SAV Interactions

<https://arxiv.org/abs/2510.08202>

Lirui Guo, Michael G. Burke, Wynita M. Griggs

Monash University

* * *

## 作った言葉で動きが変わる 機械だけの合言葉も見えてきた

Google DeepMindの研究者らは、AIに新しい架空の言葉を教えることで、その振る舞いをコントロールする方法を開発したと報告しています。  
例えば、ある新しいワードに反応してお世辞を言うようにするなど特定の行動をするようにできるそうです。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAroAAAOVAQMAAAC8vtENAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAGZJREFUGBntwTEBAAAAwiD7p14KP2AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwE+3AAB+Dii/AAAAABJRU5ErkJggg==)![](https://ai-
data-base.com/wp-content/uploads/2025/10/image-20.png)

興味深いことに、AIはその新語について意味を尋ねられると、訓練時には教わっていないにも関わらず言葉で説明できるとのことです。

さらに、AIはその新語の同義語にあたる言葉を既存の単語から選んで伝えることも出来ますが、人間にとってはその関連性が直感的ではないという状況でした。  
しかし他のAIにとっては理解できることがあるようです。  
つまり機械同士では特定の意味を共有する「機械だけの同義語」が生まれていたということです。

AIとのコミュニケーションにおける奥深さと新しい可能性を示唆するような研究です。

[この話題へのみんなの反応を見る (Xに移動)](https://x.com/ai_database/status/1976984817121268073)

#### 参考文献

Neologism Learning for Controllability and Self-Verbalization

<https://arxiv.org/abs/2510.08506>

John Hewitt, Oyvind Tafjord, Robert Geirhos, Been Kim

Google DeepMind

#### 関連記事

> [🔒 LLM専用の「新しい言葉」を導入すると何が起きるのか](https://ai-data-base.com/archives/84361)

* * *

## 問題文だけで研究案を出す 既存と違う筋の良い解も

LLMに、1,200本以上の論文から抽出した問題文だけを見せて「あなたならどう解決しますか」と尋ねる実験が実施されました。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAApwAAAPlAQMAAAD1369bAAAAA1BMVEUAAACnej3aAAAAAXRSTlMAQObYZgAAAGlJREFUGBntwTEBAAAAwiD7p14MH2AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALwFLGAABw93ixQAAAABJRU5ErkJggg==)![](https://ai-
data-base.com/wp-content/uploads/2025/10/image-21.png)

その結果、LLMは科学的に妥当で、しかも人間の提案と異なる有効な解を出せる場合があることが示されました。  
記憶の再生だけではなく、新しい発想が生まれる可能性がうかがえます。

とくにGPT-OSS-120Bを内部モデルに用いた構成では、約4回に3回の割合で筋の通った解を出せました（厳格基準）。

また、学会での評価の高低が違ってもLLMのアイデア成功率は大きく変わらず、この現象の理由は明確ではありません。

#### 参考文献

AInstein: Assessing the Feasibility of AI-Generated Approaches to Research
Problems

<https://arxiv.org/abs/2510.05432>

Shambhavi Mishra, Gaurav Sahu, Marco Pedersoli, Laurent Charlin, Jose Dolz,
Christopher Pal

Mila – Quebec AI Institute, HEC Montréal, ServiceNow Research, Université de
Montréal, Polytechnique Montréal, ÉTS Montréal, International Laboratory on
Learning Systems (ILLS)

#### 関連記事

> [🔒 100人以上の研究者が実験参加 LLMは人間より優れた研究アイデアを思いつくのか？](https://ai-data-
> base.com/archives/75562)

## まとめ

今週見えたのは、AIは人の思考と創造を「整える設計」で価値が決まるという点です。出力は小刻みに、対話は丁寧なトーンで、最終判断は人が担うという基本を守れば力になる一方、急ぎの結論や過剰な情報提示は足かせになります。モデルは感情の手がかりを扱え、専門領域で強さも示すものの、弱点や脆さは残るため、説明可能性と責任の分担を明確にして運用することが重要です。

このダイジェストでは、機能紹介だけでなく、その背後にある意図や運用上の注意点も見渡します。どこで人が関与し、どんな順番で確かめ、誰が責任を持つのか。広がる活用の中で、私たちが選び取りたい関わり方を具体的に描き直し、次回もその境目を一緒に見極めていきましょう。

