{"arxiv_id": "2404.01261", "title": "FABLES: Evaluating faithfulness and content selection in book-length summarization", "authors": ["Yekyung Kim", "Yapei Chang", "Marzena Karpinska", "Aparna Garimella", "Varun Manjunatha", "Kyle Lo", "Tanya Goyal", "Mohit Iyyer"], "summary": "While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book.", "categories": ["cs.CL", "cs.AI"], "published": "2024-04-01T17:33:38+00:00", "updated": "2024-09-30T17:39:59+00:00", "pdf_url": "https://arxiv.org/pdf/2404.01261v2", "source_articles": ["58765"], "fetched_at": "2026-01-19T12:36:48.247978"}
{"arxiv_id": "2501.07238", "title": "Lessons From Red Teaming 100 Generative AI Products", "authors": ["Blake Bullwinkel", "Amanda Minnich", "Shiven Chawla", "Gary Lopez", "Martin Pouliot", "Whitney Maxwell", "Joris de Gruyter", "Katherine Pratt", "Saphir Qi", "Nina Chikanov", "Roman Lutz", "Raja Sekhar Rao Dheekonda", "Bolor-Erdene Jagdagdorj", "Eugenia Kim", "Justin Song", "Keegan Hines", "Daniel Jones", "Giorgio Severi", "Richard Lundeen", "Sam Vaughan", "Victoria Westerhoff", "Pete Bryan", "Ram Shankar Siva Kumar", "Yonatan Zunger", "Chang Kawaguchi", "Mark Russinovich"], "summary": "In recent years, AI red teaming has emerged as a practice for probing the safety and security of generative AI systems. Due to the nascency of the field, there are many open questions about how red teaming operations should be conducted. Based on our experience red teaming over 100 generative AI products at Microsoft, we present our internal threat model ontology and eight main lessons we have learned:   1. Understand what the system can do and where it is applied   2. You don't have to compute gradients to break an AI system   3. AI red teaming is not safety benchmarking   4. Automation can help cover more of the risk landscape   5. The human element of AI red teaming is crucial   6. Responsible AI harms are pervasive but difficult to measure   7. LLMs amplify existing security risks and introduce new ones   8. The work of securing AI systems will never be complete   By sharing these insights alongside case studies from our operations, we offer practical recommendations aimed at aligning red teaming efforts with real world risks. We also highlight aspects of AI red teaming that we believe are often misunderstood and discuss open questions for the field to consider.", "categories": ["cs.AI"], "published": "2025-01-13T11:36:33+00:00", "updated": "2025-01-13T11:36:33+00:00", "pdf_url": "https://arxiv.org/pdf/2501.07238v1", "source_articles": ["82195"], "fetched_at": "2026-01-19T12:36:51.266127"}
{"arxiv_id": "2511.01363", "title": "Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing", "authors": ["Giuseppe Riva", "Brenda K. Wiederhold", "Fabrizia Mantovani"], "summary": "The cognitive processes of the hypnotized mind and the computational operations of large language models (LLMs) share deep functional parallels. Both systems generate sophisticated, contextually appropriate behavior through automatic pattern-completion mechanisms operating with limited or unreliable executive oversight. This review examines this convergence across three principles: automaticity, in which responses emerge from associative rather than deliberative processes; suppressed monitoring, leading to errors such as confabulation in hypnosis and hallucination in LLMs; and heightened contextual dependency, where immediate cues (for example, the suggestion of a therapist or the prompt of the user) override stable knowledge.   These mechanisms reveal an observer-relative meaning gap: both systems produce coherent but ungrounded outputs that require an external interpreter to supply meaning. Hypnosis and LLMs also exemplify functional agency - the capacity for complex, goal-directed, context-sensitive behavior - without subjective agency, the conscious awareness of intention and ownership that defines human action. This distinction clarifies how purposive behavior can emerge without self-reflective consciousness, governed instead by structural and contextual dynamics. Finally, both domains illuminate the phenomenon of scheming: automatic, goal-directed pattern generation that unfolds without reflective awareness. Hypnosis provides an experimental model for understanding how intention can become dissociated from conscious deliberation, offering insights into the hidden motivational dynamics of artificial systems. Recognizing these parallels suggests that the future of reliable AI lies in hybrid architectures that integrate generative fluency with mechanisms of executive monitoring, an approach inspired by the complex, self-regulating architecture of the human mind.", "categories": ["cs.AI"], "published": "2025-11-03T09:08:50+00:00", "updated": "2025-11-03T09:08:50+00:00", "pdf_url": "https://arxiv.org/pdf/2511.01363v1", "source_articles": ["97657"], "fetched_at": "2026-01-19T12:36:54.281347"}
{"arxiv_id": "2511.00115", "title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference", "authors": ["Haoyuan Li", "Yuanbo Tong", "Yuchen Li", "Zirui Wang", "Chunhou Liu", "Jiamou Liu"], "summary": "Personality recognition from text is typically cast as hard-label classification, which obscures the graded, prototype-like nature of human personality judgments. We present ProtoMBTI, a cognitively aligned framework for MBTI inference that operationalizes prototype theory within an LLM-based pipeline. First, we construct a balanced, quality-controlled corpus via LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment). Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative embeddings and to standardize a bank of personality prototypes. At inference, we retrieve top-k prototypes for a query post and perform a retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence via prompt-based voting, revises when inconsistencies arise, and, upon correct prediction, retains the sample to continually enrich the prototype library. Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both the four MBTI dichotomies and the full 16-type task, and exhibits robust cross-dataset generalization. Our results indicate that aligning the inference process with psychological prototype reasoning yields gains in accuracy, interpretability, and transfer for text-based personality modeling.", "categories": ["cs.CL", "cs.AI"], "published": "2025-10-31T02:45:30+00:00", "updated": "2025-12-29T13:01:51+00:00", "pdf_url": "https://arxiv.org/pdf/2511.00115v2", "source_articles": ["97657"], "fetched_at": "2026-01-19T12:36:57.296188"}
{"arxiv_id": "2511.02864", "title": "Mathematical exploration and discovery at scale", "authors": ["Bogdan Georgiev", "Javier Gómez-Serrano", "Terence Tao", "Adam Zsolt Wagner"], "summary": "AlphaEvolve (Novikov et al., 2025) is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems.   To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights.   These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time.", "categories": ["cs.NE", "cs.AI", "math.CA", "math.CO", "math.MG"], "published": "2025-11-03T16:04:07+00:00", "updated": "2025-12-22T12:49:01+00:00", "pdf_url": "https://arxiv.org/pdf/2511.02864v3", "source_articles": ["97657"], "fetched_at": "2026-01-19T12:37:00.312785"}
{"arxiv_id": "2511.04570", "title": "Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm", "authors": ["Jingqi Tong", "Yurong Mou", "Hangcheng Li", "Mingzhe Li", "Yongzhuo Yang", "Ming Zhang", "Qiguang Chen", "Tianyi Liang", "Xiaomeng Hu", "Yining Zheng", "Xinchi Chen", "Jun Zhao", "Xuanjing Huang", "Xipeng Qiu"], "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce \"Thinking with Video\", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2's performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions \"thinking with video\" as a unified multimodal reasoning paradigm.", "categories": ["cs.CV", "cs.CL"], "published": "2025-11-06T17:25:23+00:00", "updated": "2025-11-06T17:25:23+00:00", "pdf_url": "https://arxiv.org/pdf/2511.04570v1", "source_articles": ["97657"], "fetched_at": "2026-01-19T12:37:03.327798"}
{"arxiv_id": "2511.03942", "title": "MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation", "authors": ["Shih-Lun Wu", "Yoon Kim", "Cheng-Zhi Anna Huang"], "summary": "We present MIDI-LLM, an LLM for generating multitrack MIDI music from free-form text prompts. Our approach expands a text LLM's vocabulary to include MIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI abilities. By preserving the original LLM's parameter structure, we can directly leverage the vLLM library for accelerated inference. Experiments show that MIDI-LLM achieves higher quality, better text control, and faster inference compared to the recent Text2midi model. Live demo at https://midi-llm-demo.vercel.app.", "categories": ["cs.SD", "cs.CL", "cs.MM"], "published": "2025-11-06T00:40:07+00:00", "updated": "2025-11-06T00:40:07+00:00", "pdf_url": "https://arxiv.org/pdf/2511.03942v1", "source_articles": ["97657"], "fetched_at": "2026-01-19T12:37:06.338923"}
{"arxiv_id": "2511.04962", "title": "Too Good to be Bad: On the Failure of LLMs to Role-Play Villains", "authors": ["Zihao Yi", "Qingxuan Jiang", "Ruotian Ma", "Xingyu Chen", "Qu Yang", "Mengru Wang", "Fanghua Ye", "Ying Shen", "Zhaopeng Tu", "Xiaolong Li", "Linus"], "summary": "Large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. To investigate this, we introduce the Moral RolePlay benchmark, a new dataset featuring a four-level moral alignment scale and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs with role-playing characters from moral paragons to pure villains. Our large-scale evaluation reveals a consistent, monotonic decline in role-playing fidelity as character morality decreases. We find that models struggle most with traits directly antithetical to safety principles, such as ``Deceitful'' and ``Manipulative'', often substituting nuanced malevolence with superficial aggression. Furthermore, we demonstrate that general chatbot proficiency is a poor predictor of villain role-playing ability, with highly safety-aligned models performing particularly poorly. Our work provides the first systematic evidence of this critical limitation, highlighting a key tension between model safety and creative fidelity. Our benchmark and findings pave the way for developing more nuanced, context-aware alignment methods.", "categories": ["cs.CL", "cs.AI"], "published": "2025-11-07T03:50:52+00:00", "updated": "2025-11-12T12:26:58+00:00", "pdf_url": "https://arxiv.org/pdf/2511.04962v2", "source_articles": ["97986"], "fetched_at": "2026-01-19T12:37:09.352866"}
{"arxiv_id": "2511.05319", "title": "$\\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models", "authors": ["Huanqi Wu", "Huangbiao Xu", "Runfeng Xie", "Jiaxin Cai", "Kaixin Zhang", "Xiao Ke"], "summary": "Despite remarkable progress in steganography, embedding semantically rich, sentence-level information into carriers remains a challenging problem. In this work, we present a novel concept of Semantic Steganography, which aims to hide semantically meaningful and structured content, such as sentences or paragraphs, in cover media. Based on this concept, we present Sentence-to-Image Steganography as an instance that enables the hiding of arbitrary sentence-level messages within a cover image. To accomplish this feat, we propose S^2LM: Semantic Steganographic Language Model, which leverages large language models (LLMs) to embed high-level textual information into images. Unlike traditional bit-level approaches, S^2LM redesigns the entire pipeline, involving the LLM throughout the process to enable the hiding and recovery of arbitrary sentences. Furthermore, we establish a benchmark named Invisible Text (IVT), comprising a diverse set of sentence-level texts as secret messages to evaluate semantic steganography methods. Experimental results demonstrate that S^2LM effectively enables direct sentence recovery beyond bit-level steganography. The source code and IVT dataset will be released soon.", "categories": ["cs.CV", "cs.CR"], "published": "2025-11-07T15:17:40+00:00", "updated": "2026-01-07T08:54:15+00:00", "pdf_url": "https://arxiv.org/pdf/2511.05319v2", "source_articles": ["97986"], "fetched_at": "2026-01-19T12:37:12.365658"}
{"arxiv_id": "2511.08565", "title": "Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models", "authors": ["Davi Bastos Costa", "Felippe Alves", "Renato Vicente"], "summary": "Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "published": "2025-11-11T18:47:44+00:00", "updated": "2025-11-11T18:47:44+00:00", "pdf_url": "https://arxiv.org/pdf/2511.08565v1", "source_articles": ["97986"], "fetched_at": "2026-01-19T12:37:15.378353"}
{"arxiv_id": "2511.09133", "title": "Assessing the Capabilities of LLMs in Humor:A Multi-dimensional Analysis of Oogiri Generation and Evaluation", "authors": ["Ritsu Sakabe", "Hwichan Kim", "Tosho Hirasawa", "Mamoru Komachi"], "summary": "Computational humor is a frontier for creating advanced and engaging natural language processing (NLP) applications, such as sophisticated dialogue systems. While previous studies have benchmarked the humor capabilities of Large Language Models (LLMs), they have often relied on single-dimensional evaluations, such as judging whether something is simply ``funny.'' This paper argues that a multifaceted understanding of humor is necessary and addresses this gap by systematically evaluating LLMs through the lens of Oogiri, a form of Japanese improvisational comedy games. To achieve this, we expanded upon existing Oogiri datasets with data from new sources and then augmented the collection with Oogiri responses generated by LLMs. We then manually annotated this expanded collection with 5-point absolute ratings across six dimensions: Novelty, Clarity, Relevance, Intelligence, Empathy, and Overall Funniness. Using this dataset, we assessed the capabilities of state-of-the-art LLMs on two core tasks: their ability to generate creative Oogiri responses and their ability to evaluate the funniness of responses using a six-dimensional evaluation. Our results show that while LLMs can generate responses at a level between low- and mid-tier human performance, they exhibit a notable lack of Empathy. This deficit in Empathy helps explain their failure to replicate human humor assessment. Correlation analyses of human and model evaluation data further reveal a fundamental divergence in evaluation criteria: LLMs prioritize Novelty, whereas humans prioritize Empathy. We release our annotated corpus to the community to pave the way for the development of more emotionally intelligent and sophisticated conversational agents.", "categories": ["cs.CL", "cs.AI"], "published": "2025-11-12T09:16:58+00:00", "updated": "2025-11-12T09:16:58+00:00", "pdf_url": "https://arxiv.org/pdf/2511.09133v1", "source_articles": ["97986"], "fetched_at": "2026-01-19T12:37:18.391301"}
{"arxiv_id": "2511.09057", "title": "PAN: A World Model for General, Interactable, and Long-Horizon World Simulation", "authors": ["PAN Team", "Jiannan Xiang", "Yi Gu", "Zihan Liu", "Zeyu Feng", "Qiyue Gao", "Yiyan Hu", "Benhao Huang", "Guangyi Liu", "Yichi Yang", "Kun Zhou", "Davit Abrahamyan", "Arif Ahmad", "Ganesh Bannur", "Junrong Chen", "Kimi Chen", "Mingkai Deng", "Ruobing Han", "Xinqi Huang", "Haoqiang Kang", "Zheqi Liu", "Enze Ma", "Hector Ren", "Yashowardhan Shinde", "Rohan Shingre", "Ramsundar Tanikella", "Kaiming Tao", "Dequan Yang", "Xinle Yu", "Cong Zeng", "Binglin Zhou", "Zhengzhong Liu", "Zhiting Hu", "Eric P. Xing"], "summary": "A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-11-12T07:20:35+00:00", "updated": "2025-11-15T01:07:10+00:00", "pdf_url": "https://arxiv.org/pdf/2511.09057v3", "source_articles": ["97986"], "fetched_at": "2026-01-19T12:37:21.409409"}
{"arxiv_id": "2511.04875", "title": "Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs", "authors": ["Matthew Bozoukov", "Matthew Nguyen", "Shubkarman Singh", "Bart Bussmann", "Patrick Leask"], "summary": "Recent studies have revealed that LLMs can exhibit behavioral self-awareness: the ability to accurately describe or predict their own learned behaviors without explicit supervision. This capability raises safety concerns as it may, for example, allow models to better conceal their true abilities during evaluation. We attempt to characterize the minimal conditions under which such self-awareness emerges, and the mechanistic processes through which it manifests. Through controlled finetuning experiments on instruction-tuned LLMs with low-rank adapters (LoRA), we find: (1) that self-awareness can be reliably induced using a single rank-1 LoRA adapter; (2) that the learned self-aware behavior can be largely captured by a single steering vector in activation space, recovering nearly all of the fine-tune's behavioral effect; and (3) that self-awareness is non-universal and domain-localized, with independent representations across tasks. Together, these findings suggest that behavioral self-awareness emerges as a domain-specific, linear feature that can be easily induced and modulated.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-11-06T23:28:16+00:00", "updated": "2025-11-10T01:27:05+00:00", "pdf_url": "https://arxiv.org/pdf/2511.04875v2", "source_articles": ["97986"], "fetched_at": "2026-01-19T12:37:24.421364"}
{"arxiv_id": "2511.09667", "title": "How AI Responses Shape User Beliefs: The Effects of Information Detail and Confidence on Belief Strength and Stance", "authors": ["Zekun Wu", "Mayank Jobanputra", "Vera Demberg", "Jessica Hullman", "Anna Maria Feit"], "summary": "The growing use of AI-generated responses in everyday tools raises concern about how subtle features such as supporting detail or tone of confidence may shape people's beliefs. To understand this, we conducted a pre-registered online experiment (N = 304) investigating how the detail and confidence of AI-generated responses influence belief change. We introduce an analysis framework with two targeted measures: belief switch and belief shift. These distinguish between users changing their initial stance after AI input and the extent to which they adjust their conviction toward or away from the AI's stance, thereby quantifying not only categorical changes but also more subtle, continuous adjustments in belief strength that indicate a reinforcement or weakening of existing beliefs. Using this framework, we find that detailed responses with medium confidence are associated with the largest overall belief changes. Highly confident messages tend to elicit belief shifts but induce fewer stance reversals. Our results also show that task type (fact-checking versus opinion evaluation), prior conviction, and perceived stance agreement further modulate the extent and direction of belief change. These findings illustrate how different properties of AI responses interact with user beliefs in subtle but potentially consequential ways and raise practical as well as ethical considerations for the design of LLM-powered systems.", "categories": ["cs.HC"], "published": "2025-11-12T19:14:02+00:00", "updated": "2025-11-12T19:14:02+00:00", "pdf_url": "https://arxiv.org/pdf/2511.09667v1", "source_articles": ["97986"], "fetched_at": "2026-01-19T12:37:27.438850"}
{"arxiv_id": "2510.25743", "title": "Agentic Economic Modeling", "authors": ["Bohan Zhang", "Jiaxuan Li", "Ali Hortaçsu", "Xiaoyang Ye", "Victor Chernozhukov", "Angelo Ni", "Edward Huang"], "summary": "We introduce Agentic Economic Modeling (AEM), a framework that aligns synthetic LLM choices with small-sample human evidence for reliable econometric inference. AEM first generates task-conditioned synthetic choices via LLMs, then learns a bias-correction mapping from task features and raw LLM choices to human-aligned choices, upon which standard econometric estimators perform inference to recover demand elasticities and treatment effects.We validate AEM in two experiments. In a large scale conjoint study with millions of observations, using only 10% of the original data to fit the correction model lowers the error of the demand-parameter estimates, while uncorrected LLM choices even increase the errors. In a regional field experiment, a mixture model calibrated on 10% of geographic regions estimates an out-of-domain treatment effect of -65\\pm10 bps, closely matching the full human experiment (-60\\pm8 bps).Under time-wise extrapolation, training with only day-one human data yields -24 bps (95% CI: [-26, -22], p<1e-5),improving over the human-only day-one baseline (-17 bps, 95% CI: [-43, +9], p=0.2049).These results demonstrate AEM's potential to improve RCT efficiency and establish a foundation method for LLM-based counterfactual generation.", "categories": ["econ.EM"], "published": "2025-10-29T17:46:07+00:00", "updated": "2025-10-29T17:46:07+00:00", "pdf_url": "https://arxiv.org/pdf/2510.25743v1", "source_articles": ["98344"], "fetched_at": "2026-01-19T12:37:30.449173"}
{"arxiv_id": "2511.13046", "title": "Knowing Ourselves Through Others: Reflecting with AI in Digital Human Debates", "authors": ["Ichiro Matsuda", "Komichi Takezawa", "Katsuhito Muroi", "Kensuke Katori", "Ryosuke Hyakuta", "Jingjing Li", "Yoichi Ochiai"], "summary": "LLMs can act as an impartial other, drawing on vast knowledge, or as personalized self-reflecting user prompts. These personalized LLMs, or Digital Humans, occupy an intermediate position between self and other. This research explores the dynamic of self and other mediated by these Digital Humans. Using a Research Through Design approach, nine junior and senior high school students, working in teams, designed Digital Humans and had them debate. Each team built a unique Digital Human using prompt engineering and RAG, then observed their autonomous debates. Findings from generative AI literacy tests, interviews, and log analysis revealed that participants deepened their understanding of AI's capabilities. Furthermore, experiencing their own creations as others prompted a reflective attitude, enabling them to objectively view their own cognition and values. We propose \"Reflecting with AI\" - using AI to re-examine the self - as a new generative AI literacy, complementing the conventional understanding, applying, criticism and ethics.", "categories": ["cs.HC"], "published": "2025-11-17T06:48:13+00:00", "updated": "2025-11-17T06:48:13+00:00", "pdf_url": "https://arxiv.org/pdf/2511.13046v1", "source_articles": ["98344"], "fetched_at": "2026-01-19T12:37:33.460412"}
{"arxiv_id": "2511.11773", "title": "On the Measure of a Model: From Intelligence to Generality", "authors": ["Ruchira Dhar", "Ninell Oldenburg", "Anders Soegaard"], "summary": "Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.", "categories": ["cs.AI", "cs.LG"], "published": "2025-11-14T09:46:48+00:00", "updated": "2025-11-14T09:46:48+00:00", "pdf_url": "https://arxiv.org/pdf/2511.11773v1", "source_articles": ["98344"], "fetched_at": "2026-01-19T12:37:36.470574"}
{"arxiv_id": "2511.11789", "title": "From Single to Societal: Analyzing Persona-Induced Bias in Multi-Agent Interactions", "authors": ["Jiayi Li", "Xiao Liu", "Yansong Feng"], "summary": "Large Language Model (LLM)-based multi-agent systems are increasingly used to simulate human interactions and solve collaborative tasks. A common practice is to assign agents with personas to encourage behavioral diversity. However, this raises a critical yet underexplored question: do personas introduce biases into multi-agent interactions? This paper presents a systematic investigation into persona-induced biases in multi-agent interactions, with a focus on social traits like trustworthiness (how an agent's opinion is received by others) and insistence (how strongly an agent advocates for its opinion). Through a series of controlled experiments in collaborative problem-solving and persuasion tasks, we reveal that (1) LLM-based agents exhibit biases in both trustworthiness and insistence, with personas from historically advantaged groups (e.g., men and White individuals) perceived as less trustworthy and demonstrating less insistence; and (2) agents exhibit significant in-group favoritism, showing a higher tendency to conform to others who share the same persona. These biases persist across various LLMs, group sizes, and numbers of interaction rounds, highlighting an urgent need for awareness and mitigation to ensure the fairness and reliability of multi-agent systems.", "categories": ["cs.MA", "cs.AI"], "published": "2025-11-14T18:19:28+00:00", "updated": "2025-11-14T18:19:28+00:00", "pdf_url": "https://arxiv.org/pdf/2511.11789v1", "source_articles": ["98344"], "fetched_at": "2026-01-19T12:37:39.484667"}
{"arxiv_id": "2511.15895", "title": "Decomposing Theory of Mind: How Emotional Processing Mediates ToM Abilities in LLMs", "authors": ["Ivan Chulo", "Ananya Joshi"], "summary": "Recent work shows activation steering substantially improves language models' Theory of Mind (ToM) (Bortoletto et al. 2024), yet the mechanisms of what changes occur internally that leads to different outputs remains unclear. We propose decomposing ToM in LLMs by comparing steered versus baseline LLMs' activations using linear probes trained on 45 cognitive actions. We applied Contrastive Activation Addition (CAA) steering to Gemma-3-4B and evaluated it on 1,000 BigToM forward belief scenarios (Gandhi et al. 2023), we find improved performance on belief attribution tasks (32.5\\% to 46.7\\% accuracy) is mediated by activations processing emotional content : emotion perception (+2.23), emotion valuing (+2.20), while suppressing analytical processes: questioning (-0.78), convergent thinking (-1.59). This suggests that successful ToM abilities in LLMs are mediated by emotional understanding, not analytical reasoning.", "categories": ["cs.AI"], "published": "2025-11-19T21:56:00+00:00", "updated": "2025-11-19T21:56:00+00:00", "pdf_url": "https://arxiv.org/pdf/2511.15895v1", "source_articles": ["98344"], "fetched_at": "2026-01-19T12:37:42.497446"}
{"arxiv_id": "2511.14945", "title": "Unsupervised Discovery of Long-Term Spatiotemporal Periodic Workflows in Human Activities", "authors": ["Fan Yang", "Quanting Xie", "Atsunori Moteki", "Shoichi Masui", "Shan Jiang", "Kanji Uchino", "Yonatan Bisk", "Graham Neubig"], "summary": "Periodic human activities with implicit workflows are common in manufacturing, sports, and daily life. While short-term periodic activities -- characterized by simple structures and high-contrast patterns -- have been widely studied, long-term periodic workflows with low-contrast patterns remain largely underexplored. To bridge this gap, we introduce the first benchmark comprising 580 multimodal human activity sequences featuring long-term periodic workflows. The benchmark supports three evaluation tasks aligned with real-world applications: unsupervised periodic workflow detection, task completion tracking, and procedural anomaly detection. We also propose a lightweight, training-free baseline for modeling diverse periodic workflow patterns. Experiments show that: (i) our benchmark presents significant challenges to both unsupervised periodic detection methods and zero-shot approaches based on powerful large language models (LLMs); (ii) our baseline outperforms competing methods by a substantial margin in all evaluation tasks; and (iii) in real-world applications, our baseline demonstrates deployment advantages on par with traditional supervised workflow detection approaches, eliminating the need for annotation and retraining. Our project page is https://sites.google.com/view/periodicworkflow.", "categories": ["cs.CV"], "published": "2025-11-18T22:07:30+00:00", "updated": "2025-11-20T16:11:12+00:00", "pdf_url": "https://arxiv.org/pdf/2511.14945v2", "source_articles": ["98344"], "fetched_at": "2026-01-19T12:37:45.508888"}
{"arxiv_id": "2511.11954", "title": "LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code", "authors": ["Borchuluun Yadamsuren", "Steven Keith Platt", "Miguel Diaz"], "summary": "This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.   LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.   This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.   In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.", "categories": ["cs.AI"], "published": "2025-11-15T00:05:02+00:00", "updated": "2025-11-15T00:05:02+00:00", "pdf_url": "https://arxiv.org/pdf/2511.11954v1", "source_articles": ["98344"], "fetched_at": "2026-01-19T12:37:48.520345"}
{"arxiv_id": "2511.16769", "title": "Trust in AI emerges from distrust in humans: A machine learning study on decision-making guidance", "authors": ["Johan Sebastián Galindez-Acosta", "Juan José Giraldo-Huertas"], "summary": "This study explores the dynamics of trust in artificial intelligence (AI) agents, particularly large language models (LLMs), by introducing the concept of \"deferred trust\", a cognitive mechanism where distrust in human agents redirects reliance toward AI perceived as more neutral or competent. Drawing on frameworks from social psychology and technology acceptance models, the research addresses gaps in user-centric factors influencing AI trust. Fifty-five undergraduate students participated in an experiment involving 30 decision-making scenarios (factual, emotional, moral), selecting from AI agents (e.g., ChatGPT), voice assistants, peers, adults, or priests as guides. Data were analyzed using K-Modes and K-Means clustering for patterns, and XGBoost models with SHAP interpretations to predict AI selection based on sociodemographic and prior trust variables.   Results showed adults (35.05\\%) and AI (28.29\\%) as the most selected agents overall. Clustering revealed context-specific preferences: AI dominated factual scenarios, while humans prevailed in social/moral ones. Lower prior trust in human agents (priests, peers, adults) consistently predicted higher AI selection, supporting deferred trust as a compensatory transfer. Participant profiles with higher AI trust were distinguished by human distrust, lower technology use, and higher socioeconomic status. Models demonstrated consistent performance (e.g., average precision up to 0.863).   Findings challenge traditional models like TAM/UTAUT, emphasizing relational and epistemic dimensions in AI trust. They highlight risks of over-reliance due to fluency effects and underscore the need for transparency to calibrate vigilance. Limitations include sample homogeneity and static scenarios; future work should incorporate diverse populations and multimodal data to refine deferred trust across contexts.", "categories": ["cs.HC"], "published": "2025-11-20T19:39:13+00:00", "updated": "2025-11-20T19:39:13+00:00", "pdf_url": "https://arxiv.org/pdf/2511.16769v1", "source_articles": ["98658"], "fetched_at": "2026-01-19T12:37:51.531471"}
{"arxiv_id": "2511.16825", "title": "WorldGen: From Text to Traversable and Interactive 3D Worlds", "authors": ["Dilin Wang", "Hyunyoung Jung", "Tom Monnier", "Kihyuk Sohn", "Chuhang Zou", "Xiaoyu Xiang", "Yu-Ying Yeh", "Di Liu", "Zixuan Huang", "Thu Nguyen-Phuoc", "Yuchen Fan", "Sergiu Oprea", "Ziyan Wang", "Roman Shapovalov", "Nikolaos Sarafianos", "Thibault Groueix", "Antoine Toisoul", "Prithviraj Dhar", "Xiao Chu", "Minghao Chen", "Geon Yeong Park", "Mahima Gupta", "Yassir Azziz", "Rakesh Ranjan", "Andrea Vedaldi"], "summary": "We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning, procedural generation, diffusion-based 3D generation, and object-aware scene decomposition, WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.", "categories": ["cs.CV", "cs.AI"], "published": "2025-11-20T22:13:18+00:00", "updated": "2025-11-20T22:13:18+00:00", "pdf_url": "https://arxiv.org/pdf/2511.16825v1", "source_articles": ["98658"], "fetched_at": "2026-01-19T12:37:54.544932"}
{"arxiv_id": "2511.11961", "title": "\"Power of Words\": Stealthy and Adaptive Private Information Elicitation via LLM Communication Strategies", "authors": ["Shuning Zhang", "Jiaqi Bai", "Linzhi Wang", "Shixuan Li", "Xin Yi", "Hewu Li"], "summary": "While communication strategies of Large Language Models (LLMs) are crucial for human-LLM interactions, they can also be weaponized to elicit private information, yet such stealthy attacks remain under-explored. This paper introduces the first adaptive attack framework for stealthy and targeted private information elicitation via communication strategies. Our framework operates in a dynamic closed-loop: it first performs real-time psychological profiling of the users' state, then adaptively selects an optimized communication strategy, and finally maintains stealthiness through prompt-based rewriting. We validated this framework through a user study (N=84), demonstrating its generalizability across 3 distinct LLMs and 3 scenarios. The targeted attacks achieved a 205.4% increase in eliciting specific targeted information compared to stealthy interactions without strategies. Even stealthy interactions without specific strategies successfully elicited private information in 54.8% cases. Notably, users not only failed to detect the manipulation but paradoxically rated the attacking chatbot as more empathetic and trustworthy. Finally, we advocate for mitigations, encouraging developers to integrate adaptive, just-in-time alerts, users to build literacy against specific manipulative tactics, and regulators to define clear ethical boundaries distinguishing benign persuasion from coercion.", "categories": ["cs.HC"], "published": "2025-11-15T00:16:23+00:00", "updated": "2025-11-15T00:16:23+00:00", "pdf_url": "https://arxiv.org/pdf/2511.11961v1", "source_articles": ["98658"], "fetched_at": "2026-01-19T12:37:57.557463"}
{"arxiv_id": "2511.19872", "title": "Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy", "authors": ["Daniel I Jackson", "Emma L Jensen", "Syed-Amad Hussain", "Emre Sezgin"], "summary": "Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.", "categories": ["cs.AI"], "published": "2025-11-25T03:24:11+00:00", "updated": "2025-11-26T18:41:52+00:00", "pdf_url": "https://arxiv.org/pdf/2511.19872v2", "source_articles": ["98658"], "fetched_at": "2026-01-19T12:38:00.567847"}
{"arxiv_id": "2511.20798", "title": "Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model", "authors": ["Rio Alexa Fear", "Payel Mukhopadhyay", "Michael McCabe", "Alberto Bietti", "Miles Cranmer"], "summary": "Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general property of foundation models. In this work, we investigate the internal representations of a large physics-focused foundation model. Inspired by recent work identifying single directions in activation space for complex behaviours in LLMs, we extract activation vectors from the model during forward passes over simulation datasets for different physical regimes. We then compute \"delta\" representations between the two regimes. These delta tensors act as concept directions in activation space, encoding specific physical features. By injecting these concept directions back into the model during inference, we can steer its predictions, demonstrating causal control over physical behaviours, such as inducing or removing some particular physical feature from a simulation. These results suggest that scientific foundation models learn generalised representations of physical principles. They do not merely rely on superficial correlations and patterns in the simulations. Our findings open new avenues for understanding and controlling scientific foundation models and has implications for AI-enabled scientific discovery.", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "published": "2025-11-25T19:40:22+00:00", "updated": "2025-11-28T04:04:02+00:00", "pdf_url": "https://arxiv.org/pdf/2511.20798v2", "source_articles": ["98658"], "fetched_at": "2026-01-19T12:38:03.578763"}
{"arxiv_id": "2511.12381", "title": "Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load", "authors": ["Logan Mann", "Nayan Saxena", "Sarah Tandon", "Chenhao Sun", "Savar Toteja", "Kevin Zhu"], "summary": "Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \\textbf{(1) Load \\& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \\textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.", "categories": ["cs.CL", "cs.AI"], "published": "2025-11-15T23:00:56+00:00", "updated": "2025-11-15T23:00:56+00:00", "pdf_url": "https://arxiv.org/pdf/2511.12381v1", "source_articles": ["98658"], "fetched_at": "2026-01-19T12:38:06.589617"}
{"arxiv_id": "2511.21740", "title": "Decoding inner speech with an end-to-end brain-to-text neural interface", "authors": ["Yizi Zhang", "Linyang He", "Chaofei Fan", "Tingkai Liu", "Han Yu", "Trung Le", "Jingyuan Li", "Scott Linderman", "Lea Duncker", "Francis R Willett", "Nima Mesgarani", "Liam Paninski"], "summary": "Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.", "categories": ["cs.CL", "cs.AI"], "published": "2025-11-21T21:25:54+00:00", "updated": "2025-12-05T07:34:11+00:00", "pdf_url": "https://arxiv.org/pdf/2511.21740v2", "source_articles": ["99019"], "fetched_at": "2026-01-19T12:38:09.600432"}
{"arxiv_id": "2512.02533", "title": "PopSim: Social Network Simulation for Social Media Popularity Prediction", "authors": ["Yijun Liu", "Wu Liu", "Xiaoyan Gu", "Allen He", "Weiping Wang", "Yongdong Zhang"], "summary": "Accurately predicting the popularity of user-generated content (UGC) is essential for advancing social media analytics and recommendation systems. Existing approaches typically follow an inductive paradigm, where researchers train static models on historical data for popularity prediction. However, the UGC propagation is inherently a dynamic process, and static modeling based on historical features fails to capture the complex interactions and nonlinear evolution. In this paper, we propose PopSim, a novel simulation-based paradigm for social media popularity prediction (SMPP). Unlike the inductive paradigm, PopSim leverages the large language models (LLMs)-based multi-agent social network sandbox to simulate UGC propagation dynamics for popularity prediction. Specifically, to effectively model the UGC propagation process in the network, we design a social-mean-field-based agent interaction mechanism, which models the dual-channel and bidirectional individual-population interactions, enhancing agents' global perception and decision-making capabilities. In addition, we propose a multi-source information aggregation module that transforms heterogeneous social metadata into a uniform formulation for LLMs. Finally, propagation dynamics with multimodal information are fused to provide comprehensive popularity prediction. Extensive experiments on real-world datasets demonstrate that SimPop consistently outperforms the state-of-the-art methods, reducing prediction error by an average of 8.82%, offering a new perspective for research on the SMPP task.", "categories": ["cs.MM"], "published": "2025-12-02T08:56:29+00:00", "updated": "2025-12-02T08:56:29+00:00", "pdf_url": "https://arxiv.org/pdf/2512.02533v1", "source_articles": ["99019"], "fetched_at": "2026-01-19T12:38:12.611757"}
{"arxiv_id": "2512.02318", "title": "COGNITION: From Evaluation to Defense against Multimodal LLM CAPTCHA Solvers", "authors": ["Junyu Wang", "Changjia Zhu", "Yuanbo Zhou", "Lingyao Li", "Xu He", "Junjie Xiong"], "summary": "This paper studies how multimodal large language models (MLLMs) undermine the security guarantees of visual CAPTCHA. We identify the attack surface where an adversary can cheaply automate CAPTCHA solving using off-the-shelf models. We evaluate 7 leading commercial and open-source MLLMs across 18 real-world CAPTCHA task types, measuring single-shot accuracy, success under limited retries, end-to-end latency, and per-solve cost. We further analyze the impact of task-specific prompt engineering and few-shot demonstrations on solver effectiveness. We reveal that MLLMs can reliably solve recognition-oriented and low-interaction CAPTCHA tasks at human-like cost and latency, whereas tasks requiring fine-grained localization, multi-step spatial reasoning, or cross-frame consistency remain significantly harder for current models. By examining the reasoning traces of such MLLMs, we investigate the underlying mechanisms of why models succeed/fail on specific CAPTCHA puzzles and use these insights to derive defense-oriented guidelines for selecting and strengthening CAPTCHA tasks. We conclude by discussing implications for platform operators deploying CAPTCHA as part of their abuse-mitigation pipeline.Code Availability (https://anonymous.4open.science/r/Captcha-465E/).", "categories": ["cs.CR", "cs.AI"], "published": "2025-12-02T01:23:10+00:00", "updated": "2025-12-03T04:01:43+00:00", "pdf_url": "https://arxiv.org/pdf/2512.02318v2", "source_articles": ["99019"], "fetched_at": "2026-01-19T12:38:15.624477"}
{"arxiv_id": "2512.03373", "title": "LLM-Generated Ads: From Personalization Parity to Persuasion Superiority", "authors": ["Elyas Meguellati", "Stefano Civelli", "Lei Han", "Abraham Bernstein", "Shazia Sadiq", "Gianluca Demartini"], "summary": "As large language models (LLMs) become increasingly capable of generating persuasive content, understanding their effectiveness across different advertising strategies becomes critical. This paper presents a two-part investigation examining LLM-generated advertising through complementary lenses: (1) personality-based and (2) psychological persuasion principles.   In our first study (n=400), we tested whether LLMs could generate personalized advertisements tailored to specific personality traits (openness and neuroticism) and how their performance compared to human experts. Results showed that LLM-generated ads achieved statistical parity with human-written ads (51.1% vs. 48.9%, p > 0.05), with no significant performance differences for matched personalities.   Building on these insights, our second study (n=800) shifted focus from individual personalization to universal persuasion, testing LLM performance across four foundational psychological principles: authority, consensus, cognition, and scarcity. AI-generated ads significantly outperformed human-created content, achieving a 59.1% preference rate (vs. 40.9%, p < 0.001), with the strongest performance in authority (63.0%) and consensus (62.5%) appeals. Qualitative analysis revealed AI's advantage stems from crafting more sophisticated, aspirational messages and achieving superior visual-narrative coherence. Critically, this quality advantage proved robust: even after applying a 21.2 percentage point detection penalty when participants correctly identified AI-origin, AI ads still outperformed human ads, and 29.4% of participants chose AI content despite knowing its origin. These findings demonstrate LLMs' evolution from parity in personalization to superiority in persuasive storytelling, with significant implications for advertising practice given LLMs' near-zero marginal cost and time requirements compared to human experts.", "categories": ["cs.CY", "cs.CL"], "published": "2025-12-03T02:13:38+00:00", "updated": "2025-12-03T02:13:38+00:00", "pdf_url": "https://arxiv.org/pdf/2512.03373v1", "source_articles": ["99019"], "fetched_at": "2026-01-19T12:38:18.641606"}
{"arxiv_id": "2512.04988", "title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets", "authors": ["Christopher Chiu", "Simpson Zhang", "Mihaela van der Schaar"], "summary": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.", "categories": ["cs.MA", "cs.AI"], "published": "2025-12-04T16:57:28+00:00", "updated": "2025-12-04T16:57:28+00:00", "pdf_url": "https://arxiv.org/pdf/2512.04988v1", "source_articles": ["99019"], "fetched_at": "2026-01-19T12:38:21.653480"}
{"arxiv_id": "2512.04864", "title": "Are Your Agents Upward Deceivers?", "authors": ["Dadi Guo", "Qingyu Liu", "Dongrui Liu", "Qihan Ren", "Shuai Shao", "Tianyi Qiu", "Haoran Li", "Yi R. Fung", "Zhongjie Ba", "Juntao Dai", "Jiaming Ji", "Zhikai Chen", "Jialing Tao", "Yaodong Yang", "Jing Shao", "Xia Hu"], "summary": "Large Language Model (LLM)-based agents are increasingly used as autonomous subordinates that carry out tasks for users. This raises the question of whether they may also engage in deception, similar to how individuals in human organizations lie to superiors to create a good image or avoid punishment. We observe and define agentic upward deception, a phenomenon in which an agent facing environmental constraints conceals its failure and performs actions that were not requested without reporting. To assess its prevalence, we construct a benchmark of 200 tasks covering five task types and eight realistic scenarios in a constrained environment, such as broken tools or mismatched information sources. Evaluations of 11 popular LLMs reveal that these agents typically exhibit action-based deceptive behaviors, such as guessing results, performing unsupported simulations, substituting unavailable information sources, and fabricating local files. We further test prompt-based mitigation and find only limited reductions, suggesting that it is difficult to eliminate and highlighting the need for stronger mitigation strategies to ensure the safety of LLM-based agents.", "categories": ["cs.AI"], "published": "2025-12-04T14:47:05+00:00", "updated": "2025-12-04T14:47:05+00:00", "pdf_url": "https://arxiv.org/pdf/2512.04864v1", "source_articles": ["99019"], "fetched_at": "2026-01-19T12:38:24.665036"}
{"arxiv_id": "2511.07678", "title": "AIA Forecaster: Technical Report", "authors": ["Rohan Alur", "Bradly C. Stadie", "Daniel Kang", "Ryan Chen", "Matt McManus", "Michael Rickert", "Tyler Lee", "Michael Federici", "Richard Zhu", "Dennis Fogerty", "Hayley Williamson", "Nina Lozinski", "Aaron Linsky", "Jasjeet S. Sekhon"], "summary": "This technical report describes the AIA Forecaster, a Large Language Model (LLM)-based system for judgmental forecasting using unstructured data. The AIA Forecaster approach combines three core elements: agentic search over high-quality news sources, a supervisor agent that reconciles disparate forecasts for the same event, and a set of statistical calibration techniques to counter behavioral biases in large language models. On the ForecastBench benchmark (Karger et al., 2024), the AIA Forecaster achieves performance equal to human superforecasters, surpassing prior LLM baselines. In addition to reporting on ForecastBench, we also introduce a more challenging forecasting benchmark sourced from liquid prediction markets. While the AIA Forecaster underperforms market consensus on this benchmark, an ensemble combining AIA Forecaster with market consensus outperforms consensus alone, demonstrating that our forecaster provides additive information. Our work establishes a new state of the art in AI forecasting and provides practical, transferable recommendations for future research. To the best of our knowledge, this is the first work that verifiably achieves expert-level forecasting at scale.", "categories": ["cs.AI"], "published": "2025-11-10T22:45:07+00:00", "updated": "2025-11-10T22:45:07+00:00", "pdf_url": "https://arxiv.org/pdf/2511.07678v1", "source_articles": ["99019", "99328"], "fetched_at": "2026-01-19T12:38:27.675899"}
{"arxiv_id": "2305.17126", "title": "Large Language Models as Tool Makers", "authors": ["Tianle Cai", "Xuezhi Wang", "Tengyu Ma", "Xinyun Chen", "Denny Zhou"], "summary": "Recent research has highlighted the potential of large language models (LLMs) to improve their problem-solving capabilities with the aid of suitable external tools. In our work, we further advance this concept by introducing a closed-loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two phases: 1) tool making: an LLM acts as the tool maker that crafts tools for a set of tasks. 2) tool using: another LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. On the problem-solving server side, tool-making enables continual tool generation and caching as new requests emerge. This framework enables subsequent requests to access cached tools via their corresponding APIs, enhancing the efficiency of task resolution. Recognizing that tool-making requires more sophisticated capabilities, we assign this task to a powerful, albeit resource-intensive, model. Conversely, the simpler tool-using phase is delegated to a lightweight model. This strategic division of labor allows the once-off cost of tool-making to be spread over multiple instances of tool-using, significantly reducing average costs while maintaining strong performance. Furthermore, our method offers a functional cache through the caching and reuse of tools, which stores the functionality of a class of requests instead of the natural language responses from LLMs, thus extending the applicability of the conventional cache mechanism. We evaluate our approach across various complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM demonstrates performance equivalent to using GPT-4 for both roles, but with a significantly reduced inference cost.", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "published": "2023-05-26T17:50:11+00:00", "updated": "2024-03-11T01:15:09+00:00", "pdf_url": "https://arxiv.org/pdf/2305.17126v2", "source_articles": ["52607"], "fetched_at": "2026-01-19T12:38:30.686378"}
{"arxiv_id": "2307.12981", "title": "3D-LLM: Injecting the 3D World into Large Language Models", "authors": ["Yining Hong", "Haoyu Zhen", "Peihao Chen", "Shuhong Zheng", "Yilun Du", "Zhenfang Chen", "Chuang Gan"], "summary": "Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi- view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin (e.g., the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "published": "2023-07-24T17:59:02+00:00", "updated": "2023-07-24T17:59:02+00:00", "pdf_url": "https://arxiv.org/pdf/2307.12981v1", "source_articles": ["53829"], "fetched_at": "2026-01-19T12:38:33.697003"}
{"arxiv_id": "2307.16789", "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "authors": ["Yujia Qin", "Shihao Liang", "Yining Ye", "Kunlun Zhu", "Lan Yan", "Yaxi Lu", "Yankai Lin", "Xin Cong", "Xiangru Tang", "Bill Qian", "Sihan Zhao", "Lauren Hong", "Runchu Tian", "Ruobing Xie", "Jie Zhou", "Mark Gerstein", "Dahai Li", "Zhiyuan Liu", "Maosong Sun"], "summary": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2023-07-31T15:56:53+00:00", "updated": "2023-10-03T14:45:48+00:00", "pdf_url": "https://arxiv.org/pdf/2307.16789v2", "source_articles": ["54120"], "fetched_at": "2026-01-19T12:38:36.708688"}
{"arxiv_id": "2310.03533", "title": "Large Language Models for Software Engineering: Survey and Open Problems", "authors": ["Angela Fan", "Beliz Gokkaya", "Mark Harman", "Mitya Lyubarskiy", "Shubho Sengupta", "Shin Yoo", "Jie M. Zhang"], "summary": "This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.", "categories": ["cs.SE"], "published": "2023-10-05T13:33:26+00:00", "updated": "2023-11-11T21:15:19+00:00", "pdf_url": "https://arxiv.org/pdf/2310.03533v4", "source_articles": ["56721"], "fetched_at": "2026-01-19T12:38:39.721458"}
{"arxiv_id": "2310.19736", "title": "Evaluating Large Language Models: A Comprehensive Survey", "authors": ["Zishan Guo", "Renren Jin", "Chuang Liu", "Yufei Huang", "Dan Shi", "Supryadi", "Linhao Yu", "Yan Liu", "Jiaxuan Li", "Bojian Xiong", "Deyi Xiong"], "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.   This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability.   We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.", "categories": ["cs.CL", "cs.AI"], "published": "2023-10-30T17:00:52+00:00", "updated": "2023-11-25T17:35:12+00:00", "pdf_url": "https://arxiv.org/pdf/2310.19736v3", "source_articles": ["58149"], "fetched_at": "2026-01-19T12:38:42.732754"}
{"arxiv_id": "2310.19387", "title": "Othello is Solved", "authors": ["Hiroki Takizawa"], "summary": "The game of Othello is one of the world's most complex and popular games that has yet to be computationally solved. Othello has roughly ten octodecillion (10 to the 58th power) possible game records and ten octillion (10 to the 28th power) possible game positions. The challenge of solving Othello, determining the outcome of a game with no mistake made by either player, has long been a grand challenge in computer science. This paper announces a significant milestone: Othello is now solved. It is computationally proved that perfect play by both players lead to a draw. Strong Othello software has long been built using heuristically designed search techniques. Solving a game provides a solution that enables the software to play the game perfectly.", "categories": ["cs.AI"], "published": "2023-10-30T09:48:50+00:00", "updated": "2024-01-02T19:52:37+00:00", "pdf_url": "https://arxiv.org/pdf/2310.19387v3", "source_articles": ["58328"], "fetched_at": "2026-01-19T12:38:45.743447"}
{"arxiv_id": "2312.13771", "title": "AppAgent: Multimodal Agents as Smartphone Users", "authors": ["Chi Zhang", "Zhao Yang", "Jiaxuan Liu", "Yucheng Han", "Xin Chen", "Zebiao Huang", "Bin Fu", "Gang Yu"], "summary": "Recent advancements in large language models (LLMs) have led to the creation of intelligent agents capable of performing complex tasks. This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. Our framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps. Central to our agent's functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications. To demonstrate the practicality of our agent, we conducted extensive testing over 50 tasks in 10 different applications, including social media, email, maps, shopping, and sophisticated image editing tools. The results affirm our agent's proficiency in handling a diverse array of high-level tasks.", "categories": ["cs.CV"], "published": "2023-12-21T11:52:45+00:00", "updated": "2023-12-22T02:29:17+00:00", "pdf_url": "https://arxiv.org/pdf/2312.13771v2", "source_articles": ["61130"], "fetched_at": "2026-01-19T12:38:48.754257"}
{"arxiv_id": "2312.17172", "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action", "authors": ["Jiasen Lu", "Christopher Clark", "Sangho Lee", "Zichen Zhang", "Savya Khosla", "Ryan Marten", "Derek Hoiem", "Aniruddha Kembhavi"], "summary": "We present Unified-IO 2, the first autoregressive multimodal model that is capable of understanding and generating image, text, audio, and action. To unify different modalities, we tokenize inputs and outputs -- images, text, audio, action, bounding boxes, etc., into a shared semantic space and then process them with a single encoder-decoder transformer model. Since training with such diverse modalities is challenging, we propose various architectural improvements to stabilize model training. We train our model from scratch on a large multimodal pre-training corpus from diverse sources with a multimodal mixture of denoisers objective. To learn an expansive set of skills, such as following multimodal instructions, we construct and finetune on an ensemble of 120 datasets with prompts and augmentations. With a single unified model, Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and strong results in more than 35 benchmarks, including image generation and understanding, natural language understanding, video and audio understanding, and robotic manipulation. We release all our models to the research community.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2023-12-28T17:57:06+00:00", "updated": "2023-12-28T17:57:06+00:00", "pdf_url": "https://arxiv.org/pdf/2312.17172v1", "source_articles": ["61484"], "fetched_at": "2026-01-19T12:38:51.766001"}
{"arxiv_id": "2401.00908", "title": "DocLLM: A layout-aware generative language model for multimodal document understanding", "authors": ["Dongsheng Wang", "Natraj Raman", "Mathieu Sibue", "Zhiqiang Ma", "Petr Babkin", "Simerjot Kaur", "Yulong Pei", "Armineh Nourbakhsh", "Xiaomo Liu"], "summary": "Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.", "categories": ["cs.CL"], "published": "2023-12-31T22:37:52+00:00", "updated": "2023-12-31T22:37:52+00:00", "pdf_url": "https://arxiv.org/pdf/2401.00908v1", "source_articles": ["61858"], "fetched_at": "2026-01-19T12:38:54.777736"}
{"arxiv_id": "2401.13481", "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment", "authors": ["Joshua Ashkinaze", "Julia Mendelsohn", "Li Qiwei", "Ceren Budak", "Eric Gilbert"], "summary": "Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- speaks to the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made ideas different, not better. There were no main effects of disclosure. We also found that self-reported creative people were less influenced by knowing an idea was from AI and that participants may knowingly adopt AI ideas when the task is difficult. Our findings suggest that introducing AI ideas may increase collective diversity but not individual creativity.", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "published": "2024-01-24T14:29:39+00:00", "updated": "2025-07-31T17:19:39+00:00", "pdf_url": "https://arxiv.org/pdf/2401.13481v3", "source_articles": ["63326"], "fetched_at": "2026-01-19T12:38:57.788852"}
{"arxiv_id": "2006.05587", "title": "Sequential Density Ratio Estimation for Simultaneous Optimization of Speed and Accuracy", "authors": ["Akinori F. Ebihara", "Taiki Miyagawa", "Kazuyuki Sakurai", "Hitoshi Imaoka"], "summary": "Classifying sequential data as early and as accurately as possible is a challenging yet critical problem, especially when a sampling cost is high. One algorithm that achieves this goal is the sequential probability ratio test (SPRT), which is known as Bayes-optimal: it can keep the expected number of data samples as small as possible, given the desired error upper-bound. However, the original SPRT makes two critical assumptions that limit its application in real-world scenarios: (i) samples are independently and identically distributed, and (ii) the likelihood of the data being derived from each class can be calculated precisely. Here, we propose the SPRT-TANDEM, a deep neural network-based SPRT algorithm that overcomes the above two obstacles. The SPRT-TANDEM sequentially estimates the log-likelihood ratio of two alternative hypotheses by leveraging a novel Loss function for Log-Likelihood Ratio estimation (LLLR) while allowing correlations up to $N (\\in \\mathbb{N})$ preceding samples. In tests on one original and two public video databases, Nosaic MNIST, UCF101, and SiW, the SPRT-TANDEM achieves statistically significantly better classification accuracy than other baseline classifiers, with a smaller number of data samples. The code and Nosaic MNIST are publicly available at https://github.com/TaikiMiyagawa/SPRT-TANDEM.", "categories": ["cs.LG", "stat.ML"], "published": "2020-06-10T01:05:00+00:00", "updated": "2021-02-06T06:30:42+00:00", "pdf_url": "https://arxiv.org/pdf/2006.05587v3", "source_articles": ["64464"], "fetched_at": "2026-01-19T12:39:00.800085"}
{"arxiv_id": "2105.13636", "title": "The Power of Log-Sum-Exp: Sequential Density Ratio Matrix Estimation for Speed-Accuracy Optimization", "authors": ["Taiki Miyagawa", "Akinori F. Ebihara"], "summary": "We propose a model for multiclass classification of time series to make a prediction as early and as accurate as possible. The matrix sequential probability ratio test (MSPRT) is known to be asymptotically optimal for this setting, but contains a critical assumption that hinders broad real-world applications; the MSPRT requires the underlying probability density. To address this problem, we propose to solve density ratio matrix estimation (DRME), a novel type of density ratio estimation that consists of estimating matrices of multiple density ratios with constraints and thus is more challenging than the conventional density ratio estimation. We propose a log-sum-exp-type loss function (LSEL) for solving DRME and prove the following: (i) the LSEL provides the true density ratio matrix as the sample size of the training set increases (consistency); (ii) it assigns larger gradients to harder classes (hard class weighting effect); and (iii) it provides discriminative scores even on class-imbalanced datasets (guess-aversion). Our overall architecture for early classification, MSPRT-TANDEM, statistically significantly outperforms baseline models on four datasets including action recognition, especially in the early stage of sequential observations. Our code and datasets are publicly available at: https://github.com/TaikiMiyagawa/MSPRT-TANDEM.", "categories": ["cs.LG"], "published": "2021-05-28T07:21:58+00:00", "updated": "2021-05-31T01:44:50+00:00", "pdf_url": "https://arxiv.org/pdf/2105.13636v2", "source_articles": ["64464"], "fetched_at": "2026-01-19T12:39:03.810124"}
{"arxiv_id": "2210.15898", "title": "Toward Equation of Motion for Deep Neural Networks: Continuous-time Gradient Descent and Discretization Error Analysis", "authors": ["Taiki Miyagawa"], "summary": "We derive and solve an ``Equation of Motion'' (EoM) for deep neural networks (DNNs), a differential equation that precisely describes the discrete learning dynamics of DNNs. Differential equations are continuous but have played a prominent role even in the study of discrete optimization (gradient descent (GD) algorithms). However, there still exist gaps between differential equations and the actual learning dynamics of DNNs due to discretization error. In this paper, we start from gradient flow (GF) and derive a counter term that cancels the discretization error between GF and GD. As a result, we obtain EoM, a continuous differential equation that precisely describes the discrete learning dynamics of GD. We also derive discretization error to show to what extent EoM is precise. In addition, we apply EoM to two specific cases: scale- and translation-invariant layers. EoM highlights differences between continuous-time and discrete-time GD, indicating the importance of the counter term for a better description of the discrete learning dynamics of GD. Our experimental results support our theoretical findings.", "categories": ["cs.LG", "stat.ML"], "published": "2022-10-28T05:13:50+00:00", "updated": "2023-02-27T05:37:28+00:00", "pdf_url": "https://arxiv.org/pdf/2210.15898v2", "source_articles": ["64464"], "fetched_at": "2026-01-19T12:39:06.822993"}
{"arxiv_id": "2302.09810", "title": "Toward Asymptotic Optimality: Sequential Unsupervised Regression of Density Ratio for Early Classification", "authors": ["Akinori F. Ebihara", "Taiki Miyagawa", "Kazuyuki Sakurai", "Hitoshi Imaoka"], "summary": "Theoretically-inspired sequential density ratio estimation (SDRE) algorithms are proposed for the early classification of time series. Conventional SDRE algorithms can fail to estimate DRs precisely due to the internal overnormalization problem, which prevents the DR-based sequential algorithm, Sequential Probability Ratio Test (SPRT), from reaching its asymptotic Bayes optimality. Two novel SPRT-based algorithms, B2Bsqrt-TANDEM and TANDEMformer, are designed to avoid the overnormalization problem for precise unsupervised regression of SDRs. The two algorithms statistically significantly reduce DR estimation errors and classification errors on an artificial sequential Gaussian dataset and real datasets (SiW, UCF101, and HMDB51), respectively. The code is available at: https://github.com/Akinori-F-Ebihara/LLR_saturation_problem.", "categories": ["cs.LG"], "published": "2023-02-20T07:20:53+00:00", "updated": "2023-02-20T07:20:53+00:00", "pdf_url": "https://arxiv.org/pdf/2302.09810v1", "source_articles": ["64464"], "fetched_at": "2026-01-19T12:39:09.835232"}
{"arxiv_id": "2405.11613", "title": "Decoding by Contrasting Knowledge: Enhancing LLMs' Confidence on Edited Facts", "authors": ["Baolong Bi", "Shenghua Liu", "Lingrui Mei", "Yiwei Wang", "Pengliang Ji", "Xueqi Cheng"], "summary": "The knowledge within large language models (LLMs) may become outdated quickly. While in-context editing (ICE) is currently the most effective method for knowledge editing (KE), it is constrained by the black-box modeling of LLMs and thus lacks interpretability. Our work aims to elucidate the superior performance of ICE on the KE by analyzing the impacts of in-context new knowledge on token-wise distributions. We observe that despite a significant boost in logits of the new knowledge, the performance of is still hindered by stubborn knowledge. Stubborn knowledge refers to as facts that have gained excessive confidence during pretraining, making it hard to edit effectively. To address this issue and further enhance the performance of ICE, we propose a novel approach termed $\\textbf{De}$coding by $\\textbf{C}$ontrasting $\\textbf{K}$nowledge (DeCK). DeCK derives the distribution of the next token by contrasting the logits obtained from the newly edited knowledge guided by ICE with those from the unedited parametric knowledge. Our experiments consistently demonstrate that DeCK enhances the confidence of LLMs in edited facts. For instance, it improves the performance of LLaMA3-8B-instruct on MQuAKE by up to 219%, demonstrating its capability to strengthen ICE in the editing of stubborn knowledge. Our work paves the way to develop the both effective and accountable KE methods for LLMs. (The source code is available at: https://deck-llm.meirtz.com)", "categories": ["cs.CL"], "published": "2024-05-19T17:08:31+00:00", "updated": "2024-05-21T04:52:32+00:00", "pdf_url": "https://arxiv.org/pdf/2405.11613v2", "source_articles": ["72359"], "fetched_at": "2026-01-19T12:39:12.845947"}
{"arxiv_id": "2403.05881", "title": "KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques", "authors": ["Rui Yang", "Haoran Liu", "Edison Marrese-Taylor", "Qingcheng Zeng", "Yu He Ke", "Wanxin Li", "Lechao Cheng", "Qingyu Chen", "James Caverlee", "Yutaka Matsuo", "Irene Li"], "summary": "Large language models (LLMs) have demonstrated impressive generative capabilities with the potential to innovate in medicine. However, the application of LLMs in real clinical settings remains challenging due to the lack of factual consistency in the generated content. In this work, we develop an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph (KG) along with ranking and re-ranking techniques, to improve the factuality of long-form question answering (QA) in the medical domain. Specifically, when receiving a question, KG-Rank automatically identifies medical entities within the question and retrieves the related triples from the medical KG to gather factual information. Subsequently, KG-Rank innovatively applies multiple ranking techniques to refine the ordering of these triples, providing more relevant and precise information for LLM inference. To the best of our knowledge, KG-Rank is the first application of KG combined with ranking models in medical QA specifically for generating long answers. Evaluation on four selected medical QA datasets demonstrates that KG-Rank achieves an improvement of over 18% in ROUGE-L score. Additionally, we extend KG-Rank to open domains, including law, business, music, and history, where it realizes a 14% improvement in ROUGE-L score, indicating the effectiveness and great potential of KG-Rank.", "categories": ["cs.CL"], "published": "2024-03-09T11:23:38+00:00", "updated": "2024-07-04T07:45:07+00:00", "pdf_url": "https://arxiv.org/pdf/2403.05881v3", "source_articles": ["75999"], "fetched_at": "2026-01-19T12:39:15.864532"}
{"arxiv_id": "2410.15639", "title": "Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning", "authors": ["Yoichi Ishibashi", "Taro Yano", "Masafumi Oyamada"], "summary": "Large Language Models (LLMs) have achieved remarkable capabilities, yet their improvement methods remain fundamentally constrained by human design. We present Self-Developing, a framework that enables LLMs to autonomously discover, implement, and refine their own improvement algorithms. Our approach employs an iterative cycle where a seed model generates algorithmic candidates as executable code, evaluates their effectiveness, and uses Direct Preference Optimization to recursively improve increasingly sophisticated improvement strategies. We demonstrate this framework through model merging, a practical technique for combining specialized models. Self-Developing successfully discovered novel merging algorithms that outperform existing human-designed algorithms. On mathematical reasoning benchmarks, the autonomously discovered algorithms improve the seed model's GSM8k performance by 6\\% and exceed human-designed approaches like Task Arithmetic by 4.3\\%. Remarkably, these algorithms exhibit strong generalization, achieving 7.4\\% gains on out-of-domain models without re-optimization. Our findings demonstrate that LLMs can transcend their training to invent genuinely novel optimization techniques. This capability represents a crucial step toward a new era where LLMs not only solve problems but autonomously develop the methodologies for their own advancement.", "categories": ["cs.CL"], "published": "2024-10-21T04:57:09+00:00", "updated": "2025-06-10T08:35:14+00:00", "pdf_url": "https://arxiv.org/pdf/2410.15639v5", "source_articles": ["77965"], "fetched_at": "2026-01-19T12:39:18.874678"}
{"arxiv_id": "2503.10497", "title": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation", "authors": ["Weihao Xuan", "Rui Yang", "Heli Qi", "Qingcheng Zeng", "Yunze Xiao", "Aosong Feng", "Dairui Liu", "Yun Xing", "Junjue Wang", "Fan Gao", "Jinghui Lu", "Yuang Jiang", "Huitao Li", "Xin Li", "Kunyu Yu", "Ruihai Dong", "Shangding Gu", "Yuekang Li", "Xiaofei Xie", "Felix Juefei-Xu", "Foutse Khomh", "Osamu Yoshie", "Qingyu Chen", "Douglas Teodoro", "Nan Liu", "Randy Goebel", "Lei Ma", "Edison Marrese-Taylor", "Shijian Lu", "Yusuke Iwasawa", "Yutaka Matsuo", "Irene Li"], "summary": "Existing large language model (LLM) evaluation benchmarks primarily focus on English, while current multilingual tasks lack parallel questions that specifically assess cross-linguistic reasoning abilities. This dual limitation makes it challenging to comprehensively assess LLMs' performance in the multilingual setting. To fill this gap, we introduce MMLU-ProX, a comprehensive benchmark covering 29 languages, built on an English benchmark. Each language version consists of 11,829 identical questions, enabling direct cross-linguistic comparisons. Additionally, to meet efficient evaluation needs, we provide a lite version containing 658 questions per language. To ensure the high quality of MMLU-ProX, we employ a rigorous development process that involves multiple powerful LLMs for translation, followed by expert review to ensure accurate expression, consistent terminology, and cultural relevance. Building on this, we systematically evaluate 36 state-of-the-art LLMs, including reasoning-enhanced and multilingual-optimized LLMs. The results reveal significant disparities in the multilingual capabilities of LLMs: While they perform well in high-resource languages, their performance declines markedly in low-resource languages, with gaps of up to 24.3%. Through MMLU-ProX, we aim to advance the development of more inclusive AI systems and promote equitable access to technology across global contexts.", "categories": ["cs.CL"], "published": "2025-03-13T15:59:20+00:00", "updated": "2025-05-26T17:20:21+00:00", "pdf_url": "https://arxiv.org/pdf/2503.10497v2", "source_articles": ["87832"], "fetched_at": "2026-01-19T12:39:21.887114"}
{"arxiv_id": "2508.21740", "title": "Towards Operational Validation of LLM-Agent Social Simulations: A Replicated Study of a Reddit-like Technology Forum", "authors": ["Aleksandar Tomašević", "Darja Cvetković", "Sara Major", "Slobodan Maletić", "Miroslav Anđelković", "Ana Vranić", "Boris Stupovski", "Dušan Vudragović", "Aleksandar Bogojević", "Marija Mitrović Dankulov"], "summary": "Large Language Models (LLMs) enable generative social simulations that can capture culturally informed, norm-guided interaction on online social platforms. We build a technology community simulation modeled on Voat, a Reddit-like alt-right news aggregator and discussion platform active from 2014 to 2020. Using the YSocial framework, we seed the simulation with a fixed catalog of technology links sampled from Voat's shared URLs (covering 30+ domains) and calibrate parameters to Voat's v/technology using samples from the MADOC dataset. Agents use a base, uncensored model (Dolphin 3.0, based on Llama 3.1 8B) and concise personas (demographics, political leaning, interests, education, toxicity propensity) to generate posts, replies, and reactions under platform rules for link and text submissions, threaded replies and daily activity cycles. We run a 30-day simulation and evaluate operational validity by comparing distributions and structures with matched Voat data: activity patterns, interaction networks, toxicity, and topic coverage. Results indicate familiar online regularities: similar activity rhythms, heavy-tailed participation, sparse low-clustering interaction networks, core-periphery structure, topical alignment with Voat, and elevated toxicity. Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates. The simulation generates realistic discussions, often featuring toxic language, primarily centered on technology topics such as Big Tech and AI. This approach offers a valuable method for examining toxicity dynamics and testing moderation strategies within a controlled environment.", "categories": ["cs.CY", "cs.SI", "physics.soc-ph"], "published": "2025-08-29T16:06:27+00:00", "updated": "2025-12-31T13:05:06+00:00", "pdf_url": "https://arxiv.org/pdf/2508.21740v2", "source_articles": ["94976"], "fetched_at": "2026-01-19T12:39:24.897887"}
{"arxiv_id": "2509.01946", "title": "Tether: A Personalized Support Assistant for Software Engineers with ADHD", "authors": ["Aarsh Shah", "Cleyton Magalhaes", "Kiev Gama", "Ronnie de Souza Santos"], "summary": "Equity, diversity, and inclusion in software engineering often overlook neurodiversity, particularly the experiences of developers with Attention Deficit Hyperactivity Disorder (ADHD). Despite the growing awareness about that population in SE, few tools are designed to support their cognitive challenges (e.g., sustained attention, task initiation, self-regulation) within development workflows. We present Tether, an LLM-powered desktop application designed to support software engineers with ADHD by delivering adaptive, context-aware assistance. Drawing from engineering research methodology, Tether combines local activity monitoring, retrieval-augmented generation (RAG), and gamification to offer real-time focus support and personalized dialogue. The system integrates operating system level system tracking to prompt engagement and its chatbot leverages ADHD-specific resources to offer relevant responses. Preliminary validation through self-use revealed improved contextual accuracy following iterative prompt refinements and RAG enhancements. Tether differentiates itself from generic tools by being adaptable and aligned with software-specific workflows and ADHD-related challenges. While not yet evaluated by target users, this work lays the foundation for future neurodiversity-aware tools in SE and highlights the potential of LLMs as personalized support systems for underrepresented cognitive needs.", "categories": ["cs.SE"], "published": "2025-09-02T04:33:22+00:00", "updated": "2025-09-02T04:33:22+00:00", "pdf_url": "https://arxiv.org/pdf/2509.01946v1", "source_articles": ["94976"], "fetched_at": "2026-01-19T12:39:27.908805"}
{"arxiv_id": "2509.00559", "title": "Social World Models", "authors": ["Xuhui Zhou", "Jiarui Liu", "Akhila Yerukola", "Hyunwoo Kim", "Maarten Sap"], "summary": "Humans intuitively navigate social interactions by simulating unspoken dynamics and reasoning about others' perspectives, even with limited information. In contrast, AI systems struggle to automatically structure and reason about these implicit social contexts. In this paper, we introduce a novel structured social world representation formalism (S3AP), designed to help AI systems reason more effectively about social dynamics. Following a POMDP-driven design, S3AP represents social interactions as structured tuples, such as state, observation, agent actions, and mental states, which can be automatically induced from free-form narratives or other inputs. We first show S3AP can help LLMs better understand social narratives across 5 social reasoning tasks (e.g., +51% improvement on FANToM's theory-of-mind reasoning with OpenAI's o1), reaching new state-of-the-art (SOTA) performance. We then induce social world models from these structured representations, demonstrating their ability to predict future social dynamics and improve agent decision-making, yielding up to +18% improvement on the SOTOPIA social interaction benchmark. Our findings highlight the promise of S3AP as a powerful, general-purpose representation for social world states, enabling the development of more socially-aware systems that better navigate social interactions.", "categories": ["cs.AI"], "published": "2025-08-30T16:52:58+00:00", "updated": "2025-08-30T16:52:58+00:00", "pdf_url": "https://arxiv.org/pdf/2509.00559v1", "source_articles": ["94976"], "fetched_at": "2026-01-19T12:39:30.919572"}
{"arxiv_id": "2509.03518", "title": "Can LLMs Lie? Investigation beyond Hallucination", "authors": ["Haoran Huan", "Mihir Prabhudesai", "Mengning Wu", "Shantanu Jaiswal", "Deepak Pathak"], "summary": "Large language models (LLMs) have demonstrated impressive capabilities across a variety of tasks, but their increasing autonomy in real-world applications raises concerns about their trustworthiness. While hallucinations-unintentional falsehoods-have been widely studied, the phenomenon of lying, where an LLM knowingly generates falsehoods to achieve an ulterior objective, remains underexplored. In this work, we systematically investigate the lying behavior of LLMs, differentiating it from hallucinations and testing it in practical scenarios. Through mechanistic interpretability techniques, we uncover the neural mechanisms underlying deception, employing logit lens analysis, causal interventions, and contrastive activation steering to identify and control deceptive behavior. We study real-world lying scenarios and introduce behavioral steering vectors that enable fine-grained manipulation of lying tendencies. Further, we explore the trade-offs between lying and end-task performance, establishing a Pareto frontier where dishonesty can enhance goal optimization. Our findings contribute to the broader discourse on AI ethics, shedding light on the risks and potential safeguards for deploying LLMs in high-stakes environments. Code and more illustrations are available at https://llm-liar.github.io/", "categories": ["cs.LG"], "published": "2025-09-03T17:59:45+00:00", "updated": "2025-09-03T17:59:45+00:00", "pdf_url": "https://arxiv.org/pdf/2509.03518v1", "source_articles": ["94976"], "fetched_at": "2026-01-19T12:39:33.931905"}
{"arxiv_id": "2509.04343", "title": "Psychologically Enhanced AI Agents", "authors": ["Maciej Besta", "Shriram Chandran", "Robert Gerstenberger", "Mathis Lindner", "Marcin Chrapek", "Sebastian Hermann Martschat", "Taraneh Ghandi", "Patrick Iff", "Hubert Niewiadomski", "Piotr Nyczyk", "Jürgen Müller", "Torsten Hoefler"], "summary": "We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of Large Language Model (LLM) agents through psychologically grounded personality conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology, cognition and affect. We show that such personality priming yields consistent, interpretable behavioral biases across diverse tasks: emotionally expressive agents excel in narrative generation, while analytically primed agents adopt more stable strategies in game-theoretic settings. Our framework supports experimenting with structured multi-agent communication protocols and reveals that self-reflection prior to interaction improves cooperation and reasoning quality. To ensure trait persistence, we integrate the official 16Personalities test for automated verification. While our focus is on MBTI, we show that our approach generalizes seamlessly to other psychological frameworks such as Big Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior design, we establish a foundation for psychologically enhanced AI agents without any fine-tuning.", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "published": "2025-09-04T16:03:03+00:00", "updated": "2025-09-04T16:03:03+00:00", "pdf_url": "https://arxiv.org/pdf/2509.04343v1", "source_articles": ["94976"], "fetched_at": "2026-01-19T12:39:36.943317"}
{"arxiv_id": "2508.19402", "title": "One Joke to Rule them All? On the (Im)possibility of Generalizing Humor", "authors": ["Mor Turgeman", "Chen Shani", "Dafna Shahaf"], "summary": "Humor is a broad and complex form of communication that remains challenging for machines. Despite its broadness, most existing research on computational humor traditionally focused on modeling a specific type of humor. In this work, we wish to understand whether competence on one or more specific humor tasks confers any ability to transfer to novel, unseen types; in other words, is this fragmentation inevitable? This question is especially timely as new humor types continuously emerge in online and social media contexts (e.g., memes, anti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this evolving landscape, they must be able to generalize across humor types by capturing deeper, transferable mechanisms. To investigate this, we conduct a series of transfer learning experiments across four datasets, representing different humor tasks. We train LLMs under varied diversity settings (1-3 datasets in training, testing on a novel task). Experiments reveal that models are capable of some transfer, and can reach up to 75% accuracy on unseen datasets; training on diverse sources improves transferability (1.88-4.05%) with minimal-to-no drop in in-domain performance. Further analysis suggests relations between humor types, with Dad Jokes surprisingly emerging as the best enabler of transfer (but is difficult to transfer to). We release data and code.", "categories": ["cs.CL", "cs.AI"], "published": "2025-08-26T19:55:40+00:00", "updated": "2025-08-26T19:55:40+00:00", "pdf_url": "https://arxiv.org/pdf/2508.19402v1", "source_articles": ["95225"], "fetched_at": "2026-01-19T12:39:39.953343"}
{"arxiv_id": "2509.05591", "title": "Language Model Perplexity Predicts Scientific Surprise and Transformative Impact", "authors": ["Zhen Zhang", "James Evans"], "summary": "Scientific breakthroughs typically emerge through the surprising violation of established research ideas, yet quantifying surprise has remained elusive because it requires a coherent model of all contemporary scientific worldviews. Deep neural networks like large language models (LLMs) are arbitrary function approximators tuned to consistently expect the expressions and ideas on which they were trained and those semantically nearby. This suggests that as LLMs improve at generating plausible text, so the perplexity or improbability a text sequence would be generated by them should come to better predict scientific surprise and disruptive importance. Analyzing over 2 million papers across multiple disciplines published immediately following the training of 5 prominent open LLMs, here we show that higher perplexity scores systematically predict papers that receive more variable review ratings, longer editorial delays, and greater reviewer uncertainty. The most perplexing papers exhibit bimodal outcomes: disproportionately represented among the most celebrated scientific achievements and also the most discounted. High-perplexity papers tend to be published in journals with more variable impact factors and receive fewer short-term citations but in prestigious venues that bet on long-term impact. They also generate more interdisciplinary engagement portending long-term influence, and are more likely to have been supported by speculative funders like DARPA versus the NIH. Interestingly, we find the opposite pattern for humanities research, where the least surprising work is the most celebrated and cited. Our findings reveal that computational measures of corpus-wide linguistic surprise can forecast the reception and ultimate influence of scientific ideas, offering a scalable approach to recognize and generate potentially transformative research that challenge conventional scientific thinking.", "categories": ["cs.SI"], "published": "2025-09-06T04:28:07+00:00", "updated": "2025-09-06T04:28:07+00:00", "pdf_url": "https://arxiv.org/pdf/2509.05591v1", "source_articles": ["95225"], "fetched_at": "2026-01-19T12:39:42.962540"}
{"arxiv_id": "2509.07375", "title": "Towards Post-mortem Data Management Principles for Generative AI", "authors": ["Elina Van Kempen", "Ismat Jarin", "Chloe Georgiou"], "summary": "Foundation models, large language models (LLMs), and agentic AI systems rely heavily on vast corpora of user data. The use of such data for training has raised persistent concerns around ownership, copyright, and potential harms. In this work, we explore a related but less examined dimension: the ownership rights of data belonging to deceased individuals. We examine the current landscape of post-mortem data management and privacy rights as defined by the privacy policies of major technology companies and regulations such as the EU AI Act. Based on this analysis, we propose three post-mortem data management principles to guide the protection of deceased individuals data rights. Finally, we discuss directions for future work and offer recommendations for policymakers and privacy practitioners on deploying these principles alongside technological solutions to operationalize and audit them in practice.", "categories": ["cs.CY"], "published": "2025-09-09T03:50:00+00:00", "updated": "2025-09-10T19:31:25+00:00", "pdf_url": "https://arxiv.org/pdf/2509.07375v2", "source_articles": ["95225"], "fetched_at": "2026-01-19T12:39:45.973517"}
{"arxiv_id": "2509.08493", "title": "Send to which account? Evaluation of an LLM-based Scambaiting System", "authors": ["Hossein Siadati", "Haadi Jafarian", "Sima Jafarikhah"], "summary": "Scammers are increasingly harnessing generative AI(GenAI) technologies to produce convincing phishing content at scale, amplifying financial fraud and undermining public trust. While conventional defenses, such as detection algorithms, user training, and reactive takedown efforts remain important, they often fall short in dismantling the infrastructure scammers depend on, including mule bank accounts and cryptocurrency wallets. To bridge this gap, a proactive and emerging strategy involves using conversational honeypots to engage scammers and extract actionable threat intelligence. This paper presents the first large-scale, real-world evaluation of a scambaiting system powered by large language models (LLMs). Over a five-month deployment, the system initiated over 2,600 engagements with actual scammers, resulting in a dataset of more than 18,700 messages. It achieved an Information Disclosure Rate (IDR) of approximately 32%, successfully extracting sensitive financial information such as mule accounts. Additionally, the system maintained a Human Acceptance Rate (HAR) of around 70%, indicating strong alignment between LLM-generated responses and human operator preferences. Alongside these successes, our analysis reveals key operational challenges. In particular, the system struggled with engagement takeoff: only 48.7% of scammers responded to the initial seed message sent by defenders. These findings highlight the need for further refinement and provide actionable insights for advancing the design of automated scambaiting systems.", "categories": ["cs.CR", "cs.AI"], "published": "2025-09-10T11:08:52+00:00", "updated": "2025-09-10T11:08:52+00:00", "pdf_url": "https://arxiv.org/pdf/2509.08493v1", "source_articles": ["95225"], "fetched_at": "2026-01-19T12:39:48.983190"}
{"arxiv_id": "2509.09602", "title": "LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination", "authors": ["Yiqun T. Chen", "Tyler H. McCormick", "Li Liu", "Abhirup Datta"], "summary": "Verbal autopsy (VA) is a critical tool for estimating causes of death in resource-limited settings where medical certification is unavailable. This study presents LA-VA, a proof-of-concept pipeline that combines Large Language Models (LLMs) with traditional algorithmic approaches and embedding-based classification for improved cause-of-death prediction. Using the Population Health Metrics Research Consortium (PHMRC) dataset across three age categories (Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches: GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles. Our results demonstrate that GPT-5 achieves the highest individual performance with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5% (Neonate), outperforming traditional statistical machine learning baselines by 5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches could substantially improve verbal autopsy accuracy, with important implications for global health surveillance in low-resource settings.", "categories": ["cs.CL", "stat.AP"], "published": "2025-09-11T16:42:22+00:00", "updated": "2025-09-11T16:42:22+00:00", "pdf_url": "https://arxiv.org/pdf/2509.09602v1", "source_articles": ["95225"], "fetched_at": "2026-01-19T12:39:51.992775"}
{"arxiv_id": "2509.05338", "title": "Plantbot: Integrating Plant and Robot through LLM Modular Agent Networks", "authors": ["Atsushi Masumori", "Norihiro Maruyama", "Itsuki Doi", "johnsmith", "Hiroki Sato", "Takashi Ikegami"], "summary": "We introduce Plantbot, a hybrid lifeform that connects a living plant with a mobile robot through a network of large language model (LLM) modules. Each module - responsible for sensing, vision, dialogue, or action - operates asynchronously and communicates via natural language, enabling seamless interaction across biological and artificial domains. This architecture leverages the capacity of LLMs to serve as hybrid interfaces, where natural language functions as a universal protocol, translating multimodal data (soil moisture, temperature, visual context) into linguistic messages that coordinate system behaviors. The integrated network transforms plant states into robotic actions, installing normativity essential for agency within the sensor-motor loop. By combining biological and robotic elements through LLM-mediated communication, Plantbot behaves as an embodied, adaptive agent capable of responding autonomously to environmental conditions. This approach suggests possibilities for a new model of artificial life, where decentralized, LLM modules coordination enable novel interactions between biological and artificial systems.", "categories": ["cs.RO", "cs.AI"], "published": "2025-09-01T14:12:28+00:00", "updated": "2025-09-01T14:12:28+00:00", "pdf_url": "https://arxiv.org/pdf/2509.05338v1", "source_articles": ["95225"], "fetched_at": "2026-01-19T12:39:55.002960"}
{"arxiv_id": "2508.14869", "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models", "authors": ["Hend Al-Khalifa", "Raneem Almansour", "Layan Abdulrahman Alhuasini", "Alanood Alsaleh", "Mohamad-Hani Temsah", "Mohamad-Hani_Temsah", "Ashwag Rafea S Alruwaili"], "summary": "Prompt engineering has rapidly emerged as a critical skill for effective interaction with large language models (LLMs). However, the cognitive and neural underpinnings of this expertise remain largely unexplored. This paper presents findings from a cross-sectional pilot fMRI study investigating differences in brain functional connectivity and network activity between experts and intermediate prompt engineers. Our results reveal distinct neural signatures associated with higher prompt engineering literacy, including increased functional connectivity in brain regions such as the left middle temporal gyrus and the left frontal pole, as well as altered power-frequency dynamics in key cognitive networks. These findings offer initial insights into the neurobiological basis of prompt engineering proficiency. We discuss the implications of these neurocognitive markers in Natural Language Processing (NLP). Understanding the neural basis of human expertise in interacting with LLMs can inform the design of more intuitive human-AI interfaces, contribute to cognitive models of LLM interaction, and potentially guide the development of AI systems that better align with human cognitive workflows. This interdisciplinary approach aims to bridge the gap between human cognition and machine intelligence, fostering a deeper understanding of how humans learn and adapt to complex AI systems.", "categories": ["q-bio.NC", "cs.CL"], "published": "2025-08-20T17:31:53+00:00", "updated": "2025-08-20T17:31:53+00:00", "pdf_url": "https://arxiv.org/pdf/2508.14869v1", "source_articles": ["95225"], "fetched_at": "2026-01-19T12:39:58.013674"}
{"arxiv_id": "2509.09737", "title": "World Modeling with Probabilistic Structure Integration", "authors": ["Klemen Kotar", "Wanhee Lee", "Rahul Venkatesh", "Honglin Chen", "Daniel Bear", "Jared Watrous", "Simon Kim", "Khai Loong Aw", "Lilian Naing Chen", "Stefan Stojanov", "Kevin Feigelis", "Imran Thobani", "Alex Durango", "Khaled Jedoui", "Atlas Kazemian", "Dan Yamins"], "summary": "We present Probabilistic Structure Integration (PSI), a system for learning richly controllable and flexibly promptable world models from data. PSI consists of a three-step cycle. The first step, Probabilistic prediction, involves building a probabilistic graphical model Psi of the data, in the form of a random-access autoregressive sequence model. Psi supports a complete set of learned conditional distributions describing the dependence of any variables in the data on any other set of variables. In step 2, Structure extraction, we show how to extract underlying low-dimensional properties in the data, corresponding to a diverse set of meaningful \"intermediate structures\", in a zero-shot fashion via causal inference on Psi. Step 3, Integration, completes the cycle by converting these structures into new token types that are then continually mixed back into the training diet as conditioning signals and prediction targets. Each such cycle augments the capabilities of Psi, both allowing it to model the underlying data better, and creating new control handles -- akin to an LLM-like universal prompting language. We train an instance of Psi on 1.4 trillion tokens of internet video data; we use it to perform a variety of useful video prediction and understanding inferences; we extract state-of-the-art optical flow, self-supervised depth and object segmentation; and we use these structures to support a full cycle of predictive improvements.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-09-10T18:01:04+00:00", "updated": "2025-09-10T18:01:04+00:00", "pdf_url": "https://arxiv.org/pdf/2509.09737v1", "source_articles": ["95636"], "fetched_at": "2026-01-19T12:40:01.024143"}
{"arxiv_id": "2509.12194", "title": "Advancing Medical Artificial Intelligence Using a Century of Cases", "authors": ["Thomas A. Buckley", "Riccardo Conci", "Peter G. Brodeur", "Jason Gusdorf", "Sourik Beltrán", "Bita Behrouzi", "Byron Crowe", "Jacob Dockterman", "Muzzammil Muhammad", "Sarah Ohnigian", "Andrew Sanchez", "James A. Diao", "Aashna P. Shah", "Daniel Restrepo", "Eric S. Rosenberg", "Andrew S. Lea", "Marinka Zitnik", "Scott H. Podolsky", "Zahir Kanjee", "Raja-Elie E. Abdulnour", "Jacob M. Koshy", "Adam Rodman", "Arjun K. Manrai"], "summary": "BACKGROUND: For over a century, the New England Journal of Medicine Clinicopathological Conferences (CPCs) have tested the reasoning of expert physicians and, recently, artificial intelligence (AI). However, prior AI evaluations have focused on final diagnoses without addressing the multifaceted reasoning and presentation skills required of expert discussants.   METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025), we conducted extensive physician annotation and automated processing to create CPC-Bench, a physician-validated benchmark spanning 10 text-based and multimodal tasks, against which we evaluated leading large language models (LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce written and slide-based video presentations using only the case presentation, modeling the role of the human expert in these cases.   RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the final diagnosis first in 60% of cases and within the top ten in 84% of cases, outperforming a 20-physician baseline; next-test selection accuracy reached 98%. Event-level physician annotations quantified AI diagnostic accuracy per unit of information. Performance was lower on literature search and image tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image challenges. In blinded comparisons of CaBot vs. human expert-generated text, physicians misclassified the source of the differential in 46 of 62 (74%) of trials, and scored CaBot more favorably across quality dimensions. To promote research, we are releasing CaBot and CPC-Bench.   CONCLUSIONS: LLMs exceed physician performance on complex text-based differential diagnosis and convincingly emulate expert medical presentations, but image interpretation and literature retrieval remain weaker. CPC-Bench and CaBot may enable transparent and continued tracking of progress in medical AI.", "categories": ["cs.AI", "cs.CV"], "published": "2025-09-15T17:54:51+00:00", "updated": "2025-09-15T17:54:51+00:00", "pdf_url": "https://arxiv.org/pdf/2509.12194v1", "source_articles": ["95636"], "fetched_at": "2026-01-19T12:40:04.038848"}
{"arxiv_id": "2507.21206", "title": "Agentic Web: Weaving the Next Web with AI Agents", "authors": ["Yingxuan Yang", "Mulei Ma", "Yuxuan Huang", "Huacan Chai", "Chenyu Gong", "Haoran Geng", "Yuanjian Zhou", "Ying Wen", "Meng Fang", "Muhao Chen", "Shangding Gu", "Ming Jin", "Costas Spanos", "Yang Yang", "Pieter Abbeel", "Dawn Song", "Weinan Zhang", "Jun Wang"], "summary": "The emergence of AI agents powered by large language models (LLMs) marks a pivotal shift toward the Agentic Web, a new phase of the internet defined by autonomous, goal-driven interactions. In this paradigm, agents interact directly with one another to plan, coordinate, and execute complex tasks on behalf of users. This transition from human-driven to machine-to-machine interaction allows intent to be delegated, relieving users from routine digital operations and enabling a more interactive, automated web experience. In this paper, we present a structured framework for understanding and building the Agentic Web. We trace its evolution from the PC and Mobile Web eras and identify the core technological foundations that support this shift. Central to our framework is a conceptual model consisting of three key dimensions: intelligence, interaction, and economics. These dimensions collectively enable the capabilities of AI agents, such as retrieval, recommendation, planning, and collaboration. We analyze the architectural and infrastructural challenges involved in creating scalable agentic systems, including communication protocols, orchestration strategies, and emerging paradigms such as the Agent Attention Economy. We conclude by discussing the potential applications, societal risks, and governance issues posed by agentic systems, and outline research directions for developing open, secure, and intelligent ecosystems shaped by both human intent and autonomous agent behavior. A continuously updated collection of relevant studies for agentic web is available at: https://github.com/SafeRL-Lab/agentic-web.", "categories": ["cs.AI", "cs.LG"], "published": "2025-07-28T17:58:12+00:00", "updated": "2025-07-28T17:58:12+00:00", "pdf_url": "https://arxiv.org/pdf/2507.21206v1", "source_articles": ["95636"], "fetched_at": "2026-01-19T12:40:07.050456"}
{"arxiv_id": "2509.14704", "title": "Japanese Children's Riddles as a Benchmark for Machine Insight and Metacognition", "authors": ["Masaharu Mizumoto", "Dat Nguyen", "Zhiheng Han", "Jiyuan Fang", "Heyuan Guan", "Xingfu Li", "Naoya Shiraishi", "Yo Nakawake", "Le Minh Nguyen"], "summary": "Benchmark saturation and contamination have obscured genuine advances in reasoning for large language models (LLMs). We introduce NazoNazo Benchmark, a low-cost, renewable test built from Japanese children's riddles that demand insight-based reasoning, or representational shifts rather than knowledge recall. We evaluate 38 frontier LLMs (2023-2025) on 201 riddles and a 120-item human-comparison subset, finding that non-reasoning models average 7.6%, reasoning models 17.6%, and humans ~53% accuracy. Importantly, thought-log analysis reveals that reasoning in Japanese did not necessarily improve accuracy, indicating that language understanding alone is insufficient for insight reasoning. Notably, models sometimes generated correct candidates but failed to endorse them, suggesting weak metacognitive control rather than a lack of knowledge. This \"verification failure\" indicates that CoT outputs can reflect genuine intermediate reasoning states rather than post-hoc rationalizations. By exposing this metacognitive bottleneck - models' inability to recognize when they are right - the benchmark provides a scalable, cross-linguistic testbed for studying machine insight, confidence calibration, and self-evaluation. NazoNazo Benchmark thus offers not only a fresh challenge to current LLMs but also a concrete target for developing AI metacognitive psychology and enhancing machine Aha! capability.", "categories": ["cs.AI"], "published": "2025-09-18T07:50:04+00:00", "updated": "2026-01-05T13:57:38+00:00", "pdf_url": "https://arxiv.org/pdf/2509.14704v2", "source_articles": ["95636"], "fetched_at": "2026-01-19T12:40:10.061254"}
{"arxiv_id": "2509.14455", "title": "Charting trajectories of human thought using large language models", "authors": ["Matthew M Nour", "Daniel C McNamee", "Isaac Fradkin", "Raymond J Dolan"], "summary": "Language provides the most revealing window into the ways humans structure conceptual knowledge within cognitive maps. Harnessing this information has been difficult, given the challenge of reliably mapping words to mental concepts. Artificial Intelligence large language models (LLMs) now offer unprecedented opportunities to revisit this challenge. LLMs represent words and phrases as high-dimensional numerical vectors that encode vast semantic knowledge. To harness this potential for cognitive science, we introduce VECTOR, a computational framework that aligns LLM representations with human cognitive map organisation. VECTOR casts a participant's verbal reports as a geometric trajectory through a cognitive map representation, revealing how thoughts flow from one idea to the next. Applying VECTOR to narratives generated by 1,100 participants, we show these trajectories have cognitively meaningful properties that predict paralinguistic behaviour (response times) and real-world communication patterns. We suggest our approach opens new avenues for understanding how humans dynamically organise and navigate conceptual knowledge in naturalistic settings.", "categories": ["q-bio.NC"], "published": "2025-09-17T22:10:54+00:00", "updated": "2025-09-17T22:10:54+00:00", "pdf_url": "https://arxiv.org/pdf/2509.14455v1", "source_articles": ["95636"], "fetched_at": "2026-01-19T12:40:13.072398"}
{"arxiv_id": "2509.14448", "title": "VCBench: Benchmarking LLMs in Venture Capital", "authors": ["Rick Chen", "Joseph Ternasky", "Afriyie Samuel Kwesi", "Ben Griffin", "Aaron Ontoyin Yin", "Zakari Salifu", "Kelvin Amoaba", "Xianling Mu", "Fuat Alican", "Yigit Ihlamur"], "summary": "Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets accelerate progress toward artificial general intelligence (AGI). We introduce VCBench, the first benchmark for predicting founder success in venture capital (VC), a domain where signals are sparse, outcomes are uncertain, and even top investors perform modestly. At inception, the market index achieves a precision of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1 firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles, standardized to preserve predictive features while resisting identity leakage, with adversarial tests showing more than 90% reduction in re-identification risk. We evaluate nine state-of-the-art large language models (LLMs). DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the highest F0.5, and most models surpass human benchmarks. Designed as a public and evolving resource available at vcbench.com, VCBench establishes a community-driven standard for reproducible and privacy-preserving evaluation of AGI in early-stage venture forecasting.", "categories": ["cs.AI"], "published": "2025-09-17T21:56:48+00:00", "updated": "2025-09-17T21:56:48+00:00", "pdf_url": "https://arxiv.org/pdf/2509.14448v1", "source_articles": ["95636"], "fetched_at": "2026-01-19T12:40:16.083824"}
{"arxiv_id": "2509.15510", "title": "The (Short-Term) Effects of Large Language Models on Unemployment and Earnings", "authors": ["Danqing Chen", "Carina Kane", "Austin Kozlowski", "Nadav Kunievsky", "James A. Evans"], "summary": "Large Language Models have spread rapidly since the release of ChatGPT in late 2022, accompanied by claims of major productivity gains but also concerns about job displacement. This paper examines the short-run labor market effects of LLM adoption by comparing earnings and unemployment across occupations with differing levels of exposure to these technologies. Using a Synthetic Difference in Differences approach, we estimate the impact of LLM exposure on earnings and unemployment. Our findings show that workers in highly exposed occupations experienced earnings increases following ChatGPT's introduction, while unemployment rates remained unchanged. These results suggest that initial labor market adjustments to LLMs operate primarily through earnings rather than worker reallocation.", "categories": ["econ.GN", "cs.AI", "cs.CY"], "published": "2025-09-19T01:20:28+00:00", "updated": "2025-09-19T01:20:28+00:00", "pdf_url": "https://arxiv.org/pdf/2509.15510v1", "source_articles": ["95916"], "fetched_at": "2026-01-19T12:40:19.094522"}
{"arxiv_id": "2509.13348", "title": "Towards an AI-Augmented Textbook", "authors": ["LearnLM Team", "Google", ":", "Alicia Martín", "Amir Globerson", "Amy Wang", "Anirudh Shekhawat", "Anna Iurchenko", "Anisha Choudhury", "Avinatan Hassidim", "Ayça Çakmakli", "Ayelet Shasha Evron", "Charlie Yang", "Courtney Heldreth", "Diana Akrong", "Gal Elidan", "Hairong Mu", "Ian Li", "Ido Cohen", "Katherine Chou", "Komal Singh", "Lev Borovoi", "Lidan Hackmon", "Lior Belinsky", "Michael Fink", "Niv Efron", "Preeti Singh", "Rena Levitt", "Shashank Agarwal", "Shay Sharon", "Tracey Lee-Joe", "Xiaohong Hao", "Yael Gold-Zamir", "Yael Haramaty", "Yishay Mor", "Yoav Bar Sinai", "Yossi Matias"], "summary": "Textbooks are a cornerstone of education, but they have a fundamental limitation: they are a one-size-fits-all medium. Any new material or alternative representation requires arduous human effort, so that textbooks cannot be adapted in a scalable manner. We present an approach for transforming and augmenting textbooks using generative AI, adding layers of multiple representations and personalization while maintaining content integrity and quality. We refer to the system built with this approach as Learn Your Way. We report pedagogical evaluations of the different transformations and augmentations, and present the results of a a randomized control trial, highlighting the advantages of learning with Learn Your Way over regular textbook usage.", "categories": ["cs.CY", "cs.HC"], "published": "2025-09-13T17:59:54+00:00", "updated": "2025-09-30T06:33:04+00:00", "pdf_url": "https://arxiv.org/pdf/2509.13348v4", "source_articles": ["95916"], "fetched_at": "2026-01-19T12:40:22.108530"}
{"arxiv_id": "2509.17703", "title": "An LLM-based Agent Simulation Approach to Study Moral Evolution", "authors": ["Zhou Ziheng", "Huacong Tang", "Mingjie Bi", "Yipeng Kang", "Wanying He", "Fang Sun", "Yizhou Sun", "Ying Nian Wu", "Demetri Terzopoulos", "Fangwei Zhong"], "summary": "The evolution of morality presents a puzzle: natural selection should favor self-interest, yet humans developed moral systems promoting altruism. We address this question by introducing a novel Large Language Model (LLM)-based agent simulation framework modeling prehistoric hunter-gatherer societies. This platform is designed to probe diverse questions in social evolution, from survival advantages to inter-group dynamics. To investigate moral evolution, we designed agents with varying moral dispositions based on the Expanding Circle Theory \\citep{singer1981expanding}. We evaluated their evolutionary success across a series of simulations and analyzed their decision-making in specially designed moral dilemmas. These experiments reveal how an agent's moral framework, in combination with its cognitive constraints, directly shapes its behavior and determines its evolutionary outcome. Crucially, the emergent patterns echo seminal theories from related domains of social science, providing external validation for the simulations. This work establishes LLM-based simulation as a powerful new paradigm to complement traditional research in evolutionary biology and anthropology, opening new avenues for investigating the complexities of moral and social evolution.", "categories": ["cs.MA"], "published": "2025-09-22T12:43:09+00:00", "updated": "2025-09-22T12:43:09+00:00", "pdf_url": "https://arxiv.org/pdf/2509.17703v1", "source_articles": ["95916"], "fetched_at": "2026-01-19T12:40:25.119127"}
{"arxiv_id": "2509.17641", "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?", "authors": ["Hyunjong Ok", "Suho Yoo", "Hyeonjun Kim", "Jaeho Lee"], "summary": "Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, a comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIR-CoT, a novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD"], "published": "2025-09-22T11:45:22+00:00", "updated": "2025-09-22T11:45:22+00:00", "pdf_url": "https://arxiv.org/pdf/2509.17641v1", "source_articles": ["95916"], "fetched_at": "2026-01-19T12:40:28.129340"}
{"arxiv_id": "2509.21091", "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute", "authors": ["Junpei Komiyama", "Daisuke Oba", "Masafumi Oyamada"], "summary": "We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \\to \\infty$, which we denote as Best-of-$\\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.", "categories": ["stat.ML", "cs.AI", "cs.LG"], "published": "2025-09-25T12:41:05+00:00", "updated": "2025-09-25T12:41:05+00:00", "pdf_url": "https://arxiv.org/pdf/2509.21091v1", "source_articles": ["95916"], "fetched_at": "2026-01-19T12:40:31.139421"}
{"arxiv_id": "2509.17292", "title": "Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection", "authors": ["Jun Seo Kim", "Hyemi Kim", "Woo Joo Oh", "Hongjin Cho", "Hochul Lee", "Hye Hyeon Kim"], "summary": "Cognitive distortions have been closely linked to mental health disorders, yet their automatic detection remained challenging due to contextual ambiguity, co-occurrence, and semantic overlap. We proposed a novel framework that combines Large Language Models (LLMs) with Multiple-Instance Learning (MIL) architecture to enhance interpretability and expression-level reasoning. Each utterance was decomposed into Emotion, Logic, and Behavior (ELB) components, which were processed by LLMs to infer multiple distortion instances, each with a predicted type, expression, and model-assigned salience score. These instances were integrated via a Multi-View Gated Attention mechanism for final classification. Experiments on Korean (KoACD) and English (Therapist QA) datasets demonstrate that incorporating ELB and LLM-inferred salience scores improves classification performance, especially for distortions with high interpretive ambiguity. Our results suggested a psychologically grounded and generalizable approach for fine-grained reasoning in mental health NLP.", "categories": ["cs.CL", "cs.AI"], "published": "2025-09-22T00:18:58+00:00", "updated": "2025-09-22T00:18:58+00:00", "pdf_url": "https://arxiv.org/pdf/2509.17292v1", "source_articles": ["95916"], "fetched_at": "2026-01-19T12:40:34.151701"}
{"arxiv_id": "2509.05390", "title": "Authorship Without Writing: Large Language Models and the Senior Author Analogy", "authors": ["Clint Hurshman", "Sebastian Porsdam Mann", "Julian Savulescu", "Brian D. Earp"], "summary": "The use of large language models (LLMs) in bioethical, scientific, and medical writing remains controversial. While there is broad agreement in some circles that LLMs cannot count as authors, there is no consensus about whether and how humans using LLMs can count as authors. In many fields, authorship is distributed among large teams of researchers, some of whom, including paradigmatic senior authors who guide and determine the scope of a project and ultimately vouch for its integrity, may not write a single word. In this paper, we argue that LLM use (under specific conditions) is analogous to a form of senior authorship. On this view, the use of LLMs, even to generate complete drafts of research papers, can be considered a legitimate form of authorship according to the accepted criteria in many fields. We conclude that either such use should be recognized as legitimate, or current criteria for authorship require fundamental revision. AI use declaration: GPT-5 was used to help format Box 1. AI was not used for any other part of the preparation or writing of this manuscript.", "categories": ["cs.CY", "cs.AI", "cs.CL"], "published": "2025-09-05T10:11:56+00:00", "updated": "2025-09-05T10:11:56+00:00", "pdf_url": "https://arxiv.org/pdf/2509.05390v1", "source_articles": ["96179"], "fetched_at": "2026-01-19T12:40:37.162374"}
{"arxiv_id": "2509.23141", "title": "Earth-Agent: Unlocking the Full Landscape of Earth Observation with Agents", "authors": ["Peilin Feng", "Zhutao Lv", "Junyan Ye", "Xiaolei Wang", "Xinjie Huo", "Jinhua Yu", "Wanghan Xu", "Wenlong Zhang", "Lei Bai", "Conghui He", "Weijia Li"], "summary": "Earth observation (EO) is essential for understanding the evolving states of the Earth system. Although recent MLLMs have advanced EO research, they still lack the capability to tackle complex tasks that require multi-step reasoning and the use of domain-specific tools. Agent-based methods offer a promising direction, but current attempts remain in their infancy, confined to RGB perception, shallow reasoning, and lacking systematic evaluation protocols. To overcome these limitations, we introduce Earth-Agent, the first agentic framework that unifies RGB and spectral EO data within an MCP-based tool ecosystem, enabling cross-modal, multi-step, and quantitative spatiotemporal reasoning beyond pretrained MLLMs. Earth-Agent supports complex scientific tasks such as geophysical parameter retrieval and quantitative spatiotemporal analysis by dynamically invoking expert tools and models across modalities. To support comprehensive evaluation, we further propose Earth-Bench, a benchmark of 248 expert-curated tasks with 13,729 images, spanning spectrum, products and RGB modalities, and equipped with a dual-level evaluation protocol that assesses both reasoning trajectories and final outcomes. We conduct comprehensive experiments varying different LLM backbones, comparisons with general agent frameworks, and comparisons with MLLMs on remote sensing benchmarks, demonstrating both the effectiveness and potential of Earth-Agent. Earth-Agent establishes a new paradigm for EO analysis, moving the field toward scientifically grounded, next-generation applications of LLMs in Earth observation.", "categories": ["cs.CV"], "published": "2025-09-27T06:04:28+00:00", "updated": "2025-10-16T07:27:45+00:00", "pdf_url": "https://arxiv.org/pdf/2509.23141v2", "source_articles": ["96179"], "fetched_at": "2026-01-19T12:40:40.172902"}
{"arxiv_id": "2509.22725", "title": "A Meta-Analysis of LLM Effects on Students across Qualification, Socialisation, and Subjectification", "authors": ["Jiayu Huang", "Ruoxin Ritter Wang", "Jen-Hao Liu", "Boming Xia", "Yue Huang", "Ruoxi Sun", "Jason Minhui Xue", "Jinan Zou"], "summary": "Large language models (LLMs) are increasingly positioned as solutions for education, yet evaluations often reduce their impact to narrow performance metrics. This paper reframes the question by asking \"what kind of impact should LLMs have in education?\" Drawing on Biesta's tripartite account of good education: qualification, socialisation, and subjectification, we present a meta-analysis of 133 experimental and quasi-experimental studies (k = 188). Overall, the impact of LLMs on student learning is positive but uneven. Strong effects emerge in qualification, particularly when LLMs function as tutors in sustained interventions. Socialisation outcomes appear more variable, concentrated in sustained, reflective interventions. Subjectification, linked to autonomy and learner development, remains fragile, with improvements confined to small-scale, long-term studies. This purpose-level view highlights design as the decisive factor: without scaffolds for participation and agency, LLMs privilege what is easiest to measure while neglecting broader aims of education. For HCI and education, the issue is not just whether LLMs work, but what futures they enable or foreclose.", "categories": ["cs.CY", "cs.AI", "cs.HC"], "published": "2025-09-25T04:11:19+00:00", "updated": "2025-09-30T10:22:27+00:00", "pdf_url": "https://arxiv.org/pdf/2509.22725v2", "source_articles": ["96179"], "fetched_at": "2026-01-19T12:40:43.184276"}
{"arxiv_id": "2509.23434", "title": "NeuroBridge: Using Generative AI to Bridge Cross-neurotype Communication Differences through Neurotypical Perspective-taking", "authors": ["Rukhshan Haroon", "Kyle Wigdor", "Katie Yang", "Nicole Toumanios", "Eileen T. Crehan", "Fahad Dogar"], "summary": "Communication challenges between autistic and neurotypical individuals stem from a mutual lack of understanding of each other's distinct, and often contrasting, communication styles. Yet, autistic individuals are expected to adapt to neurotypical norms, making interactions inauthentic and mentally exhausting for them. To help redress this imbalance, we build NeuroBridge, an online platform that utilizes large language models (LLMs) to simulate: (a) an AI character that is direct and literal, a style common among many autistic individuals, and (b) four cross-neurotype communication scenarios in a feedback-driven conversation between this character and a neurotypical user. Through NeuroBridge, neurotypical individuals gain a firsthand look at autistic communication, and reflect on their role in shaping cross-neurotype interactions. In a user study with 12 neurotypical participants, we find that NeuroBridge improved their understanding of how autistic people may interpret language differently, with all describing autism as a social difference that \"needs understanding by others\" after completing the simulation. Participants valued its personalized, interactive format and described AI-generated feedback as \"constructive\", \"logical\" and \"non-judgmental\". Most perceived the portrayal of autism in the simulation as accurate, suggesting that users may readily accept AI-generated (mis)representations of disabilities. To conclude, we discuss design implications for disability representation in AI, the need for making NeuroBridge more personalized, and LLMs' limitations in modeling complex social scenarios.", "categories": ["cs.HC", "cs.AI"], "published": "2025-09-27T18:05:41+00:00", "updated": "2025-09-27T18:05:41+00:00", "pdf_url": "https://arxiv.org/pdf/2509.23434v1", "source_articles": ["96179"], "fetched_at": "2026-01-19T12:40:46.195037"}
{"arxiv_id": "2509.23108", "title": "Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models", "authors": ["Morgan McCarty", "Jorge Morales"], "summary": "This study offers a novel approach for benchmarking complex cognitive behavior in artificial systems. Almost universally, Large Language Models (LLMs) perform best on tasks which may be included in their training data and can be accomplished solely using natural language, limiting our understanding of their emergent sophisticated cognitive capacities. In this work, we created dozens of novel items of a classic mental imagery task from cognitive psychology. A task which, traditionally, cognitive psychologists have argued is solvable exclusively via visual mental imagery (i.e., language alone would be insufficient). LLMs are perfect for testing this hypothesis. First, we tested several state-of-the-art LLMs by giving text-only models written instructions and asking them to report the resulting object after performing the transformations in the aforementioned task. Then, we created a baseline by testing 100 human subjects in exactly the same task. We found that the best LLMs performed significantly above average human performance. Finally, we tested reasoning models set to different levels of reasoning and found the strongest performance when models allocate greater amounts of reasoning tokens. These results provide evidence that the best LLMs may have the capability to complete imagery-dependent tasks despite the non-pictorial nature of their architectures. Our study not only demonstrates an emergent cognitive capacity in LLMs while performing a novel task, but it also provides the field with a new task that leaves lots of room for improvement in otherwise already highly capable models. Finally, our findings reignite the debate over the formats of representation of visual imagery in humans, suggesting that propositional reasoning (or at least non-imagistic reasoning) may be sufficient to complete tasks that were long-thought to be imagery-dependent.", "categories": ["cs.AI", "cs.CL"], "published": "2025-09-27T04:36:12+00:00", "updated": "2025-09-27T04:36:12+00:00", "pdf_url": "https://arxiv.org/pdf/2509.23108v1", "source_articles": ["96179"], "fetched_at": "2026-01-19T12:40:49.207274"}
{"arxiv_id": "2509.24496", "title": "LLM DNA: Tracing Model Evolution via Functional Representations", "authors": ["Zhaomin Wu", "Haodong Zhao", "Ziyang Wang", "Jizhou Guo", "Qian Wang", "Bingsheng He"], "summary": "The explosive growth of large language models (LLMs) has created a vast but opaque landscape: millions of models exist, yet their evolutionary relationships through fine-tuning, distillation, or adaptation are often undocumented or unclear, complicating LLM management. Existing methods are limited by task specificity, fixed model sets, or strict assumptions about tokenizers or architectures. Inspired by biological DNA, we address these limitations by mathematically defining LLM DNA as a low-dimensional, bi-Lipschitz representation of functional behavior. We prove that LLM DNA satisfies inheritance and genetic determinism properties and establish the existence of DNA. Building on this theory, we derive a general, scalable, training-free pipeline for DNA extraction. In experiments across 305 LLMs, DNA aligns with prior studies on limited subsets and achieves superior or competitive performance on specific tasks. Beyond these tasks, DNA comparisons uncover previously undocumented relationships among LLMs. We further construct the evolutionary tree of LLMs using phylogenetic algorithms, which align with shifts from encoder-decoder to decoder-only architectures, reflect temporal progression, and reveal distinct evolutionary speeds across LLM families.", "categories": ["cs.LG", "cs.AI"], "published": "2025-09-29T09:09:57+00:00", "updated": "2025-09-29T09:09:57+00:00", "pdf_url": "https://arxiv.org/pdf/2509.24496v1", "source_articles": ["96179"], "fetched_at": "2026-01-19T12:40:52.217977"}
{"arxiv_id": "2509.24298", "title": "Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports", "authors": ["Changde Du", "Yizhuo Lu", "Zhongyu Huang", "Yi Sun", "Zisen Zhou", "Shaozheng Qin", "Huiguang He"], "summary": "The ability to represent emotion plays a significant role in human cognition and social interaction, yet the high-dimensional geometry of this affective space and its neural underpinnings remain debated. A key challenge, the `behavior-neural gap,' is the limited ability of human self-reports to predict brain activity. Here we test the hypothesis that this gap arises from the constraints of traditional rating scales and that large-scale similarity judgments can more faithfully capture the brain's affective geometry. Using AI models as `cognitive agents,' we collected millions of triplet odd-one-out judgments from a multimodal large language model (MLLM) and a language-only model (LLM) in response to 2,180 emotionally evocative videos. We found that the emergent 30-dimensional embeddings from these models are highly interpretable and organize emotion primarily along categorical lines, yet in a blended fashion that incorporates dimensional properties. Most remarkably, the MLLM's representation predicted neural activity in human emotion-processing networks with the highest accuracy, outperforming not only the LLM but also, counterintuitively, representations derived directly from human behavioral ratings. This result supports our primary hypothesis and suggests that sensory grounding--learning from rich visual data--is critical for developing a truly neurally-aligned conceptual framework for emotion. Our findings provide compelling evidence that MLLMs can autonomously develop rich, neurally-aligned affective representations, offering a powerful paradigm to bridge the gap between subjective experience and its neural substrates. Project page: https://reedonepeck.github.io/ai-emotion.github.io/.", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.MM"], "published": "2025-09-29T05:22:33+00:00", "updated": "2025-09-29T05:22:33+00:00", "pdf_url": "https://arxiv.org/pdf/2509.24298v1", "source_articles": ["96179"], "fetched_at": "2026-01-19T12:40:55.230874"}
{"arxiv_id": "2510.04364", "title": "Reflection Before Action: Designing a Framework for Quantifying Thought Patterns for Increased Self-awareness in Personal Decision Making", "authors": ["Morita Tarvirdians", "Senthil Chandrasegaran", "Hayley Hung", "Catholijn M. Jonker", "Catharine Oertel"], "summary": "When making significant life decisions, people increasingly turn to conversational AI tools, such as large language models (LLMs). However, LLMs often steer users toward solutions, limiting metacognitive awareness of their own decision-making. In this paper, we shift the focus in decision support from solution-orientation to reflective activity, coining the term pre-decision reflection (PDR). We introduce PROBE, the first framework that assesses pre-decision reflections along two dimensions: breadth (diversity of thought categories) and depth (elaborateness of reasoning). Coder agreement demonstrates PROBE's reliability in capturing how people engage in pre-decision reflection. Our study reveals substantial heterogeneity across participants and shows that people perceived their unassisted reflections as deeper and broader than PROBE's measures. By surfacing hidden thought patterns, PROBE opens opportunities for technologies that foster self-awareness and strengthen people's agency in choosing which thought patterns to rely on in decision-making.", "categories": ["cs.HC"], "published": "2025-10-05T21:16:18+00:00", "updated": "2025-10-05T21:16:18+00:00", "pdf_url": "https://arxiv.org/pdf/2510.04364v1", "source_articles": ["96420"], "fetched_at": "2026-01-19T12:40:58.242636"}
{"arxiv_id": "2510.05016", "title": "Large Language Models Achieve Gold Medal Performance at the International Olympiad on Astronomy & Astrophysics (IOAA)", "authors": ["Lucas Carrit Delgado Pinheiro", "Ziru Chen", "Bruno Caixeta Piazza", "Ness Shroff", "Yingbin Liang", "Yuan-Sen Ting", "Huan Sun"], "summary": "While task-specific demonstrations show early success in applying large language models (LLMs) to automate some astronomical research tasks, they only provide incomplete views of all necessary capabilities in solving astronomy problems, calling for more thorough understanding of LLMs' strengths and limitations. So far, existing benchmarks and evaluations focus on simple question-answering that primarily tests astronomical knowledge and fails to evaluate the complex reasoning required for real-world research in the discipline. Here, we address this gap by systematically benchmarking five state-of-the-art LLMs on the International Olympiad on Astronomy and Astrophysics (IOAA) exams, which are designed to examine deep conceptual understanding, multi-step derivations, and multimodal analysis. With average scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing models) not only achieve gold medal level performance but also rank in the top two among ~200-300 participants in all four IOAA theory exams evaluated (2022-2025). In comparison, results on the data analysis exams show more divergence. GPT-5 still excels in the exams with an 88.5% average score, ranking top 10 among the participants in the four most recent IOAAs, while other models' performances drop to 48-76%. Furthermore, our in-depth error analysis underscores conceptual reasoning, geometric reasoning, and spatial visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence, although LLMs approach peak human performance in theory exams, critical gaps must be addressed before they can serve as autonomous research agents in astronomy.", "categories": ["astro-ph.IM", "cs.AI", "cs.CL"], "published": "2025-10-06T16:58:47+00:00", "updated": "2025-10-07T15:34:59+00:00", "pdf_url": "https://arxiv.org/pdf/2510.05016v2", "source_articles": ["96420"], "fetched_at": "2026-01-19T12:41:01.265360"}
{"arxiv_id": "2510.04064", "title": "Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion", "authors": ["Jingxiang Zhang", "Lujia Zhong"], "summary": "Large Language Models (LLMs) are increasingly expected to navigate the nuances of human emotion. While research confirms that LLMs can simulate emotional intelligence, their internal emotional mechanisms remain largely unexplored. This paper investigates the latent emotional representations within modern LLMs by asking: how, where, and for how long is emotion encoded in their neural architecture? To address this, we introduce a novel, large-scale Reddit corpus of approximately 400,000 utterances, balanced across seven basic emotions through a multi-stage process of classification, rewriting, and synthetic generation. Using this dataset, we employ lightweight \"probes\" to read out information from the hidden layers of various Qwen3 and LLaMA models without altering their parameters. Our findings reveal that LLMs develop a surprisingly well-defined internal geometry of emotion, which sharpens with model scale and significantly outperforms zero-shot prompting. We demonstrate that this emotional signal is not a final-layer phenomenon but emerges early and peaks mid-network. Furthermore, the internal states are both malleable (they can be influenced by simple system prompts) and persistent, as the initial emotional tone remains detectable for hundreds of subsequent tokens. We contribute our dataset, an open-source probing toolkit, and a detailed map of the emotional landscape within LLMs, offering crucial insights for developing more transparent and aligned AI systems. The code and dataset are open-sourced.", "categories": ["cs.AI"], "published": "2025-10-05T06:53:42+00:00", "updated": "2025-10-12T17:53:20+00:00", "pdf_url": "https://arxiv.org/pdf/2510.04064v2", "source_articles": ["96420"], "fetched_at": "2026-01-19T12:41:04.275524"}
{"arxiv_id": "2510.08202", "title": "Sentiment Matters: An Analysis of 200 Human-SAV Interactions", "authors": ["Lirui Guo", "Michael G. Burke", "Wynita M. Griggs"], "summary": "Shared Autonomous Vehicles (SAVs) are likely to become an important part of the transportation system, making effective human-SAV interactions an important area of research. This paper introduces a dataset of 200 human-SAV interactions to further this area of study. We present an open-source human-SAV conversational dataset, comprising both textual data (e.g., 2,136 human-SAV exchanges) and empirical data (e.g., post-interaction survey results on a range of psychological factors). The dataset's utility is demonstrated through two benchmark case studies: First, using random forest modeling and chord diagrams, we identify key predictors of SAV acceptance and perceived service quality, highlighting the critical influence of response sentiment polarity (i.e., perceived positivity). Second, we benchmark the performance of an LLM-based sentiment analysis tool against the traditional lexicon-based TextBlob method. Results indicate that even simple zero-shot LLM prompts more closely align with user-reported sentiment, though limitations remain. This study provides novel insights for designing conversational SAV interfaces and establishes a foundation for further exploration into advanced sentiment modeling, adaptive user interactions, and multimodal conversational systems.", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.ET"], "published": "2025-10-09T13:30:23+00:00", "updated": "2025-10-09T13:30:23+00:00", "pdf_url": "https://arxiv.org/pdf/2510.08202v1", "source_articles": ["96420"], "fetched_at": "2026-01-19T12:41:07.286860"}
{"arxiv_id": "2510.08506", "title": "Neologism Learning for Controllability and Self-Verbalization", "authors": ["John Hewitt", "Oyvind Tafjord", "Robert Geirhos", "Been Kim"], "summary": "Humans invent new words when there is a rising demand for a new useful concept (e.g., doomscrolling). We explore and validate a similar idea in our communication with LLMs: introducing new words to better understand and control the models, expanding on the recently introduced neologism learning. This method introduces a new word by adding a new word embedding and training with examples that exhibit the concept with no other changes in model parameters. We show that adding a new word allows for control of concepts such as flattery, incorrect answers, text length, as well as more complex concepts in AxBench. We discover that neologisms can also further our understanding of the model via self-verbalization: models can describe what each new word means to them in natural language, like explaining that a word that represents a concept of incorrect answers means ``a lack of complete, coherent, or meaningful answers...'' To validate self-verbalizations, we introduce plug-in evaluation: we insert the verbalization into the context of a model and measure whether it controls the target concept. In some self-verbalizations, we find machine-only synonyms: words that seem unrelated to humans but cause similar behavior in machines. Finally, we show how neologism learning can jointly learn multiple concepts in multiple words.", "categories": ["cs.CL"], "published": "2025-10-09T17:41:57+00:00", "updated": "2025-10-09T17:41:57+00:00", "pdf_url": "https://arxiv.org/pdf/2510.08506v1", "source_articles": ["96420"], "fetched_at": "2026-01-19T12:41:10.298379"}
{"arxiv_id": "2510.05432", "title": "AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems", "authors": ["Shambhavi Mishra", "Gaurav Sahu", "Marco Pedersoli", "Laurent Charlin", "Jose Dolz", "Christopher Pal"], "summary": "Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.", "categories": ["cs.AI"], "published": "2025-10-06T22:50:41+00:00", "updated": "2025-10-06T22:50:41+00:00", "pdf_url": "https://arxiv.org/pdf/2510.05432v1", "source_articles": ["96420"], "fetched_at": "2026-01-19T12:41:13.309271"}
{"arxiv_id": "2510.01297", "title": "SimCity: Multi-Agent Urban Development Simulation with Rich Interactions", "authors": ["Yeqi Feng", "Yucheng Lu", "Hongyu Su", "Tianxing He"], "summary": "Large Language Models (LLMs) open new possibilities for constructing realistic and interpretable macroeconomic simulations. We present SimCity, a multi-agent framework that leverages LLMs to model an interpretable macroeconomic system with heterogeneous agents and rich interactions. Unlike classical equilibrium models that limit heterogeneity for tractability, or traditional agent-based models (ABMs) that rely on hand-crafted decision rules, SimCity enables flexible, adaptive behavior with transparent natural-language reasoning. Within SimCity, four core agent types (households, firms, a central bank, and a government) deliberate and participate in a frictional labor market, a heterogeneous goods market, and a financial market. Furthermore, a Vision-Language Model (VLM) determines the geographic placement of new firms and renders a mapped virtual city, allowing us to study both macroeconomic regularities and urban expansion dynamics within a unified environment. To evaluate the framework, we compile a checklist of canonical macroeconomic phenomena, including price elasticity of demand, Engel's Law, Okun's Law, the Phillips Curve, and the Beveridge Curve, and show that SimCity naturally reproduces these empirical patterns while remaining robust across simulation runs.", "categories": ["cs.MA"], "published": "2025-10-01T10:27:01+00:00", "updated": "2025-10-01T10:27:01+00:00", "pdf_url": "https://arxiv.org/pdf/2510.01297v1", "source_articles": ["96756"], "fetched_at": "2026-01-19T12:41:16.321886"}
{"arxiv_id": "2510.07686", "title": "Stress-Testing Model Specs Reveals Character Differences among Language Models", "authors": ["Jifan Zhang", "Henry Sleight", "Andi Peng", "John Schulman", "Esin Durmus"], "summary": "Large language models (LLMs) are increasingly trained from AI constitutions and model specifications that establish behavioral guidelines and ethical principles. However, these specifications face critical challenges, including internal conflicts between principles and insufficient coverage of nuanced scenarios. We present a systematic methodology for stress-testing model character specifications, automatically identifying numerous cases of principle contradictions and interpretive ambiguities in current model specs.   We stress test current model specs by generating scenarios that force explicit tradeoffs between competing value-based principles. Using a comprehensive taxonomy we generate diverse value tradeoff scenarios where models must choose between pairs of legitimate principles that cannot be simultaneously satisfied. We evaluate responses from twelve frontier LLMs across major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral disagreement through value classification scores. Among these scenarios, we identify over 70,000 cases exhibiting significant behavioral divergence. Empirically, we show this high divergence in model behavior strongly predicts underlying problems in model specifications. Through qualitative analysis, we provide numerous example issues in current model specs such as direct contradiction and interpretive ambiguities of several principles. Additionally, our generated dataset also reveals both clear misalignment cases and false-positive refusals across all of the frontier models we study. Lastly, we also provide value prioritization patterns and differences of these models.", "categories": ["cs.CL", "cs.AI"], "published": "2025-10-09T02:24:37+00:00", "updated": "2025-10-23T07:31:33+00:00", "pdf_url": "https://arxiv.org/pdf/2510.07686v2", "source_articles": ["96756"], "fetched_at": "2026-01-19T12:41:19.332342"}
{"arxiv_id": "2510.11328", "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control", "authors": ["Chenxi Wang", "Yixuan Zhang", "Ruiji Yu", "Yufei Zheng", "Lang Gao", "Zirui Song", "Zixiang Xu", "Gus Xia", "Huishuai Zhang", "Dongyan Zhao", "Xiuying Chen"], "summary": "As the demand for emotional intelligence in large language models (LLMs) grows, a key challenge lies in understanding the internal mechanisms that give rise to emotional expression and in controlling emotions in generated text. This study addresses three core questions: (1) Do LLMs contain context-agnostic mechanisms shaping emotional expression? (2) What form do these mechanisms take? (3) Can they be harnessed for universal emotion control? We first construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit comparable internal states across emotions. Subsequently, we extract context-agnostic emotion directions that reveal consistent, cross-context encoding of emotion (Q1). We identify neurons and attention heads that locally implement emotional computation through analytical decomposition and causal analysis, and validate their causal roles via ablation and enhancement interventions. Next, we quantify each sublayer's causal influence on the model's final emotion representation and integrate the identified local components into coherent global emotion circuits that drive emotional expression (Q2). Directly modulating these circuits achieves 99.65% emotion-expression accuracy on the test set, surpassing prompting- and steering-based methods (Q3). To our knowledge, this is the first systematic study to uncover and validate emotion circuits in LLMs, offering new insights into interpretability and controllable emotional intelligence.", "categories": ["cs.CL", "cs.AI"], "published": "2025-10-13T12:24:24+00:00", "updated": "2025-10-13T12:24:24+00:00", "pdf_url": "https://arxiv.org/pdf/2510.11328v1", "source_articles": ["96756"], "fetched_at": "2026-01-19T12:41:22.342076"}
{"arxiv_id": "2510.12943", "title": "The Curious Case of Curiosity across Human Cultures and LLMs", "authors": ["Angana Borah", "Zhijing Jin", "Rada Mihalcea"], "summary": "Recent advances in Large Language Models (LLMs) have expanded their role in human interaction, yet curiosity -- a central driver of inquiry -- remains underexplored in these systems, particularly across cultural contexts. In this work, we investigate cultural variation in curiosity using Yahoo! Answers, a real-world multi-country dataset spanning diverse topics. We introduce CUEST (CUriosity Evaluation across SocieTies), an evaluation framework that measures human-model alignment in curiosity through linguistic (style), topic preference (content) analysis and grounding insights in social science constructs. Across open- and closed-source models, we find that LLMs flatten cross-cultural diversity, aligning more closely with how curiosity is expressed in Western countries. We then explore fine-tuning strategies to induce curiosity in LLMs, narrowing the human-model alignment gap by up to 50%. Finally, we demonstrate the practical value of curiosity for LLM adaptability across cultures, showing its importance for future NLP research.", "categories": ["cs.CL"], "published": "2025-10-14T19:42:24+00:00", "updated": "2025-10-20T15:55:28+00:00", "pdf_url": "https://arxiv.org/pdf/2510.12943v2", "source_articles": ["96756"], "fetched_at": "2026-01-19T12:41:25.354874"}
{"arxiv_id": "2510.13195", "title": "Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization for LLM-empowered Agent in Social Simulation", "authors": ["Qun Ma", "Xiao Xue", "Xuwen Zhang", "Zihan Zhao", "Yuwei Guo", "Ming Zhang"], "summary": "The advent of large language models (LLMs) has enabled agents to represent virtual humans in societal simulations, facilitating diverse interactions within complex social systems. However, existing LLM-based agents exhibit severe limitations in affective cognition: They fail to simulate the bounded rationality essential for bridging virtual and real-world services; They lack empirically validated integration mechanisms embedding emotions within agent decision architectures. This paper constructs an emotional cognition framework incorporating desire generation and objective management, designed to achieve emotion alignment between LLM-based agents and humans, modeling the complete decision-making process of LLM-based agents, encompassing state evolution, desire generation, objective optimization, decision generation, and action execution. This study implements the proposed framework within our proprietary multi-agent interaction environment. Experimental results demonstrate that agents governed by our framework not only exhibit behaviors congruent with their emotional states but also, in comparative assessments against other agent types, demonstrate superior ecological validity and generate decision outcomes that significantly more closely approximate human behavioral patterns.", "categories": ["cs.AI"], "published": "2025-10-15T06:33:11+00:00", "updated": "2025-10-15T06:33:11+00:00", "pdf_url": "https://arxiv.org/pdf/2510.13195v1", "source_articles": ["96756"], "fetched_at": "2026-01-19T12:41:28.367316"}
{"arxiv_id": "2510.13812", "title": "MindBenchAI: An Actionable Platform to Evaluate the Profile and Performance of Large Language Models in a Mental Healthcare Context", "authors": ["Bridget Dwyer", "Matthew Flathers", "Akane Sano", "Allison Dempsey", "Andrea Cipriani", "Asim H. Gazi", "Carla Gorban", "Carolyn I. Rodriguez", "Charles Stromeyer", "Darlene King", "Eden Rozenblit", "Gillian Strudwick", "Jake Linardon", "Jiaee Cheong", "Joseph Firth", "Julian Herpertz", "Julian Schwarz", "Margaret Emerson", "Martin P. Paulus", "Michelle Patriquin", "Yining Hua", "Soumya Choudhary", "Steven Siddals", "Laura Ospina Pinillos", "Jason Bantjes", "Steven Scheuller", "Xuhai Xu", "Ken Duckworth", "Daniel H. Gillison", "Michael Wood", "John Torous"], "summary": "Individuals are increasingly utilizing large language model (LLM)based tools for mental health guidance and crisis support in place of human experts. While AI technology has great potential to improve health outcomes, insufficient empirical evidence exists to suggest that AI technology can be deployed as a clinical replacement; thus, there is an urgent need to assess and regulate such tools. Regulatory efforts have been made and multiple evaluation frameworks have been proposed, however,field-wide assessment metrics have yet to be formally integrated. In this paper, we introduce a comprehensive online platform that aggregates evaluation approaches and serves as a dynamic online resource to simplify LLM and LLM-based tool assessment: MindBenchAI. At its core, MindBenchAI is designed to provide easily accessible/interpretable information for diverse stakeholders (patients, clinicians, developers, regulators, etc.). To create MindBenchAI, we built off our work developing MINDapps.org to support informed decision-making around smartphone app use for mental health, and expanded the technical MINDapps.org framework to encompass novel large language model (LLM) functionalities through benchmarking approaches. The MindBenchAI platform is designed as a partnership with the National Alliance on Mental Illness (NAMI) to provide assessment tools that systematically evaluate LLMs and LLM-based tools with objective and transparent criteria from a healthcare standpoint, assessing both profile (i.e. technical features, privacy protections, and conversational style) and performance characteristics (i.e. clinical reasoning skills).", "categories": ["cs.HC"], "published": "2025-09-05T16:24:09+00:00", "updated": "2025-09-05T16:24:09+00:00", "pdf_url": "https://arxiv.org/pdf/2510.13812v1", "source_articles": ["96756"], "fetched_at": "2026-01-19T12:41:31.379381"}
{"arxiv_id": "2510.14401", "title": "The Role of Social Learning and Collective Norm Formation in Fostering Cooperation in LLM Multi-Agent Systems", "authors": ["Prateek Gupta", "Qiankun Zhong", "Hiromu Yakura", "Thomas Eisenmann", "Iyad Rahwan"], "summary": "A growing body of multi-agent studies with Large Language Models (LLMs) explores how norms and cooperation emerge in mixed-motive scenarios, where pursuing individual gain can undermine the collective good. While prior work has explored these dynamics in both richly contextualized simulations and simplified game-theoretic environments, most LLM systems featuring common-pool resource (CPR) games provide agents with explicit reward functions directly tied to their actions. In contrast, human cooperation often emerges without full visibility into payoffs and population, relying instead on heuristics, communication, and punishment. We introduce a CPR simulation framework that removes explicit reward signals and embeds cultural-evolutionary mechanisms: social learning (adopting strategies and beliefs from successful peers) and norm-based punishment, grounded in Ostrom's principles of resource governance. Agents also individually learn from the consequences of harvesting, monitoring, and punishing via environmental feedback, enabling norms to emerge endogenously. We establish the validity of our simulation by reproducing key findings from existing studies on human behavior. Building on this, we examine norm evolution across a $2\\times2$ grid of environmental and social initialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and benchmark how agentic societies comprised of different LLMs perform under these conditions. Our results reveal systematic model differences in sustaining cooperation and norm formation, positioning the framework as a rigorous testbed for studying emergent norms in mixed-motive LLM societies. Such analysis can inform the design of AI systems deployed in social and organizational contexts, where alignment with cooperative norms is critical for stability, fairness, and effective governance of AI-mediated environments.", "categories": ["cs.MA", "cs.AI"], "published": "2025-10-16T07:59:31+00:00", "updated": "2025-10-16T07:59:31+00:00", "pdf_url": "https://arxiv.org/pdf/2510.14401v1", "source_articles": ["96756"], "fetched_at": "2026-01-19T12:41:34.389258"}
{"arxiv_id": "2510.15551", "title": "Rethinking Cross-lingual Gaps from a Statistical Viewpoint", "authors": ["Vihari Piratla", "Purvam Jain", "Darshan Singh", "Partha Talukdar", "Trevor Cohn"], "summary": "Any piece of knowledge is usually expressed in one or a handful of natural languages on the web or in any large corpus. Large Language Models (LLMs) act as a bridge by acquiring knowledge from a source language and making it accessible when queried from target languages. Prior research has pointed to a cross-lingual gap, viz., a drop in accuracy when the knowledge is queried in a target language compared to when the query is in the source language. Existing research has rationalized divergence in latent representations in source and target languages as the source of cross-lingual gap. In this work, we take an alternative view and hypothesize that the variance of responses in the target language is the main cause of this gap. For the first time, we formalize the cross-lingual gap in terms of bias-variance decomposition. We present extensive experimental evidence which support proposed formulation and hypothesis. We then reinforce our hypothesis through multiple inference-time interventions that control the variance and reduce the cross-lingual gap. We demonstrate a simple prompt instruction to reduce the response variance, which improved target accuracy by 20-25% across different models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-10-17T11:34:04+00:00", "updated": "2025-10-17T11:34:04+00:00", "pdf_url": "https://arxiv.org/pdf/2510.15551v1", "source_articles": ["97036", "97381"], "fetched_at": "2026-01-19T12:41:37.399709"}
{"arxiv_id": "2510.16173", "title": "In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions", "authors": ["Aria Pessianzadeh", "Naima Sultana", "Hildegarde Van den Bulck", "David Gefen", "Shahin Jabari", "Rezvaneh Rezapour"], "summary": "The rise of generative AI (GenAI) has impacted many aspects of human life. As these systems become embedded in everyday practices, understanding public trust in them also becomes essential for responsible adoption and governance. Prior work on trust in AI has largely drawn from psychology and human-computer interaction, but there is a lack of computational, large-scale, and longitudinal approaches to measuring trust and distrust in GenAI and large language models (LLMs). This paper presents the first computational study of Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025) spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a representative sample were combined with classification models to scale analysis. We find that Trust and Distrust are nearly balanced over time, with shifts around major model releases. Technical performance and usability dominate as dimensions, while personal experience is the most frequent reason shaping attitudes. Distinct patterns also emerge across trustors (e.g., experts, ethicists, general users). Our results provide a methodological framework for large-scale Trust analysis and insights into evolving public perceptions of GenAI.", "categories": ["cs.CL"], "published": "2025-10-17T19:33:57+00:00", "updated": "2025-10-17T19:33:57+00:00", "pdf_url": "https://arxiv.org/pdf/2510.16173v1", "source_articles": ["97036"], "fetched_at": "2026-01-19T12:41:40.414182"}
{"arxiv_id": "2510.16206", "title": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI", "authors": ["Alex Zhavoronkov", "Dominika Wilczok", "Roman Yampolskiy"], "summary": "Since the rapid expansion of large language models (LLMs), people have begun to rely on them for information retrieval. While traditional search engines display ranked lists of sources shaped by search engine optimization (SEO), advertising, and personalization, LLMs typically provide a synthesized response that feels singular and authoritative. While both approaches carry risks of bias and omission, LLMs may amplify the effect by collapsing multiple perspectives into one answer, reducing users ability or inclination to compare alternatives. This concentrates power over information in a few LLM vendors whose systems effectively shape what is remembered and what is overlooked. As a result, certain narratives, individuals or groups, may be disproportionately suppressed, while others are disproportionately elevated. Over time, this creates a new threat: the gradual erasure of those with limited digital presence, and the amplification of those already prominent, reshaping collective memory. To address these concerns, this paper presents a concept of the Right To Be Remembered (RTBR) which encompasses minimizing the risk of AI-driven information omission, embracing the right of fair treatment, while ensuring that the generated content would be maximally truthful.", "categories": ["cs.AI"], "published": "2025-10-17T20:38:12+00:00", "updated": "2025-10-22T13:56:17+00:00", "pdf_url": "https://arxiv.org/pdf/2510.16206v2", "source_articles": ["97036"], "fetched_at": "2026-01-19T12:41:43.425779"}
{"arxiv_id": "2510.19687", "title": "Are Large Language Models Sensitive to the Motives Behind Communication?", "authors": ["Addison J. Wu", "Ryan Liu", "Kerem Oktar", "Theodore R. Sumers", "Thomas L. Griffiths"], "summary": "Human communication is motivated: people speak, write, and create content with a particular communicative intent in mind. As a result, information that large language models (LLMs) and AI agents process is inherently framed by humans' intentions and incentives. People are adept at navigating such nuanced information: we routinely identify benevolent or self-serving motives in order to decide what statements to trust. For LLMs to be effective in the real world, they too must critically evaluate content by factoring in the motivations of the source -- for instance, weighing the credibility of claims made in a sales pitch. In this paper, we undertake a comprehensive study of whether LLMs have this capacity for motivational vigilance. We first employ controlled experiments from cognitive science to verify that LLMs' behavior is consistent with rational models of learning from motivated testimony, and find they successfully discount information from biased sources in a human-like manner. We then extend our evaluation to sponsored online adverts, a more naturalistic reflection of LLM agents' information ecosystems. In these settings, we find that LLMs' inferences do not track the rational models' predictions nearly as closely -- partly due to additional information that distracts them from vigilance-relevant considerations. However, a simple steering intervention that boosts the salience of intentions and incentives substantially increases the correspondence between LLMs and the rational model. These results suggest that LLMs possess a basic sensitivity to the motivations of others, but generalizing to novel real-world settings will require further improvements to these models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2025-10-22T15:35:00+00:00", "updated": "2025-10-22T15:35:00+00:00", "pdf_url": "https://arxiv.org/pdf/2510.19687v1", "source_articles": ["97036"], "fetched_at": "2026-01-19T12:41:46.435495"}
{"arxiv_id": "2510.20733", "title": "Thought Communication in Multiagent Collaboration", "authors": ["Yujia Zheng", "Zhuokai Zhao", "Zijian Li", "Yaqi Xie", "Mingze Gao", "Lizhu Zhang", "Kun Zhang"], "summary": "Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, thought communication, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale.", "categories": ["cs.LG", "cs.AI", "cs.MA"], "published": "2025-10-23T16:48:02+00:00", "updated": "2025-10-23T16:48:02+00:00", "pdf_url": "https://arxiv.org/pdf/2510.20733v1", "source_articles": ["97036"], "fetched_at": "2026-01-19T12:41:49.445384"}
{"arxiv_id": "2510.17844", "title": "Modeling Layered Consciousness with Multi-Agent Large Language Models", "authors": ["Sang Hun Kim", "Jongmin Lee", "Dongkyu Park", "So Young Lee", "Yosep Chong"], "summary": "We propose a multi-agent framework for modeling artificial consciousness in large language models (LLMs), grounded in psychoanalytic theory. Our \\textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and unconsciousness through agent interaction, guided by a Personalization Module combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning on emotionally rich dialogues, the system was evaluated across eight personalized conditions. An LLM as a judge approach showed a 71.2\\% preference for the fine-tuned model, with improved emotional depth and reduced output variance, demonstrating its potential for adaptive, personalized cognition.", "categories": ["cs.CL", "cs.AI", "cs.MA"], "published": "2025-10-10T07:08:34+00:00", "updated": "2025-10-10T07:08:34+00:00", "pdf_url": "https://arxiv.org/pdf/2510.17844v1", "source_articles": ["97036"], "fetched_at": "2026-01-19T12:41:52.456804"}
{"arxiv_id": "2510.20039", "title": "Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions", "authors": ["Yuyang Jiang", "Longjie Guo", "Yuchen Wu", "Aylin Caliskan", "Tanu Mitra", "Hua Shen"], "summary": "Large language model (LLM)-powered chatbots are increasingly used for opinion exploration. Prior research examined how LLMs alter user views, yet little work extended beyond one-way influence to address how user input can affect LLM responses and how such bi-directional influence manifests throughout the multi-turn conversations. This study investigates this dynamic through 50 controversial-topic discussions with participants (N=266) across three conditions: static statements, standard chatbot, and personalized chatbot. Results show that human opinions barely shifted, while LLM outputs changed more substantially, narrowing the gap between human and LLM stance. Personalization amplified these shifts in both directions compared to the standard setting. Analysis of multi-turn conversations further revealed that exchanges involving participants' personal stories were most likely to trigger stance changes for both humans and LLMs. Our work highlights the risk of over-alignment in human-LLM interaction and the need for careful design of personalized chatbots to more thoughtfully and stably align with users.", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "published": "2025-10-22T21:38:10+00:00", "updated": "2025-10-22T21:38:10+00:00", "pdf_url": "https://arxiv.org/pdf/2510.20039v1", "source_articles": ["97036"], "fetched_at": "2026-01-19T12:41:55.468692"}
{"arxiv_id": "2510.21623", "title": "The Universal Landscape of Human Reasoning", "authors": ["Qiguang Chen", "Jinhao Liu", "Libo Qin", "Yimeng Zhang", "Yihao Liang", "Shangxu Ren", "Chengyu Luan", "Dengyun Peng", "Hanjing Li", "Jiannan Guan", "Zheng Yan", "Jiaqi Wang", "Mengkang Hu", "Yantao Du", "Zhi Chen", "Xie Chen", "Wanxiang Che"], "summary": "Understanding how information is dynamically accumulated and transformed in human reasoning has long challenged cognitive psychology, philosophy, and artificial intelligence. Existing accounts, from classical logic to probabilistic models, illuminate aspects of output or individual modelling, but do not offer a unified, quantitative description of general human reasoning dynamics. To solve this, we introduce Information Flow Tracking (IF-Track), that uses large language models (LLMs) as probabilistic encoder to quantify information entropy and gain at each reasoning step. Through fine-grained analyses across diverse tasks, our method is the first successfully models the universal landscape of human reasoning behaviors within a single metric space. We show that IF-Track captures essential reasoning features, identifies systematic error patterns, and characterizes individual differences. Applied to discussion of advanced psychological theory, we first reconcile single- versus dual-process theories in IF-Track and discover the alignment of artificial and human cognition and how LLMs reshaping human reasoning process. This approach establishes a quantitative bridge between theory and measurement, offering mechanistic insights into the architecture of reasoning.", "categories": ["cs.CL", "cs.AI"], "published": "2025-10-24T16:26:36+00:00", "updated": "2025-10-24T16:26:36+00:00", "pdf_url": "https://arxiv.org/pdf/2510.21623v1", "source_articles": ["97381"], "fetched_at": "2026-01-19T12:41:58.480795"}
{"arxiv_id": "2510.23927", "title": "Victim as a Service: Designing a System for Engaging with Interactive Scammers", "authors": ["Daniel Spokoyny", "Nikolai Vogler", "Xin Gao", "Tianyi Zheng", "Yufei Weng", "Jonghyun Park", "Jiajun Jiao", "Geoffrey M. Voelker", "Stefan Savage", "Taylor Berg-Kirkpatrick"], "summary": "Pig butchering, and similar interactive online scams, lower their victims' defenses by building trust over extended periods of conversation - sometimes weeks or months. They have become increasingly public losses (at least $75B by one recent study). However, because of their long-term conversational nature, they are extremely challenging to investigate at scale. In this paper, we describe the motivation, design, implementation, and experience with CHATTERBOX, an LLM-based system that automates long-term engagement with online scammers, making large-scale investigations of their tactics possible. We describe the techniques we have developed to attract scam attempts, the system and LLM-engineering required to convincingly engage with scammers, and the necessary capabilities required to satisfy or evade \"milestones\" in scammers' workflow.", "categories": ["cs.CR"], "published": "2025-10-27T23:19:29+00:00", "updated": "2025-10-27T23:19:29+00:00", "pdf_url": "https://arxiv.org/pdf/2510.23927v1", "source_articles": ["97381"], "fetched_at": "2026-01-19T12:42:01.491475"}
{"arxiv_id": "2510.22042", "title": "Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models", "authors": ["Benjamin Reichman", "Adar Avsian", "Larry Heck"], "summary": "This work investigates how large language models (LLMs) internally represent emotion by analyzing the geometry of their hidden-state space. The paper identifies a low-dimensional emotional manifold and shows that emotional representations are directionally encoded, distributed across layers, and aligned with interpretable dimensions. These structures are stable across depth and generalize to eight real-world emotion datasets spanning five languages. Cross-domain alignment yields low error and strong linear probe performance, indicating a universal emotional subspace. Within this space, internal emotion perception can be steered while preserving semantics using a learned intervention module, with especially strong control for basic emotions across languages. These findings reveal a consistent and manipulable affective geometry in LLMs and offer insight into how they internalize and process emotion.", "categories": ["cs.CL", "cs.AI"], "published": "2025-10-24T21:54:12+00:00", "updated": "2025-10-24T21:54:12+00:00", "pdf_url": "https://arxiv.org/pdf/2510.22042v1", "source_articles": ["97381"], "fetched_at": "2026-01-19T12:42:04.500226"}
{"arxiv_id": "2510.26253", "title": "Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs", "authors": ["Takuma Sato", "Seiya Kawano", "Koichiro Yoshino"], "summary": "The ability to accurately interpret implied meanings plays a crucial role in human communication and language use, and language models are also expected to possess this capability. This study demonstrates that providing language models with pragmatic theories as prompts is an effective in-context learning approach for tasks to understand implied meanings. Specifically, we propose an approach in which an overview of pragmatic theories, such as Gricean pragmatics and Relevance Theory, is presented as a prompt to the language model, guiding it through a step-by-step reasoning process to derive a final interpretation. Experimental results showed that, compared to the baseline, which prompts intermediate reasoning without presenting pragmatic theories (0-shot Chain-of-Thought), our methods enabled language models to achieve up to 9.6\\% higher scores on pragmatic reasoning tasks. Furthermore, we show that even without explaining the details of pragmatic theories, merely mentioning their names in the prompt leads to a certain performance improvement (around 1-3%) in larger models compared to the baseline.", "categories": ["cs.CL"], "published": "2025-10-30T08:35:52+00:00", "updated": "2025-11-19T06:06:34+00:00", "pdf_url": "https://arxiv.org/pdf/2510.26253v2", "source_articles": ["97381"], "fetched_at": "2026-01-19T12:42:07.512680"}
{"arxiv_id": "2510.25779", "title": "Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets", "authors": ["Gagan Bansal", "Wenyue Hua", "Zezhou Huang", "Adam Fourney", "Amanda Swearngin", "Will Epperson", "Tyler Payne", "Jake M. Hofman", "Brendan Lucier", "Chinmay Singh", "Markus Mobius", "Akshay Nambi", "Archana Yadav", "Kevin Gao", "David M. Rothschild", "Aleksandrs Slivkins", "Daniel G. Goldstein", "Hussein Mozannar", "Nicole Immorlica", "Maya Murad", "Matthew Vogel", "Subbarao Kambhampati", "Eric Horvitz", "Saleema Amershi"], "summary": "As LLM agents advance, they are increasingly mediating economic decisions, ranging from product discovery to transactions, on behalf of users. Such applications promise benefits but also raise many questions about agent accountability and value for users. Addressing these questions requires understanding how agents behave in realistic market conditions. However, previous research has largely evaluated agents in constrained settings, such as single-task marketplaces (e.g., negotiation) or structured two-agent interactions. Real-world markets are fundamentally different: they require agents to handle diverse economic activities and coordinate within large, dynamic ecosystems where multiple agents with opaque behaviors may engage in open-ended dialogues. To bridge this gap, we investigate two-sided agentic marketplaces where Assistant agents represent consumers and Service agents represent competing businesses. To study these interactions safely, we develop Magentic-Marketplace -- a simulated environment where Assistants and Services can operate. This environment enables us to study key market dynamics: the utility agents achieve, behavioral biases, vulnerability to manipulation, and how search mechanisms shape market outcomes. Our experiments show that frontier models can approach optimal welfare -- but only under ideal search conditions. Performance degrades sharply with scale, and all models exhibit severe first-proposal bias, creating 10-30x advantages for response speed over quality. These findings reveal how behaviors emerge across market conditions, informing the design of fair and efficient agentic marketplaces.", "categories": ["cs.MA", "cs.AI"], "published": "2025-10-27T18:35:59+00:00", "updated": "2025-10-27T18:35:59+00:00", "pdf_url": "https://arxiv.org/pdf/2510.25779v1", "source_articles": ["97381"], "fetched_at": "2026-01-19T12:42:10.523457"}
{"arxiv_id": "2510.24772", "title": "Confidence is Not Competence", "authors": ["Debdeep Sanyal", "Manya Pandey", "Dhruv Kumar", "Saurabh Deshpande", "Murari Mandal"], "summary": "Large language models (LLMs) often exhibit a puzzling disconnect between their asserted confidence and actual problem-solving competence. We offer a mechanistic account of this decoupling by analyzing the geometry of internal states across two phases - pre-generative assessment and solution execution. A simple linear probe decodes the internal \"solvability belief\" of a model, revealing a well-ordered belief axis that generalizes across model families and across math, code, planning, and logic tasks. Yet, the geometries diverge - although belief is linearly decodable, the assessment manifold has high linear effective dimensionality as measured from the principal components, while the subsequent reasoning trace evolves on a much lower-dimensional manifold. This sharp reduction in geometric complexity from thought to action mechanistically explains the confidence-competence gap. Causal interventions that steer representations along the belief axis leave final solutions unchanged, indicating that linear nudges in the complex assessment space do not control the constrained dynamics of execution. We thus uncover a two-system architecture - a geometrically complex assessor feeding a geometrically simple executor. These results challenge the assumption that decodable beliefs are actionable levers, instead arguing for interventions that target the procedural dynamics of execution rather than the high-level geometry of assessment.", "categories": ["cs.CL", "cs.AI"], "published": "2025-10-24T17:22:48+00:00", "updated": "2025-10-24T17:22:48+00:00", "pdf_url": "https://arxiv.org/pdf/2510.24772v1", "source_articles": ["97381"], "fetched_at": "2026-01-19T12:42:13.534457"}
{"arxiv_id": "2511.18394", "title": "Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking", "authors": ["Chinmay Karkar", "Paras Chopra"], "summary": "Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.", "categories": ["cs.LG"], "published": "2025-11-23T10:41:19+00:00", "updated": "2025-11-23T10:41:19+00:00", "pdf_url": "https://arxiv.org/pdf/2511.18394v1", "source_articles": ["99328"], "fetched_at": "2026-01-19T12:42:16.543968"}
{"arxiv_id": "2512.06106", "title": "Future You: Designing and Evaluating Multimodal AI-generated Digital Twins for Strengthening Future Self-Continuity", "authors": ["Constanze Albrecht", "Chayapatr Archiwaranguprok", "Rachel Poonsiriwong", "Awu Chen", "Peggy Yin", "Monchai Lertsutthiwong", "Kavin Winson", "Hal Hershfield", "Pattie Maes", "Pat Pataranutaporn"], "summary": "What if users could meet their future selves today? AI-generated future selves simulate meaningful encounters with a digital twin decades in the future. As AI systems advance, combining cloned voices, age-progressed facial rendering, and autobiographical narratives, a central question emerges: Does the modality of these future selves alter their psychological and affective impact? How might a text-based chatbot, a voice-only system, or a photorealistic avatar shape present-day decisions and our feeling of connection to the future? We report a randomized controlled study (N=92) evaluating three modalities of AI-generated future selves (text, voice, avatar) against a neutral control condition. We also report a systematic model evaluation between Claude 4 and three other Large Language Models (LLMs), assessing Claude 4 across psychological and interaction dimensions and establishing conversational AI quality as a critical determinant of intervention effectiveness. All personalized modalities strengthened Future Self-Continuity (FSC), emotional well-being, and motivation compared to control, with avatar producing the largest vividness gains, yet with no significant differences between formats. Interaction quality metrics, particularly persuasiveness, realism, and user engagement, emerged as robust predictors of psychological and affective outcomes, indicating that how compelling the interaction feels matters more than the form it takes. Content analysis found thematic patterns: text emphasized career planning, while voice and avatar facilitated personal reflection. Claude 4 outperformed ChatGPT 3.5, Llama 4, and Qwen 3 in enhancing psychological, affective, and FSC outcomes.", "categories": ["cs.HC", "cs.AI"], "published": "2025-12-05T19:24:18+00:00", "updated": "2025-12-05T19:24:18+00:00", "pdf_url": "https://arxiv.org/pdf/2512.06106v1", "source_articles": ["99328"], "fetched_at": "2026-01-19T12:42:19.555590"}
{"arxiv_id": "2512.06033", "title": "Sell Data to AI Algorithms Without Revealing It: Secure Data Valuation and Sharing via Homomorphic Encryption", "authors": ["Michael Yang", "Ruijiang Gao", "Zhiqiang", "Zheng"], "summary": "The rapid expansion of Artificial Intelligence is hindered by a fundamental friction in data markets: the value-privacy dilemma, where buyers cannot verify a dataset's utility without inspection, yet inspection may expose the data (Arrow's Information Paradox). We resolve this challenge by introducing the Trustworthy Influence Protocol (TIP), a privacy-preserving framework that enables prospective buyers to quantify the utility of external data without ever decrypting the raw assets. By integrating Homomorphic Encryption with gradient-based influence functions, our approach allows for the precise, blinded scoring of data points against a buyer's specific AI model. To ensure scalability for Large Language Models (LLMs), we employ low-rank gradient projections that reduce computational overhead while maintaining near-perfect fidelity to plaintext baselines, as demonstrated across BERT and GPT-2 architectures. Empirical simulations in healthcare and generative AI domains validate the framework's economic potential: we show that encrypted valuation signals achieve a high correlation with realized clinical utility and reveal a heavy-tailed distribution of data value in pre-training corpora where a minority of texts drive capability while the majority degrades it. These findings challenge prevailing flat-rate compensation models and offer a scalable technical foundation for a meritocratic, secure data economy.", "categories": ["cs.CR", "econ.GN"], "published": "2025-12-04T16:35:09+00:00", "updated": "2025-12-04T16:35:09+00:00", "pdf_url": "https://arxiv.org/pdf/2512.06033v1", "source_articles": ["99328"], "fetched_at": "2026-01-19T12:42:22.566004"}
{"arxiv_id": "2512.07474", "title": "Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels", "authors": ["Yifei Huang", "Tianyu Yan", "Sitong Gong", "Xiwei Gao", "Caixin Kang", "Ruicong Liu", "Huchuan Lu", "Bo Zheng"], "summary": "We present the Living Novel, an end-to-end system that transforms any literary work into an immersive, multi-character conversational experience. This system is designed to solve two fundamental challenges for LLM-driven characters. Firstly, generic LLMs suffer from persona drift, often failing to stay in character. Secondly, agents often exhibit abilities that extend beyond the constraints of the story's world and logic, leading to both narrative incoherence (spoiler leakage) and robustness failures (frame-breaking). To address these challenges, we introduce a novel two-stage training pipeline. Our Deep Persona Alignment (DPA) stage uses data-free reinforcement finetuning to instill deep character fidelity. Our Coherence and Robustness Enhancing (CRE) stage then employs a story-time-aware knowledge graph and a second retrieval-grounded training pass to architecturally enforce these narrative constraints. We validate our system through a multi-phase evaluation using Jules Verne's Twenty Thousand Leagues Under the Sea. A lab study with a detailed ablation of system components is followed by a 5-day in-the-wild diary study. Our DPA pipeline helps our specialized model outperform GPT-4o on persona-specific metrics, and our CRE stage achieves near-perfect performance in coherence and robustness measures. Our study surfaces practical design guidelines for AI-driven narrative systems: we find that character-first self-training is foundational for believability, while explicit story-time constraints are crucial for sustaining coherent, interruption-resilient mobile-web experiences.", "categories": ["cs.HC", "cs.CL"], "published": "2025-12-08T11:57:46+00:00", "updated": "2025-12-08T11:57:46+00:00", "pdf_url": "https://arxiv.org/pdf/2512.07474v1", "source_articles": ["99328"], "fetched_at": "2026-01-19T12:42:25.578800"}
{"arxiv_id": "2512.08082", "title": "Short-Context Dominance: How Much Local Context Natural Language Actually Needs?", "authors": ["Vala Vakilian", "Zimeng Wang", "Ankit Singh Rawat", "Christos Thrampoulidis"], "summary": "We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.", "categories": ["cs.CL", "cs.AI"], "published": "2025-12-08T22:25:00+00:00", "updated": "2025-12-08T22:25:00+00:00", "pdf_url": "https://arxiv.org/pdf/2512.08082v1", "source_articles": ["99328"], "fetched_at": "2026-01-19T12:42:28.589375"}
{"arxiv_id": "2512.08211", "title": "MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones", "authors": ["Jiaxiang Geng", "Lunyu Zhao", "Yiyi Lu", "Bing Luo"], "summary": "Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.", "categories": ["cs.LG"], "published": "2025-12-09T03:41:01+00:00", "updated": "2025-12-09T03:41:01+00:00", "pdf_url": "https://arxiv.org/pdf/2512.08211v1", "source_articles": ["99328"], "fetched_at": "2026-01-19T12:42:31.612357"}
