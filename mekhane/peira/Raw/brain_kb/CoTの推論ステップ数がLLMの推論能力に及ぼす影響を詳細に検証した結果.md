---
created: 2026-01-01T09:37:19 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/62364
author: AIDB Research
---

# CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB

> ## Excerpt
> LLMの連鎖的な思考を促す『CoT』プロンプトにおいて、推論ステップ数と効果（影響）の関係を綿密に調べた結果が報告されています。 実験では、「ステップバイステップで考え、さらに、もっと多くのステップで考えてください（Let’s think step by step, you must think more steps）」といったプロンプトが試され、推論精度が顕著に向上したことが示されています。 参…

---
LLMの連鎖的な思考を促す『CoT』プロンプトにおいて、推論ステップ数と効果（影響）の関係を綿密に調べた結果が報告されています。

実験では、「ステップバイステップで考え、さらに、もっと多くのステップで考えてください（Let’s think step by step, you must think more steps）」といったプロンプトが試され、推論精度が顕著に向上したことが示されています。

![[CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB/AIDB_62364_thum2-1024x576.jpg]]

**参照論文情報**

-   タイトル：The Impact of Reasoning Step Length on Large Language Models
-   著者：Mingyu Jin, Qinkai Yu, Dong shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du
-   所属：Northwestern University, University of Liverpool, New Jersey Institute of Technology, Rutgers University
-   URL：[https://doi.org/10.48550/arXiv.2401.04925](https://doi.org/10.48550/arXiv.2401.04925)

LLMを活用するにあたって、プロンプトの書き方を工夫することがタスクのパフォーマンスに大きな効果をもらたすことはよく知られています。

プロンプトの書き方にはいくつか有名な手法があり、最もシンプルかつ効果的だとされているものの一つに「CoT（Chain of Thought）」があります。

しかし、CoTはなぜ有効なのか、そしてどのようにLLMの性能に影響するのか詳細はまだわかっていません。

### CoT（Chain of Thought）について

LLMに複雑な問題に取り組ませる際に、人間のような論理的思考、すなわち連続的な推論プロセスを取り入れる手法として考案されたのがCoTです。

連鎖的な思考を促す機能を持つため、Chain of Thoughtと名付けられ、略してCoTと呼ばれています。

![[CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB/AIDB_62364_1-1024x466.png]]

CoTを実行するための代表的なプロンプトテンプレートは次のとおりです。

“Let’s think step by step”

すなわち「一歩一歩考えよう」という文言で、この指示を添えられるとLLMは自らの推論に中間ステップを入れ、段階的な論理的思考ができるようになるとのことです。そして、本プロンプトを指示に加えるだけで、結果的にLLMの能力は向上すると言います。

### CoTの謎

これまで、さまざまなタスクに対してCoTが有効であることは示されてきました。しかし、いくつかの疑問が残されています。それは以下のようなものです。

-   結局のところ、なぜ効果があるのか？
-   どのように使用するのが効果的なのか？

要するに、基本的なことがまだ根本的にわかっていない状況です。LLMの有効な活用方法はまだまだ未知の領域であること、試行錯誤に頼っていることが浮き彫りになっているとも言えます。そのため、内部動作レベルで構造的に理解することに対するニーズも深い状況です。

そこで今回研究者らは、推論ステップを増やすことがCoTにとって、ひいてはLLMにとって重要なのかを調査することにしました。なお、データセットは算術問題に焦点が当てられています。

**関連研究**：

1.  [算術タスクでGPT-4を圧倒的に上回るコンパクトなモデル『MathGLM』登場。やはりステップ・バイ・ステップが重要](https://ai-data-base.com/archives/55122)
2.  [ユーザープロンプトをLLMが言い換えて、LLM自身が理解しやすくする手法『RaR』](https://ai-data-base.com/archives/51160)
3.  [LLMにまず前提から尋ることで出力精度を向上させる『ステップバック・プロンプティング』と実行プロンプト](https://ai-data-base.com/archives/56671)
4.  [プロンプトの原則26ヶ条をまとめた報告](https://ai-data-base.com/archives/61417)
5.  [ChatGPTの効果的なプロンプト手法における「基本のキ」を理論とテンプレート両方で紹介](https://ai-data-base.com/archives/58361)

以下では、CoTの影響を詳細に調べるアプローチと実験結果、結論を紹介していきます。

なお、推論ステップを長くするプロンプト手法や圧縮する手法もまとめています。

## 推論ステップ数はどう影響するのか

研究者らのアプローチは、簡単にいうと、

ここから限定コンテンツ

推論ステップ数を制御して推論の精度の変化を追跡することで、両者の関連性を分析するというものでした。

ポイントは、新しい情報を追加することなく、推論ステップ数のみを増やしたり減らしたりする試みを行ったことです。

なお、ゼロショットのCoT（基本的には「一歩一歩考えよう」を添えるだけ）と、フューショットのCoTでの方法が比較されています。なおフューショットのCoTとは、プロンプトに推論の例をデモンストレーションとしていくつか含ませながらCoTを促す手法です。

論文では一般的なフューショットのCoTとして2つのアプローチが紹介されています。

**Manual-CoT**：質問と、答えにつながる推論ステップを手動で添える  
**Auto-CoT**：類似する質問の例をいくつか提示することで、モデルが推論ステップを自動的に想起する

### ゼロショットCoTで推論ステップを長くする

CoTというのは前述した通り、LLMが複雑な問題を解く時に、ステップバイステップで推論を行う方法です。そして、研究者らはCoTをゼロショットでどう改善するかを考えました。

そこで、普通のCoTだと”Let’s think step by step”となるところを、プロンプトを少し変えて、以下のようにして試しました。

“Let’s think step by step, you must think more steps”

すなわち、「ステップバイステップで考え、さらに、もっと多くのステップで考えてください」という文言です。要するに、モデルにもっと長く推論を続けるように促しているというわけです。

ゼロショットでの実験では例を示せませんが、その制約の中でモデルに深く考えさせるために、このような戦略がとられています。

### フューショットCoTで推論ステップを長くする

次にフューショットのCoTにおいて、推論のステップ数をどのように変更するのかを紹介します。

新しい情報を追加したり、既存の情報を削除することなく、単純に推論ステップの数だけを変えるために、以下のような手法が使用されました。

**（１）Think About The Word（言葉について考える）**

モデルに言葉を解釈させて知識を再構築させる戦略です。一つの言葉にはいろいろな意味がある場合が多いですが、モデルに文脈に沿った言葉の解釈をさせるのが重要です。新しい情報を加えるわけでもなく、質問に対する解像度を上げる手法です。（以下、適用例）

![[CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB/AIDB_62364_3-1024x527.png]]

**（２）Read the question again（質問をもう一度読む）**

質問を繰り返し読ませて、他のテキストの干渉を減らす方法です。「干渉を減らす」と言うと難しいですが、要するにモデルに質問を覚えさせます。

**（３）Repeat State（**現状を繰り返す**）**

質問を繰り返し読ませるのと似ていますが、長い推論の連鎖の後に、現状を軽く要約させます。モデルは記憶をシンプルにすることができて、思考の連鎖における助けになるとのことです。

**（４）Self-Verification（自己検証）**

人間は問題に答える時に自分の答えが正しいかどうかを（自分の中でもう一度）確認しますが、同じようにモデルにも基本情報に基づいて自分の回答を検証させる方法が有効のようです。

**（５）Make Equation（方程式を作る）**

数学の問題において、人間は単純化を図るために未知数（xなど）を用いて式を立てます。このように、LLMにも数学の問題に対しては方程式を作らせるのが有効です。

以上のように戦略は色々と使用されましたが、後ほど紹介する実験結果では、モデルの反応には一貫したパターンが見られたようです。

なお下の図は、コインが表か裏かというシンプルな問題を例にして、モデルが問題を解く際に各戦略を適用する様子を図解しています。

![[CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB/AIDB_62364_2-1024x670.png]]

## 実験と結果

### 実験設計

まず、課題は以下のように設定されました。

1.  CoTの推論ステップ数が、LLMのタスクにおけるパフォーマンスにどう影響しているか
2.  ステップ数だけがパフォーマンスに影響するのか
3.  フューショットでの推論ステップ数を減らすとパフォーマンスは下がるのか
4.  モデルの大きさに応じて推論ステップ数は変わるのか  
    （スケーリング現象と言う）
5.  LLMの推論能力における論理性に対して質問はどう影響するのか

データセット（すべて算術に関わる問題）：

-   MultiArith：小学1年生から中学3年生までの算術問題
-   GSM8K：小学校から大学までの算術問題
-   AQuA：算術の答えを説明する問題
-   SingleEq：単一の方程式を解く問題
-   SAVMP：算術問題に答えるため複数の推論をする問題
-   Letter：算術問題がテキストと画像で提供される
-   Coin：コインの確率に関わる算術問題
-   Strategyqa：算術問題を解くために必要な戦略を説明する問題

モデル：

-   GPT-4
-   GPT-3.5
-   text-davinci-002

CoTの適用パターン

-   ただのゼロショット（適用なし）
-   ゼロショットCoT
-   フューショットCoT
    -   Manual-CoT
    -   Auto-CoT

![[CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB/AIDB_62364_4-1024x284.png]]

なお、推論ステップの追加に関しては前章で説明した通りの手法で行われました。

逆に推論ステップを圧縮する際には、「次の2つの文を、情報を失うことなく、できるだけ簡潔に圧縮してください」“Please compress the following two sentences without losing any information, and make them as concise as possible”といったプロンプトで推論の連鎖を圧縮することを試みたそうです。（ただし、この戦略が本当に推論ステップ圧縮につながっているかは検証が必要かもしれません）

### 推論ステップ数と精度の関係

まずステップというのは、問題を解くための手順のようなもので、その手順の数が性能にどれくらい関係があるのか？というのが、知りたかったことの一つ目でした。

結論から言うと、今回の実験対象モデルに関しては総じて、ステップ数が上がるにつれて精度も上がったのでした。以下の図はその詳細を示しています。

![[CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB/AIDB_62364_6-1024x430.png]]

### 間違った答えを含むプロンプトの影響

今回の実験ではユニークなことに、プロンプトにあえて間違った答えを紛れ込ませて、LLMがどのように推論を進めるかを測定する実験も行われました。言うなれば、わざと道に迷わすようなヒントを与えてみたのです。

その結果、問題によって影響が異なることがわかりました。例えば単純な算数の問題の場合は、ヒントが間違っていても最終的に答えにたどり着きます。（下が例）

![[CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB/AIDB_62364_5-1024x249.png]]

![[CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB/AIDB_62364_8-1024x444.png]]

しかし、コインの確率のような問題の場合には、他の問題タイプと比べると比較的答えに辿り着きにくくなることがわかりました。（下グラフ参照）

![[CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB/AIDB_62364_7-1024x356.png]]

単純な算数の場合には手順が大事で、論理問題においては細部も大事だと言えるのかもしれません。

### 推論ステップの圧縮

ここまでの話で、考えの段取りを増やすことによってLLMは賢くなるという傾向が分かったかと思います。しかしここで、段取りを減らすとどうなるのかという点にも注目してみましょう。

実験では、LLMが自動で考えた思考の連鎖（Auto-CoT）と、少しのヒントを与えた場合の思考の連鎖（フューショットCoT）を両方短くしてみました。要するに、推論のステップ数が圧縮されるような工夫が行われたのです。すると、正解率がガクッと減りました。精度は、CoT適用なしのゼロショットと同レベルでした。

つまり、CoTを適用するのであればステップ数をしっかり確保しなければいけないという仮説が得られたのです。

![[CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB/AIDB_62364_9-1024x353.png]]

### 異なるサイズのモデルにおけるパフォーマンス

次に、モデルサイズとステップ数の関係がどうなっているかの実験結果を見てみましょう。

GPT-4のような基本が賢いモデルは、推論ステップ数を増やしてもあまり変化はありません。

しかし、GPT-3.5やtext-davinci-002のように、GPT-4と比べると少し見劣りするようなモデルに関しては、思考の連鎖を促すとグンと性能が向上することがわかります。

つまり少なくとも本実験に限って言えば、標準性能が悪いモデルの方がCoTを適用する効能が高いという示唆が得られています。以下の表のステップ数4までの数値を見ると明確に分かります。

しかし更なる重大な事実として、もっとステップ数を上げていくと、GPT-3.5とtext-davinci-002は推論性能が下降してしまいます。つまり、複雑で長い推論プロセスを適切に処理するためには、基盤モデルの性能が必要という結果も出ているのです。

![[CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB/AIDB_62364_10-1024x448.png]]

### 質問の仕方は影響するか

ここまで推論ステップの数ばかりに焦点を当ててきましたが、質問の書き方はどのように影響するでしょうか？「MultiArith」と「GSM8K」という巨大なデータセットを用いて、LLMが自動で考えた思考の連鎖（Auto-CoT）と、少しのヒントを与えた場合の思考の連鎖（フューショットCoT）の両方で性能を試しました。

結論としては、質問の仕方が多少変わったとしても正解率に影響はあまりないことが判明しました（下の表を参照）。

つまり、推論ステップの長さのほうが質問内容よりも重要である可能性が示唆されたのです。

![[CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果 - AIDB/AIDB_62364_11-1024x490.png]]

## 結論

結論として、本研究では、CoTの重要性についていくつかの知見が得られています。

**推論ステップと正確性の関係**

フューショットCoTにおいて、推論ステップの数と正確性には、ほぼ比例関係があることがわかりました。

**推論ステップの長さと推論能力**

重要な情報を保持しながら推論ステップを短くすると、モデルの推論能力が著しく低下する可能性もあります。

**不完全な推論でも答えに辿り着く可能性**

推論の長さが十分であれば、途中に誤ったヒントが入り込む不完全な推論であっても良好な結果が得られる場合があります。

**タスク依存**

推論ステップ数が増えた時のパフォーマンス向上度合いは、タスクによって異なります。単純なタスクは少ないステップで済みますが、複雑なタスクはより長い推論を必要とします。

**ゼロショットCoT**

「Let’s think step by step」から「Let’s think step by step, you must think more steps」に変更しただけで、LLMの推論能力が顕著に向上しました（数学問題で検証したことに注意）。

## まとめ

以上、CoTにおける推論ステップの長さとLLMの推論能力の関係についての研究を紹介しました。

ステップ数の重要性やタスク依存性についてこれまでに判明していなかった詳細な知見が報告された論文でした。LLMに複雑な問題解決をさせたい場合には、基本的には推論ステップ数を適度に増やしつつ、モデルの種類などにも注意が必要という結果でした。

検証データセットは算術にフォーカスしているため、少し偏っている結果である可能性もあります。そのため、一概に鵜呑みにはせず、ご自身の扱う問題に応じて参考にしてみてください。
