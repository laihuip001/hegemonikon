---
created: 2026-01-01T09:37:48 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/60538
author: AIDB Research
---

# DeepMindの研究者らが有効性を検証した、LLMに自ら高品質な訓練データを生成させる「自己学習」 - AIDB

> ## Excerpt
> DeepMindの研究チームは、現在の大規模言語モデル（LLM）は人間によって生成されたデータに過度に依存しており、LLMの発展にとって望ましい状況ではないと考えました。 この問題に対処するために、彼らはLLMが自律的に高品質な訓練データを生成し、データセットを自ら拡充する「自己学習」アプローチの有効性を検証しました。 実施された実験では、自己生成データによって、数学やコード生成の分野におけるLL…

---
DeepMindの研究チームは、現在の大規模言語モデル（LLM）は人間によって生成されたデータに過度に依存しており、LLMの発展にとって望ましい状況ではないと考えました。

この問題に対処するために、彼らはLLMが自律的に高品質な訓練データを生成し、データセットを自ら拡充する「自己学習」アプローチの有効性を検証しました。

実施された実験では、自己生成データによって、数学やコード生成の分野におけるLLMの能力が顕著に向上したことが確認されました。

本記事では、研究内容を詳しく見ていきます。

![[DeepMindの研究者らが有効性を検証した、LLMに自ら高品質な訓練データを生成させる「自己学習」 - AIDB/AIDB_60538-1024x576.jpg]]

**参照論文情報**

-   タイトル：Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
-   著者：Avi Singh et al.（多数）
-   所属：Google DeepMind
-   URL：[https://doi.org/10.48550/arXiv.2312.06585](https://doi.org/10.48550/arXiv.2312.06585)

## 自己学習の登場以前における課題

この研究の背景として、既存の言語モデルが人間生成データに依存しているゆえの限界があります。新しい学習法の必要性につながる流れです。

### 言語モデルの人間生成データへの依存

現状の言語モデルは、人間が生成したデータによってファインチューニングする方法が一般的なアプローチになっています。しかしこの方法では、データの量と多様性、質によってモデルの限界が決まってしまいます。

### データ収集の課題

高品質な人間生成データを収集すれば問題ない（あるいは収集するのが鍵である）とも言われていますが、複雑な問題解決タスクにおいてデータ収集は大きなボトルネックとなりえます。多大なリソースと専門知識をもってしても、完全な解決に至らないことがあり、課題ははっきりしているのにプロジェクトが前に進まないといった状況に陥る恐れがあります。

そのため、仮にモデル生成データの品質が保証されれば、拡張性とコスト効率の面で有望な代替手段となりえます。

### 既存の自己教師あり学習法の限界

既存の自己教師あり学習法である監督されたファインチューニング（Supervised Fine-Tuning, SFT）にもネックとなる要因があります。この学習法においては、データセットから人間生成の出力に対して、負の損失を最小化することでポリシーを訓練しています。

オンライン強化学習（RL）メソッドを使用して損失を直接最適化するには、訓練中に何度もポリシーの更新とサンプリングが必要となります。しかし、新しいサンプルの連続的なフローに対するファインチューニングの計算コストは、時に大規模なポリシーネットワークでの大きな障壁になります。

そこでReST（Reinforced Self-Training）という新しい自己学習手法が台頭しています。今回、DeepMindの研究者らは、ReSTのアプローチに工夫を加え、正しさを検証しやすい分野で有効性の検証を行うことにしました。

**本記事の関連研究**：[従来の小さなニューラルネットワークでも「メタ学習」でChatGPTを凌駕するほど高度な生成AIができるとの報告、Nature誌](https://ai-data-base.com/archives/57838)

## 今回のメインアイデア

今回紹介する研究のメインアイデアは、言語モデルが自ら高品質なデータを生成する自己学習のアプローチを工夫し、有効性を確かめることです。人間生成データへの依存を減らし、問題解決能力を向上させるための試みです。

### 自己学習アプローチの導入

前述したように、従来の言語モデルは、人間生成データにファインチューニングされていますが、その性能は人間データによって左右されてしまいます。

そこでDeepMindの研究者らは、まずは数学問題など正しさを検証できるタスクにおいて、人間データを超えることができるかを探求することにしました。

### ReST（Reinforced Self-Training）の概要

研究者らが考案した自己学習アプローチは、モデルが自身でサンプルを生成し、これらのサンプルをスコアリングメカニズムで評価するものです。

このアプローチは期待値最大化に基づく強化学習の一種です。期待値ステップ（E-step）と最大化ステップ（M-step）を交互に行うことで達成されるとのことです。

**生成（E-step）**：言語モデルは各入力に対して複数の出力サンプルを生成します。

**改善（M-step）**：元の言語モデルを、前の生成ステップからのトレーニングデータセットに基づいて教師ありファインチューニングし、次の生成ステップで使用します。

### ReSTの応用とその成果

ReSTは、これまでにも、機械翻訳、意味解析、好みの整合性、基本的な推論など、多様なドメインでの言語モデルの強化に成功しています。

今回は、数学的推論能力とコード生成能力の検証に用いられ、ReSTにより能力が大幅に向上したことが明らかになりました（詳細は後述します）。モデル生成データに基づいてファインチューニングされたモデルは、人間生成データに基づいてトレーニングされたモデルに比べて顕著に大きな性能向上を達成しているとのことです。

要するに、基本的なReST自体は以前から存在するアプローチですが、これまでは限定的な範囲や状況で評価されていました。しかしDeepMindのこの研究は、より広範囲の問題解決タスクでReSTの効果を示すことに成功しています。

**本記事の関連研究**：[AGI（汎用人工知能）の原則6箇条とレベル5段階](https://ai-data-base.com/archives/58565)

## 実験された内容のまとめ

### 使用されたデータセット

実験は、

ここから限定コンテンツ

数学問題解決においてはHendrycks’ MATHデータセットと、コード生成においてはAPPS（Introductory）データセットを使用して行われました。

MATHデータセットには7,500の問題が含まれ、APPSデータセットには2,342の問題が含まれています。モデルの出力を自動的に正誤評価するのに適しているタスクであり、ReST（の有効性を検証すること）に向いていると考えられました。

### 使用されたモデル

実験では、GoogleのPaLM 2モデル（Google et al. 2023）は、Google Cloudの公開APIを通じて実行されました。

詳しいモデルのバージョンとしては、PaLM 2-S（Bison）、PaLM 2-S\*（Codey）、およびPaLM 2-L（Unicorn）が使用されています。

### 評価方法

ReSTによる自己学習の一般化性能は、MATHおよびAPPS（Introductory）データセットのテスト分割を使用して評価されました。

また転移性能の測定には、GSM8K（Cobbe et al. 2021）、ハンガリー高校期末試験（Paster 2023）、およびHumanEval（Chen et al. 2021）データセットが使用されました。

また、モデルの一般能力評価には、Big-Bench Hard（Suzgun et al. 2022）ベンチマークが使用されました。

### 実装の詳細

ReSTの各イテレーション（訓練の繰り返し）では、問題ごとに特定の数、解答が生成されました。なお、top-Kサンプリング（K=40）と温度0.7が使用されました。

生成されたすべての解答を直接使用すると、データセットが不均衡になる可能性があるため、問題ごとの解答の最大数にカットオフ閾値（両データセットで10）が設定されました。

ファインチューニングでは、フューショットプロンプト（および質問）をモデルの入力として使用し、モデル生成の解答をターゲットとして使用しました。ターゲットには次のトークン予測損失（Equation 1）のみが適用されました。

**本記事の関連研究**：[OpenAI、大規模言語モデルの数学能力を大きく向上させることに成功](https://ai-data-base.com/archives/52505)

## 実験結果のまとめ

![[DeepMindの研究者らが有効性を検証した、LLMに自ら高品質な訓練データを生成させる「自己学習」 - AIDB/AIDB_60538_1-1024x421.png]]

### 数学問題解決タスクの結果

ReSTを使用して訓練されたモデルは、数学のテストセットおよびGSM8Kにおける転移性能の両方で、複数のイテレーションによる成果が確認されました。

MATHデータセットでは、ReSTの適用によってPaLM 2-Sモデルのテスト精度が5.94%向上し、より大きなPaLM 2-Lモデルでは6.34%向上しました。

![[DeepMindの研究者らが有効性を検証した、LLMに自ら高品質な訓練データを生成させる「自己学習」 - AIDB/AIDB_60538_2-1024x488.png]]

### コード生成タスクの結果

APPSデータセットに関しては、最初のイテレーションによってほぼ改善されました。逆に、追加のイテレーションではパフォーマンスが低下する傾向にありました。

つまり、言い換えると、コード生成タスクにおいては1回の自己学習でほぼ改善が完了するため、2回目以降はやらない方がいいということが示唆されました。

これは自己学習の意味はないということではなく、タスクによってファインチューニングの回数を調整すべきだということです。

なおReSTを適用したモデルは、特に大型モデルで人間による解決策を用いたファインチューニングよりも優れた性能を示しました。

![[DeepMindの研究者らが有効性を検証した、LLMに自ら高品質な訓練データを生成させる「自己学習」 - AIDB/AIDB_60538_3-1024x413.png]]

![[DeepMindの研究者らが有効性を検証した、LLMに自ら高品質な訓練データを生成させる「自己学習」 - AIDB/AIDB_60538_4-1024x413.png]]

![[DeepMindの研究者らが有効性を検証した、LLMに自ら高品質な訓練データを生成させる「自己学習」 - AIDB/AIDB_60538_6-1024x485.png]]

### Pass@Kと多数決投票のパフォーマンス

ReSTを適用した後のPaLM 2-Lモデルの性能は、「pass@k」と「多数決投票」のパフォーマンス（予測能力を測定するための評価指標）によって評価されています。

「Pass@K」は、モデルが生成した上位K個の解答の中に正しい解答が含まれている確率を測定する指標です。

「多数決投票」は、モデルが生成した複数の解答の中から最も可能性が高いと判断される解答を選ぶ指標です。

上記のパフォーマンスが、ベースモデルよりも強くなったことが確認されています。

![[DeepMindの研究者らが有効性を検証した、LLMに自ら高品質な訓練データを生成させる「自己学習」 - AIDB/AIDB_60538_5-1024x362.png]]

### Big-Bench Hardタスクスイートにおけるパフォーマンス

ReSTを使って改善されたモデルは、Big-Bench Hardタスクスイートと呼ばれるテストで、元々のモデルと比べても性能が低下することはありませんでした。要するに、新しい学習方法を使っても、プログラムの全体的な能力が悪くならないことを意味しています。

下記のグラフは、ReSTを適用したPaLM 2-Lモデルが多様なタスクでどれだけ効果的かBig-Bench Hardによる検証結果を示しています。

![[DeepMindの研究者らが有効性を検証した、LLMに自ら高品質な訓練データを生成させる「自己学習」 - AIDB/AIDB_60538_7-1024x409.png]]

また、トレーニング後のモデルは、「連鎖思考（CoT）プロンプト」によって元のモデルよりも良い結果を示しました。

### ハンガリー高校最終試験でのパフォーマンス

さらに、実世界の評価セットとして、2023年のハンガリー高校期末試験の数学問題でモデルを評価しました。ReSTでファインチューニングされたPaLM 2-Lモデルは良好なパフォーマンスを示し、GPT-4を除くすべての既存モデルよりも優れていたとのことです。

下記のグラフは、さまざまなモデルがハンガリーの高校最終試験とGSM8Kベンチマークでどれだけ上手くスコアを出せるかを比較しています。

![[DeepMindの研究者らが有効性を検証した、LLMに自ら高品質な訓練データを生成させる「自己学習」 - AIDB/AIDB_60538_8-1024x500.png]]

**本記事の関連研究**：[算術タスクでGPT-4を圧倒的に上回るコンパクトなモデル『MathGLM』登場。やはりステップ・バイ・ステップが重要](https://ai-data-base.com/archives/55122)

## 展望と注意点

研究者らは自己学習をLLMにとって将来性のあるアプローチとして位置付けており、さらに他の領域での試用も推奨しています。

これまでの検証は特定のモデルと限られたタスクに焦点を当てていますが、今後は適用性の範囲を広げることが重要とされています。

さらに、言語モデルにおける自己改善に関する研究は、パイプラインの手動部分を自動化することや、pass@Kパフォーマンスのギャップを縮めるためのアルゴリズム改善に焦点を当てるべきであると提案されています。

ただし、現時点ではReSTにはいくつかの限界があります。

まず、新しいタスクに取り組むためには、適度な規模のトレーニングセットが必要であり、それは恐らく初期の段階では人間によって収集される必要があります。

また、手動で設計された、または学習された報酬関数も必要です。ただし理想的には、これは自動的に設計されたほうがいいとのことです。

## まとめ

本記事では、DeepMindの研究者らが検証した「自己学習(Self-Training)」アプローチに焦点を当てています。この研究は、大規模言語モデル（LLM）が自身で高品質な訓練データを生成し、それを使用して学習することで、人間が生成したデータへの依存を減らすことを目指しています。

実験では、数学問題解決とコード生成タスクを中心に行われ、ReST（Reinforced Self-Training）という方法が用いられました。この方法はモデルが自らサンプルを生成し、それらをフィルタリングして再び学習に使用することを繰り返します。

実験結果からは、特に数学問題解決において、モデルの正答率が人間生成データを使用した場合に比べて向上することが確認されました。

しかし、覚えておくべき注意点として、新しいタスクに取り組むためのトレーニングセットの収集や、適切な報酬関数の設計などは現在は手動に依存するため、今後自動化が必要です。

このようなアプローチが多岐にわたる領域で活用されていくことが期待されますね。
