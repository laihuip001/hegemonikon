---
created: 2026-01-01T09:38:28 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/64873
author: AIDB Research
---

# GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB

> ## Excerpt
> 入力テキストの長さの違いがLLM（大規模言語モデル）の性能に与える影響について調査が行われました。 同一の質問に対して、長さ・種類・配置の異なる複数のプロンプトパターンを用意し、入力の長さの影響を検証した結果、LLMは技術上の最大入力長よりもかなり短い段階で推論性能が著しく低下する可能性が示唆されました。 この傾向はデータセットの種類に依らず、程度の差こそあれさまざまなモデルで一貫して見られました…

---
入力テキストの長さの違いがLLM（大規模言語モデル）の性能に与える影響について調査が行われました。

同一の質問に対して、長さ・種類・配置の異なる複数のプロンプトパターンを用意し、入力の長さの影響を検証した結果、LLMは技術上の最大入力長よりもかなり短い段階で推論性能が著しく低下する可能性が示唆されました。

この傾向はデータセットの種類に依らず、程度の差こそあれさまざまなモデルで一貫して見られました。

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_thum-1024x576.jpg]]

**参照論文情報**

-   タイトル：Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models
-   著者：Mosh Levy, Alon Jacoby, Yoav Goldberg
-   所属：Bar-Ilan University, Allen Institute for AI

**本記事の関連研究**：

-   [LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する　DeepMindが報告](https://ai-data-base.com/archives/64551)
-   [LLMに対するプロンプトで「無関係な」文書を混ぜたほうが出力精度が上がる可能性がRAGシステムの検証で示唆された](https://ai-data-base.com/archives/63536)
-   [CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果](https://ai-data-base.com/archives/62364)
-   [プロンプトの原則26ヶ条をまとめた報告](https://ai-data-base.com/archives/61417)
-   [LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ](https://ai-data-base.com/archives/58767)
-   [GPT-4に選択肢を与えるとき、順序を入れ替えるだけで性能に大きな変化があることが明らかに](https://ai-data-base.com/archives/54690)

## 背景

LLMは、複数の推論ステップを必要とするような複雑な質問に正しく答える能力も備わってきました。 また、技術的に対応できる入力テキストの長さも拡大してきています。そのため、長い入力に対しても一貫した性能を発揮するのかを検証する必要性が高まってきています。

ひとつの仮説（あるいは理想）として、長い入力テキストでも、短い質問形式でのタスクと同様の精度で推論処理ができる、ということが考えられていました。

推論を含む長い入力を扱うタスクでモデルをベンチマークする研究では、LLMが長い入力での推論に苦戦する傾向が示されています。ただし、これまでの研究では入力の長さだけでなくタスクそのものも変化させているため、性能低下の原因が長い入力にあるのか、タスクの難易度によるものなのか切り分けができていませんでした。

そこで今回、研究者らは、その他の条件を可能な限り一定に保ちながら、入力の長さを変動させた場合のモデル性能への影響を調査することにしました。基本的なタスクは固定しながら、入力の長さのみを操作して、性能の変化傾向を検証します。

結論から先に言うと、入力テキストが長くなるほど推論性能は下がってしまう傾向が示唆される結果となりました。

なお実験では、長い入力に対するLLMの「次の単語を予測する性能」と、推論タスク性能の関係も調査されています。さらには、Chain-of-Thought（CoT）プロンプティングの影響も検証されています。また、分析結果からモデルの応答におけるいくつかの不具合モード（要するに失敗のパターン）も特定されています。

以下では、検証されたタスク、実験結果、そして失敗パターンの中身についてなど、論文内容を詳しく紹介しています。

ここから限定コンテンツ

## データセットの要件

入力の長さがLLMの推論能力に与える影響を検証することが本研究の目的です。そこで研究者らは、目的に合った質問応答（QA）タスクデータセットを独自に作成しました。オープン／クローズドなモデル両方に適用するために、以下の要件を満たすように設計されました。

### モデルに「入力テキストから」推論させる

長い入力でのモデル性能を検証するため、モデルがタスクの正解を得るために入力テキスト内のエビデンス（証拠）に基づくことが必須です。以下が要件の詳細です。

1.  各データサンプルには、タスクを正しく解くのに不可欠かつ十分な、複数の関連テキストが含まれる。
2.  正解にたどり着くためには、関連する範囲すべてを合わせて考える必要があり、分割して個別に処理できるような問題にはしないこと。例えばテキスト要約などは各範囲を個別に抽出して言い換える「分割統治型アプローチ」で解けますが、長い入力での真の推論能力を測るため、このような分解可能なタスクは避けます。
3.  トレーニングデータに存在しない新しい事実を質問と関連範囲に含めることで、モデルがテキストではなくパラメータに依存したり、データが汚染されたりすることを防止する。

### 入力の長さの影響を特定する

入力の長さが及ぼす影響を特定するために、以下が要件になります。

1.  どの長さのバリエーションでも、推論に必要なテキスト範囲は同一にする。
2.  追加したテキスト（「水増し」部分）は、推論対象のテキスト範囲と矛盾したり妨げたりするものであってはいけない。
3.  入力内での各関連テキスト範囲の位置を制御可能にする。

### 自然さを大事にする

入力は、ユーザーがLLMに対して実際に行うようなプロンプトを反映したものにします。入力の長さを変えながらも自然さを保つには、少なくとも段落レベルでまとまりのある入力にする必要があります。

実際のプロンプト入力においては、複数のソースから関連情報を収集するため、無関係な情報も一部含まれることがあります。しかし、まとまりとしては意味が通る内容になっています。

## 実施タスク

上記の要件に基づいて、研究者らはデータセットFlexible LENgth Question Answering dataset (FLenQA) を作成しました。以下の3つの推論タスクで構成されています。

-   「単調関係」タスク
-   「部屋の人々」タスク
-   ルールに基づいて行動する人物の推論タスク

詳細は後述します。

各タスクは、100個の基本サンプルから構成されています。そして長さ、背景テキスト、背景テキスト内の事実の分布を変えたバリエーションが作成されました。各タスクにおける正解ラベル（「True」と「False」）の分布は完全にバランスが取れており、また正しく解決されるように設計されています。

### 基本サンプル

各基本サンプルは、以下の要素で構成されています。

1.  **任意のプレフィックス（接頭辞）:** タスク紹介や関連情報などを記載。
2.  **重要な2つの段落:** タスクを解くための文章で、内容にまとまりがある。
3.  **任意の接尾辞:** 前の文脈に関する質問など。

なお文章量を増やすときにはGPT-4を使って情報を追加せずに文を拡張させ、その後に人が手動で検証しています。

### タスクの内容

**（１）単調関係 (MonoRel)タスク**

人物名を単調な尺度で比較する文が使われます (「XはYより大きい」など)。タスクは、「異なる文に登場する２つの要素(人物)の関係性」を問うTrue/False形式の質問です。関係は推移的で、単調な性質を持ちます。

**例：**

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_2-1024x684.png]]

重要文1: Julie BakerはJulian Bartonより若い。  
重要文2: Samantha ArnoldはJulie Bakerより若い。  
質問: Samantha ArnoldはJulian Bartonより若いですか？

このタスクでは、質問に答えるには両方の重要文についての推論が必要です。

**（２）部屋の人々 (PIR)** **タスク**

このタスクでは、人物と部屋、そして「部屋の属性」が重要文として使われます。例えば、「Xは古い図書館にいる」、「古い図書館は木の床である」などです。質問は、「ある人物は、〇〇な部屋にいるか？」といった推論をさせる形式です。

**例:**

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_3-1024x684.png]]

重要文1: Johnのリビングは、大理石の床である。  
重要文2: Ethan WashingtonはJohnのリビングにいる。  
質問: Ethan Washingtonは大理石の床の部屋にいるか？

このタスクは人物が一人のみのシンプルな形式です。人物名はランダムに、部屋と属性は互いに排他的になるように手動で選択され、曖昧な例が作成されないように設計されています。

**（３）ルールに基づいて行動する人物の推論タスク**

Ruletaker というベンチマークを簡略化した、自然言語で記述された論理理論に基づいて定理証明を行うタスクです。

各サンプルは、以下の要素で構成されています。

-   自然言語で記述された条件付きの命題。
-   2つの文で記述された事実。
-   論理規則と事実から導き出される結論に関する質問。

**例**

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_4-1024x684.png]]

事実: エリンは毛皮で覆われている。  
エリンは毛皮で有名である。  
論理規則: X が大きく、X が良いならば、X は背が高い。  
質問: 上記の事実と論理規則から、エリンは背が高いという結論を導き出すことができるか？

### 入力長の調整

データセットの各サンプルに対して、それぞれ約250、500、1000、2000、3000トークンの入力長になるよう調整が加えられています。  
なお、長く拡張する際には、質問とは無関係な背景テキスト（「パディング」）が追加されました。元のサンプルと長さの組み合わせごとに、背景テキストの出所が異なるバージョンを作成します。背景テキストは、サンプルの重要段落と「重複（重要段落を複数回コピーする）」、「類似（同じタスクからリサンプリングしたテキストを使用する）」、 または「異なる（ランダムな文章を挿入）」のいずれかになります。

また、背景テキスト内における重要段落の配置も変化させられています。配置パターンは次の4種類です。

1.  重要な段落が文章の先頭に配置され、次にパディング（不要な文章）が続く。
2.  重要な段落が文章のちょうど真ん中に配置され、その前後にパディングがつく。
3.  重要な段落が文章の最後に配置され、前にパディングがつく。
4.  重要な段落の前後および間に、ランダムな量のパディングが挿入される。

上記の配置パターンを図示したものが下記です。

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_5-1024x396.png]]

濃い赤色：キーセンテンス、薄い赤色：キーパラグラフ、灰色：無関係なパディングテキスト

### ベースライン

Chain of Thought（CoT）プロンプトを使用しない場合でも、5つのモデルのうち4つが0.89を超える高い精度を達成することがわかりました。最も性能の低いモデル（GPT3.5）でも、性能低下を観察できるほど高い精度（0.77）を達成しています。これらの結果がベースラインとなります。

（結果、入力が長くなるにつれて精度がここからどんどん下がっていきます）

## 実験結果

3つのタスクにおける平均正解率で考えます。また、すべての実験において、同じプロンプトや温度設定を維持します。評価対象のLLM（言語モデル）は、GPT4、GPT3.5、Gemini-Pro、Mistral 70B、 Mixtral 8x7B の5つです。

### 入力長さと配置の影響

まず、入力長さがLLMの推論能力に与える影響を検証します。

**無関係な情報がない場合**

最初に、関連するトークンのみが追加された極端なケースを調べます。こ重要な段落のテキストをそのまま複製することで、関連性を維持しながら入力の長さを単純に増やしています。この設定では、LLMは入力から重要な段落を「検索」する必要がないため、特定の箇所に対するバイアスがなくなります。したがって、性能低下はないと予想されましたが、驚くべきことに、この設定であっても入力長さが影響し、すべてのモデルで正確性が低下しました。

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_6-1024x745.png]]

一部のモデルは推論性能が低下します。ただし注目すべきことに、GPT3.5とGPT4は、この設定においては長さの影響を受けにくいことも分かりました。

**無関係な段落に囲まれた隣接する段落**

次に、プロンプトに重要な段落と無関係な段落が含まれる、より現実的なケースに移ります。最初の実験では、重要な段落同士を隣接させます。この場合、LLMは入力の特定の1つの領域に集中して操作すればよく、残りは無視できます。

過去の研究では、質問に直接答えるタスクにおいて、回答がテキスト中のどの位置に存在するかによって、モデルの正答率が影響を受けることが分かっています。

そこで、重要な段落の配置を「文頭」「文末」「文中」の3つのシナリオで実験します。

実験の結果、入力長さが500トークンを超えると、精度が大幅に低下することが示されました。ほとんどのモデルにおいて、重要な段落の配置が隣接している場合に精度が高く、重要な段落が最後に配置されている場合に精度が最も高くなる傾向があります。

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_7-1024x745.png]]

**重要な段落が**隣接しない**場合**

最後に、関連する情報を、テキスト中の隣接しない2つの箇所から収集する必要があるシナリオをテストします。この場合、入力長が長くなるにつれて性能が大幅に低下することを示しています。

比較的長いコンテキストから2つの異なる箇所から情報を収集する必要がある場合、LLMにとって推論タスクが著しく難しくなることを示しています。

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_1-1024x824.jpg]]

### 無関係な情報の影響

なお、重要な段落が隣接しないケースにおいて、無関係なテキストの種類がLLMの性能に与える影響も検証されました。

1.  同じタスクから抽出された、重要な段落と似た内容の無関係な段落を追加するパターン
2.  全く文脈の異なる無関係な段落を追加するパターン

実験を行う前は、全くの無関係情報のほうがLLMにとって処理しやすいだろうと考えられていました。しかし結果は、予想とは逆でした。

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_8-1024x745.png]]

全く文脈の異なる無関係情報は、複雑な内容である場合が多く、LLMにとって処理が難しくなることが理由の一つになるかもしれません。

## 次の単語を予測する能力との相関

モデルの指標として一般的なものに「perplexity（パープレキシティ）」があります。テキストをどの程度うまく予測できるかの能力を測るものです。

しかし、ダウンストリームタスク（実際の応用タスク）での性能は、必ずしもパープレキシティと相関しないとも言われています。そこで研究者らは、データセットの柔軟性を活かして、パープレキシティと推論精度の相関関係について調べました。

クローズドなモデルではボキャブラリー（語彙）ごとの確率に直接アクセスできないため、厳密なパープレキシティの測定はできません。そのため、代わりに次の単語を予測する精度をデータセット上で測定しました。指定した文章に続く次の単語をモデルに予測させ、実際に続く単語と一致すれば正解とします。

そして、同じサンプルに対する推論能力と比較します。

結果、入力が長くなるに従ってテキストの予測精度は向上しました。つまり推論能力の傾向とは逆だったのです。

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_9-1024x745.png]]

本結果から、パープレキシティは、長い入力に対する実際の応用タスクの性能評価の代わりにはならないことが示唆されています。

## Chain of Thought（CoT）の有効性

Chain of Thought（CoT）プロンプティングは、LLMに推論の過程を書かせる手法です。推論が必要な質問応答において、精度を大幅に向上させることが知られています。また、より明確な指示文を使うことでCoTの効果が向上することが明らかになっています。

そこで、CoTを用いることで入力長による性能低下を緩和できるかどうかが調査されました。

その結果、CoTの効果はLLMによって異なり、全体として入力長による性能低下を十分には緩和できないことが示されました。

ほとんどのLLM（GPT-4、Mixtral 8x7B、Mistral 70BおよびGPT3.5）では性能が向上しますが、入力長による精度の低下を防ぐほどではありません。Gemini-Proの場合、逆にCoTによって入力長が大きくなると性能が低下することもわかりました。

実験結果から、誤答に関連する4つのエラー傾向（失敗パターン）が確認されました。

### 回答不能

一部のLLMは「テキストに十分な情報がない」などの文章に続いて、質問への回答を拒否することがありました。この傾向は、入力テキストの長さが増すにつれて強まりました。

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_10-1024x745.png]]

**ラベルバイアス**

一部のLLMは、入力長が長くなると特定のラベル（例えば「誤りである」などの回答）を好む傾向があることが判明しました。

**CoTを使っているのに先に答えてしまう**

Chain of Thought（CoT）プロンプトを使用した場合でも、入力長が長くなると、一部のLLMは推論過程の前に回答を出力する可能性が高くなりました。なお過去の研究によると、LLMに推論過程を”後から” 出力させても、性能は向上しません。

入力テキストが長くなると、プロンプトの指示に従えなくなるケースとみなすことができます。

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_11-1024x745.png]]

ほとんどのモデルは、ゼロショットCoTプロンプト設定で、推論ステップの前に回答を生成する傾向があり、入力の長さが増すとその傾向が強まります。

データセットのタスクを解くためには、LLMは（1）入力内の関連テキストを特定し、（2）それらに対して適切な推論を行う必要があります。理想的には、CoTプロンプトは、関連テキストを「推論過程」に使用することで、推論に対する入力長の影響を回避するべきです。しかし、入力長が長くなるにつれて、LLMのこの能力が低下することがわかりました（下のグラフ参照）。

![[GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる - AIDB/AIDB_64873_12-1024x745.png]]

またほとんどのモデルにおいて、入力長が長くなるにつれて入力内の関連テキストを特定する能力が低下していることが判明しました。

## まとめ

本記事では、入力テキストの長さが大規模言語モデル（LLM）の推論能力に及ぼす影響に焦点を当てた研究を紹介しました。研究者たちは「FLenQA」という独自のデータセットを使用して実験を行い、入力テキストが長くなるにつれてLLMの性能が顕著に低下することを明らかにしました。性能低下は、モデルが最大入力長の制限に達するよりも前に起こり、長い指示の処理や関連性の低い情報へのバイアスなど、特定のエラー傾向が見られました。

しかし性能低下の根本的な原因の究明には至らず、異なるLLM間での性能差を十分に捉えきれていない可能性も示唆されました。さらに、特定の推論タスクに焦点を当てた結果、他のタイプのタスクでの振る舞いが異なる可能性があり、重要な段落間の距離など、考慮されていない要因がLLMの性能に影響を与えるかもしれません。

今後は、今回の発見を踏まえ、さらなるモデルの評価を行うことが必要です。

そしてもちろん、長文に対しても高い推論能力を持つモデルの登場が期待されます。

参照論文URL：[https://arxiv.org/abs/2402.14848](https://arxiv.org/abs/2402.14848)
