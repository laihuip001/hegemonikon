---
created: 2026-01-01T11:16:17 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/98282
author: AIDB Research
---

# GPT-5などの高性能LLMは実際に稼げるのか？実案件で大規模調査 人間が介入すべきタスクとは - AIDB

> ## Excerpt
> 本記事では、実際のクラウドソーシング案件を用いてLLMの実務能力を評価した事例を紹介します。 日々進化するLLMですが、「本当に仕事で稼げるレベルなのか？」という疑問を持つ方は多いのではないでしょうか。そこで、GPT-5などの最新モデルが、実際に報酬が発生した「本物の仕事」でどれほど通用するのかを検証されました。 その結果、LLM単独で完結できる仕事の限界とともに、人間が適切なタイミングで介入する…

---
本記事では、実際のクラウドソーシング案件を用いてLLMの実務能力を評価した事例を紹介します。

日々進化するLLMですが、「本当に仕事で稼げるレベルなのか？」という疑問を持つ方は多いのではないでしょうか。そこで、GPT-5などの最新モデルが、実際に報酬が発生した「本物の仕事」でどれほど通用するのかを検証されました。

その結果、LLM単独で完結できる仕事の限界とともに、人間が適切なタイミングで介入することで、成果物の質が劇的に向上するという事実が見えてきました。

![[GPT-5などの高性能LLMは実際に稼げるのか？実案件で大規模調査 人間が介入すべきタスクとは - AIDB/AIDB_98282_thum2-1024x576.png]]

**本記事の関連研究**

-   [LLMが仕事のタスク・生産性・労働需要に与える影響](https://ai-data-base.com/archives/95498)
-   [あなたのLLM依存度はどのくらいか 仕事面と感情面の12項目テストで傾向をチェック](https://ai-data-base.com/archives/91020)
-   [AIエージェントはどこまで使えるか　業務に取り入れる前に知っておきたい進化と現在地](https://ai-data-base.com/archives/89982)

## 背景

LLMは単なるチャットボットを超え、複雑な計画を立てて自律的にタスクをこなす「エージェント」としての能力を高めています 。しかし、これらが実際のビジネス現場で、プロとして「稼げる」レベルにあるのかを判断するのは容易ではありません。

よくAIの性能測定に使われている「ベンチマーク（性能評価テスト）」の多くは、あらかじめ答えが決まっている学校のペーパーテストのようなものです。これらは人工的に作られた問題であり、変化が激しく、正解が一つとは限らない実際の仕事現場のリアリティを反映できていない弱点があります。実社会で通用するかどうかを知るには、実際の経済活動に基づいた評価が必要。

そこで、クラウドソーシング市場で評価するというアイデアが浮上します。そこには架空の課題ではなく、過去に実際に人間が受注し、クライアントから報酬が支払われた「本物の仕事」が詰まっています。

以下では、プロのフリーランスがLLMの成果物をチェックし、「合格・不合格」の判定だけでなく、どこが良かったか、何が足りなかったかという詳細なフィードバックを行った結果を取り上げます。

ここから限定コンテンツ

### **忙しい人向けに、重要なポイント5選**

1.  世界最大級のクラウドソーシング「Upwork」の実案件322件を使い、LLMの実務能力を経済的な観点で厳しく評価した
2.  GPT-5などの最新モデルでも、単独でのタスク完遂率は2割〜4割程度という厳しい現実が明らかになった
3.  一方で、人間が一度フィードバックを行うと、成功率が最大で70%以上も向上するケースが確認された
4.  失敗したタスクの約2割は、人間の適切な介入によって納品可能なレベルまで「救済」されている
5.  コストと品質のバランスを考えると、完全自動化より「人間とLLMの協働」が最も利益を生むと結論づけられています

**参照文献情報**

-   タイトル：UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI
-   URL：[https://doi.org/10.48550/arXiv.2511.12306](https://doi.org/10.48550/arXiv.2511.12306)
-   著者：Darvin Yi, Teng Liu, Mattie Terzolo, Lance Hasson, Ayan Sinh, Pablo Mendes, Andrew Rabinovich
-   所属：Upwork

## 徹底した検証

それでは、具体的にどのような手順で調査が行われたのか、その手法について見ていきます。研究チームが最もこだわったのは、プロセスの全段階に人間の専門家を関与させるという点でした。

### トップクラスの専門家による評価体制

まず、データ収集や評価を担当した人々の質に感心します。一般的にこの手のアノテーション作業は、不特定多数のクラウドワーカーに安価に依頼されがちです。しかし本研究では、実際にUpworkで活躍する検証済みのエキスパートだけを採用しました。

彼らは顧客満足度100%を誇り、累計で100万ドル以上を稼ぎ出しているようなトップ層のフリーランサーたちです。会計やデータサイエンス、ライティングなど8つの専門分野ごとにチームが組まれ、それぞれの専門知識を活かして評価に参加しています。プロの仕事はプロにしか評価できないという、非常に納得感のある体制です。

### 案件の選別と安全性の確保

評価に使う仕事の選び方も慎重です。Upwork上のすべての仕事を使ったわけではありません。時給制の仕事は完了の定義が曖昧になりがちなため除外され、成果物が明確な固定報酬制の仕事のみが厳選されました。

選ばれた仕事はさらに、人間の専門家によって中身を精査されます。ここで重要になるのがタスク完遂性と個人情報のチェックです。仕事の依頼文だけで作業が完結するよう情報が揃っているか、そして個人の特定につながる情報が含まれていないかを確認し、安全かつ公平にテストできる土台を整えました。

### 曖昧さを排除する厳格な評価基準の作成

この研究の最大のハイライトと言えるのがルーブリックと呼ばれる評価基準表の作成です。実際のクライアントの依頼は曖昧なこと（たとえば「高い品質でよろしく」など）が多々ありますが、これをそのままLLMに投げても良し悪しの判定が主観的になってしまいます。そこで専門家たちは、一つの仕事につき5個から20個程度の客観的なチェック項目を作成しました。

さらに、それぞれの項目には以下のような重要度が設定されています。

-   プロジェクトの成功に不可欠であり必ず満たす必要がある必須項目
-   クライアントの要望と強く一致していることを示す重要項目
-   あると望ましいが必須ではない任意項目
-   事実誤認や不適切な内容など避けるべき地雷項目

単なる合格や不合格の二択ではなく、きめ細やかな採点をする体制にしたということです。

### 専門家による詳細なフィードバック

最後に、LLMが生成した成果物を専門家が評価します。ここでも先ほどの評価基準表が活躍します。専門家は各項目について合否を判定するだけでなく、なぜダメだったのか、どうすれば良くなるのかという具体的なフィードバックを記述します。このフィードバックデータこそが、後の実験でLLMの性能を向上させるための重要な鍵となります。

## リアルな仕事のデータセット

評価に使われたデータの中身、つまり「どんな仕事が選ばれたのか」について、もう詳しく見ていきます。

### 322件の多種多様な「生きた」仕事

今回用意されたのは、世界最大級の仕事マッチングサイトUpworkから抽出された計322件の実際の案件です。数は多くないように感じるかもしれませんが、すべて人間の専門家が手作業で精査していることを考えると、非常に濃密なデータセットだと言えます。

その内訳を見ると、エンジニアリングや建築、セールス＆マーケティング、ライティング、事務サポートなど、非常に幅広い職種が網羅されています。単にプログラムコードを書くだけ、あるいは文章を要約するだけでなく、Excelでのデータ分析や、特定のフォーマットへの入力作業といった、地味ながらも確実な遂行能力が求められる仕事が含まれているのが特徴です。

![[GPT-5などの高性能LLMは実際に稼げるのか？実案件で大規模調査 人間が介入すべきタスクとは - AIDB/AIDB_98282_1-1024x432.png]]

![[GPT-5などの高性能LLMは実際に稼げるのか？実案件で大規模調査 人間が介入すべきタスクとは - AIDB/AIDB_98282_2-1024x395.png]]

### 添付ファイルやツールの指定という「リアル」

実際の仕事では、テキストで指示されるだけとは限りません。「このPDFを参考にして」「この画像を加工して」といった具合に、ファイルが添付されることがよくあります。

今回のデータセットでも、JPGやPDF、Excelファイルなどが多くの案件に含まれています 。また、使用するツールも「Excelを使って」「Unityで」といった具体的な指定があり、LLMはただ言葉を操るだけでなく、指示されたファイルやツールを適切に扱う能力まで試されることになります。学校のテストと違い、実務では「資料が開けませんでした」では済まされない厳しさがここにあります。

![[GPT-5などの高性能LLMは実際に稼げるのか？実案件で大規模調査 人間が介入すべきタスクとは - AIDB/AIDB_98282_3-1024x597.png]]

### 4段階の重要度で評価を重み付け

先ほどの手法パートで触れた評価基準（ルーブリック）ですが、データセットとしてはさらに詳細な構造を持っています。

チェック項目には、「必須（Critical）」「重要（Important）」「任意（Optional）」、そしてやってはいけない「地雷（Pitfall）」という4つのラベルが付与されています。

たとえば、「期日を守る」は必須ですが、「フォントは見やすいものにする」は任意かもしれません。あるいは、「事実と異なるデータを捏造する」は地雷に当たります。このように項目を重み付けしてデータ化することで、LLMが「致命的なミスを犯したのか」それとも「惜しいミスだったのか」を後から詳しく分析できるようになっています。

なお、このデータセットはウェブ上に全部を公開はされていないようです。しかし下記のページに情報が追加されています。

[https://www.upwork.com/human-agent-productivity-index](https://www.upwork.com/human-agent-productivity-index)

## 実験内容と結果

実際に3つのモデルを使って行われた検証の結果と、そこから見えてきた「LLMの使いどころ」について見ていきます。

### 3つの最新モデルによる実力テスト

実験には、現在利用可能な最高性能のLLMである「Claude Sonnet 4」「Gemini 2.5 Pro」「GPT-5」が選ばれました。これらを単にチャットボットとして使うのではなく、自分で計画を立て、実行し、振り返るという自律的な動きをするエージェントとして設定しています。ただし、条件は公平かつ厳格です。インターネット検索や外部ツールの使用は禁止され、あくまでモデル自身の能力だけで勝負させました。

### 厳格な「合格」と部分点

評価には主に2つの指標が使われています。1つは完了率です。これは先ほどの評価基準のうち、必須と重要の項目をすべて満たさないと合格にならないという、非常に厳しい基準です。1つでもミスがあれば不合格となります。もう1つはルーブリック・スコアです。これは全項目のうち何割をクリアできたかという部分点のようなものです。

### 人間のフィードバックによる敗者復活戦

この実験の最大の特徴は、LLM単独の結果（一発勝負）だけでなく、人間が介入した場合の結果も計測した点です。これをHuman-in-the-Loop（HITL）と呼びます。具体的には、LLMが提出した成果物が不合格だった場合、人間の専門家が「ここは間違っている」「もっとこうすべき」というフィードバックを与えます。LLMはその指摘をもとにもう一度作業を行い、再提出します。これで合格できるかが試されました。

### 劇的な改善と「救済率」

結果は数字にはっきりと表れました。LLM単独の場合、タスクを完全にこなせた割合はClaude Sonnet 4で約40%、他の2つは約20%にとどまりました。

![[GPT-5などの高性能LLMは実際に稼げるのか？実案件で大規模調査 人間が介入すべきタスクとは - AIDB/AIDB_98282_5-1024x707.png]]

しかし、人間がフィードバックを与えて再挑戦させると、成功率は大幅に向上しました。特筆すべきは救済率というデータです。これは、最初に不合格だった仕事のうち、人間のアドバイスによって合格ラインまで引き上げられた割合を示します。実験では、失敗した仕事の約2割が、たった1回のフィードバックで納品可能なレベルに生まれ変わりました。

![[GPT-5などの高性能LLMは実際に稼げるのか？実案件で大規模調査 人間が介入すべきタスクとは - AIDB/AIDB_98282_6-1024x585.png]]

人間が介入すると時間はかかりますが、品質がその分しっかり上がるというトレードオフがあります。

![[GPT-5などの高性能LLMは実際に稼げるのか？実案件で大規模調査 人間が介入すべきタスクとは - AIDB/AIDB_98282_7.png]]

### 結局、どこでLLMを使うべきか

最後に、経済的な価値についての分析も行われています。タスクの単価と成功率、そして人件費を天秤にかけた結果、興味深い結論が導き出されました。単価の安い仕事なら、失敗してもダメージが少ないためLLMに任せるのが合理的です。逆に、非常に高額で失敗が許されない仕事は、最初から人間がやるべきです。そして、その中間にある中程度の単価の仕事こそが、LLMが下書きをして人間が修正するという協働スタイルが最も利益を生むゾーンだということが判明しました。

![[GPT-5などの高性能LLMは実際に稼げるのか？実案件で大規模調査 人間が介入すべきタスクとは - AIDB/AIDB_98282_4-1024x584.png]]

## まとめ

今回の検証を通じて見えてきたのは、LLMは決して「何でも叶える魔法の杖」ではないという現実です。最新モデルであっても、プロの厳しい基準で見ると、単独で完璧な仕事をこなせるケースはまだ限定的でした 。

しかし、そこに「人の目」が入るだけで、成果物の質は飛躍的に向上することも証明されました 。私たちに今求められているのは、LLMに仕事を奪われることを過度に恐れることではなく、むしろ、LLMを良き「パートナー」として使いこなし、的確な指示とフィードバックを与える「監督者」としてのスキルこそが、これからの時代を生き抜く鍵になるのかもしれません 。

**本記事の関連研究**

-   [LLMが仕事のタスク・生産性・労働需要に与える影響](https://ai-data-base.com/archives/95498)
-   [あなたのLLM依存度はどのくらいか 仕事面と感情面の12項目テストで傾向をチェック](https://ai-data-base.com/archives/91020)
-   [AIエージェントはどこまで使えるか　業務に取り入れる前に知っておきたい進化と現在地](https://ai-data-base.com/archives/89982)
