---
created: 2026-01-01T09:38:01 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/61597
author: AIDB Research
---

# Geminiの「常識を推論する能力」を網羅的に調査した結果 間違えやすいタイプの問題も明らかに - AIDB

> ## Excerpt
> Geminiは、テキストと画像を組み合わせたマルチモーダル推論に特化したLLMです。開発元であるGoogleは、Geminiは人間のような柔軟性と理解力を持つことを目指す存在だとしています。 本記事では、様々な常識推論タスクを通じてGeminiの性能を評価した研究を紹介します。 Geminiは登場からまだ間もなく、実験結果は貴重なデータかもしれません。 関連研究：Googleが「人間の専門家レベル…

---
Geminiは、テキストと画像を組み合わせたマルチモーダル推論に特化したLLMです。開発元であるGoogleは、Geminiは人間のような柔軟性と理解力を持つことを目指す存在だとしています。

本記事では、様々な常識推論タスクを通じてGeminiの性能を評価した研究を紹介します。

Geminiは登場からまだ間もなく、実験結果は貴重なデータかもしれません。

**関連研究**：[Googleが「人間の専門家レベルを超える最初のモデル」とする『Gemini』発表、GPT-4を凌駕](https://ai-data-base.com/archives/60035)

![[Geminiの「常識を推論する能力」を網羅的に調査した結果 間違えやすいタイプの問題も明らかに - AIDB/AIDB_61597-1024x576.jpg]]

**参照論文情報**

-   タイトル：Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models
-   著者：Yuqing Wang, Yun Zhao
-   所属：スタンフォード大学、Meta
-   URL：[https://doi.org/10.48550/arXiv.2312.17661](https://doi.org/10.48550/arXiv.2312.17661)
-   GitHub：[https://github.com/EternityYW/Gemini-Commonsense-Evaluation/](https://github.com/EternityYW/Gemini-Commonsense-Evaluation/)

## 研究に至る背景

私たち人間が日常で必要とする思考力の一つに常識推論があります。たとえば、天気予報をもとに服を選ぶ際には、気象や交通の知識や知恵が求められます。AIもこのような常識を理解するようになることが期待されています。

最近ではGPT-4VやGeminiのようなモデルが登場し、文章だけでなく画像なども理解するマルチモーダルLLM（MLLM）が注目され、常識推論の力も向上することが見込まれています。  
現状ではモデルが「常識」をどれだけうまく扱えるのかは分かっていません。世の中の研究者たちはLLMの理解力を測定するために、色々な方法を試しているところです。

**本記事の関連研究**：

-   [Geminiの高い推論能力を活かして、過去最高水準のプログラミングAI『AlphaCode 2』も誕生したとの報告](https://ai-data-base.com/archives/60201)
-   [AGIを目指すLLMのために専門家レベルの問題を集めたベンチマーク「MMMU」、GPT-4VやGemini Ultraでも正解率6割未満](https://ai-data-base.com/archives/61463)

## そもそも常識推論とはなにか

常識というのは、私たちが何気なく使っている日常の知識や考え方を指します。直感的に世界を理解するために無意識のうちに働く思考とも言えます。たとえば、曇った日に誰かが傘を持っているのを見たら、雨が降るかもしれないと私たちは予想します。また、例えば図書館などで部屋のドアが閉じられていたら、その部屋の中では静かでいるべきだと理解します。私たちは日々の生活の中で「常識」に基づいた推測を自然と行っているのです。

今回紹介する研究では、

ここから限定コンテンツ

研究者らはモデルを使ってさまざまなタイプの常識問題に挑戦しています。例えば以下のような問題が含まれています。

-   日常の知識や文脈に応じた解釈
-   物事の背後にある理由を見つける力
-   出来事の順序とそれらの関連性を把握すること
-   時間の概念、数値、物理的な法則、科学の原理を日常生活に応用すること
-   クリエイティブな問題解決
-   社会的なやり取り
-   道徳的な判断
-   画像やシーンから情報を読み取る

下の表は、LLMが常識推論する際に最も頻繁に起こす誤りのタイプを、ゼロショットとフューショットに分けて示しています。一般にLLMは文脈を誤って解釈するのが最も多いようです。

![[Geminiの「常識を推論する能力」を網羅的に調査した結果 間違えやすいタイプの問題も明らかに - AIDB/AIDB_61597_9.png]]

## 実験方法

### データセットの選択

本研究では、異なるタイプの常識推論に関連する12件のデータセットが使用されました。そのうち11件はテキストベースのデータセットで、1件はマルチモーダルデータセットでした。

テキストベースのデータセットは、文脈的推論、知識推論、倫理的推論の3つの主要なカテゴリーをカバーしています。一方マルチモーダルデータセットでは、認知レベルの視覚的理解に関する問題が含まれています。

下図は、様々な種類の常識推論タスクに関するデータセットの一覧を示しています。一般的な常識／文脈／抽象的な推論／イベント／時間／数値／物理／科学／謎解き／社会／道徳／視覚的推論といった幅広いカテゴリーが含まれています。

![[Geminiの「常識を推論する能力」を網羅的に調査した結果 間違えやすいタイプの問題も明らかに - AIDB/AIDB_61597_1.jpg]]

### モデルの選択と評価

テキストベースのデータセットによる評価では、Geminiを含む以下4つのポピュラーなLLMを使用します。

-   Llama-2-70B
-   Gemini Pro
-   GPT-3.5
-   GPT-4

マルチモーダルデータセット（VCR）では、GPT-4（V）とGemini Proが使用されます。

なお下の画像はマルチモーダルデータセットの一例で、複数の人物が崖の上にいるシーンを示しています。

![[Geminiの「常識を推論する能力」を網羅的に調査した結果 間違えやすいタイプの問題も明らかに - AIDB/AIDB_61597_8.png]]

### プロンプトの設定

テキストベースのデータセットの評価では、ゼロショットの標準プロンプトとフューショットCoTプロンプトの2つの設定を使用します。マルチモーダルデータセットでは、ゼロショットの標準プロンプトを使用して、エンドツーエンドの視覚的常識推論能力を評価します。

## 実験結果

### 全体的な性能比較

Gemini Proは、GPT-3.5よりわずかに高い精度を示し、Llama-2-70Bよりも大幅に優れていました。

また全体を通してGPT-4が他モデルよりも優れた性能を示し、ゼロショットとフューショットで顕著です。

下の表は、異なる常識推論タスクでの四つのLLMsの性能が整理されています。

![[Geminiの「常識を推論する能力」を網羅的に調査した結果 間違えやすいタイプの問題も明らかに - AIDB/AIDB_61597_2.png]]

また下の表はマルチモーダルタスクにおけるGPT-4VとGemini Proの性能比較を示しており、GPT-4Vが全てのサブタスクでGemini Pro Visionを上回っています。

![[Geminiの「常識を推論する能力」を網羅的に調査した結果 間違えやすいタイプの問題も明らかに - AIDB/AIDB_61597_3.png]]

さらに下の表は、GPT-4VとGemini Pro Visionの評価で見られた誤りの種類とその割合を示しています。文脈、空間認識、感情認識、論理に関する誤りなどが含まれています。

![[Geminiの「常識を推論する能力」を網羅的に調査した結果 間違えやすいタイプの問題も明らかに - AIDB/AIDB_61597_10.png]]

なお、GeminiとGPT-4Vの画像認識能力に関しては他にも網羅的に確かめた研究があります：[Gemini vs GPT-4V、画像認識能力でどちらが優秀なのか](https://ai-data-base.com/archives/61286)

常識推論にフォーカスしているのが、本記事で紹介している研究の特徴です。

### ドメインの影響

常識推論をカテゴリーで分類した際にも。GPT-4は全てで一貫して最高の性能を示しました。Gemini Proは、GPT-3.5と同等の性能を示しましたが、倫理的（社会的）推論においては比較的低い性能が観測されました。

下図はLLMの性能比較を表しており、カテゴリー別でのLLMの平均的な性能を示しています。

![[Geminiの「常識を推論する能力」を網羅的に調査した結果 間違えやすいタイプの問題も明らかに - AIDB/AIDB_61597_4.png]]

### マルチモーダルデータセットにおける性能

下記は、マルチモーダルデータセット上でのGPT-4VとGemini Pro Visionのサブタスクにおける性能を、質問タイプ別に比較したグラフです。すべてのサブタスクで、GPT-4VはGemini Pro Visionよりも優れた性能を示しました。

![[Geminiの「常識を推論する能力」を網羅的に調査した結果 間違えやすいタイプの問題も明らかに - AIDB/AIDB_61597_5.png]]

### 推論の正当化

LLMの推論能力を評価するために、正しい答えと間違った答えの両方に対する論理性が手動でレビューされました。GPT-4は正しい答えだけでなく間違った答えにおいても優れた論理性を示し、同様にGemini Proも優れた能力を示しました。この評価が何を意味しているかというと、両モデルは最終的には間違った回答をした場合においても、途中まではロジカルに考えることができるということです。

下のグラフは、正しい／間違った質問に対するテキストベースのデータセットでのLLMの平均推論正確性を比較しています。

![[Geminiの「常識を推論する能力」を網羅的に調査した結果 間違えやすいタイプの問題も明らかに - AIDB/AIDB_61597_6.png]]

また下の画像は、Gemini ProとGPT-3.5が質問に正しく答え、その理由を正しく説明した具体例を示しています。

![[Geminiの「常識を推論する能力」を網羅的に調査した結果 間違えやすいタイプの問題も明らかに - AIDB/AIDB_61597_7.png]]

## 展望や注意点

この研究では、Gemini Proなどのモデルの能力を、様々な常識推論タスクで評価しました。モデルは優れている一方で、まだ改善の余地があることもわかりました。例えば複雑なタイムラインや抽象的な問題は苦手のようです。

ただし、評価はデータセットに大きく依存しており、常識推論の全ての側面をカバーしているわけではありません。また、今回の分析は英語に限定されていることにも注意です。今後は常識推論をより正確に評価することにも力を入れる必要があります。そして、モデルの開発は日進月歩であり、今回の結果が覆される瞬間が遠くない将来に訪れる可能性が高いことも留意しましょう。

## まとめ

本記事では、Gemini Proを含む最先端のLLMを12の常識推論データセットを用いて評価した研究を紹介しました。

研究者らは実験によってモデルが様々な課題においてどれだけ効果的に働くかを明らかにし、同時にその限界も浮き彫りにしました。複雑なタスクや抽象的な推論を必要とする場面での課題が指摘され、今後の道筋を示しています。

Geminiは登場から間もなく、このような研究結果も使い道を考える上での一つの参考になるかもしれません。
