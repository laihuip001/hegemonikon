---
created: 2026-01-01T11:17:27 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/95848
author: AIDB Research
---

# JSONなどの構造化出力はLLMの質にどう影響するか - AIDB

> ## Excerpt
> 本記事では、構造化出力がLLMの質にどう影響するかを取り上げます。出力の形式を変えることで、モデルのふるまいがどう変わるのか。タスクや指示の内容によって、その影響は変わるのか。こうした因果関係を見ていきます。 背景 LLMの出力をJSON、XML、YAMLなどの決まった形式にそろえて返す「構造化出力」が注目されています。 モデル単体で使う時代から、複数のモジュールを組み合わせた複雑なシステムで使用…

---
本記事では、構造化出力がLLMの質にどう影響するかを取り上げます。  
出力の形式を変えることで、モデルのふるまいがどう変わるのか。タスクや指示の内容によって、その影響は変わるのか。  
こうした因果関係を見ていきます。

![[JSONなどの構造化出力はLLMの質にどう影響するか - AIDB/AIDB_95848-1024x576.png]]

## 背景

LLMの出力をJSON、XML、YAMLなどの決まった形式にそろえて返す「構造化出力」が注目されています。

モデル単体で使う時代から、複数のモジュールを組み合わせた複雑なシステムで使用する時代に変化してきているためです。

ただ、構造化出力が自由な文章形式と比べて、出力の質にどのような影響を与えるのかは、まだはっきりしていない面があります。事実性が高まるという報告もあれば、推論が弱くなるという指摘もあります。

これまで行われた研究の多くは限られた分野やタスクを対象としており、結果を広く当てはめるには注意が必要です。プロンプトの設計も揃っておらず、単純なスコアの比較だけでは見落としもあると考えられます。

そこで本記事、複数の推論タスクに対して、構造化出力がLLMのふるまいにどう影響するかを、原因と結果の関係に注目して丁寧に検証した事例を取り上げます。

ここから限定コンテンツ

**参照文献情報**

-   タイトル：Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference
-   URL：[https://doi.org/10.48550/arXiv.2509.21791](https://doi.org/10.48550/arXiv.2509.21791)
-   著者：Han Yuan, Yue Zhao, Li Zhang, Wuqiong Luo, Zheng Ma
-   所属：Global Decision Science, American Express

## 構造化出力の効果を精密に評価する方法論

構造化出力がどのような条件で有効になるかを確かめるために、各要素が結果に与える影響が統計的に分析されました。

### 三つの要素に分けて分析を実施

検証では、評価の軸として三つの要素が設定されました。  
・指示（役割の設定、状況の説明、問いの出し方）  
・出力フォーマット（自由文、JSON、XML、YAML）  
・LLMの生成結果（最終的な回答）

これら以外の要因が結果に影響しないよう、実験設計でできる限り統制が加えられています。

![[JSONなどの構造化出力はLLMの質にどう影響するか - AIDB/AIDB_95848_1-1024x441.png]]

自由文、JSON、XML、YAML

### 指示のつくりかた

検証は、ゼロショットの条件で行われました。  
つまり、モデルに対して事前の例示や外部の知識検索などは与えず、純粋に与えられた指示だけで答えを生成させています。

指示の変化は、システムプロンプトに含まれる「役割の定義」を言い換えることで行いました。  
たとえば、標準の役割として「数学教師」を与えた場合と、「探偵」や「裁判官」など異なる役割を与えた場合で、モデルのふるまいがどう変わるかを比較しました。

### 出力フォーマットはテンプレートで統一

構造化出力を促すため、プロンプト内で出力形式が明示されました。  
自由文と構造化形式が同じ構成となるよう、「推論」と「答え」の2要素を含むテンプレートが用いられました。

### 因果構造の判定

まず最初に行われたのは、出力形式や指示がLLMの出力に影響を与えているかを確かめる検定です。  
単に一つの指示だけを使って比較するのではなく、複数の指示パターンそれぞれについて検定を行い、その結果を統合するという方法が採られました。

出力形式の影響を調べるときには、使う指示はすべて同じにそろえた状態で検定が行われました。  
逆に、指示の影響を調べるときには、出力形式を固定した上で、指示だけを変えて比較しています。

上記の段階で、出力形式または指示のいずれか、あるいは両方が有意に影響していることが確認された場合、LLMの出力結果ごとにグループを分けたうえで、出力形式と指示のあいだに「隠れた関連」が残っているかが調べられるという流れです。

## 多様なデータとモデルで構造化出力の影響を検証

構造化出力の影響を確かめるために、複数のデータセットとLLMを使った検証が行われました。

### 幅広い推論タスクを含む8種類のデータセット

検証では、推論の内容が異なる7つの既存データセットが使われました。

・GSM8K（数学の文章題）  
・LLC（単語の最後の文字を連結）  
・SOT（物体の追跡）  
・GCF（事実性の検証）  
・GCC（因果関係の判断）  
・OpsEval（システム運用と障害分析）  
・XCodeEval（コードのコンパイル可否）

これらに加えて、独自のデータセットELLCが新たに作られました。

元のLLC（単語の最後の文字を連結させるタスク）は文字を抜き出すだけでしたが、ELLCではさらに並び替えを行って有効な英単語を作らせます。推論の難度が上がり、最先端のモデルでも正答率は1割未満。より挑戦的なベンチマークとして設計されています。

### 大規模から小規模まで5つのモデルを比較

主に使われたのはGPT-4oです。そのほかにも、小型のLLMとしてGPT-oss-20B、Llama-3.1-8B、Phi-3.5-mini、Gemma-2-9Bが検証に加えられました。

### 因果構造は統計的に分類された

出力形式や指示が出力に影響しているかを、統計的な検定で調べました。  
有意水準は0.05と0.1の2通り。どちらも超えなければ「影響なし」、どちらかが下回れば「影響あり」とされます。両方に影響があるときは、変数間に“見かけの関連（m-bias）”があるかも検証されました。

その結果、タスクと出力形式の組み合わせによって因果構造は大きく異なりました。

### GPT-4oは構造化出力に対して安定

GPT-4oでは48通りの組み合わせのうち、43で構造化出力の影響は見られませんでした。

しかし、残り5つのうち3つでは、指示と出力形式の両方が出力に影響を与える結果となりました。  
単純なスコア比較では見えない違いがあることがわかります。

### 外部から形式を指定する方法でも同様の結果に

形式の指定方法を変えて、GPT-4oの関数呼び出し機能も使いました。  
8つのうち5つのデータセットでは、プロンプトで指定するよりも高い出力品質が確認されました。

### 小規模モデルは影響を受けやすい

小型モデルでは、構造化出力の影響を受ける傾向が強くなっています。  
24通りのうち7つで影響ありと判定されました。GPT-4oの48中5つより割合が高くなっています。  
応答の安定性もやや低く、一部のタスクでは成功率が50％を下回ることもありました。  
モデルの規模や設計が、出力の正確さに大きく関わっているといえます。

### 問題の言い回しは出力に影響する

形式をそろえた状態では、GPT-4oは「役割のちがい」にはあまり左右されませんでした。  
一方で、文脈の説明や問題の言い回しを変えると、出力に変化が出るケースが見られました。

### 出力の安定性も確認された

LLMの出力は、温度をゼロにしても完全には一定になりません。  
そこで同じ条件で複数回の試行を行い、出力にどれだけ違いが出るかも調べました。

たとえばGCF（事実性の検証）データセットでは、1回目と同じ因果構造が2回目・3回目でも再現されました。  
ただし、試行回数を増やすと結果が揺れる可能性もあることが示されています。

## 結果から見えてきたこと

### 出力形式の影響はタスクによって変わる

構造化出力がLLMの推論に与える影響は、一律ではありませんでした。どんな指示を与えるか、どのような推論を求めるかによって、影響の出方が変わります。

### モデル内部では三つの仕組みが働いていると考えられる

公開情報をもとに、構造化出力を支えるしくみについて整理できます。  
GPT-4oでは、次の三つの技術が組み合わされていると推測されます。

・出力形式を固定するための設計ルール  
・スキーマの理解に特化したモデルの追加学習  
・スキーマに合う語だけを使う、出力時の選択制限

これらが連動することで、自由形式とは異なるふるまいが生まれていると見られます。

### 性能差の背景には、学習と出力の制約がある

構造化出力と自由形式のあいだに見られた性能の違いは、モデルの学習調整と出力時の制限に由来する可能性があります。  
基盤モデルの精度が上がれば、構造化フォーマットでも自由形式と同等の品質が出せるようになるかもしれません。

## まとめ

構造化出力がLLMに与える影響は、タスクや指示の内容によって異なることが分かりました。また、プロンプトの書き方や出力形式の指定方法によって、モデルのふるまいが変わる場面も確認されています。

ただし、小さなモデルでは出力が揺れやすい傾向がありましたが、大きなモデル（GPT-4o）では多くのケースで影響は見られませんでした。

実務へのヒントとしては、以下が言えます。

・タスクごとに出力の変化を確かめる  
・指示と出力形式を分けて検証する  
・テンプレートで比較条件をそろえる  
・関数呼び出しとプロンプト指定の両方を試す  
・指示だけでなく、文脈や問い方も変えてみる  
・小さなモデルは影響を受けやすいため注意する  
・判定があいまいなときは複数回ためす  
・設定と結果は簡単に記録しておく

日々のLLM活用で、少しずつ取り入れてみるとよいかもしれません。
