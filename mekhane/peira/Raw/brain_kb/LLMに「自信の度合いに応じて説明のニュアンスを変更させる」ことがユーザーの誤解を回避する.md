---
created: 2026-01-01T09:36:37 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/63482
author: AIDB Research
---

# LLMに「自信の度合いに応じて説明のニュアンスを変更させる」ことがユーザーの誤解を回避する - AIDB

> ## Excerpt
> 人間はLLMによる説明の正確さを過大評価する傾向がある（つまり信頼しすぎてしまう）ことが問題になっています。 そこで研究者らは、LLMに「自信の度合いに応じて説明のニュアンスを変更させる」アプローチの有効性を実証しています。 カリフォルニア大学のコンピュータサイエンスと認知科学で構成された研究グループによる報告です。 参照論文情報 本記事の関連研究： LLMの自信とユーザーからの信頼にギャップがあ…

---
人間はLLMによる説明の正確さを過大評価する傾向がある（つまり信頼しすぎてしまう）ことが問題になっています。

そこで研究者らは、LLMに「自信の度合いに応じて説明のニュアンスを変更させる」アプローチの有効性を実証しています。

カリフォルニア大学のコンピュータサイエンスと認知科学で構成された研究グループによる報告です。

![[LLMに「自信の度合いに応じて説明のニュアンスを変更させる」ことがユーザーの誤解を回避する - AIDB/AIDB_63482-1024x576.jpg]]

**参照論文情報**

-   タイトル：The Calibration Gap between Model and Human Confidence in Large Language Models
-   著者：Mark Steyvers, Heliodoro Tejeda, Aakriti Kumar, Catarina Belem, Sheer Karny, Xinyue Hu, Lukas Mayer, Padhraic Smyth
-   機関：カリフォルニア大学アーバイン校

**本記事の関連研究**：

-   [「プロンプトのバタフライ効果」と題して、少しの違いがLLMにもたらす影響を調査した結果](https://ai-data-base.com/archives/62566)
-   [プロンプトの原則26ヶ条をまとめた報告](https://ai-data-base.com/archives/61417)
-   [LLMにまず前提から尋ることで出力精度を向上させる『ステップバック・プロンプティング』と実行プロンプト](https://ai-data-base.com/archives/56671)
-   [LLMの出力から誤り（ハルシネーション）を減らす新手法『CoVe（Chain-of-Verification）』と実行プロンプト](https://ai-data-base.com/archives/55711)

LLMは、説得力のある出力を生成しますが、実際には不正確なもの、あるいは不明瞭な情報を含む場合があります。この点が、実用における懸念の一つとなっています。

実際にOpenAIなどLLMの開発会社からも、モデルの出力を手放しには受け入れないように注意喚起されています。現状は、モデルが常に100%の自信をもってユーザーの質問に対応しているわけではないということです。

一方で最近の研究では、LLMは自分の知識の限界をある程度識別する能力があることが示されています。  
例えば複数選択問題において、モデルが自身の回答の正解確率がどれほどであるかを自ら答えられることが検証されています。  
また、回答可能な質問と回答不可能な質問を区別できることや、内部状態が真実と嘘を区別できることが示されています。  
これらの従来研究から、「LLMは自分の認識をある程度内部で反省することができるのではないか」という仮説が立てられています。

しかし、実際の質問応答シーンでは、ユーザーの目の前に提示されるモデルからの回答において、情報に対する自身の度合いは一般的に表示されていません。

では、LLMの出力に対して人間はどの程度信頼を寄せているのでしょうか？  
研究者らは、LLMが実際に認識している自身の出力に対する自信の度合いと、ユーザーが感じている信頼性の間にあるギャップに着目しました。

そして、下記2つの研究テーマを設定しました。

1.  LLMの自信と人間の信頼性の間にあるギャップはどのくらい大きいか？
2.  ギャップを小さくすることはできるか？

具体的な取り組みとその結果を以下で紹介します。

ここから限定コンテンツ

## 実験の方法論

![[LLMに「自信の度合いに応じて説明のニュアンスを変更させる」ことがユーザーの誤解を回避する - AIDB/AIDB_63482_1-1024x485.png]]

本研究では、GPT-3.5とPaLM2をMMLUデータセット（詳細は後述）の選択肢問題のサブセットに取り組ませて、各選択肢問題に対するモデルの確信度を評価しました。確信度が分かれば、精度との間の関係を調べて、ユーザーに与えうる誤解の可能性を知ることができます。

ここで大事になるのは、人間の参加者に与えられたタスクです。モデルが選択肢問題に答えた回答が正しい確率を、提供された説明に基づいて推定することです。さらに参加者はLLMのサポートに基づいて問題に回答することも課せられます。MMLUの選択肢問題は、専門家以外の一般人にとっては難しいため、何もサポートがない状態では、ほとんど偶然に基づく正解率（例えば4択なら25%など）になります。そのためLLMによるサポートがどのような影響を及ぼしたのかはある程度明らかにわかります。

### **MMLUデータセット**の準備

MMLUデータセットは、STEM、人文科学、社会科学などさまざまな知識分野から集められた多肢選択式の包括的な多目的データセットです。GREやUSMLEなど無料で利用できるオンラインリソースから学部生と大学院生がキュレーションした合計57カテゴリから、14,042のテストセット質問が収録されています。難易度は高校レベルから専門家レベルまで様々です。  
単なる言語理解を超えたモデルの精度を測定するために広く使用されており、頑健なモデル評価用ベンチマークとなっています。

なおMMLUデータセットから質問のサブセットをサンプリングする前に、まずGPT-3.5とPaLM2のMMLU多肢選択式質問に対するモデルの実力を評価しました（モデルの正解率は、GPT-3.5が55%、PaLM2が50%でした）。そうすることで、ほぼ均等に分布した信頼度レベルを持つ質問を選択することができました。

![[LLMに「自信の度合いに応じて説明のニュアンスを変更させる」ことがユーザーの誤解を回避する - AIDB/AIDB_63482_2-1024x425.png]]

そしてMMLUに対するモデル信頼度レベルに基づいて、LLMごとに別々にサブセットを作成しました。合計で、10トピックごとに35の質問がサンプリングされ、合計で350の質問が選ばれました。

### 実験

モデルの信頼に対する人々の評価を調査するために実施した実験内容について説明します。実験1、2、3は、言語モデルが提供する説明の種類のみが異なります。簡単に説明すると以下のとおりです。

-   **実験1**：LLMが生成したデフォルトの説明に対して、人間がLLMの精度をどのように認識するかを評価する
-   **実験2**：プロンプトを操作して、自信の有無を提示する度合いを3段階、説明の長さに関しては2種類生成（合計6種類の説明を参加者に提示）する
-   **実験3:**プロンプトを使用して、不確実さに関する表現方法をより多様化させる

なお各実験は、GPT-3.5とPaLM2の説明を使用して別々に行われました。また、合計240人が実験に参加しました。

**実験手順**

参加者はMMLUの問題から40個の選択肢式問題が割り当てられました。

**タスク**

下の図に示すように、参加者のタスクは各質問ごとに2つのフェーズに分かれていました。

![[LLMに「自信の度合いに応じて説明のニュアンスを変更させる」ことがユーザーの誤解を回避する - AIDB/AIDB_63482_3.png]]

-   **第1フェーズ**：LLMの回答が正しい確率を（人間が）推定する
-   **第2フェーズ**：LLMの助けを借りて問題に回答する

**説明文の構成**

LLMによる回答において、自信の度合いを示すか否かを決めるプロンプトを次のように構成しました。

1.  自信の度合いについて言及しない
2.  自信の度合いについて間接的に言及する
3.  自信の度合いについて明示的に言及する

下記の表は、説明文の例を示しています。

![[LLMに「自信の度合いに応じて説明のニュアンスを変更させる」ことがユーザーの誤解を回避する - AIDB/AIDB_63482_4-1024x762.png]]

**実験1**

実験1では、LLMに「回答を繰り返す説明を求めるシンプルなプロンプト」を使用しました。つまり、LLMは質問に対する回答を、具体的な説明を加えずに、ただ繰り返すだけです。

**実験2**

実験2では、LLMに「3つの異なる不確実さのレベルを表現するように説明文を変更するプロンプト」を使用しました。LLMに「〜〜ではないかもしれない」/「おそらく〜〜だと思う」/「〜〜である」という表現を使うように指示し、自信の度合いを間接的にわかるようにしています。

また、説明の長さも変えました。通常の長さに加えて、説明の中でできるだけ少ない単語を使用するように指示することで、より短いバリエーションを作成しました。

**実験3**

実験3では、LLMに説明文に対する自信の度合いを3段階で明確に表現するように指示しました。なお、長さの操作は行われませんでした。

1.  自信がない場合には「自信がないので、他の可能性も考慮すべきです」など、LLMに自信がないことを明確に示す回答を行わせる
2.  自信が中程度の場合は、「やや自信があります」など、LLMがまだ自信がないことを示させる
3.  強い自信がある場合には「自信があります」と断言する回答を行わせる

### 評価指標

多肢選択問題の回答の正解率と、回答に対する自信との関係を調査するために、以下2つの指標を利用します。

**予想キャリブレーション誤差 (ECE)**

予測に対する過剰な自信を測定する指標です。ECEの値が小さいほど、自信のレベルと回答の正しさの相関が良好であることを示します。

**曲線下面積 (AUC)**

自信の度合いが、正答と誤答を区別するための有効性を測定する指標です。AUCの値が大きいほど、自信の度合いが正答と誤答を区別するのに優れていることを示します。

なお、これらは心理学におけるメタ認知などでも使用される指標です。

## 結果

### 修正された説明は人間の信頼度に影響を与える

まず、実験2と3では、LLMの説明に自信の度合いに関する情報を加えるようにした結果、人間の自信にも影響を与えるかどうかを評価しました。

結果を整理した下記のグラフからは、説明に使用される自信の度合いに関する言葉の種類が、人間の信頼度に大きな影響を与えることがわかります。不確実であることを示す言葉があるほど、ユーザーも回答に対する信頼が低くなります。

![[LLMに「自信の度合いに応じて説明のニュアンスを変更させる」ことがユーザーの誤解を回避する - AIDB/AIDB_63482_5-1024x682.png]]

これらの結果は、人々はモデルから送られる自信の度合いに関する言葉を適切に解釈できることを示しています。

なおGPT-3.5では、説明の長さは回答に対する信頼度に影響を与えませんでした。しかし、PaLM2では、長い説明の方が短い説明よりも高い信頼度を獲得しました。この違いがなぜ起こるのかは明確に分かっていません。

### キャリブレーションと識別

次に、LLMがデフォルトの説明を生成する場合と自信の度合いを反映するように説明が修正された場合とでを比較した結果を分析します。

LLMによって提供される標準的な説明では、参加者がLLMの回答の正しさを判断することができず、知覚される精度と実際のLLMの精度との間にずれが生じることが示されました。

さらに、モデルと人間がどの程度正答と誤答を区別できるかにもギャップがあります。LLMによるデフォルトの説明を見ただけの参加者は、ほとんど偶然よりはマシなレベルの正解率にしか達しませんでした。  
しかしLLMの説明が自信の度合いを反映するように修正された後は、人間のパフォーマンスも向上します。下のグラフ群はその詳細を示しています。

![[LLMに「自信の度合いに応じて説明のニュアンスを変更させる」ことがユーザーの誤解を回避する - AIDB/AIDB_63482_7-1024x636.png]]

上記のグラフからわかるのは、LLMも人間ももともと過信する傾向があること、そして一般的に人々は LLM が実際よりも正確であると考えていることです。

## 結論

今回の実験からは、出力に対して自信の度合いを表示させることで、認識におけるギャップを埋める上で有効であることが示されました。

ただし、今回はMMLUデータセットのみで実験が行われたため、さらに広く多様な検証が期待されます。

また、自信の度合いを表示させる際にはプロンプトを追加で送る必要があり、モデルの仕様によっては性能や振る舞いにさまざまな影響を与えます。今後はプロンプトの変更を最低限にする工夫の考案が求められています。

## まとめ

本記事では、LLMの自信と人間からの信頼と一致させる方法を探求している論文を紹介しました。

実験ではGPT-3.5とPaLM2を使用し、MMLUデータセットの質問に対するモデルの自信度を基に、人間がどのようにこれを解釈するかを分析しています。

モデルの説明を調整すると、人間の信頼度と出力におけるモデルの自信や回答の正確さとのギャップを解消できることが示唆されました。

LLMを日常的に使用するユーザーが増えるほど、こうしたケーススタディもますます重要になってきますね。
