---
created: 2026-01-01T09:37:08 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/61996
author: AIDB Research
---

# LLMに「自分自身との対戦」で能力を向上させる手法『Self-Play Fine-Tuning（SPIN）』 - AIDB

> ## Excerpt
> LLMに自身の出力をアップデートさせ続け、品質を向上させる自己学習手法の一種が考案されました。実験では様々なテストスコアが上昇したとされています。 手法の名称は『Self-Play Fine-Tuning（SPIN）』と付けられています。 本記事では手法と実験結果を見ていきます。 参照論文情報 研究背景 人間の注釈データから脱却したい LLMの実用性を高めるための調整はまだ人間の手による注釈付きデ…

---
LLMに自身の出力をアップデートさせ続け、品質を向上させる自己学習手法の一種が考案されました。実験では様々なテストスコアが上昇したとされています。

手法の名称は『Self-Play Fine-Tuning（SPIN）』と付けられています。

本記事では手法と実験結果を見ていきます。

![[LLMに「自分自身との対戦」で能力を向上させる手法『Self-Play Fine-Tuning（SPIN）』 - AIDB/AIDB_61996-1024x576.jpg]]

**参照論文情報**

-   タイトル：Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models
-   著者：Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu
-   所属：カリフォルニア大学
-   URL：[https://doi.org/10.48550/arXiv.2401.01335](https://doi.org/10.48550/arXiv.2401.01335)
-   コード：[https://github.com/uclaml/SPIN](https://github.com/uclaml/SPIN)
-   データセット：[https://huggingface.co/collections/UCLA-AGI/datasets-spin-65c3624e98d4b589bbc76f3a](https://huggingface.co/collections/UCLA-AGI/datasets-spin-65c3624e98d4b589bbc76f3a)
-   モデル：[https://huggingface.co/collections/UCLA-AGI/zephyr-7b-sft-full-spin-65c361dfca65637272a02c40](https://huggingface.co/collections/UCLA-AGI/zephyr-7b-sft-full-spin-65c361dfca65637272a02c40)
-   プロジェクトページ：[https://uclaml.github.io/SPIN/](https://uclaml.github.io/SPIN/)

## 研究背景

### 人間の注釈データから脱却したい

LLMの実用性を高めるための調整はまだ人間の手による注釈付きデータに依存しているのが現状です。よく使用されている手法はファインチューニングと言って、特定のデータセットに基づいてLLMをあとから強化する方法です。

人間の手による注釈データに依存する現状はあまり効率的とは言えません。今後は手間がかからない方法でLLMが強くなるようにできるのが理想と言えます。現在、そのような合理的なプロセスへの関心が高まっています。

そこで今回研究者らは、LLMが自分自身で強くなる方法を模索しています。追加のトレーニングデータを必要とせず、弱いモデルを強化するアプローチの探求です。

**関連研究**：[OpenAIが開発中の「人間を超えたAIを制御する」方法](https://ai-data-base.com/archives/61116)

### 先行研究

研究者らは、昔からAlpha Zeroなどは自分自身と対戦することによって能力を高めることに成功していると引き合いに出しています。さらに最近では、自己学習の研究がLLMの分野で、より盛り上がってきているとのことです。彼らはそれらの事例にインスパイアされて新しい手法を開発しようとしています。

ただし、先行研究で見られるようなバイナリフィードバックとは異なる方法をとるのが今回の肝のようです。

バイナリフィードバックとは、LLMの応答が良いか悪いかの２択でフィードバックし続ける方法で、要するにモデルの出力が適切かどうかを評価することで、賢くさせる仕組みです。これは実装しやすい一方で、大雑把な強化の仕方になるのが玉にきずのようです。

では、どんな研究が今回の手法の参考になったのでしょうか。

#### 参考1：Self-play（自己対戦）

AIが自分自身とゲームなどで対戦し、新しい戦略や戦法を学ぶようになる強化学習アプローチをSelf-playと言います。囲碁やチェスなどの分野で有効性が示されてきた手法です。自分の分身と戦ううちに、学習環境内での問題と解法の難易度や網羅性がどんどん増していき、次第に高みの境地に到達するというものです。

本アプローチをLLMに適用するのは、今回研究者らが考案した方法が（本人達曰く）初めての試みだそうです。

**関連研究**：[DeepMindの『MuZero』は、ルールをゼロから学んで極める](https://ai-data-base.com/archives/47252)

#### 参考2：LLMの合成データ

コード生成タスクを中心に、人間作成データはLLMの強化に非常に役立っています。しかし、LLMが自分自身のためにデータを作るアプローチの有効性も検証され始めています。

本論文では、合成データ研究の一環として、LLMが適切な応答を行えるようにユーザープロンプトをLLM自身が書き換える研究を例に挙げています。AIDBでもすでに紹介している『RaR』という手法です：[ユーザープロンプトをLLMが言い換えて、LLM自身が理解しやすくする手法『RaR』](https://ai-data-base.com/archives/51160)

なお、本論文では紹介されていませんが、DeepMindの研究者らによって「LLM自身の手で訓練データを生成する手法」も探求されています：[DeepMindの研究者らが有効性を検証した、LLMに自ら高品質な訓練データを生成させる「自己学習」](https://ai-data-base.com/archives/60538)

#### 参考3：カリキュラム学習

LLMに限らず深層学習は前々から、ランダムな順序よりも体系立てて順序を整えて学習させる手法の有効性が確認されてきました。カリキュラム学習と呼ばれるアプローチです。

典型的なアプローチとしては、難易度を「簡単」から「難しい」へシフトさせていくもので、人間の教育と似ています。今回、研究者らはこのカリキュラム学習の考え方も取り入れているようです。

なお、本論文では触れられていませんが、LLMのチューニングにカリキュラム学習の考え方を応用している他の研究でも効果が確認されています：[](https://ai-data-base.com/archives/61555)[人間のカリキュラム教育のような学習でLLMの性能は向上するとの報告](https://ai-data-base.com/archives/61555)

## セルフプレイでファインチューニング

今回研究者らは、

ここから限定コンテンツ

人間やAIによる追加のフィードバックを必要としないLLMのファインチューニング手法を考案しています。手法を簡単に言うと、自分自身の過去と対戦した結果をもとにファインチューニングを行なっていくというコンセプトに基づくものです。

概念図は下記になります。人間らしい応答とLLMの応答にギャップが生まれる原則を利用して、合成データをもとにLLMのパフォーマンスを継続的に上げていく方式が採用されています。

![[LLMに「自分自身との対戦」で能力を向上させる手法『Self-Play Fine-Tuning（SPIN）』 - AIDB/AIDB_61996_1-1024x552.png]]

なお本手法の名称はSelf-Play fIne-tuNingの文字を抜き出してSPINと付けられました。

SPINのプロセスを順番に並べると以下のようになります。

1.  過去の自分（イテレーション）を参照する
2.  生成したデータに対して応答し改善する
3.  上記のプロセスを繰り返す

これをアルゴリズムで表すと以下になるようです。

![[LLMに「自分自身との対戦」で能力を向上させる手法『Self-Play Fine-Tuning（SPIN）』 - AIDB/AIDB_61996_8-1024x302.png]]

上記のアルゴリズムをイメージで伝えると次のようなものだと言います。

1.  2人のLLMプレイヤーがいます。
2.  メインプレイヤーの役割は、LLMの応答と人間の応答を区別することです。
3.  対戦相手プレイヤーの役割は、人間の応答と区別することができない応答を生み出すことです。
4.  それぞれのプレイヤーは異なる反復によって育成されるLLMです。
5.  先にメインプレイヤーを更新し、次に対戦相手プレイヤーを更新するといった順序になります。
6.  そして二つのプレイヤーは、相乗効果的に、各々の役割を上手くこなせるように強化されていきます。
7.  最終的には、LLMが人間の応答と区別がつかないほど自然なテキストを生成できるようになるまで続くとのことです。

## 実験と結果

結論から述べると、SPINは従来の監督付きファインチューニングの限界を超えてモデルの出力品質をかなり良くすることがわかりました。新しい注釈付きデータを追加せずとも、反復的なトレーニングを行うだけで達成するとのことです。

### 実験条件

Mistral-7Bをベースにした zephyr-7b-sft-fullというモデルが実験対象モデルとして使用されました。このモデルに対して、UltraChat200kというデータセットからランダムにサンプリングした50kプロンプトで合成応答を生成します。

そして、前章で述べた反復トレーニングが行われました。

評価ベンチマークとしてはHuggingface Open LLM Leaderboardが使用されました。6つのデータセットが含まれるものであり、それぞれのデータセットは下記のような異なる能力を測定します。

-   **Arc**: 科学的な問題解決能力
-   **TruthfulQA**: 真実性のある回答を生成する能力
-   **Winogrande**: 文脈を理解する能力
-   **GSM8k**: 算数・数学問題を解く論理的思考力
-   **HellaSwag**: 物語や説明文の次の一文を予測する能力
-   **MMLU**: 広範な一般知識

下の表では6つのデータセットごとの本実験においての設定数値が示されています。

![[LLMに「自分自身との対戦」で能力を向上させる手法『Self-Play Fine-Tuning（SPIN）』 - AIDB/AIDB_61996_9-1024x202.png]]

### 実験結果

下のグラフは、SPINによる成果を示す実験結果の一部です。横軸が反復トレーニングのイテレーション（反復）数で、縦軸が平均スコアです。反復を繰り返すたびに性能向上の角度は下がっていきますが、継続的にスコアが上昇しています。

![[LLMに「自分自身との対戦」で能力を向上させる手法『Self-Play Fine-Tuning（SPIN）』 - AIDB/AIDB_61996_2-1024x493.png]]

データセット別の結果は下の表で確認できます。イテレーション0、つまり反復トレーニングを行う前の初回のファインチューニング時でもTruthfulQAとGSM8kベンチマークで顕著な改善が見られます。  
そして反復トレーニングの1回目では、Arc ChallengeとTruthfulQA ベンチマークで顕著に性能が向上しており、その後の反復ではさまざまなタスク能力が段階的に向上していきます。なお反復を繰り返すと改善の度合いがほとんど0になるタスクもあります。

![[LLMに「自分自身との対戦」で能力を向上させる手法『Self-Play Fine-Tuning（SPIN）』 - AIDB/AIDB_61996_10-1024x292.png]]

下のグラフは、本手法とDPO（Direct Preference Optimization）を比較した結果です。DPOとは人間のフィードバックに基づいて直接モデルを調整する手法で、要するに従来型の、人間の手による注釈データが必要になるものです。

この結果から、SPINはDPOと比べても遜色ない性能向上をもたらすものだと示唆されています。

![[LLMに「自分自身との対戦」で能力を向上させる手法『Self-Play Fine-Tuning（SPIN）』 - AIDB/AIDB_61996_3-1024x415.png]]

なお、アブレーション研究（一部を変更して影響を確かめるテスト）もいくつか行われています。その中の一つに、さらに多くのタスクに対する調査も含まれます。

因果推論（Causal Judgment）タスク、常識推論（Sports Understanding）タスク、論理推論（Formal Fallacies）タスクが試されています。

さらに、さまざまな質問に答える能力をためすMT-Benchでも評価が行われました。結果の表とグラフを下記に紹介します。このアブレーション実験においても、SPINの有効性が示されました。

![[LLMに「自分自身との対戦」で能力を向上させる手法『Self-Play Fine-Tuning（SPIN）』 - AIDB/AIDB_61996_6-1024x367.png]]

![[LLMに「自分自身との対戦」で能力を向上させる手法『Self-Play Fine-Tuning（SPIN）』 - AIDB/AIDB_61996_7-1024x474.png]]

## 結論と展望

以上の実験結果から、研究者らはSPINが弱いモデルを強くする手法として有効である可能性があると述べています。本手法SPINの最大の特徴は、追加の注釈データを必要としないことであり、従来の追加データを用いるトレーニング手法に匹敵すると考えられています。

ただし、注意点もあります。SPINが有効なのは、LLMの能力が人間によって生成されたデータが参考になる場合に限られるという点です。人間の回答とLLMの回答における品質のギャップを埋めていくプロセスが中心となっており、人間を超えるあるいは逸脱した性能を示すようにするテクニックではないということです。

今後もし超人的な力をもつようになることを目指すのであれば、目標データを人間基準ではなく新しいものにする必要があるとのことです。

## まとめ

本記事ではLLMが自ら強くなる「Self-Play Fine-Tuning（SPIN）」という手法の研究を紹介しました。

人間の注釈データを大量に必要とする従来のファインチューニング手法に匹敵する結果が出たという結論です。注意点であるように、論文で述べられている範囲においては本アプローチは人間を超えるような能力をLLMが持つようにはできていませんが、広い範囲で効果が期待できる点で大きな成果だと言えるでしょう。

なお、すでに人間基準ではない自己学習アプローチも他の研究では試されています。先行研究の章で紹介した他の事例も併せて確認してみてください。
