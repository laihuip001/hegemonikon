---
created: 2026-01-01T09:36:50 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/64057
author: AIDB Research
---

# LLMに敢えて間違わせてルールを覚えさせるプロンプト手法 Google DeepMindなどが考案 - AIDB

> ## Excerpt
> LLMが同じミスを避けるために、意図的に間違わせてルールを覚えさせる新しいプロンプト手法が提案されています。 本手法によって、モデルは自ら原則をまとめ上げ、未見の例にも適応します。実験では、GPT-3.5とGPT-4における複雑な質問応答や、数学の問題への性能向上が報告されています。 参照論文情報 本記事の関連研究： 背景 LLMが急速に進化する中、実用においては下流タスク（具体的で細分化されたタ…

---
LLMが同じミスを避けるために、意図的に間違わせてルールを覚えさせる新しいプロンプト手法が提案されています。

本手法によって、モデルは自ら原則をまとめ上げ、未見の例にも適応します。実験では、GPT-3.5とGPT-4における複雑な質問応答や、数学の問題への性能向上が報告されています。

![[LLMに敢えて間違わせてルールを覚えさせるプロンプト手法 Google DeepMindなどが考案 - AIDB/AIDB_64057-1024x576.jpg]]

**参照論文情報**

-   タイトル：In-Context Principle Learning from Mistakes
-   著者：Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, Uri Alon
-   所属：UC Berkeley, Carnegie Mellon University, Google DeepMind, AI2.

**本記事の関連研究**：

-   [CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果](https://ai-data-base.com/archives/62364)
-   [基盤モデル（GPT-4）はプロンプトの工夫で専門特化モデルに匹敵するほど性能が向上することが「医学分野」で示唆される](https://ai-data-base.com/archives/59798)
-   [LLMにまず前提から尋ることで出力精度を向上させる『ステップバック・プロンプティング』と実行プロンプト](https://ai-data-base.com/archives/56671)
-   [GPT-4などのLLMに「自らの論理的な整合性をチェック」させるフレームワーク『LogiCoT』と実行プロンプト](https://ai-data-base.com/archives/55805)

## 背景

LLMが急速に進化する中、実用においては下流タスク（具体的で細分化されたタスク）の性能向上に注目が集まっています。そこで低コストな手法であるコンテキスト内学習（In-Context learning）が注目されています。いわゆるプロンプトエンジニアリングとも呼ばれる技術分野であり、OpenAIやGoogleといった基盤モデルの提供元も理論的なプロンプト構築を推奨しています。

コンテキスト内学習の中でも、「フューショットプロンプティング（Few-Shot Prompting）」と呼ばれる手法は汎用的に有用と言われています。LLMに少数の入出力例を提示し、未知の新しい入力に対する出力を生成するよう促すアプローチです。多くのタスクで非常に効率的ですが、これまでは正解の例から学習することにフォーカスされており、間違いから学ぶという重要な戦略は十分に活用されていません。

過去に行われてきた研究では、LLMが自身の出力を反省して改善する方法（Self-RefineやOptimization by PROmpting（OPRO）など）が探求されています。しかし、直接的なフィードバックや大量の訓練データを必要とするといった課題があります。

また、否定的な例から学習を促す手法（Contrastive Chain-of-Thought）も提案されましたが、一貫した改善をもたらすことまでは示せていませんでした。

こうした背景から、研究者らは「LEAP（Learning Principles）」という新しい手法を提案しています。モデルが与えられた例から間違いを犯し、それを振り返ることを促す手法です。最終的には明確なタスク固有の原理を形成することで、類似の誤りを避けることを可能にします。

下記では具体的な方法論と実際に使用できるプロンプトテンプレート、そして本手法の性能を示す実験結果を紹介します。

## 方法論

ここから限定コンテンツ

研究者らの目標は、モデルが「下流タスクで起こり得るミスを回避するための一般的な原則を学習する」ことでした。そこで以下のような方法論を組み立てました。

まず、特定のタスクに対してフューショットの例を提供し、そして意図的にミスを生成させます。つまり各入力出力ペアに対して、多様な解答をゼロショット方式で生成し、その中から誤ったものを選び出します。

次に、LLMが自分自身の間違いについて自然言語で説明を生成するようにします。そこで得られる洞察から、低レベルの原則をまとめ上げ、それらをさらに5つの主要なポイントに集約して高レベルの原則を作成させます。

最終的に、原則（低レベルでも高レベルでも）を組み込んだ強化されたプロンプトを作成します。この一回限りの原則生成プロセスを通じて、モデルは過去のミスから得られた洞察を学習し、未知の例に対しても回答の正確さと推論力を向上させる可能性があります。

この方法論は、下記のようにアルゴリズムで簡潔に要約されています。なお後ほど紹介する実験では一貫して同一のLLMを使用していますが（手法の効果を検証しやすくするため）、理論上は各段階で異なるLLMを使用することが可能です。

![[LLMに敢えて間違わせてルールを覚えさせるプロンプト手法 Google DeepMindなどが考案 - AIDB/AIDB_64057_4-1-1024x352.png]]

このロジックを入力プロンプトテンプレートに落とし込むと以下のようになります。

![[LLMに敢えて間違わせてルールを覚えさせるプロンプト手法 Google DeepMindなどが考案 - AIDB/AIDB_64057_3-1024x536.png]]

入力プロンプトテンプレートの例：

（あらかじめ問題に対するLLMの解答を受け取り、正解を提示した上で、）

_生成された回答と正しい回答を比較して徹底的な分析を行ってください。また、生成された推論が正しい推論とどのように異なるかを観察してください。不一致、誤解、またはエラーを特定してください。この分析から導き出される明確な洞察、原則、またはガイドラインを提供して、将来の回答を改善してください。なお1つのデータポイントに焦点を当てるのではなく、一般的な原則に焦点を当ててください。_

方法論をあらためて時系列でまとめます。

1.  特定の下流タスクに対して、フューショットの入出力例を提示する
2.  フューショットの例に基づいて、意図的にミスを生成させる  
    （多様な解答を生み出し、その中から間違ったものを選ぶ）
3.  LLMに、生成された間違いについて正解との比較を自然言語での説明を生成させる
4.  生成された説明から洞察を集約し、低レベルの原則を形成させる
5.  集約された低レベルの原則をさらに分析し、約5つの主要ポイントに凝縮して高レベルの原則を作成させる
6.  作成された原則（低レベルおよび高レベル）をプロンプトに組み込む
7.  強化されたプロンプトを使用して、未見の例に対する回答を生成する
8.  生成された回答の精度と推論能力を評価し、LEAPの効果を検証する

## 実験

標準のCoT（”ステップバイステップ”で考えさせる）に基づくフューショットプロンプティングをベースラインとし、LEAPの二つのバージョン、「LEAP HIGH-LEVEL」と「LEAP LOW-LEVEL」を比較しました。

-   LEAP HIGH-LEVEL：高レベルの原則を使用する
-   LEAP LOW-LEVEL：低レベルの原則を使用する

ベンチマークタスクは以下の通りです。

-   テキスト推論：HotpotQA（複数ステップ質問応答）とDROP（数値・論理推論を要求する読解）
-   数学推論：GSM8K（学校レベルの数学単語問題）とMATH（数学コンテストの問題）
-   Big-Bench Hard：LLMの様々な推論能力を試す27の比較的難しいタスク集

評価には主に以下のモデルが使用されました。

-   GPT-3.5
-   GPT-4
-   Claude-2.1
-   Gemini Pro　など

## テキスト推論タスクの結果

LEAPはHotpotQAとDROPの両方でベースラインを上回る成果を示しています。HotpotQAで最大3.5％、DROPで7.5％の改善が見られました。

![[LLMに敢えて間違わせてルールを覚えさせるプロンプト手法 Google DeepMindなどが考案 - AIDB/AIDB_64057_t1-1024x326.png]]

### タスク別の改善

HotpotQAでは、GPT-3.5-TurboとGPT-4は一貫して改善されました。

またDROPではGPT-4の性能が大幅に改善されています（絶対値で7.5％の増加）。なおGemini Proは3％改善され、GPT-3.5-TurboとGPT-4-turboの改善は小さいものでした。

### 原則レベルの高低

ほとんどのタスクとベースモデルで、LEAP LOW-LEVELとLEAP HIGH-LEVELの両方がフューショットCoTベースラインを上回りました。

例外的に、HotpotQAでGemini Proを使用した場合、フューショットCoTベースラインがLEAP LOW-LEVELとLEAP HIGH-LEVELの両方よりも優れていました。Gemini Proが生成した低レベルの原則は、例に対して過度に特化しており、かつ冗長でした。そのため未知の例に適応するほど一般化されていなかったのではと考察されています。

### 数学の推論タスク

数学のベンチマークにおいても、GPT-3.5とGPT-4を使用した場合は、LEAP LOW-LEVELとLEAP HIGH-LEVELはベースラインを上回る成果を示しました。

![[LLMに敢えて間違わせてルールを覚えさせるプロンプト手法 Google DeepMindなどが考案 - AIDB/AIDB_64057_5-1024x342.png]]

#### 他のモデルにおける結果

Claude-2では、GSM8KではフューショットCoTがLEAPより優れていますが、MATHではLEAP HIGH-LEVELがフューショットCoTをわずかに上回りました。

そしてGemini Proにおいては、GSM8KではLEAP LOW-LEVELとLEAP HIGH-LEVELがベースラインを上回りますが、MATHでは若干悪いパフォーマンスを示しました。結論が出せない状態と言えます。

なおオープンソースモデルを使用した予備実験では、LEAPがフューショットCoTベースラインを上回ることはありませんでした。有用な原則を生み出しましたが、原則に従うことはありませんでした。

![[LLMに敢えて間違わせてルールを覚えさせるプロンプト手法 Google DeepMindなどが考案 - AIDB/AIDB_64057_6-1024x324.png]]

### Big-Bench Hard

LLMの性能を多角的に見るための比較的難しいタスクを集めたベンチマークがBig-Bench Hardです。ベースモデルとしてLlama-2-chat-70Bを使用し、原則の生成には、Llama-2-chat-70BとGPT-4が使用されました。

実験の結果、GPT-4によって生成された原則は有用でしたが、Llama-2-chat-70Bは原則を利用して回答の改善をすることができませんでした。

![[LLMに敢えて間違わせてルールを覚えさせるプロンプト手法 Google DeepMindなどが考案 - AIDB/AIDB_64057_7-1024x571.jpg]]

上記の実験結果から、LEAPは、指示に従う能力と反省する能力が十分に強いベースのLLMを必要とすることが言えます。そして残念ながら、多くのオープンソースモデルは、まだクローズドソースモデルよりも強力ではないと考えられます。

下の図は、Big-Bench Hard（BBH）からの質問例と、そのタスクで学習された原則、標準のフューショットCoTによる応答、そしてLEAP LOW-LEVEL応答がGPT-3.5-turboによって生成されている様子が示されています。

![[LLMに敢えて間違わせてルールを覚えさせるプロンプト手法 Google DeepMindなどが考案 - AIDB/AIDB_64057_8-1024x614.png]]

この例からは、各ケースで学習された原則は重要であること、CoT出力で誤りが出る場合もLEAPによって正しい推論が生成されることが示されています。

## まとめ

本記事では、LLMの下流タスク性能を向上させる新しいアプローチ「LEAP」の研究を紹介しました。

LEAPは、与えられた例に基づいて、意図的に間違いを犯し、最終的にはタスクの原則を学ぶことで、LLMが未知の例にも対応できるようにする手法です。

実験ではフューショットプロンプティングをベースラインとし、DROP、HotpotQA、GSM8K、MATH、Big-Bench Hardといった幅広い推論タスクで、GPT-3、GPT-4、Gemini Proといった強力なLLMを改善することが実証されました。

「間違いから学ぶ」のは非常に重要な観点ながら、これまで十分に手法が確立されていなかった背景があり、本研究は重要な進展と言えるかもしれません。

参照論文URL：[https://doi.org/10.48550/arXiv.2402.05403](https://doi.org/10.48550/arXiv.2402.05403)
