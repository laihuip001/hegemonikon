---
created: 2026-01-01T09:37:36 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/63048
author: AIDB Research
---

# LLMに自分自身の内部動作を説明させる手法 - AIDB

> ## Excerpt
> 研究者らは、LLMの内部表現を調べる新しいフレームワークを開発しました。LLMの内部表現とは、モデルがテキストを処理する際に内部的に生成する、ある種のデータやパターンのことを指します。 今回、LLM自身によって内部の動作を説明させることに取り組まれており、モデルの決定や出力がどのように行われているかがより明らかになることを目指しています。 本記事の関連研究： LLMの内部表現とは何か LLMに限ら…

---
研究者らは、LLMの内部表現を調べる新しいフレームワークを開発しました。LLMの内部表現とは、モデルがテキストを処理する際に内部的に生成する、ある種のデータやパターンのことを指します。

今回、LLM自身によって内部の動作を説明させることに取り組まれており、モデルの決定や出力がどのように行われているかがより明らかになることを目指しています。

![[LLMに自分自身の内部動作を説明させる手法 - AIDB/AIDB_63048-1024x576.png]]

**本記事の関連研究**：

-   [LLMが「教えてはいけない事実」を抑制するパーツが明らかに　Llama-2を1057個に分解](https://ai-data-base.com/archives/61767)
-   [LLMの内部状態を観察することで「出力がハルシネーションか否かを判別する」手法『LLMファクトスコープ』](https://ai-data-base.com/archives/61651)
-   [LLMは世界モデルを持ち「物事がどのように位置づけられ、時間がどのように進行するか」を理解する可能性](https://ai-data-base.com/archives/56365)
-   [LLMなどの生成AIの背後にある思考プロセスは人間とは全く異なるかもしれないことを示す仮説『生成AIのパラドックス』](https://ai-data-base.com/archives/58414)

LLMに限らず機械学習モデルを理解してコントロールする上で、モデルの内部表現にどんな情報が含まれているのか知るのはとても重要です。

内部表現とは、モデルが入力データ（例えばテキスト）を受け取ったときに、そのデータを解析し理解するために内部で生成されるデータの表現です。表現はモデルの各層で異なる形式を持ち、最終的な出力（例えば文章生成など）に至るまでのプロセスに密接に関わります。内部表現は隠れた表現という言い方もできます。

これまで、内部表現を理解するための研究はたくさん行われてきました。しかし、スケールの限界や精度の悪さ、表現力の不足などが問題となり、なかなか実用的なアプローチとは言えるものはなかったと言われています。

そこでGoogleの研究者らは、LLMならば自分自身の内部表現を人間のために「翻訳」することが可能であると考えました。LLMがテキストを生成する高度な能力を持つことを利用しての発想です。

以下では手法の方法論と実験結果を見ていきます。

ここから限定コンテンツ

## LLM自身に自分の内部表現を語らせる「Patchscopes」

研究者たちは、ある内部表現（モデルが処理中に生成する情報やパターン）に含まれる具体的な情報をLLM自身によって解読する方法を提案しています。表現をモデルの元の処理フローから切り離し、情報をより効果的に抽出するための新しい「推論パス」に組み込む（これを「パッチング」と呼びます）手法です。

![[LLMに自分自身の内部動作を説明させる手法 - AIDB/AIDB_63048_2.png]]

今回は、既存の手法の良いところは引き継ぎつつ、新しい機能も取り入れるような形が目指されました。

なお既存研究のアプローチは主に以下3つに分類されます。

-   内部表現の上に線形分類器（プローブ）を訓練する
-   モデルの語彙空間に表現を投影する
-   ある出力に対して表現が重要かどうかを特定するために計算に介入する

今回のフレームワークの流れを以下にまとめます。

（１）**入力シーケンスとモデルの選択**

まず、処理するための入力シーケンスを選びます。n個のトークン（単語や文字など）から構成されています。  
次に、入力シーケンスを処理するためのモデルを選びます。モデルは、L層（レイヤー）から成り立っています。

（２）**隠れた表現の選択**

モデルが入力シーケンスを処理する際に、特定の層と位置で得られる「隠れた表現」（モデルが内部で生成するデータのパターン）を選びます。

（３）**ターゲットシーケンスと変換関数**

ターゲットシーケンス（m個のトークンから成る）と、隠れた表現を操作するための変換関数（例えば、単純な恒等関数やより複雑な関数）を定義します。

（４）**パッチング操作**

選択した隠れた表現を、ターゲットモデルの推論パスにおける特定の層と位置にパッチングします。元のコンテキストから隔離された表現を使用して情報を翻訳します。

![[LLMに自分自身の内部動作を説明させる手法 - AIDB/AIDB_63048_3-1024x626.png]]

## 実験と結果

以下、複雑な内容になりますがさまざまな実験と結果です。

### 実験1. 次のトークン予測のデコード

まず、LLMが与えられた文脈から次のトークン（単語や文字）をどのくらい早く正確に予測できるかを評価しました。具体的には、モデルの中間層の情報から最終層までの予測をどの程度正確に行えるかを調べたのです。

実験では3つの異なる手法を使用しました。

1.  **Logit Lens**: 変更を加えずに、パッチされた表現をそのまま使用する方法。
2.  **Tuned Lens**: 層間でアフィンマッピング関数を使用して表現を調整する方法。
3.  **Token Identity Patchscope**: 元のプロンプトとは異なるターゲットプロンプトを使用し、隠れた表現のトークンを特定する方法。

評価は以下の2つの指標で行われました。

**Precision@1**: 推定された確率分布の最も高確率のトークンが、元の出力分布の最も高確率のトークンと一致する割合。

**Surprisal**: 予測された分布内の最も高確率のトークンのマイナスログ確率。

結果は以下の通りです。

10層以上では、Token Identity Patchscopeは他の手法より優れており、18〜22層で最大98%の改善を達成しました。モデルが早期の層で既に次のトークンについての予測をかなり正確に行っていることを示しています。

しかし最初の10層ではすべての手法の性能は低く、Token Identity PatchscopeはLogit Lensと同等で、Tuned Lensよりも精度が低い結果となりました。最初の層が入力をコンテキスト化するため、この段階ではまだ完全な予測が難しいことを示唆しています。

![[LLMに自分自身の内部動作を説明させる手法 - AIDB/AIDB_63048_4.png]]

要するにこの実験はLLMが早い段階でどの程度正確に次のトークンを予測できるかを確かめ、結果として中間層以降ではToken Identity Patchscopeが他の手法に比べて優れた結果を示しています。

### 実験2: 特定属性の抽出

次に、LLMが特定の情報や知識をどのように処理しているかを調べました。具体的には、事実や常識に基づく三つの情報（例えば、「アメリカ合衆国」が主語で、「最大の都市は」が関係で、「ニューヨーク市」が対象）をモデルがどれだけ正確に理解しているかを見ます。

実験では、データセットには8つの常識知識タスクと25の事実知識タスクを使用。モデルにはGPT-J (6B)を使いました。

Patchscope（ゼロショット特徴抽出）: 特定の関係を一般的に表現するプロンプトを作成し、主語の表現をこのプロンプトにパッチングして対象が生成されるかを確認します。トレーニングが不要で、既定のラベルに制限されません。

ロジスティック回帰プローブとは、与えられた関係に対する可能な対象を予測するために訓練されたプローブです。

評価の軸は属性抽出精度が選ばれました。Patchscopeが生成したテキストに対象が含まれているか、またプローブが対象を正確に予測しているかを評価しました。

評価実験の結果、全体的なパフォーマンスとしてPatchscopeはトレーニングデータを使用せずに、12のタスク中6つでプローブよりも高い精度を達成し、残りの6つのタスクのうち5つでも同等の結果を示しました。

![[LLMに自分自身の内部動作を説明させる手法 - AIDB/AIDB_63048_5-1.png]]

また層別パフォーマンスでは、Patchscopeは、モデルの初期層で一貫してプローブより優れており、中間層でも同様またはそれ以上のパフォーマンスを示しています。

つまりPatchscopeを使用すると、LLMが隠れた表現の中にある特定の属性を効果的に抽出できることがわかりました。

![[LLMに自分自身の内部動作を説明させる手法 - AIDB/AIDB_63048_6.png]]

### 実験3. 初期層でのエンティティ名の解析

次の実験では、LLMが複数の層を通じて、エンティティ（例えば、「1996年の夏季オリンピック」のような名前）をどのように理解し、処理するかを分析しました。

まず特定のエンティティについての説明を生成するように設計されたプロンプトを作成し、そのエンティティ名の最後のトークンの表現に同プロンプトを適用します。そうすることで、モデルがそれぞれの層でそのエンティティをどのように表現しているかを見ることができます。

PopQAデータセットから、頻繁に使われるエンティティとあまり使われないエンティティを選び、それらを分析します。頻繁に使われるエンティティはモデルによってより正確に捉えられると予想されますが、珍しいエンティティの理解は難しいとされています。

生成されたエンティティの説明をWikipediaでの説明と比較し、RougeLメトリックを用いて評価しました。その結果は以下の通りです。

パッチされた表現を異なる層でプロンプトに適用した結果、ほとんどのエンティティが初期層にわたって徐々に理解され、最終的にはより遠い位置の情報も含むようになりました。

ただし最初の5層で、生成された説明の精度が向上し、その後は減少します。この減少は、初期層の表現が後の層で適用される際に、プレースホルダートークン「x」の影響を受けるためと考えられます。

![[LLMに自分自身の内部動作を説明させる手法 - AIDB/AIDB_63048_7.png]]

この実験結果を一言で言うと、LLMが初期の層でエンティティ名をどのように理解しているかを調査するためにPatchscopesが有用であることが示されました。

### 実験4. クロスモデルパッチング

最後の実験として、より表現力のあるモデルを使って、別のモデルを解釈する方法（クロスモデルパッチング）が検証されました。あるモデルの表現を、より表現力の高い別のモデルに「パッチ」するものです。

ただし、異なるアーキテクチャや最適化プロセスなどによるモデル間の違いが、この手法の効果に影響を与えるリスクが考えられました。そのため、同じモデルファミリー内で、異なるサイズのモデル間（例：Vicunaの7Bと13B、Pythiaの6.9Bと12B）で表現をパッチすることで、より大きなモデルが小さなモデルの次のトークン予測やエンティティ解決プロセスをどれだけよく推定できるかを測定する実験が行われました。

方法としては実験1で行った次のトークン予測の実験を繰り返し、モデル間の違いを克服するために、層間のアフィンマッピングを学習します。

その結果、より大きなモデルへのパッチングは効果的であり、特に早い層へのパッチングが最も効果的ということがわかりました。VicunaとPythiaの両方で、精度は最大で0.7と0.8に達しました。また、モデルの一部の層間には微妙な一致があり、対角線上で高い値が一貫して観測されました。

![[LLMに自分自身の内部動作を説明させる手法 - AIDB/AIDB_63048_8.png]]

![[LLMに自分自身の内部動作を説明させる手法 - AIDB/AIDB_63048_9.png]]

### 上記実験結果のまとめ

**実験1の結論**

Patchscopesは、LLMが早期の層でどの程度正確に次のトークンを予測できるかを示し、中間層以降ではToken Identity Patchscopeが他の手法に比べて優れた結果を示しました。

**実験2の結論**

Patchscopeは、LLMの隠れた表現から特定の属性を効果的に抽出できることが示されました。モデルの初期層において、従来の方法よりもPatchscopeが優れた結果を示すことが確認されました。

**実験3の結論**

Patchscopesを使用してLLMが初期の層で特定の名前や用語（エンティティ名）をどのように処理するかを調べました。LLMは初期の層から始めて、徐々にその名前や用語を理解し、それを自分の知識に組み込んでいきます。

（例えば、モデルが「東京」という言葉を聞いたとき、最初の層では単なる都市名として捉えますが、次の層では日本の首都としての意味を加えるなどして、徐々に深い理解を形成していきます。さらにモデルは、名前や用語に関する情報を様々な層を通じて少しずつ統合していきます）

**実験4の結論**

異なるサイズのモデル間で表現をパッチすることで、より表現力の高いモデルが小さいモデルの情報処理をより深く理解し、解釈するのに役立つことが示されました。

## CoTの精度を向上させることに活用

LLMは各推論ステップを個別に正しく処理できても、異なるステップ間の接続を処理する際に失敗することがあるという問題を抱えています。精度を向上させるための手法として有名なChain of Thoughtが線形的な思考を促すことに強く関係する問題です。

そこで今回研究者らは、Patchscopesを使用して、多段階推論のパフォーマンスを向上させる方法が提案されています。Patchscopesを用いることで、モデルの中間表現を操作し、推論タスクの一部に対するモデルの中間回答をリルートすることができ、結果的に最終的な予測を修正することができると主張されています。

そして実験の結果、Patchscopeを使用したことにより精度は19.57%から50%に向上しました。トレーニングなしで、事前に定義されたラベルセットに制限されることなく精度を向上させる可能性のある有望なアプローチと言えます。

![[LLMに自分自身の内部動作を説明させる手法 - AIDB/AIDB_63048_1.png]]

## まとめ

この記事では、「Patchscopes」という新しいフレームワークに関する研究を紹介しました。LLMの内部表現を解釈し、人間が理解しやすい形式に変換するためのツールです。

Patchscopesの役割は、モデルの内部動作をより透明にし、複雑な情報を自然言語で説明することです

実験の結果、さまざまなタスクでLLMの性能を向上させることが示され、また多段階推論問題の自己修正に有効であることも示されました。

PatchscopesはLLMのの解釈可能性を高めるために有力であると結論づけられています。

こうした、LLMの内部を理解する研究はこれまからも重要になってくると考えられます。

**参照論文情報**

-   タイトル：Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models
-   著者：Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, Mor Geva
-   所属：Google Research、Tel Aviv University
-   URL：[https://doi.org/10.48550/arXiv.2401.06102](https://doi.org/10.48550/arXiv.2401.06102)
