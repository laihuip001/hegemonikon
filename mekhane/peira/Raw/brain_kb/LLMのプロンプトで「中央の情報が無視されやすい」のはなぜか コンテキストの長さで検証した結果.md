---
created: 2026-01-01T11:18:01 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/93962
author: AIDB Research
---

# LLMのプロンプトで「中央の情報が無視されやすい」のはなぜか コンテキストの長さで検証した結果 - AIDB

> ## Excerpt
> 本記事では、LLMが入力のどこに注目するのかを「コンテキストの長さ」という観点から検証した研究を紹介します。 プロンプトの中で中央の情報が無視されやすくなる「Lost in the Middle」という現象は知られていますが、これまでの研究では結果にばらつきがありました。その理由を明らかにするため、今回は入力の長さではなく「どの程度ウィンドウを使っているか」に着目しています。 プロンプト設計やタス…

---
本記事では、LLMが入力のどこに注目するのかを「コンテキストの長さ」という観点から検証した研究を紹介します。

プロンプトの中で中央の情報が無視されやすくなる「Lost in the Middle」という現象は知られていますが、これまでの研究では結果にばらつきがありました。その理由を明らかにするため、今回は入力の長さではなく「どの程度ウィンドウを使っているか」に着目しています。

プロンプト設計やタスク実行の際に、情報をどこに置くべきかを考えるヒントになる内容です。

![[LLMのプロンプトで「中央の情報が無視されやすい」のはなぜか コンテキストの長さで検証した結果 - AIDB/AIDB_93962-1024x576.png]]

## 背景

LLMでは、扱える入力の長さ（いわゆるコンテキストウィンドウ）が拡大しつづけています。現在では十数万から数十万トークンに及び、大量の情報を一度に処理できるようになってきました。

ただし、何でもかんでも情報を詰め込めばうまくいく、というわけではありません。モデルが入力のどの部分に注目するかは、情報の配置によって大きく左右されます。冒頭や末尾にある情報は重視されやすい一方で、中央にある情報は無視されやすい傾向があります。こうした現象は「Lost in the Middle」効果と呼ばれています。

この位置バイアスの影響は、モデルの種類やコンテキストウィンドウの長さによって異なると考えられています。

そこで本記事では、入力の絶対的な長さではなく、「そのモデルのコンテキストウィンドウに対して、どれくらいの長さか」という相対的な視点から位置バイアスをとらえ直し、どのような条件でこの効果が現れるのかを詳しく分析した取り組みを取り上げます。

ここから限定コンテンツ

## これまでの評価では分かっていなかったこと

### 同じ長さで比べてもフェアじゃない？

これまでは全てのモデルに対して同じ長さの入力を与えて性能を測る方法がよく使われてきました。たとえば32Kトークンの入力を使えば、あるモデルにとっては上限ギリギリの負荷になりますが、もっと大きなウィンドウを持つモデルにとっては余裕のある設定になります。つまり、「各モデルにとっての相対的な負荷」という視点が欠けているケースが大半。しかし「絶対的な長さ」で評価してしまうと、モデル間の比較が公平ではなくなってしまうかもしれません。

### 観察される位置バイアスは研究ごとにバラバラ

入力のどの位置に情報を置くとモデルがうまく使えるのか。この「位置バイアス」については注目が集まっているものの、研究結果は一枚岩ではありません。

「Lost in the Middle」は一般的には「存在する」と思われていますが、実は同じような設定で効果が確認できなかったという報告もあります。また、先頭の情報が優先されやすい「初頭効果」は比較的安定して観察されていますが、これもその程度は研究によってまちまちです。

このような食い違いの背景には、評価に使われている入力の長さやタスクの種類が大きく異なることがあります。たとえば、短めの入力（6K以下）に絞っている研究もあれば、100K以上の長大な入力を扱う研究もあります。タスクの難易度や、事前知識に頼るかどうかといった前提もまちまちです。

### 検索と推論のつながりは意外と手つかず

また、最近では、まず関連情報を検索し、そのあとで推論するという段階的な処理をさせる設計が注目されています。人間が情報を集めてから考えるのと似た構造で、LLMに長い文章を理解させるうえで有効とされています。

ただし、「検索の精度が推論にどれほど影響するのか」「検索の位置バイアスが推論にも伝わるのか」といった点については、あまり深く分析されていません。検索に失敗すれば当然ながら推論も間違いやすくなりますが、その因果関係やバイアスの伝播については、まだ十分に解明されていない状況です。

## コンテキストウィンドウの使い方で変わる？

モデルがプロンプト上のどこにある情報を重視するかを調べるには、少し工夫が必要です。

### 検索と推論をセットで扱う

まず評価の前提として、検索と推論の両方を同じ情報から導けるように設計されたデータセットが用意されました。

これまでは、検索タスクと推論タスクが別々に扱われることが多く、それぞれの特性を比較するのが難しい状況でした。そこで研究チームは「検索と推論のペア問題」を導入しました。

たとえば「AはBより若い」「BはCより若い」という文が与えられていたとします。このとき、検索問題としては「AはBより若いか？」、推論問題としては「AはCより若いか？」といった問いが考えられます。前者は明示的な情報に基づくもの、後者は複数の情報を組み合わせた論理的な推論が必要になります。

このようにペアで設計された問題を使うことで、同じ情報に依存しつつ、検索と推論の違いを明確に比較できるようになります。

扱われたデータセットは全部で4種類。比較関係（MonoRel）、空間的関係（PIR）、論理ルール（RuleTaker）、状態追跡（BoxTracker）という4つのタイプです。各セットは100問で構成され、真と偽の答えが半々になるよう調整されています。

### 絶対量ではなく割合で測る「相対入力長」

研究の中心となるアイデアは、「入力の長さをモデルごとの相対値で表す」という考え方です。

従来のように「32Kトークン」といった絶対値ではなく、「そのモデルが処理できる最大長（コンテキストウィンドウ）のうち何％を使っているか」を基準にします。これを相対入力長（Lrel）と呼び、以下の式で定義されます。

> 相対入力長 = 入力テキストの長さ ÷ コンテキストウィンドウサイズ

今回の実験では、6%、12%、25%、38%、50%、75%、100%の7段階が設定されました。コンテキストウィンドウが異なるモデル同士でも、入力負荷を公平に比較できるようにするためです。

また、文中のどこに重要な情報が置かれているか（先頭・中央・末尾）によってモデルの応答がどう変わるかを見るため、関係ないテキストで残りの部分を埋める工夫も加えられました。

![[LLMのプロンプトで「中央の情報が無視されやすい」のはなぜか コンテキストの長さで検証した結果 - AIDB/AIDB_93962_2-1024x274.png]]

評価手順の概要。関連文を先頭・中央・末尾に配置し、相対入力長を段階的に変えて測定

### バイアスを見える化する3つの指標

位置による応答の違いを数値で捉えるために、新たに3つの指標が定義されました。

-   **Lost in the Middle強度**  
    先頭と末尾が中央よりも高い精度を示した場合にだけ値を持ち、「真ん中で情報を見失う」傾向の強さを表します。
-   **初頭バイアス強度**  
    先頭と中央の精度の差から、最初の情報がどれだけ重視されているかを測ります。
-   **終末バイアス強度**  
    末尾と中央の差から、最後の情報がどれだけ優先されているかを示します。

これらを組み合わせることで、モデルがどの位置にある情報を信頼しているのかを、相対入力長の変化に応じて数値的に分析します。

### モデルごとの傾向もチェック

今回の検証では、8K〜128Kのコンテキストウィンドウを持つ6種類のオープンソースLLMが使われました。

-   Llama-3.1-70B（128K）
-   Llama-3.3-70B（128K）
-   Llama-3-70B（8K）
-   Mistral-Small-24B（32K）
-   Qwen-2.5-32B（32K）
-   Gemma-2-27B（8K）

評価に用いたモデルとコンテキストウィンドウ、ベース精度は以下の通りです。

|        モデル        | コンテキストウィンドウ | MonoRel RT | MonoRel RA | PIR RT | PIR RA | RuleTaker RT | RuleTaker RA | BoxTracker RT | BoxTracker RA |
|-------------------|-------------|------------|------------|--------|--------|--------------|--------------|---------------|---------------|
|   Llama-3.1-70B   |    128K     |    1.00    |    1.00    |  1.00  |  1.00  |     0.98     |     0.65     |     1.00      |     0.60      |
|   Llama-3.3-70B   |    128K     |    1.00    |    1.00    |  1.00  |  1.00  |     0.98     |     0.73     |     1.00      |     0.88      |
|    Llama-3-70B    |     8K      |    1.00    |    0.98    |  1.00  |  1.00  |     0.77     |     0.63     |     0.88      |     0.86      |
| Mistral-Small-24B |     32K     |    1.00    |    1.00    |  1.00  |  1.00  |     1.00     |     0.91     |     0.96      |     0.78      |
|   Qwen-2.5-32B    |     32K     |    0.95    |    0.92    |  1.00  |  0.98  |     0.97     |     0.89     |     0.97      |     0.92      |
|    Gemma-2-27B    |     8K      |    1.00    |    1.00    |  1.00  |  1.00  |     0.96     |     0.88     |     0.87      |     0.83      |

モデル選定の基準は以下の2点でした。

-   コンテキストウィンドウの大きさが幅広く異なること
-   基本的な検索・推論問題に対応できる性能を持っていること

## 位置バイアスはコンテキストの使い方で変わる

モデルが入力のどこにある情報を重視するか。その傾向は、コンテキストウィンドウの使い方によって大きく変わることが分かりました。

![[LLMのプロンプトで「中央の情報が無視されやすい」のはなぜか コンテキストの長さで検証した結果 - AIDB/AIDB_93962_1.png]]

入力が長くなると初頭バイアスが低下

### 「50％」が境目？Lost in the Middleが出やすい条件

まず注目されたのは、相対入力長が50％を下回ると、どのモデルでも「Lost in the Middle（真ん中の情報を見失いやすい）」傾向が顕著になることです。

先頭や末尾の情報にはきちんと反応できていても、中央部分の内容だけは見落としがちになる。そんな傾向が、入力が短いときほど強く現れました。

一方で、入力がコンテキストウィンドウの50％を超えると、その傾向は急激に薄れます。モデルによっては、ほぼ消失する場合もありました。

過去の研究で「Lost in the Middleが起きた」と報告されることもあれば、「起きなかった」とされることもあった背景には、この入力長の違いがあると考えられます。短めの入力では効果が観察され、長大な入力では消える。この発見は、これまでの研究の違いをうまく説明しています。

### 初頭バイアスは弱まり、終末バイアスは安定

なぜ中央部分が見失われにくくなるのでしょうか。その理由を探るため、初頭バイアスと終末バイアスの変化が調べられました。

すると、先頭の情報を重視する「初頭バイアス」は、入力が長くなるにつれて急激に弱まることが分かりました。とくに検索タスクでは、相対入力長が50％を超えると、中央の情報の方が先頭より正確に処理されるという逆転現象まで確認されました。

一方で、末尾の情報を重視する「終末バイアス」は比較的安定して存在し、入力が長くなっても強さを維持する傾向が見られました。

このことから、Lost in the Middleの消失は「初頭バイアスが消える」ことで起きていると考えられます。

![[LLMのプロンプトで「中央の情報が無視されやすい」のはなぜか コンテキストの長さで検証した結果 - AIDB/AIDB_93962_3-1024x272.png]]

位置バイアスの定量評価。相対入力長に伴う変化（検索／推論）

### 「近さ」が効くようになる距離バイアス

さらに、入力が長くなると「どこにあるか」より「どれだけ近いか」が重要になる傾向も確認されました。

たとえばLlama-3-70Bでは、短い入力のうちは典型的なV字型の精度分布（先頭と末尾が高く、中央が低い）を示しましたが、入力が長くなると先頭の精度が大きく下がり、最終的には「末尾 > 中央 > 先頭」という順序に変化しました。

![[LLMのプロンプトで「中央の情報が無視されやすい」のはなぜか コンテキストの長さで検証した結果 - AIDB/AIDB_93962_4-1024x347.png]]

長くなるほど先頭の精度が低下し、末尾優位の距離バイアスへ移行

一部のモデルでは、コンテキストウィンドウを100％使った状態で、中央や末尾の精度が短い入力よりも高くなるケースも見られました。これは、モデルが限界まで入力を使うときに、処理の仕方を切り替えている可能性を示しています。

### 推論は「検索の成功」にかかっている

検索と推論の関係にも重要な発見がありました。うまく検索できているかどうかが、推論の成功を左右するのです。

検索に成功したときの推論精度は、失敗時に比べて〜116％も高くなりました。

|      モデル      | 検索失敗時 | 検索成功時 | 向上率  |
|---------------|-------|-------|------|
| Llama-3.1-70B |  42%  |  76%  | 81%  |
| Qwen-2.5-32B  |  38%  |  82%  | 116% |
|  Gemma-2-27B  |  50%  |  79%  | 58%  |

この結果は、LLMが正しく推論するには、まず適切な情報を見つけることが何より大事だということを示しています。

### 推論の位置バイアスは検索から引き継がれる

最後に、検索と推論の位置バイアスのつながりも調べられました。

![[LLMのプロンプトで「中央の情報が無視されやすい」のはなぜか コンテキストの長さで検証した結果 - AIDB/AIDB_93962_5-1024x361.png]]

検索の成否で変わる推論の位置バイアス。失敗時は中央が落ち込み、成功時はほぼフラット

検索に失敗したときの推論では、中央にある情報に対する精度の落ち込みが目立ちました。これはまさにLost in the Middleの状態です。

ところが、検索に成功していた場合、情報の位置による精度差がほとんど見られませんでした。Mistral-Small-24BやLlama-3.3-70Bでは、検索成功時には推論精度がほぼフラットに近づき、どこに情報があっても対応できるようになっていました。

このことから、推論時の位置バイアスは、主に検索時の傾向をそのまま引き継いでいることが分かります。情報を見つけられれば、その後の処理では位置の影響をかなり抑えられるのです。

## まとめ

本記事では、LLMにおける位置バイアスを「相対入力長」という新たな観点から検証した研究を紹介しました。

調査では、入力がコンテキストの半分程度までに収まる場合、中央の情報が見落とされやすく、逆に入力が長くなると初頭効果が弱まり、末尾に近い情報が優先される傾向が明らかになりました。また、必要な情報をうまく検索できたかどうかが、後の推論精度に大きく影響することも確認されました。

タスク設計やプロンプトの工夫を見直すうえで、情報の配置や入力量の調整を意識する手がかりにしてみてください。

**参照文献情報**

-   タイトル：Positional Biases Shift as Inputs Approach Context Window Limits
-   URL：[https://arxiv.org/abs/2508.07479](https://arxiv.org/abs/2508.07479)
-   著者：Blerta Veseli, Julian Chibane, Mariya Toneva, Alexander Koller
-   所属：Saarland University, Max Planck Institute for Informatics, Max Planck Institute for Software Systems
