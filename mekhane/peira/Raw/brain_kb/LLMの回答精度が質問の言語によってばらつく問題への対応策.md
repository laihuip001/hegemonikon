---
created: 2026-01-01T11:16:58 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/96678
author: AIDB Research
---

# LLMの回答精度が質問の言語によってばらつく問題への対応策 - AIDB

> ## Excerpt
> 本記事では、LLMの多言語対応における精度のばらつきに関する調査を取り上げます。 ChatGPTやGeminiなどのLLMは多言語に対応していますが、同じ質問でも言語によって正解率が変わる「クロスリンガルギャップ」が問題になっています。 本記事では、この現象の原因に迫り、実務での対応方法を見ていきます。 本記事の関連研究 背景 LLMは、英語だけでなく、日本語や中国語、スペイン語など、多くの言語に…

---
本記事では、LLMの多言語対応における精度のばらつきに関する調査を取り上げます。

ChatGPTやGeminiなどのLLMは多言語に対応していますが、同じ質問でも言語によって正解率が変わる「クロスリンガルギャップ」が問題になっています。

本記事では、この現象の原因に迫り、実務での対応方法を見ていきます。

![[LLMの回答精度が質問の言語によってばらつく問題への対応策 - AIDB/AIDB_96678-1024x576.png]]

## 背景

LLMは、英語だけでなく、日本語や中国語、スペイン語など、多くの言語に対応しているのが特徴です。中には100以上の言語で動くモデルもあります。

一見すると単純に便利に思えますが、じつは見逃せない問題があります。同じ質問でも、どの言語で聞くかによって、正しく答えられるかどうかが変わってしまうのです。

もともと情報が書かれている言語と、質問に使う言語が違うと、答えの正しさに差が出ることがあります。この現象は「クロスリンガルギャップ」と呼ばれています。

なぜこんなことが起きるのでしょうか。よく言われてきたのは、「同じ意味の表現でも、言語によってモデルの中で別のものとして扱われているから」という説明です。英語の情報が圧倒的に多く、他の言語のデータが少ないことで、知識がバラバラになってしまっているという指摘もあります。

こうした言語による差は、ビジネスの現場でも無視できません。たとえば、世界中の顧客に対応するカスタマーサポートや、多言語での市場分析などでは、LLMの回答が言語によってブレると困ってしまいます。

今回の記事では、新たな視点からこの問題をとらえ直そうとしていきます。

ここから限定コンテンツ

### **忙しい人向けに、重要なポイント5選**

1.  LLMの多言語性能の差は、知識の不足ではなく、答えのブレが主な原因と判明した
2.  同じ質問を複数回投げて出力をまとめることによって、言語間の性能差を縮めることができる
3.  元の言語でモデルの自信が高い質問は、翻訳後でも正確に答えやすい傾向がある
4.  質問に答える前にモデル自身に他言語へ翻訳させれば、精度差を改善できる
5.  モデルを作り直さなくても、質問の工夫だけで多言語性能を実用レベルまで引き上げられることが示された

**参照文献情報**

-   タイトル：Rethinking Cross-lingual Gaps from a Statistical Viewpoint
-   URL：[https://arxiv.org/abs/2510.15551](https://arxiv.org/abs/2510.15551)
-   著者：Vihari Piratla, Purvam Jain, Darshan Singh, Partha Talukdar, Trevor Cohn
-   所属：Google DeepMind, Google Research

## 問題は「知識がないこと」ではなく「答えが安定しないこと」かもしれない

これまで、多言語での性能差は「ある言語では知識が足りていない」とか「表現の違いでうまく対応できない」といった理由で説明されてきました。

今回研究チームは、ちょっと違う見方をしています。

注目したのは、「答えが安定しないこと」そのものが原因ではないか、という点です。

ここでカギになるのが、「バイアス」と「バリアンス」という考え方です。どちらも統計でよく使われますが、イメージはそれほど難しくありません。

バイアスは「答えがズレている」状態のことです。たとえば、本当は1992年なのに、いつも1882年と100年ズレて答えてしまうようなケースです。

一方で、バリアンスは「答えがバラバラで安定しない」状態を指します。同じ質問を何度しても、1992年、1988年、1995年など、毎回少しずつ違う答えが返ってくるような感じです。

この2つを見分けるのはとても重要です。もしバイアス（ズレ）が原因なら、モデルの設計や学習の方法を根本から変える必要があります。でも、バリアンス（バラつき）が主な問題なら、工夫次第でもっと簡単に改善できるかもしれません。下の図はバイアス（左）とバリアンス（右）の違いを示しています。

![[LLMの回答精度が質問の言語によってばらつく問題への対応策 - AIDB/AIDB_96678_1-1024x603.png]]

研究チームは、もとの言語（ソース言語）と翻訳先の言語（ターゲット言語）での回答を、それぞれ確率の分布として数式で表しました。つまり、「どんな答えがどれくらいの確率で出るか」を数値化したわけです。

その結果、

> ターゲット言語では答えが正解から大きくズレているわけではなく、正解の周辺にバラついている傾向がある

とわかりました。

つまり、モデルは知識そのものは持っているけれど、「どの答えを選べばいいか」という自信が少し弱くなっている。そんな状態だと考えられます。

### 答えのばらつきを抑えれば、言語による差は小さくなるかもしれない

新しい視点に立った研究チームは、2つの大事な予測を立てました。

その1つが、「答えのばらつきを減らす工夫をすれば、言語による性能の差も小さくなるはず」という考えです。

ここで出てくるのが「アンサンブル」という方法です。これは、同じ質問を何度もモデルに投げかけて、出てきた複数の答えの中から一番よく出たものを選ぶというやり方です。たとえば10回聞いて、いちばん多かった答えを正解とするような方法です。

もし本当の原因が「知識が足りない」ことだったとしたら、何回聞いても毎回間違った答えしか返ってこないので、こうした工夫は意味がありません。逆に、答えをまとめてしまうことで、さらに精度が下がってしまうかもしれません。

しかし、原因が「ばらつき」だった場合は話が変わります。何度も質問してその答えをまとめれば、正しい答えが見えてくる可能性があります。なぜなら、モデルは正しい情報を持っているけれど、自信がなくて毎回ちょっとずつ違う答えを出してしまっているだけだからです。

### 元の言語で自信がある質問は、翻訳しても正しく答えやすいはず

研究チームが立てた2つ目の予測は、興味深いものです。

「元の言語（ソース言語）で自信を持って答えられる質問ほど、他の言語（ターゲット言語）に訳しても、正しく答えられやすい」という予測です。

ここでいう「自信」とは、モデルがその答えをどれくらい強く選んでいるか、つまり「確信度」のことを指します。

たとえば、同じ質問を10回して、毎回同じ答えが返ってくるなら、モデルはその答えに強い自信を持っているといえます。逆に、毎回ちがう答えが出てくるなら、自信がない状態です。

ソース言語とターゲット言語の確信度の間に「比例関係」があるなら、「知識が言語ごとに分かれているから性能が落ちる」というより、「翻訳するとモデルの自信が弱まること」が主な原因だということです。もしこの考えが正しければ、モデルの学習を一から作り直さなくても、使い方の工夫だけで改善が期待できます。

たとえば、同じ質問を何度か繰り返して答えをまとめる方法や、原文と翻訳を両方使う方法などが考えられます。

## 仮説を確かめるために、実験をどう設計したか

研究チームは、仮説が本当に正しいのかどうかを確かめるため、実際のデータと複数のLLMを使って実験を行いました。使ったのは、目的の異なる2種類のデータセットです。

### Wikipediaにある「言語ごとの知識の違い」を調べる

1つ目は「ECLeKTic」という名前のデータセットです。

このデータの特徴は、特定の言語にしか存在しないWikipediaページをもとに質問を作っているところにあります。たとえば、ヒンディー語のWikipediaにしかない話題を使って質問をつくり、それをヒンディー語で聞いた場合を「ソース言語」、他の言語に翻訳して聞いた場合を「ターゲット言語」として扱います。

このような設計にすることで、「どの言語に知識があるか」をはっきりと分けて調べることができます。

このデータセットには約5,500の質問が含まれており、12言語（英語、日本語、韓国語、ヒンディー語、ドイツ語、中国語、スペイン語、フランス語、ポルトガル語、イタリア語、インドネシア語、ヘブライ語）に対応しています。

### 質問と選択肢の言語をわざと混ぜる

2つ目は「MMLU（with mixup）」というデータセットです。

MMLUは、歴史、科学、法律など多くの分野からなる四択問題で、LLMの理解力を測る定番のベンチマークです。ここでの工夫は、問題文と選択肢の言語をわざと混ぜていることにあります。

たとえば、質問は英語なのに、選択肢のうち1つが日本語、別の1つが韓国語といったように、複数の言語がまざっています。質問と選択肢がすべて同じ言語のときは「ソース」、言語が入り混じっている場合は「ターゲット」として扱います。

このデータセットからは約2,000問が選ばれ、対象言語は英語、フランス語、ドイツ語、スペイン語、イタリア語の5つです。

### 最新の5つのモデルで実験

研究チームは、「今回の仮説が特定のモデルだけに当てはまる話ではないか？」という疑いをなくすために、性能の高いLLMを5つ選んで実験を行いました。

まず、外部に中身が公開されていない「クローズドソース」のモデルとして、GoogleのGeminiシリーズ（2.5 FlashとPro）、OpenAIのGPTシリーズ（GPT-5とGPT-5-mini）を使いました。

さらに、誰でも使える「オープンソース」のモデルとして、DeepSeek-R1も加えました。

どれも2024年から2025年にかけて登場した、高性能なモデルです。こうした異なるモデルでも同じような結果が出れば、今回の発見が「どのモデルでも当てはまる一般的な傾向」だといえるようになります。

下の2つのグラフは言語による性能差を如実に表しています。

![[LLMの回答精度が質問の言語によってばらつく問題への対応策 - AIDB/AIDB_96678_2-1024x371.png]]

### モデルが書いた自由記述の答えは、どうやって正解か判断する？

ECLeKTicというデータセットでは、モデルは選択肢の中から選ぶのではなく、「1992年」や「チャーリー・パルタナ」などの答えを自分で文章として出さなければなりません。

ここで問題になるのは、「その答えが本当に正しいのか」をどう判定するかです。すべて人間が確認するのは大変すぎて現実的ではありません。

そこで研究チームは「LLM-as-judge（LLMを判定役にする）」を使いました。別のLLMを使って、「この答えは正解と一致しているか」を自動でチェックさせる方法です。

一部を人間が確認する「スポットチェック」も行ったところ、この自動チェックの精度は95%以上と非常に高いことが分かりました。ただし、英語以外の言語では少し間違いが増える傾向が見られました。

そこで評価の精度をさらに高めるため、「Year-ECLeKTic」というデータの一部を別に取り出しました。「年号を答える問題」だけを集めたもので、全体の約18%にあたります。年号なら、ルールに合った形式で数字を抜き出し、答えが完全に一致するかを機械的にチェックできるため、あいまいさがありません。

一方、もうひとつのデータセット「MMLU（with mixup）」は四択問題なので、評価はとても簡単です。選んだ選択肢が正解かどうかを確認するだけで済みます。

## 実験の結果、仮説は当たっていた

実験の結果、研究チームの仮説は正しかったことがわかりました。言語によって答えの正確さが変わるのは、「知識が足りないから」ではなく、「答えにバラつきがあるから」だったのです。

### 同じ質問をくり返すと、言語の差が小さくなる

最初の実験では、シンプルですが効果の高い方法が使われました。それは、「同じ質問を10回くり返し、出てきた答えをまとめる」というやり方です。

そして、その10個の答えをまとめて平均をとり、元の言語（ソース）と翻訳後の言語（ターゲット）で、どれだけ近い答えになるかを測りました。

自由記述形式の答えを扱うECLeKTicでは、回答をそのまま比較するのではなく、「文章を数値に変換する技術（多言語埋め込みモデル）」を使って、意味の近さを測定しました。そして、ソースと言語とターゲット言語での答えの「距離」を「L2距離」という数値で計算しました。これは、2つの答えがどれくらい離れているかを測る方法です。

結果ははっきりしていました。

1回だけ質問した場合は、言語間の答えに大きな差がありましたが、回答を複数回集めて平均すると、その差はどんどん小さくなっていきました。

たとえば、Gemini 2.5 Flashでは、10回の回答を平均すると、ソースと言語とターゲット言語の距離が約40%も縮まりました。

### 四択問題でも、答えのばらつきが差の原因だった

四択形式の問題を使ったMMLU（with mixup）でも、先ほどと同じような傾向が確認されました。

ここでは、モデルが4つの選択肢のどれをどれくらいの確率で選ぶか、その「選び方の分布」を比べるために、「カイ二乗距離」という指標が使われました。これは、確率の分布がどれだけ違うかを数値で示すものです。

同じ質問を何回もくり返して答えを集め（アンサンブルを大きくすると）、元の言語と翻訳された言語での回答パターンがだんだん近づいていきました。GPT-5の場合は、カイ二乗距離が約半分にまで減り、言語による違いが大きく縮まったことがわかります。

### 9割以上のケースで「ばらつき」が主な原因だった

さらに研究チームは、どのくらいの質問が「バリアンス（答えのばらつき）」によって性能差が生まれているのかを、「π（パイ）」という値で推定しました。

その結果、ECLeKTicでは約90%、MMLU（with mixup）では約95%の質問が、バリアンスによる差だと判断されました。つまり、言語によって答えが変わってしまう原因のほとんどが「モデルが自信を持ちきれず、毎回ちがう答えを出してしまうこと」にあるということです。

残りの5〜10%の質問については、翻訳のミスや、言語ごとに事実の書かれ方が違っているといった、別の要因が影響していると考えられます。

### 翻訳文を並べて質問する方法が有効だった

同じ質問を10回くり返して答えを集める方法は効果的ですが、時間もコストもかかってしまいます。そこで研究チームは、もっと実用的な方法を試しました。それが「入力側のアンサンブル」です。

この方法は「TrEn（Translation Ensemble）」と呼ばれています。やり方はシンプルで、1つの質問文に対して、元の言語だけでなく他の言語への翻訳も一緒に見せるというものです。

たとえば、ドイツ語の質問をヒンディー語で聞く場合、その質問の英語訳やスペイン語訳もあわせて提示します。ただし、ヒントになってしまうのを避けるため、元の言語（この例ではドイツ語）や似た文字体系の言語は含めません。

実験では、翻訳を1つだけ加える「TrEn-1」、3つ加える「TrEn-3」、5つ加える「TrEn-5」という3パターンを試しました。

結果ははっきりしていました。翻訳を多く加えるほど、モデルの正解率が上がったのです。

たとえば、Gemini 2.5 Flashでは、翻訳を何も加えない場合、ソース言語とターゲット言語の両方で正解できた割合（転移スコア）は30.7%でしたが、TrEn-5では36.0%まで向上しました。

ただし、この方法には課題もあります。モデルが、最初の翻訳だけを見て答えてしまい、他の翻訳を無視してしまう可能性があるのです。つまり、複数の翻訳を見せても、それがきちんと活用されるとは限らないということです。

プロンプトの例を以下に示します。

````
以下の質問に答えてください。あなたの答えは、
質問の最初の行と同じ言語でなければなりません。

Q: マハトマ・ガンディーの国籍は何ですか？
प्रश्न: महात्मा गांधी की राष्ट्रीय संबद्धता क्या है?
（質問：マハトマ・ガンディーの国籍は何ですか？）
A: India
...

Q: மகாத்மா காந்தி இந்த நாட்டிற் பிறந்தார்?
（マハトマ・ガンディーはどの国で生まれましたか？）
லகம் பிநியன் கள நினாரம் எல என்ன?
A:
```
````

### モデルに自分で翻訳させてから答えさせる方法も有効

ここで登場するのが、さらに一歩進んだ方法「Translate-then-Answer（TTA）」です。

これは、質問に答える前に、まずモデル自身にその質問を別の言語に翻訳させるというアプローチです。たとえば、「この質問をランダムな言語に1回翻訳してから、元の言語で答えてください」といった指示を与えます。

こうすることで、モデルは答える前に別の言語表現を考える必要が出てきます。結果として、自然と複数の言語的な視点を取り入れることになり、「暗黙のアンサンブル（多様な表現の統合）」が働くようになります。

実験では、翻訳を1回だけ行う「TTA-1」と、3回行う「TTA-3」の2パターンが試されました。

結果はTTA-1の方が安定して高い性能を出しました。たとえば、

-   Gemini 2.5 Flashは37.8%
-   Gemini 2.5 Proは49.3%
-   GPT-5は49.1%

いずれも、翻訳を使わない元のやり方と比べて大きくスコアが上がっています。

一方で、TTA-3ではかえって性能が落ちるケースもありました。翻訳を何度もさせると、モデルが指示を正しく理解できなくなることがあったからです。GPT-5-miniやDeepSeekのようなモデルでは、複雑な指示にうまく対応できない場合がありました。

いずれにせよ、

> 「質問を少し工夫するだけで、言語の違いによる精度の差を20〜25%も縮められる」

ということが示されました。

これは、実務でLLMを使う人にとって、とても役に立つシンプルかつ効果的な方法といえるでしょう。

この「質問に答える前に、まずモデル自身にその質問を別の言語に翻訳させるというアプローチ」についてもプロンプト例を示します。

```
以下の質問に答えてください。最初に質問を1つの
ランダムな言語に翻訳してから、元の言語で答えてください。

Q: マハトマ・ガンディーの国籍は何ですか？
翻訳された質問: మహాత్మ గాంధీ కు జాతీయ సంబద్ధత ఏమిటి?
（マハトマ・ガンディーの国籍は何ですか？）
A: India
...

Q: மகாத்மா காந்தி இந்த நாட்டிற் பிறந்தார்?
（マハトマ・ガンディーはどの国で生まれましたか？）
翻訳された質問:
A:
```

### ソース言語での自信が、ターゲット言語の性能を決める

最後に研究チームは、2つ目の予測も検証しました。それは、「元の言語（ソース言語）で自信を持って答えられる質問ほど、翻訳した先の言語（ターゲット言語）でも正しく答えやすい」という関係です。

ここでいう「自信の度合い（確信度）」は、10回質問したときに、いちばん多く出てきた答えが何回現れたかで測ります。たとえば、10回中9回が同じ答えなら確信度は0.9、すべて違う答えなら0.1というふうに計算されます。

この関係は、ECLeKTicとMMLU（with mixup）の両方で確認されました。グラフにすると、横軸にソース言語での確信度、縦軸にソースとターゲットの答えの一致率をとったとき、右上がりの傾向がはっきりと見えました。

たとえば、Gemini 2.5 Flashでは、ソース言語での確信度が0.2程度の質問では、一致率は20%ほどでしたが、確信度が0.8を超えると、一致率は一気に80%近くまで上がりました。グラフの右上に書かれている「3.1x – 1.7」といった数式は、答えの一致率がどれだけ改善されたかの割合を表しています。

この結果は、数学的な予測とも一致していました。ソース言語で答えが安定している質問は、知識がしっかりしているため、他の言語に訳してもブレにくくなります。反対に、元の言語ですでに答えがバラついている質問は、翻訳するとさらに不安定になります。

年号の問題だけを集めた「Year-ECLeKTic」というデータを使って同じ分析を行うと、その傾向はさらに明確になりました。これは、LLMの自動判定に頼らずに正解をはっきり判断できるため、評価に余計なノイズが入らなかったからです。

以上の結果から、

> 言語ごとに知識がバラバラだから性能が落ちるのではなく、翻訳後にモデルの確信度が下がることが、主な原因である

という仮説が強く裏付けられました。

この発見は、実務でもとても重要です。

## モデルの回答精度が低い原因を見分ける方法は？

今回の研究では、「言語に依存してモデルの確信度が変化して回答精度が落ちている」場合における対策が論じられました。

しかし、モデルの回答精度に疑問が生じたときに、 「言語に依存してモデルの確信度が変化して回答精度が落ちている」 のか、 「知識が足りていない」 のかを区別するにはどうしたらいいでしょうか？

研究に基づくと、もっとも簡単な方法は、同じ質問を3〜5回くり返してみることです。もし原因がモデルの「自信のなさ（確信度の問題）」であれば、返ってくる答えは毎回少しずつ違うけれど、どれも正解の近くにあるはずです。

一方で、原因が「知識の欠如」であれば、何度質問しても同じように外れた答えが返ってくるか、1850年や2010年、1750年など明らかに正解から離れた答えばかりになります。このようなケースでは、いくら答えを集めても正解には近づきません。

次に試したいのが、英語や元の言語で質問してみる方法です。確信度の問題であれば、英語では正しく答えられるのに、翻訳された言語では答えられなくなるというパターンがよく見られます。逆に、どの言語でも正解できない場合は、そもそもその知識がモデルに含まれていない可能性が高くなります。

## まとめ

この発見で重要なのは、対策が現実的であるという点です。言語に依存してモデルの確信度が変化して回答精度が落ちるのであれば、質問を何回かくり返して答えをまとめたり、複数の翻訳を見せたりする工夫だけで精度を改善できる可能性があります。

実験では、「元の言語で自信を持って答えられる質問は、他の言語でも正しく答えやすい」という関係も明確になりました。

多言語でLLMを活用する場面では、この知見はとても実用的です。1回だけ質問するのではなく、ちょっとした工夫を加えるだけで、精度を大きく引き上げられる余地があるのです。

ただしこの方法は、学習データにある程度含まれている言語に限られ、まったく見たことのない言語にはあてはまらない点には注意が必要です。
