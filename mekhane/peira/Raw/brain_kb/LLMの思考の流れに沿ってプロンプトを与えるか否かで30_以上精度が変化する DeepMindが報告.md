---
created: 2026-01-01T09:36:59 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/64551
author: AIDB Research
---

# LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB

> ## Excerpt
> 大規模言語モデル（LLM）は文章の並び順に影響されやすいといった意外な弱点があります。 例えば推論タスクでは、前提を論理の構造と同じ順番で提示することでモデルの精度が大幅に向上することがわかっています。 今回Google DeepMindの研究者らは、この「前提の順序」による影響を様々なLLMで検証しました。 参照論文情報 本記事の関連研究： 背景 LLMは人間のような認知バイアスに沿ったエラーを…

---
大規模言語モデル（LLM）は文章の並び順に影響されやすいといった意外な弱点があります。

例えば推論タスクでは、前提を論理の構造と同じ順番で提示することでモデルの精度が大幅に向上することがわかっています。

今回Google DeepMindの研究者らは、この「前提の順序」による影響を様々なLLMで検証しました。

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551-1024x576.jpg]]

**参照論文情報**

-   タイトル：Premise Order Matters in Reasoning with Large Language Models
-   著者：Xinyun Chen, Ryan A. Chi, Xuezhi Wang, Denny Zhou
-   所属：Google DeepMind, Stanford University

**本記事の関連研究**：

-   [GPT-4に選択肢を与えるとき、順序を入れ替えるだけで性能に大きな変化があることが明らかに](https://ai-data-base.com/archives/54690)
-   [大規模言語モデル（LLM）のこれまでとこれから③　-使用法・拡張法、データセット編-](https://ai-data-base.com/archives/64398)
-   [プロンプトの小さな違いがLLMにもたらすバタフライ効果を調査した結果](https://ai-data-base.com/archives/62566)
-   [CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果](https://ai-data-base.com/archives/62364)

## 背景

LLMは人間のような認知バイアスに沿ったエラーを起こすことがあることが知られています。

例えば、「AはBに含まれる」 という情報で学習したLLMは「BはAを含む」と推測できない現象が報告されています。（〇〇さんの父親は××さんである、という事実から××さんの息子は〇〇さんであることが分からないといったケース。参考：[GPT-4などのLLMが「AはB」から「BはA」を導かない『逆転の呪い』における誤解なき解釈と対策](https://ai-data-base.com/archives/56074)）

また、タスクの説明に無関係な文脈が含まれると、LLMの性能が大きく低下する傾向もあります。

そんな背景を踏まえて、Google DeepMindの研究者らは、LLMの推論能力に及ぼす「前提の順序」の影響を調べることにしました。  
本来は演繹的な推論では、前提となる文の順序を変えても結論は変わりません。次の例を考えてみましょう。

1.  AならばBである
2.  BならばCである
3.  Aは真である

上記の３つの前提は順番を入れ替えても、いずれの場合も「Cは真」と結論できます。しかし、LLMは与えられる前提の順序に左右される性質があり、証明文と同じ順番で前提文が並んでいる場合に最良の結果を出します。

このような順序へのこだわりは、もともとは人間の思考傾向にもあるものです。私たち人間も、前提を一つずつ確認しながら逐次的に結論を導けるため、順番通りの文章を好む傾向にあります。このことは人間を対象とした実験が過去に行われて示されています。

研究者らはこの「前提の順序」による影響を、GPT-4-turbo、GPT-3.5-turbo、PaLM 2-L、Gemini Proなど最新のLLMを使って体系的に調査しました。

以下で詳細を紹介します。

ここから限定コンテンツ

研究者らは、主に演繹的な推論に注目し、少ない数の前提で一定以上の性能を発揮する「単純な論理問題」で検証を行っています。

なお、言語モデルがある程度は単語の順番をランダムに入れ替えたテキストも理解可能であることを示す先行研究は存在します。また他の研究でも、単語の大部分が並べ替えられてもGPT-4が複数の推論ベンチマークで健闘することは示されています。  
ただし今回は、そのように文構造の内部で要素の入れ替えをすることによって文の意味が不明になる実験ではありません。文章の中で文の順番を入れ替える、つまり文の意味も文章の意味も変わらず成り立つ場合に限定して、LLMの理解度が変化するかを検証しています。

## ベンチマーク

### 論理推論

これまでの研究では、証明が長かったり複数の定理の知識を要する場合にLLMの論理推論性能が弱くなることなどが明らかになっています。そこで今回の研究では「前提の順序」の影響に注目するため、SimpleLogicを改良したベンチマークが作成されました。なおSimpleLogicベンチマークとは、命題論理の問題を扱うために設計されたもので、タスクとしては二値分類（結論がTrueかFalseかを指示する）です。

今回作られたデータセットの構成は以下の通りです。

-   前提となる事実 A1…An
-   「XならばY」のような形のルール
-   証明すべき結論「Cは真」

SimpleLogicとは異なり、今回のベンチマークでは全ての結論が「真」となる問題のみを使っており、完全に正しい形式の証明文を生成できた場合のみ正解とします。つまり、存在しない事実やルールを作り出すようなエラーも不正解となる厳しいルールです。

今回のベンチマークの重要な点は、各論理問題について前提の順番を変えたバリエーションを作成していることです。証明文におけるルールの適用順に沿う並び方を「順方向」、それと逆の並びを「逆方向」とします。

なお、証明との相関が強い順方向や逆方向だけでなく、相関度が異なる並び方についてもLLMの性能を検証しています。

**前提の順序効果を検証する指標**

前提の順序効果を測定するために、以下の2つの要素を変えて検証を行いました。

**1\. 証明に必要なルール数**

必要なルール数が多いほど、前提の順序効果が大きくなると予想されます。ベンチマークでは、4〜12個のルールを使用する問題を生成しました。

**2\. 問題に含まれる不要なルール数**

証明に不要なルール（ディストラクター）が存在すると、問題の難易度が上がります。これは、前提の選択自体が難しくなるためです。今回、不要なルール数は0、5、10個のバリエーションを用意しました。

**問題数**

必要なルール数ごとに200個の問題を生成しました。前提の順序と不要なルール数の組み合わせにより、各問題には15個のバリエーションがあり、合計27,000個の問題がベンチマークに含まれています。

### 数学的推論における順序効果（R-GSM）

論理推論だけでなく、より広い範囲で文の順序効果を調べるため、小学生レベルの数学文章題からなるベンチマークGSM8Kをもとに、問題文章内の文の順序を入れ替えたR-GSMデータセットを作成しました。

研究者らはGSM8Kの問題文が5文以上のテスト問題に対して、問題文の末尾は変えず、それより前の文の順序を入れ替えて新たな問題を220問作成しました。 図の例では、元の記述は推論ステップが文の順と一致しますが、書き換えたバージョンでは2つ目の計算が末尾から2番目の文を参照する必要が生じます。

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551_2-1024x673.jpg]]

## 実験

### モデルと設定

GPT-4、GPT-3.5、PaLM 2-L、Gemini Proを対象として検証しました。デコードはgreedy decoding、プロンプトには zero-shot prompting（指示の追加なし）を用いました。

なおデコードとは、ベクトル情報を自然言語に直すプロセスを指します。そしてgreedy decodingとは、各ステップで最も高い確率を持つ単語やトークンを選択して次のステップの入力として使用する（シンプルで主流な）方式を指します。

R-GSMデータセットでは指示を加えず問題文のみを入力とし、論理推論タスクでは図のように各ステップで使用する前提を明示するような証明を求める指示を入力しました。

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551_1-1024x700.jpg]]

### 論理推論の結果

下の図は、証明に必要なルール数の違いによる結果です。

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551_3-1024x308.png]]

ここでは不要なルールは含まず、相関度 ? が 0.5、0、-0.5 の結果をまとめています。どのLLMでも「順方向」の並びで最良の結果が得られ、これは人間の思考傾向とも一致します。また、ルール数が増えるほど並び方の違いによる性能低下も大きくなります。（論文では明記されていませんが、これも人間と似ていますね。）

なお、推論能力の低いモデルほど前提の順序による影響を大きく受けます。GPT-4と PaLM 2-Lは精度の低下が 20〜30%程度であるのに対し、Gemini-ProとGPT-3.5は並び替えると精度が65%から25%以下に下がり、精度の低下幅が40%を超えました。

**順序の細かい違いによる分析**

下の図は、より細かいレベルでの並び方の影響を示しています。興味深いことに、どのLLMも「順方向」を一番に好むものの、それ以外の並び方に対する傾向はそれぞれ違います。

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551_4-1024x305.png]]

GPT-4は、他の並び替えよりも「逆方向」を好む傾向があり、順方向や逆方向から外れるほど性能が下がります。後方連鎖も確立された推論手法の一つなので、これは人間の思考パターンとも整合します。

PaLM 2-Lは、逆に「逆方向」で最も精度が悪くなり、順方向から変化するにつれて精度が低下します。

Gemini ProとGPT-3.5の傾向はあまり一貫していませんが、それでも順方向以外の並びではやはり「逆方向」を好むことが多いようです。

**不要なルールの影響**

不要なルールがない場合に一定の性能を発揮するGPT-4とPaLM 2-Lについて、不要なルールの影響を検証しました。下の図に示す通り、不要なルールを加えると推論性能が低下し、前提の順序による影響が大きくなります。それでも、GPT-4は他の非順方向よりも「逆方向」を好み、PaLM 2-Lは順方向から変化するにつれて性能が低下するという、両LLMの全体的な傾向は変わりません。

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551_3_2-1024x303.png]]

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551_4_2-1024x297.png]]

**エラー分析**

下の表は、前提の順序の違いによる予測エラーの内訳です。以下のエラータイプを対象としました。

1.  間違った反証： 結論が証明できないと誤判定する
2.  ルール生成ミス： 問題に存在しないルールを生成する
3.  事実生成ミス： 問題に存在しない・証明できない事実を生成する

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551_5-1024x717.png]]

すべてのLLMにおいて、最も一般的なエラーパターンは「事実生成ミス」であることが観察されました。このエラーは、順方向から変化するにつれて劇的に増加します。

主な理由は、LLMは問題に提示された順序でルールを使用する傾向があるため、次のルールがまだ適用できない場合、LLMは証明ステップを完了するために事実を生成してしまう可能性があることが考えられます。

同時に、順方向以外だと、逆方向の方が他の場合よりも「間違った反証」の割合が低くなることが観察されました。

### 数学的推論

表2aは、R-GSMデータセットでの全体的な結果です。ここでも、順序の入れ替えは全LLMにおいて性能の低下をもたらしています。元のGSM8Kの問題記述が必ずしも推論しやすい順序になっていないことに加え、手作業で並び替えた記述の方が解きやすいケースがあった点に注意が必要です。 そのため、表2bには、元の記述でLLMが解けた問題についても精度を示しています。結果、どのLLMでも元の記述で解けた問題のうち少なくとも10%を、並び替え後に解けなくなっており、GPT-3.5ではこの低下率が35%を超えました。

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551_6-1024x348.png]]

**問題の複雑さによる違い**

下の図は、それぞれ推論ステップ数、問題文の長さによる内訳です。どのLLMでも、当然ながらステップ数や文数が多いほど正解率は下がります。全体的に見ると、GPT-4と Gemini Proはステップ数・文数が増えるほど元の問題と並び替え後の問題との精度差が拡大する一方で、PaLM 2-LとGPT-3.5-turboではこの差が安定していました。

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551_7-1024x348.png]]

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551_8-1024x319.png]]

**エラー分析**

元の記述は解けて並び替え後は解けない問題について、各LLMでエラー傾向を分類したものが下記の表です。論理推論と同様、ここでもLLMが文中の数字を出現順に盲目的に使用しようとするためにエラーになる傾向が多く見られました。具体的には、どのLLMにおいても「時系列の無視」によるエラーが最も多いことがわかりました。

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551_9-1024x281.png]]

もう一つのエラータイプは、問題文を順番に処理していく中で数量が確定しないまま計算が発生し、未知変数が混入してしまうケースです。

下の図の例では、元の記述なら各動物の数は前の文だけ見れば算出できますが、並び替えるとウサギの数はそれより前の文で確定していたのに対し、モルモットの数を求める時点で魚の数が不確定なまま残ってしまっています。そのためLLMはさらに後の文を読んで魚の数を算出しなければいけません。

しかしGPT-3.5は直前のステップで出た数（＝ウサギの数）を使ってしまい、エラーとなっています。このエラー傾向はPaLM 2-Lでは少数派ですが、他のLLMではある程度見られました。

![[LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する DeepMindが報告 - AIDB/AIDB_64551_10-1024x647.jpg]]

## まとめ

本記事では、問題の前提を並べる順序が、タスクの本質を変えずとも、LLMの推論性能に大きく影響することを示した研究を紹介しました。

幅広い評価実験から、LLMは「問題を解く過程の中間推論ステップと一致する前提の順序」の時に最良の結果を出すという、人間同様の傾向があることがわかりました。逆に、問題文を前後に行き来しながら読まないと解けないような並び方だと、LLMは苦戦し、精度が30％以上低下してしまいます。数学的推論についてはベンチマークを作成して同様の順序効果を実験で確認しました。

人間も推論問題において前提の並び方に好みはありますが、LLMははるかにこの影響を受けやすいと言えます。

この「前提の順序効果」の原因は、LLMの自己回帰型の設計、学習の目的、訓練データの構成など、複数の要因が考えられますが、理論的な説明やこの制限を克服する手法の開発は今後の課題とされています。

参照論文URL：[https://doi.org/10.48550/arXiv.2402.08939](https://doi.org/10.48550/arXiv.2402.08939)

**おまけ**

冒頭で、タスクの説明に無関係な文脈が含まれると、LLMの性能が大きく低下する傾向について紹介しましたが、一方でコンテキストの中に無関係な文書が入ることで精度が上がるといった研究結果も公開されています。参考：[LLMに対するプロンプトで「無関係な」文書を混ぜたほうが出力精度が上がる可能性がRAGシステムの検証で示唆された](https://ai-data-base.com/archives/63536)
