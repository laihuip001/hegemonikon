---
created: 2026-01-01T11:16:24 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/98087
author: AIDB Research
---

# LLMの推論能力を向上させるプロンプトベースの綿密なフレームワーク - AIDB

> ## Excerpt
> 本記事では、LLMの推論精度を改善するための新しいフレームワークを取り上げます。 5つの推論ベンチマークと3つのLLMを用いた実験により、他の手法を一貫して上回る性能を示しています。 本記事の関連研究 背景 LLMは、数学的な問題解決から複雑な論理推論まで、幅広い分野で強い能力を発揮するようになっています。この成功を支えているのが「Chain-of-Thought（チェーン・オブ・ソート）」と呼ば…

---
本記事では、LLMの推論精度を改善するための新しいフレームワークを取り上げます。

5つの推論ベンチマークと3つのLLMを用いた実験により、他の手法を一貫して上回る性能を示しています。

![[LLMの推論能力を向上させるプロンプトベースの綿密なフレームワーク - AIDB/AIDB_98087-1024x576.png]]

## 背景

LLMは、数学的な問題解決から複雑な論理推論まで、幅広い分野で強い能力を発揮するようになっています。この成功を支えているのが「Chain-of-Thought（チェーン・オブ・ソート）」と呼ばれる手法です。これは、LLMに最終的な答えだけを出力させるのではなく、途中の思考過程を段階的に言語化させる技術です。モデルの意思決定プロセスを可視化できるため、解釈性が高まるだけでなく、反復的な改善の余地も生まれました。

しかし、LLMが生成する推論の流れには脆弱性が潜んでいます。それは推論の途中で一つのステップに誤りが生じると、その誤りが下流のステップに伝播し、最終的に不正確な答えや一貫性のない結論に至ってしまうことです。

そのため、複数ステップにわたる推論を信頼性高く評価し、改善していく手法が求められています。  
これまでは、主に二つのアプローチが試みられてきました。

一つはモデルに複数の回答候補を生成させ、それぞれに信頼度スコアを付与して、最も信頼できるものを選び出すという方法です。LLMを審査員として活用したり、専用のランキングモデルを用いたりしますが、完成した推論全体に対して大雑把な評価を行うため、長い推論の中に埋もれた微細なステップレベルのエラーを見逃しがちという欠点があります。

もう一つのアプローチはLLMに自分自身の回答を批評させ、反復的に修正を加えていく手法です。一定の成果は得られているものの、全体的なフィードバックに依存するため、推論のどこで具体的に誤りが生じているのかをピンポイントで特定し、修正することが困難でした。

結果として、両方とも、複雑な推論タスクにおいてエラー修正を提供するには至っていません。

そこで本記事では、こうした問題を解決するべくLLMの推論ステップを綿密に改善するフレームワークを取り上げます。

ここから限定コンテンツ

### **忙しい人向けに、重要なポイント5選**

1.  LLMの推論過程を「サブ質問とサブ回答」のペアに分解し、ステップレベルで誤りを特定・修正する新フレームワーク
2.  各ステップを複数回再解答させて自己整合性をチェックすることで、信頼度の低いステップを精密に検出できる
3.  5つの推論ベンチマーク（数学3つ、論理2つ）と3つのLLMで評価した結果、既存の自己改善手法を一貫して上回る性能を達成した
4.  テスト時の計算量を増やしても性能が飽和しにくく、標準的なChain-of-Thoughtより効果的にスケールする
5.  LLMの内部推論プロセスを評価・理解するためのブラックボックスアプローチとして、解釈可能性と制御可能性を提供

**参照文献情報**

-   タイトル：SSR: Socratic Self-Refine for Large Language Model Reasoning
-   URL：[https://doi.org/10.48550/arXiv.2511.10621](https://doi.org/10.48550/arXiv.2511.10621)
-   著者：Haizhou Shi, Ye Liu, Bo Pang, Zeyu Leo Liu, Hao Wang, Silvio Savarese, Caiming Xiong, Yingbo Zhou, Semih Yavuz
-   所属：Salesforce AI Research, Rutgers University, The University of Texas at Austin

## 仕組み

### LLMの推論をソクラテス的対話として捉える

今回提案されているアイデアのポイントは、LLMの推論過程を「ソクラテス的プロセス」として再構成することにあります。ソクラテス的プロセスとは、古代ギリシャの哲学者ソクラテスが用いた対話法に由来し、大きな問いを小さな質問に分解し、一つずつ答えていくことで真理に近づく方法です。

Chain-of-Thoughtでは、LLMは自然言語で思考の流れを生成しますが、その構造は曖昧で、どこからどこまでが一つの推論ステップなのか明確ではありません。

そこで推論プロセスを「サブ質問」と「サブ回答」のペアの連続として明示的に表現します。例えば、複雑な数学問題を解く際、全体の問題を小さな質問に分割し、それぞれに具体的な答えを対応させていくのです。

この定式化を行うことで、推論の各ステップが検証可能な単位として扱えるようになります。各サブ質問に対する回答の正しさを個別にチェックできるようになります。

### ステップの分解、検証、改善の3段階プロセス

具体的な方法は、三つのステップで構成されます。

**第一段階「分解」**

LLMが生成した自然言語の推論過程を、明示的なサブ質問とサブ回答のペアに変換します。この作業自体もLLMに任せます。LLMに「この推論過程をステップごとの質問と答えに分解してください」とプロンプトを依頼します。

**第二段階「検証」**

ここが極めて重要なポイントです。

あるサブ質問について、それまでの文脈（前のステップの回答）を与えた上で、LLMに同じサブ質問を複数回（例えば5回）独立に解かせます。もし5回とも同じ答えが返ってくれば、そのステップは高い信頼度を持つと判断できます。逆に、回答がバラバラであれば、そのステップには不確実性が高く、誤りが含まれている可能性が高いと見なせます。

得られた複数の回答を比較し、元の回答との一致度を数値化することで、各ステップに0から5のスコアを付与します。スコアが低いステップほど問題があるということです。

**第三段階「改善」**

最も信頼度が低いステップを特定し、そこに焦点を当てて推論を修正します。複数回の再解答の中から多数決で最も支持された回答を選び出し、それを正しい答えとして採用します。そして、「このステップの答えは実はこうあるべきだ」という形で明示的なフィードバックをLLMに与え、推論全体を再生成させます。

### 効率化したいとき

毎回すべてのステップを複数回再解答させるのは計算コストが高くなります。

そこで２つの派生的なアプローチが提案されています。

一つ目は、まず従来のSelf-Refine（自己改善）手法を適用して明確な誤りが検出されればそれに基づいて修正を行い、Self-Refineが誤りを見逃したり、過信して「問題なし」と判断したりした場合にのみ、より精密な検証プロセスを発動させるという仕組みです。

二つ目は、反復的な改善プロセスに入る前に、まずLLMに推論の全体計画を評価させます。もし計画自体に問題があれば、ステップレベルの修正に入る前に計画を一度洗い直します。

## 検証内容と実験結果

### 実験設定と評価方法

研究チームは、本手法の有効性を検証するため、実験を行いました。

使用したLLMは、GPT-4.1-nanoとGPT-5-miniです。

評価には二つのカテゴリのデータセットが用いられました。  
一つは数学的推論タスクで、MATHデータセットの最高難易度レベル5の問題、アメリカ数学招待試験（AIME）の2024年版と2025年版の問題、そして「Humanity’s Last Exam（人類最後の試験）」という極めて困難な数学問題集が含まれます。  
もう一つは論理推論タスクで、ゼブラパズルやミニ数独といった合成された推論問題が使われました。

比較対象としたベースライン手法は、Self-Refine（自己改善）、Debate（複数エージェントによる議論）、MCTSr（モンテカルロ木探索による自己改善）、Atom-of-Thoughts（段階的な推論グラフ構築）といった、反復的改善手法です。

なお、Self-Refine（自己改善）をプロンプト例で示すと以下のようになります。

```
「先ほどの解答をよく読んでください。
計算ミス、論理の飛躍、曖昧な説明があれば指摘してください。
どこをどう直すべきか、具体的に提案してください。」

↓

「解答を最初から書き直してください。

部分的な修正ではなく、問題を解き直すつもりで推論してください。」
```

### ステップレベルの検証がもたらす一貫した性能向上

実験結果によると、本手法はすべてのタスクにおいて競合手法を上回るか、同等の性能を達成しました。

![[LLMの推論能力を向上させるプロンプトベースの綿密なフレームワーク - AIDB/AIDB_98087_1-1024x477.png]]

興味深いのは、比較的性能の劣るGPT-4.1-nanoでも、明確な改善をもたらした点です。モデルの推論能力が限られている場合でも、明示的なステップ分解と検証によって誤りを補正できることが確認されました。小規模でリソース効率の良いモデルを実務で活用する際にも活用可能であるということです。

また、上限性能（Best-of-K）とPass@K（K回の試行のうち少なくとも1回正解する確率）という指標でも評価が行われました。これらの指標は、複数回の改善試行の中から最良のものを選べた場合にどれだけ性能が向上するかを測るものです。ここでも本手法が最良または次点の結果を示し、単に最終回答だけでなく、改善プロセス全体の信頼性が高いことが裏付けられました。

![[LLMの推論能力を向上させるプロンプトベースの綿密なフレームワーク - AIDB/AIDB_98087_2-1024x421.png]]

### Self-Refineが機能しない場面で威力を発揮

「Humanity’s Last Exam（HLE）」の915問のテキストのみの数学問題でSSRを評価しました。このデータセットは最新の強力なモデルでも苦戦するよう設計されているため、GPT-5の完全版（中程度の推論モード、中程度の冗長性）を用いて実験が行われました。

結果は以下に示されています。GPT-5-miniでは、Chain-of-Thoughtが16.18%、Self-Refineが18.58%だったのに対し、本手法（SSR-Plan）は21.53%を達成し、5.35ポイントの改善を示しました。さらに注目すべきは完全版のGPT-5での結果です。このレベルのモデルになると、従来のSelf-Refineはむしろ性能を悪化させてしまい、27.98%から26.57%に低下しました。

![[LLMの推論能力を向上させるプロンプトベースの綿密なフレームワーク - AIDB/AIDB_98087_3.png]]

これは重要な発見です。モデルの推論能力が高まると、単純な全体的フィードバックでは改善が困難になり、場合によっては有害にすえなることがあります。

### 文脈管理の重要性

二つの観点からさらに比較実験が行われました。

一つは「文脈形式」で、元の自然言語推論を残すか（Natural）、ソクラテス的ステップのみを使うか（Socratic）の違いです。

もう一つは「文脈の完全性」で、推論全体を見た後で改善するか（Reflection）、誤りを見つけた時点で即座に介入するか（Intervention）の違いです。

実験結果から、元の推論を保持しつつ、全体を振り返った後で改善する方式（Reflection + Natural）が最も優れた性能を示しました。

![[LLMの推論能力を向上させるプロンプトベースの綿密なフレームワーク - AIDB/AIDB_98087_4.png]]

AIME24では69.67%、AIME25では62.00%を達成しています。

一方、ソクラテス的形式のみを使った場合や、早期に介入する方式では性能が低下しました。元の推論の文脈的な手がかりが修正プロセスにおいて重要であり、また推論全体を見渡すことで誤りの影響範囲をより適切に判断できることを示唆しています。

### 計算量増加のスケーラビリティ

実用的な観点から重要なのは、実行時の計算資源を増やした際にどれだけ性能が向上し続けるかです。

反復的改善手法では、二つのスケーリング方向があります。

一つは逐次スケーリング（改善イテレーションを増やす）、もう一つは並列スケーリング（複数の改善を並行実行して集約する）です。

実験結果によれば、逐次スケーリングでは、SSRは改善イテレーションを増やすにつれて着実に精度が向上し、Self-Refineよりも高い性能を維持しました。Self-Refineは数回の反復で性能が頭打ちになるのに対し、本手法はより長い改善余地を持っています。

並列スケーリングでも同様の傾向が見られました。サンプル数を増やすとすべての手法が改善しますが、本手法は同じ計算コストでSelf-RefineやChain-of-Thoughtを明確に上回りました。

![[LLMの推論能力を向上させるプロンプトベースの綿密なフレームワーク - AIDB/AIDB_98087_5-1024x473.png]]

### ソクラテス的ステップの粒度が性能に与える影響

最大ステップ数を3から10まで変化させ、その影響を調べました。

基本的なやり方だとステップ数が増えるとやや性能が向上する傾向が見られましたが、変動も大きく不安定でした。

一方、反復的な改善プロセスに入る前にLLMに推論の全体計画を評価させるバージョンでは、ステップ数に関わらず安定して高い性能を維持しました。AIME24では、ステップ数が変わっても精度はほぼ一定で、AIME25では6から7ステップ程度が最適でしたが、過度に細分化した10ステップでは若干の性能低下が見られました。

![[LLMの推論能力を向上させるプロンプトベースの綿密なフレームワーク - AIDB/AIDB_98087_6-1024x465.png]]

## まとめ

本記事では、LLMの推論精度を向上させる新しいフレームワークを紹介しました。

推論過程を検証可能なサブ質問とサブ回答のペアとして明示的に分解し、各ステップの信頼度を推定することで、誤りを含む箇所をピンポイントで特定・修正するという手法です。

従来の手法が推論全体に対する漠然としたフィードバックに依存していたのに対し、細粒度のステップレベル検証を行う設計になっています。

数学的推論と論理推論の両分野における実験で、本手法は既存の反復的改善手法を一貫して上回る性能を示しました。また、テスト時の計算量を増やしても性能が飽和しにくく、強力なモデルにおいても確実な改善をもたらすことが確認されました。
