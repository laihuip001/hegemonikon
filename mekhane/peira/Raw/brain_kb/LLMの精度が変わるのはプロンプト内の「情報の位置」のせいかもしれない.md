---
created: 2026-01-01T11:18:21 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/93282
author: AIDB Research
---

# LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB

> ## Excerpt
> 本記事では、LLMの精度がプロンプト内の文章の配置によってどれほど変化するのかを検証した研究を紹介します。プロンプト設計の際に、内容だけでなく「どの情報をどこに置くか」にも気を配ることの重要性が見えてきます。 本記事の関連研究 背景 LLMは進化し、さまざまなタスクで高い精度でこなせるようになってきました。こうした性能の背景には、「文脈内学習」という仕組みがあります。プロンプトに参考事例（問題と答…

---
本記事では、LLMの精度がプロンプト内の文章の配置によってどれほど変化するのかを検証した研究を紹介します。プロンプト設計の際に、内容だけでなく「どの情報をどこに置くか」にも気を配ることの重要性が見えてきます。

![[LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB/AIDB_93282-1024x576.png]]

## 背景

LLMは進化し、さまざまなタスクで高い精度でこなせるようになってきました。こうした性能の背景には、「文脈内学習」という仕組みがあります。プロンプトに参考事例（問題と答えの組み合わせ）をいくつか含めることで、モデルが新しいタスクに対応できるようになるというものです。

ただし、この文脈内学習には不安定な面もあります。例えば、プロンプトの書き方によって結果が変わるという話が有名です。参考事例の内容や言い回しで性能が変化することが明らかになっています。

しかしプロンプトの中で参考事例を「どこに置くか」の影響については、あまり詳しく調べられていません。たとえば、指示文の前か後か、ユーザーの質問の直前か直後かといった「位置」も重要です。

そこで本記事では、プロンプト内における参考事例の位置がモデルの性能にどれくらい影響するのかを体系的に調べた研究を取り上げます。分類、推論、生成などの8種類のタスクを使い、例題の中身を変えずに位置だけを動かしたときに、出力がどう変わるかを丁寧に測定しています。

ここから限定コンテンツ

## プロンプトのどこに何を書くかで、性能が変わる

プロンプトの設計がLLMの出力に大きな影響を与えることは広く知られています。今回の研究を理解するうえでも、過去にどのような傾向が見つかってきたのかを整理しておくと、経緯がよりクリアになります。

### 並び順を変えるだけで答えが変わる

プロンプトに含める参考事例の並び順を入れ替えるだけで、モデルの出力が大きく変わることがあります。たとえば、算数や常識の問題では、順番を変えるだけで正解率が10%以上変動するという報告もあります。同じ内容でも、前に書かれたものに強く引っ張られてしまう傾向があるということです。

さらに、モデルは事例と質問の間で使われている単語が似ているかどうかにも敏感に反応します。意味的なつながりではなく、表面的な語の一致に頼って答えを出している場面も多く見られます。

分類タスクでは、選択肢の中でもリストの先頭にあるラベルを好んで選んでしまう傾向も指摘されています。

これらはすべて、プロンプトの構成がどれほど細かいレベルで出力に影響しているかを示しています。

### なぜ前にある情報を重視してしまうのか

参考事例の順番に影響を受けやすい背景には、LLMの内部構造に由来する偏りがあると考えられています。

モデルの多くは、文の最初に出てくる情報に強く注目する傾向があります。プロンプトの冒頭にある文が、後の出力を大きく左右するという構造的な性質があるのです。文章の真ん中あたりにある情報は見落とされやすく、それが全体の性能に影響を及ぼすケースもあります。

このような特性から、どの事例を先に書くかだけでなく、プロンプトの構造そのものを丁寧に設計する必要があるという認識が強まってきました。

### 「どこに置くか」は意外と見落とされてきた

これまでの研究では、参考事例の中身や言い回し、テンプレートの工夫に注目が集まっていました。たとえば、質問と似た内容の事例を選んだ方が精度が上がる、などの知見です。

一方で、参考事例を「どこに配置するか」（たとえば、システムプロンプトの中に入れるのか、それともユーザーの質問のすぐ前に置くのか）といった視点での検証はほとんど進んでいません。

今回取り上げる研究は、まさにその「空間的な配置」が焦点となっています。参考事例の中身は変えずに、置き場所だけを変えたときに、どのように出力が変わるのかを詳しく調べています。プロンプト設計において、これまで見落とされがちだったポイントに光を当てる試みといえます。

## 参考事例の位置とモデルの反応

### 配置だけが原因の変化を見極める

一般的な文脈内学習では、いくつかの参考事例と質問を一つのプロンプトにまとめてモデルに与えます。今回の実験では、指示文や質問、事例の内容はすべて固定し、参考事例を置く位置だけを変えて出力の変化を観察しました。

焦点となるのは「どの事例を使うか」ではなく「どこに置くか」です。事例の中身が変わらない状態で、配置場所だけによって出力がどれほど揺らぐのかを調べる構成です。

### 実験で使われた4つの配置パターン

多くのLLMでは、システム全体に向けた指示文（システムプロンプト）と、ユーザーの入力は分かれています。この構造に沿って、参考事例を置く位置を4パターンに分類しています。

-   システムプロンプトの冒頭に事例を置く
-   システムプロンプトの末尾に事例を続ける
-   ユーザーの質問の前に事例を置く（最も一般的な形式）
-   ユーザーの質問の後に事例を添える

![[LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB/AIDB_93282_1.png]]

事例が指示文に含まれるのか、それともユーザー入力の一部と見なされるのか。この違いが、モデルの振る舞いにどう影響するかを観察しています。

### 出力の違いを測る2つの観点

実験では、出力の変化を定量的に捉えるため、2つの観点で評価が行われました。

ひとつ目は、事例を置いたときの正解率と、事例なし（ゼロショット）での正解率の差を比較。この値がプラスであれば事例が有効だったと判断され、マイナスであれば逆効果だったことになります。

もうひとつは、事例の位置を変えたことで、出力された答えそのものがどれくらい変わってしまったかを測ります。正答率が同じであっても、個々の問題で答えがまったく変わっている可能性もあるからです。

たとえば要約や自由記述のような出力が多様になるタスクでは、ROUGE-Lスコアを用いて回答同士の近さを評価しています。基準となる答えとのスコアに0.05以上の差がある場合には、出力が変わったと判定します。

## 実験結果から見えてきた影響

### 参考事例の配置でこんなに結果が変わる

10種類のLLMを使って、8つの異なるタスクで実験を行ったところ、参考事例をどこに置くかという違いだけで、モデルの出力が大きく変わることがわかりました。

たとえば、分類や質問応答などのタスクでは、参考事例をプロンプトの前半に置いた方が安定して高い性能を出す傾向があります。中にはゼロショットと比べて正解率が18ポイント近く上がった例もありました。

![[LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB/AIDB_93282_2-1024x531.png]]

タスク平均の正答率差

逆に、質問の後に参考事例を置くと、かえって精度が落ちるケースが多く、配置の順番が成果を左右することが確認されています。

### モデルのサイズによって最適な配置が変わる

算数のような推論を要するタスクでは、モデルの大きさによって最適な配置が変わるという興味深い動きも見られました。

たとえば、小型モデルではシステムプロンプトの冒頭に参考事例を置いた方が成績が良く、LLAMA3 3Bでは配置を変えるだけで正解率が42%から11%にまで落ちたケースもあります。

一方、大型モデルのLLAMA3 70Bでは、質問の直後に参考事例を置いたほうが88%という高い改善率を示しました。文脈を広く扱えるようになったモデルほど、後ろにある情報も上手に活用できるようになっていると考えられます。

### 要約のような生成タスクでは出力が大きく揺れる

生成系のタスク、特に要約では、参考事例の配置によって出力が大きく揺れ動くことが明らかになっています。

たとえば、LLAMA3 3BでXSUMの要約タスクを行った場合、配置によって性能が大きく変動し、改善された割合が82.5%から27.5%まで落ちるという結果が得られました。CNN/DailyMailでも、改善率が49%からわずか1%にまで落ちた例が確認されています。

![[LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB/AIDB_93282_3-1024x548.png]]

回答がどれだけ入れ替わったか

### モデルの大きさと出力の安定性の関係

![[LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB/AIDB_93282_4-1024x409.png]]

![[LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB/AIDB_93282_5-1024x416.png]]

全体的な傾向としては、パラメータ数の多いモデルほど、参考事例の配置変更による出力の変動が小さくなる傾向が見られました。

分類タスクでは、QWEN 72BやLLAMA3 70Bなどの大型モデルで、配置を変えても出力の変化が10%未満に抑えられています。対照的に、小型モデルでは20%以上の変動が普通に発生しています。

ただし、算数問題のようなタスクでは、大型モデルであっても配置による影響を強く受けます。GSM8Kでは、90%を超えるケースもありました。これは、推論には特有の処理が求められるため、モデルのサイズだけでは対応しきれないという可能性を示唆しています。

![[LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB/AIDB_93282_8-1024x373.png]]

QWEN 72BでのGSM8K算数問題の遷移図

### 出力がどう変わったかも重要なポイント

正解率の上下だけでなく、「どの問題でどんな答えが出たか」まで掘り下げて見ると、参考事例の位置がいかに出力に影響を与えているかがよりクリアになります。

MMLUタスクにおいて、LLAMA3 3Bでは、参考事例を質問の後ろに移しただけで、正解していた答えが不正解に変わるパターンが多く見られました。こうした変化は、実際の利用シーンでは致命的な差につながる可能性もあります。

![[LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB/AIDB_93282_6-1024x352.png]]

Correct→Incorrectなどの遷移図

要約でも同様の傾向があり、MISTRAL 8X7BでCNN/DailyMailの要約を行った実験では、参考事例の位置が変わるだけで正解と不正解が何度も入れ替わるほど、出力が揺れました。

![[LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB/AIDB_93282_7-1024x374.png]]

MISTRAL 8×7BでのCNN/DailyMail要約の遷移図

### モデルとタスクに合わせた配置の工夫が必要

QWEN 1.5Bのような小型モデルでは、システムプロンプトの前後どちらかに事例を置く配置が比較的安定した結果を出すことが多く見られました。

![[LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB/AIDB_93282_9-1024x483.png]]

小型モデル（QWEN 1.5B）のWin–Tie–Loss

Cohere 8Bのような中型モデルになると、タスクによって最適な位置が変わり始めます。分類タスクではシステムプロンプトの冒頭が効果的な一方で、XSUMやSQUADではユーザーメッセージの冒頭の方が良い結果を示すこともあります。

![[LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB/AIDB_93282_10-1024x483.png]]

中型モデル（Cohere 8B）のWin–Tie–Loss

LLAMA3 70Bのような大型モデルでは、ユーザーメッセージの冒頭に参考事例を置く形が複数のタスクで良好な性能を出す傾向がありました。質問の近くにある情報をうまく取り込めることが影響していると考えられます。

![[LLMの精度が変わるのはプロンプト内の「情報の位置」のせいかもしれない - AIDB/AIDB_93282_11-1024x483.png]]

大型モデル（LLAMA3 70B）のWin–Tie–Loss

### 統計的にも有意な違いが出ている

MMLUのタスクを対象に統計的検定を行った結果、システムプロンプトの冒頭や後半、ユーザーメッセージの冒頭に参考事例を配置する場合には、ゼロショットに比べて有意に性能が向上していることが確認されました（p < 0.01）。

逆に、ユーザーメッセージの末尾に参考事例を置いた場合は、有意な改善は見られませんでした（p = 0.1659）。

このように、参考事例の位置は単なる形式上の問題ではなく、出力の質そのものに影響する実質的な要因として捉える必要があります。

## なぜ参考事例の位置で結果が変わってしまうのか

### モデルの仕組みが引き起こす偏り

多くのLLMは、テキストを左から右に順に読み進める構造を持っています。そして、冒頭に出てくる情報のほうがより大きく扱われやすくなりがちです。

参考事例と質問が本来は同じくらい重要な要素であっても、モデルの注意の仕方が位置によって偏ることで、性能に差が出てしまいます。研究では、こうした傾向は「induction heads」という名称で知られています。今回の実験で確認された配置の影響も、そうした構造的な性質とよく一致しています。

### 学習データがもたらす偏り

モデルが訓練時に接してきたデータの形式も、配置の好みを形づくる要因になります。

たとえば、指示調整の際に使われたデータの多くが、参考事例を特定の場所に置いていたとすれば、モデルもその形式に最適化された反応を示すようになります。こうしたデータ由来の偏りが残っていることで、配置によって性能差が生まれていると考えられます。

また、どんな種類のデータを使って訓練されたかによってもこの傾向は異なり、特定の分野や形式に偏ったデータでは影響が強まる可能性もあります。

### 偏りを抑えるためにできること

位置による差が避けられない以上、その揺れをどう扱うかが現実的な課題になります。

ひとつの方法は、実際にタスクを行う場面で最適な配置を選ぶ工夫です。  
未知の問題に対して注釈付きデータの中から近いものを探し、それらで最も成績が良かった配置を選ぶというアプローチが一つです。モデルそのものを再学習させずに対応できるため、導入のハードルも比較的低く抑えられます。

もうひとつは、モデルの学習段階から位置の偏りを減らす方法です。参考事例をランダムな場所に配置したデータを使って再学習することで、特定の位置に依存しない使い方をモデルに身につけさせることが考えられます。

### 実務で気をつけたいこと

この研究が示しているのは、プロンプトの書き方ひとつで出力が大きく変わってしまうという現実です。内容が同じでも、どこに置くかによって成否が分かれてしまう以上、配置は慎重に設計する必要があります。

LLMを業務に活用する際には、プロンプト全体の構成を意識して見直すことが重要です。既存のテンプレートや思いつきで書いたプロンプトをそのまま使うのではなく、配置のバリエーションも含めて検討する姿勢が求められます。

プロンプト最適化を行う際には、事例の書き方だけでなく、位置の頑健性もあわせて確認しておくとよいでしょう。

今回の知見は、配置による影響を避けるという目的だけでなく、LLMの振る舞いそのものをより深く理解するヒントにもなります。

最終的には、内容と配置の両方を自動的に調整しながら、より安定したアウトプットを出す仕組みづくりへとつながっていく可能性があります。配置の最適化も、今後はプロンプト設計の一部として扱われていくかもしれません。

## まとめ

本記事では、参考事例の配置位置がLLMの出力に与える影響を体系的に調べた研究を紹介しました。  
事例の位置によって精度が大きく変わることが示され、特に小型モデルや生成タスクでの変動が顕著でした。  
構造的な注意の偏りや訓練データに含まれる形式が、こうした差を生む要因として考察されています。  
実務においては、プロンプトの内容だけでなく配置のパターンも意識的に試し、最適な形を選ぶ姿勢が求められます。  
特定のテンプレートに頼るのではなく、目的に応じて柔軟に構成を見直していくことが、安定した運用につながります。

**参照文献情報**

-   タイトル：Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning
-   URL：[https://doi.org/10.48550/arXiv.2507.22887](https://doi.org/10.48550/arXiv.2507.22887)
-   著者：Kwesi Cobbina, Tianyi Zhou
-   所属：University of Maryland, College Park
