---
created: 2026-01-01T09:29:08 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/58767
author: AIDB Research
---

# LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ - AIDB

> ## Excerpt
> 大規模言語モデル（LLM）における「ハルシネーション（幻覚）」に関する網羅的な調査報告が発表されました。ハルシネーションとは、LLMの出力が現実の事実やユーザー入力と矛盾する現象です。 研究者らは、ハルシネーションはデータ、トレーニング、推論という三つの段階に根ざしていることを明らかにしました。また、LLMの実用化に対する重大な課題であるため、より信頼性の高いモデルの開発に向けた研究の方向性を示し…

---
大規模言語モデル（LLM）における「ハルシネーション（幻覚）」に関する網羅的な調査報告が発表されました。ハルシネーションとは、LLMの出力が現実の事実やユーザー入力と矛盾する現象です。

研究者らは、ハルシネーションはデータ、トレーニング、推論という三つの段階に根ざしていることを明らかにしました。また、LLMの実用化に対する重大な課題であるため、より信頼性の高いモデルの開発に向けた研究の方向性を示しています。

今後のロードマップとしては、創造性と真実性のバランスに関する議論や、LLM自身に知識の境界に関する理解を深めさせることなどが挙げられています。

本記事では、調査報告の核心部分を詳細に見ていきます。

![[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ - AIDB/AIDB_58767-1024x576.jpg]]

**参照論文情報**

-   タイトル：A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions
-   著者：Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu
-   所属：Harbin Institute of Technology, Huawei Inc.
-   URL：[https://doi.org/10.48550/arXiv.2311.05232](https://doi.org/10.48550/arXiv.2311.05232)

**本記事の関連研究**：[LLMの出力から誤り（ハルシネーション）を減らす新手法『CoVe（Chain-of-Verification）』と実行プロンプト](https://ai-data-base.com/archives/55711)

## 背景

### LLMの進展とハルシネーション

大規模言語モデル（LLM）はテキスト理解やテキスト生成に大きな進歩をもたらしました。しかし、一つの大きな課題があります。それは、現実世界の事実やユーザーの入力と矛盾する内容、すなわち「ハルシネーション（幻覚）」を生成する現象です。

ハルシネーションは、LLMの応用における懸念を引き起こしています。そのため、ハルシネーションを検出し、軽減するための方法や、今後の展開が注目されています。

### 調査の目的と範囲

今回研究者らは、LLMにおけるハルシネーションに関する最近の進歩について、徹底的な調査を行うことを目指して分析に取り組みました。  
主に分類、要因、検出方法、ベンチマークについて報告を行っています。

さらに、ハルシネーションを軽減するための代表的アプローチと、LLMの現在の限界を強調し、将来の研究のためのロードマップを提示しています。

![[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ - AIDB/AIDB_58767_2.jpg]]

本調査報告のメインコンテンツフロー

**本記事の関連研究**：[LLMに自身のハルシネーション（幻覚）を「自覚」させ、減らす方法](https://ai-data-base.com/archives/55232)

## ハルシネーションの原因

![[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ - AIDB/AIDB_58767_3.png]]

ハルシネーションの各カテゴリ例

論文によると、ハルシネーションの原因は、データ、訓練、推論の三つの段階それぞれにあります。各段階は、LLMがどのように知識を獲得し、使用するかに深く関連しています。

### 1\. データ関連の問題

#### 誤った情報源

LLMはプレトレーニングデータに依存していますが、このデータが誤った情報や偏見を含んでいると、それらがLLMによって増幅され、ハルシネーションを引き起こすことがあります。

![[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ - AIDB/AIDB_58767_6.png]]

品質の劣るデータ利用の例

#### 重複バイアスと社会的バイアス

LLMは訓練データを記憶する傾向があり、特に重複する情報は過剰に記憶されがちです。これが「重複バイアス」を生じさせ、ユーザーの質問に対して不適切な応答を引き起こすことがあります。さらに、訓練データに含まれる社会的バイアスがLLMの生成するコンテンツに影響を与えることもあります。

![[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ - AIDB/AIDB_58767_4.png]]

誤情報とバイアスによるハルシネーションの例

#### 知識の境界

LLMは広範な事実知識を保有していますが、限界があります。特定の専門分野の知識が欠けていたり、最新の事実知識が不足していることがあり、それがハルシネーションを引き起こす可能性があります。

![[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ - AIDB/AIDB_58767_5.png]]

知識境界の例

### 2\. 訓練プロセスの問題

#### プレトレーニングとアライメント

プレトレーニング段階はLLMにとって重要な段階であり、そのフェーズで不適切なトレーニング戦略が採用されると、後のアライメント段階でのハルシネーションの原因となる可能性があります。

#### 強化学習

人間のフィードバックからの強化学習（RLHF）は、LLMがユーザーの指示に従うようにするプロセスですが、このプロセスが完全に調整されていない場合、LLMがユーザーの好みに完全に沿わない結果を生み出す可能性があります。

### 3\. 推論プロセスの問題

#### デコーディング戦略の欠陥

デコーディング戦略の欠陥や不完全な表現は、LLMが推論プロセスでハルシネーションを生じる原因となる可能性があります。LLMがどのようにして入力に対する応答を生成するかに関係しています。

なお、デコーディング戦略とは、モデルが内部的な表現や文脈をもとに次に生成する単語やフレーズを決定するプロセスを指します。

**本記事の関連研究**：[LLMにナレッジグラフ（知識グラフ）を連携させることで、タスク遂行能力を大幅に向上させるフレームワーク『Graph Neural Prompting（GNP）』](https://ai-data-base.com/archives/57018)

## ハルシネーション検出方法

ハルシネーションが発生することを前提にして、適切に検出し、軽減することも重要となります。現状の検出プロセスは、

ここから限定コンテンツ

モデルが生成するコンテンツの事実性と信頼性を評価することにフォーカスを当てています。

### 検出方法のおおまかな分類

![[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ - AIDB/AIDB_58767_1.png]]

ハルシネーションの例

#### 1\. 事実性ハルシネーション検出

事実性ハルシネーションの検出は、LLMが生成したコンテンツが実際の事実と一致するかどうかを評価します。事実性ハルシネーションは、生成された情報が実際の事実と矛盾するか、または実世界の知識に対して検証不可能である場合に発生します。

![[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ - AIDB/AIDB_58767_7.png]]

外部の事実による事実性ハルシネーションの検出

![[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ - AIDB/AIDB_58767_8.png]]

事実性ハルシネーション検出における分類法

#### 2\. 信頼性ハルシネーション検出

信頼性ハルシネーション検出では、LLMがユーザー指示に従って生成したコンテンツの一貫性と論理的整合性を評価します。ユーザーからの指示や提供された文脈に対する応答の信頼性に関連しています。

### 検出方法の研究例

具体的な検出方法には以下のような研究事例があります。

#### 事実性ハルシネーションの検出

**①FactTool**

I-Chun Chern et al., “**FacTool: Factuality Detection in Generative AI — A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios**” [論文はこちら](https://arxiv.org/abs/2307.13528)

記事：[AIが生成したテキストが事実なのか確認する手法「FacTool」が登場](https://ai-data-base.com/archives/53966)

**②Evidence Retrieval**

Jifan Chen et al., “**Complex Claim Verification with Evidence Retrieved in the Wild**” [論文はこちら](https://arxiv.org/abs/2305.11859)

**③FActScore**

Sewon Min et al., “**FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation**” [論文はこちら](https://arxiv.org/abs/2305.14251)

#### 信頼性ハルシネーションの検出

**①Abstractive Summarization**

Joshua Maynez et al., “**On Faithfulness and Factuality in Abstractive Summarization**” [論文はこちら](https://aclanthology.org/2020.acl-main.173/)

**②QuestEval**

Thomas Scialom et al, “**QuestEval: Summarization Asks for Fact-based Evaluation**” [論文はこちら](https://aclanthology.org/2020.acl-main.173/)

③**QAFactEval**

Alexander Fabbri et al., “**QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization**“

![[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ - AIDB/AIDB_58767_9.png]]

信頼性ハルシネーションの検出方法を示す図

**本記事の関連研究**：[GPT-4などのLLMに「自らの論理的な整合性をチェック」させるフレームワーク『LogiCoT』と実行プロンプト](https://ai-data-base.com/archives/55805)

## ハルシネーション評価ベンチマーク

ハルシネーションの評価は、LLMが生成する情報の事実性と信頼性を中心に行われています。ハルシネーション評価ベンチマークは、モデルがどれだけ正確に現実の事実やユーザーの指示に従って情報を生成できるかを測定するために設計されています。

### ハルシネーション評価ベンチマークの例

#### 1\. ハルシネーション自体の評価ベンチマーク

**[TruthfulQA](https://github.com/sylinrl/TruthfulQA)**：LLMが事実に基づいて正確な情報を提供する能力を評価するベンチマーク

**[HalluQA](https://arxiv.org/abs/2310.03368)**：LLMが生成したコンテンツの品質を評価するためのベンチマーク

#### 2\. 検出メソッドのベンチマーク

**[HaluEval](https://github.com/RUCAIBox/HaluEval)**：ハルシネーションを検出する方法の有効性を評価

**[FELM](https://arxiv.org/abs/2310.00741)**：ハルシネーション検出手法の性能を評価するためのベンチマーク

## 将来へのロードマップ

最後に、本論文で示されている今後のロードマップを紹介します。

### 1\. 長文テキスト生成と検索の強化

長文テキスト生成においては、手動注釈付きのベンチマークが不足しているという課題があり、これを補強する必要があります。また、検索機能の強化により、より正確で信頼性の高い情報を提供できるようにすることが重要です。

### 2\. ビジョンタスクの課題

ビジョンタスクにおいても、ハルシネーションの問題は存在します。テキスト生成だけでなく、ビジュアルコンテンツの生成においても、創造性と事実性のバランスをどのように取るかという問題を提起しています。  
この分野の研究をさらに進めることが重要視されていきます。

### 3\. RAGの利用とそのリスク

Retrieval-Augmented Generation（RAG）は、検索機能を活用した生成モデルであり、有望な戦略とされています。しかし、誤った情報源からの証拠が生成フェーズに伝播し、ハルシネーションを引き起こすリスクもあります。こういったリスクに対処するためには、検索機能の改善や、誤った情報の検出と排除の機能が必要です。

### 4\. 事実性と多様性のトレードオフ

引用の不正確さから、事実性と多様性の間にトレードオフが生じることがあります。LLMが生成する情報の事実性を保ちつつ、創造的で多様な内容を提供するバランスを取ることは、今後の研究において重要なテーマとなります。

### 5\. 自己修正メカニズムの潜在力

自己修正メカニズムは、論理的な推論タスクにおけるLLMのハルシネーションを軽減する助けとなる可能性があります。このメカニズムは、LLMが独立して初期の応答を修正する能力に焦点を当てています。しかし、LLMが推論チェーンを自己修正する能力には疑問があり、この効果はさらなる探究が必要です。

### 6\. 知識の境界の理解

LLMは広範な事実知識を持っていますが、自身の知識の限界を認識することには課題があります。この特徴は、LLMが自信を持って虚偽を生産し、自身の知識の限界に気づかないハルシネーションの発生につながります。LLMの知識境界を探る研究は進行中であり、問題に対処するためのさらなる研究が必要です。

### 7\. 創造性と事実性のバランス

真実性と信頼性を担保しつつ創造性とのバランスを取るのは容易ではありませんが重要です。事実性は実用的なLLMにとって重要ですが、創造性もまた物語作成やブレインストーミングなどの取り組みにおいて洞察を提供することがあります。平たくいうと、ハルシネーションの軽減に注意を向けすぎると、堅いモデルになってしまう恐れがあります。

![[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ - AIDB/AIDB_58767_10.png]]

異なるアプローチの図解

## まとめ

本記事で紹介した論文は、大規模言語モデル（LLM）におけるハルシネーションの問題に焦点を当て、その原因、検出方法、ベンチマーク、緩和戦略を綿密に調査しています。

ハルシネーションの原因はデータ、訓練、推論の各段階にわたり、不正確なデータソース、訓練プロセスの欠陥、デコーディング戦略の不完全さなどが挙げられます。

検出方法としては、事実性と信頼性を中心にしたアプローチが取られており、ベンチマークにはTruthfulQAやHalluQAなどが存在します。

将来へのロードマップとして、自己修正メカニズム、LLMの知識境界の理解、創造性と事実性のバランスが重要なテーマとされています。また、長文テキスト生成、検索での補強、ビジョンタスクにおける課題に対応することが求められています。

LLMの有用性が知れ渡りアプリケーションへの実装や日常のタスクでの利用が進む中、ハルシネーションの問題には一層気を配るべきなのかもしれません。ぜひこういった研究事例を参考にしてみてください。
