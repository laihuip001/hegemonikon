---
created: 2026-01-01T09:36:28 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/65459
author: AIDB Research
---

# LLMは本当に推論しているか？原理から導かれる長所短所と最適なフレームワーク - AIDB

> ## Excerpt
> 現在のLLMにおいて、皆が推論と呼んでいるものは、記憶とパターンに基づく生成のため、厳密には論理に基づくものではないと主張する論文が投稿されています。 そのため、その原則に基づく長所と短所に合わせた方法論を考えることが推奨されています。 参照論文情報 本記事の関連研究： はじめに LLMは、超大規模のデータで学習された結果、誰もが予想もしなかった言語的振る舞いを示しています。一見してあまりにも汎用…

---
現在のLLMにおいて、皆が推論と呼んでいるものは、記憶とパターンに基づく生成のため、厳密には論理に基づくものではないと主張する論文が投稿されています。

そのため、その原則に基づく長所と短所に合わせた方法論を考えることが推奨されています。

![[LLMは本当に推論しているか？原理から導かれる長所短所と最適なフレームワーク - AIDB/AIDB_65459-1024x576.jpg]]

**参照論文情報**

-   タイトル：Can Large Language Models Reason and Plan?
-   著者：Subbarao Kambhampati
-   所属：Arizona State University
-   URL：[https://doi.org/10.1111/nyas.15125](https://doi.org/10.1111/nyas.15125)

**本記事の関連研究**：

-   [LLMなどの生成AIの背後にある思考プロセスは人間とは全く異なるかもしれないことを示す仮説『生成AIのパラドックス』](https://ai-data-base.com/archives/58414)
-   [GPT-4などのLLMが「AはB」から「BはA」を導かない『逆転の呪い』における誤解なき解釈と対策](https://ai-data-base.com/archives/56074)
-   [LLMの内部状態を観察することで「出力がハルシネーションか否かを判別する」手法『LLMファクトスコープ』](https://ai-data-base.com/archives/61651)
-   [LLMの思考の流れに沿ってプロンプトを与えるか否かで30%以上精度が変化する　DeepMindが報告](https://ai-data-base.com/archives/64551)
-   [大規模言語モデル（LLM）のこれまでとこれから①　-代表的なモデル編-](https://ai-data-base.com/archives/64232)

## はじめに

LLMは、超大規模のデータで学習された結果、誰もが予想もしなかった言語的振る舞いを示しています。一見してあまりにも汎用的なので、多くの研究者は、計画や推論のタスクでも優れた性能を発揮できるのではないかと考えるようになりました。

LLMが得意とするのは、一種の検索です。データベースから正確にデータを索引付けして取得するのとは異なり、LLMは、プロンプトの単語ごとに確率的に補完を再構成します。原理的には文章の自動補完ツールをもっと格段に高度にしたものだと考えるとわかりやすいかもしれません。

そのため研究者は、本質的にはLLMの長所は「創造性」であり短所は「幻覚」と考えています。そして、これこそまさにLLMの魅力の根源だと言います。

一方で現在「LLMはゼロショットで〈〇〇推論タスク〉ができる」といったタイトルの論文が非常に多く出ています。しかし改めて考えると、モデルは、本当に計画や推論ができるのでしょうか？

以下では、研究者らが考案した、LLMの短所を補い長所を伸ばす形で推論や計画タスクに活用するためのフレームワークを紹介します。まずはじめに研究者が行なった実験結果や考察から見ていきます。

ここから限定コンテンツ

## GPT-4の登場とLLMの推論・計画能力への注目

研究者は、GPT-3.5やGPT-4の登場によって、LLMの推論や計画の能力に関する議論が増えてきたことに注目しました。

ここで言う「計画」（のタスク）とは、与えられた初期状態から目標状態に到達するための行動系列を生成する問題のことを指します。例えば、ロボットが部屋の片付けを行う際、どの順番でどの物をどこに運ぶかを決めるようなタスクが該当します。

まず初期の実験において研究者らは、GPT-3からGPT-3.5、そしてGPT-4へと、生成された計画の精度が徐々に向上していることに気づきました。特に、GPT-4ではブロックワールド（机の上に積み上げられたブロックを指定された配置に並び替える問題）という問題で30%の精度を達成しました。ただし、他の分野ではまだ精度が低かったようです。

研究者は、この改善が単なるLLMの近似検索能力の向上によるものなのか、それともGPT-4が実際に計画を立てる能力を獲得したのかを明らかにしたいと考えました。

つまり、GPT-4の性能向上が、より大きなデータベースからの情報検索によるものなのか、それとも本当の意味での計画立案能力の獲得によるものなのかを見極めようとしました。

### LLMの推論能力の本質

研究者は、LLMが原理的な推論を実際に行えるかどうかに関心を持っています。つまりLLMが記憶とパターン認識によって推論タスクの正解を出すことができるかどうかではなく、本当の意味での推論ができるかどうかを知りたいと考えました。

なお、問題によっては記憶とパターン発見に基づくアプローチで真実に辿り着くことが適切な場合もあります。しかし、証明可能な正しい推論手順に対する近似的なショートカットを見つけることと、実際に推論をすることは同等ではありません。自分の直感が正しいことを説明できない限り、それは真の推論とは言えないというわけです。

（編集部注：この主張は、関連研究「[LLMなどの生成AIの背後にある思考プロセスは人間とは全く異なるかもしれないことを示す仮説『生成AIのパラドックス』](https://ai-data-base.com/archives/58414)」でも触れられています）

しかし、システムや人間が問題を暗記しているのか、一から解いているのかを判断することは実際には難しく、特に教科書が膨大になるほどより難しくなってきます。

### GPT-4の性能向上の要因

研究者は、LLMは膨大なデータから関連する情報を探し出すことに長けていると考えています。そこでGPT-4の高い性能が、大規模なデータからの情報検索によるものなのか、それとも実際に計画を立てる能力の向上によるものなのかを確認することにしました。

問題を検証するため、計画タスクに登場するアクションやオブジェクトの名前を難読化することで、GPT-4が単純にデータから答えを検索することを難しくする実験を行いました。もしGPT-4が本当に計画を立てる能力を持っているのであれば、このような難読化はそれほど大きな影響を与えないはずです。

ところが、実験の結果は驚くべきものでした。標準的なAIプランナーは難読化された問題でも何の問題もなかったのに対し、GPT-4の性能は大幅に低下したのだとか。

この結果は、GPT-4の性能向上が主にデータベースからの情報検索能力の向上によるものであり、真の計画立案能力の獲得によるものではない可能性を示唆しています（ただしまだ結論を得るのは早いとも研究者は述べています）。

### LLMの計画能力を向上させる方法

研究者は、LLMは適切な後押しがあれば計画ができるようになるのではないかと考えました。そして、そのような後押しの方法として、大きく分けて2つのアプローチがあると紹介しています。

#### ファインチューニング

1つ目は「ファインチューニング」です。一般的なLLMを計画問題における具体的な事例とその解法を用いて追加学習させることで、より良い推測ができるようになることが期待できるそうです。これまでに行われた限定的な実験では、有意な改善は見られていませんが、より大規模なファインチューニングを行えば可能性はあると述べられています。

ただし、それでもなお近似検索であることには変わりはありません。つまり、ファインチューニングで性能が上がったとしても、LLMが真に計画を立てる能力を獲得したことを証明するものではないという理屈です。

#### プロンプトを繰り返し投げる外部的なアプローチ

2つ目の方法は、LLMが生成した計画の推測に対して、ヒントやアドバイスを含むプロンプトを与えることで、計画（および推論）の性能を向上させるというものです。

ここで重要なのは、以下の三つです。

(a)そのプロンプトの生成は手動か自動か  
(b)最終的な解の正しさを保証しているのは誰か  
(c)プロンプトが新たな問題の知識を与えているのか（それとも単にLLMに再試行を促しているだけなのか）

最もシンプルかつ明確なアプローチは、外部のモデルベースの検証器にプロンプトの生成を任せ、最終的な解の正しさを保証させることだそうです。このようなフレームワークを「LLM-Modulo」と呼んでいます。

また、人間がLLMとの対話の中で、繰り返しプロンプトを与えていくというアプローチもあるにはあります。しかし、この方法にはリスクがあります。LLMは単に推測を生成しているだけなのに、正解を知っている人間が意図せずLLMを誘導してしまう可能性があるというのです。この場合、得られた結果の良し悪しは、全面的に人間の責任になってしまいます。人間自身が推論・計画問題の答えを知らない（あるいは検証できない）場合、このようなアプローチの妥当性は疑わしいと研究者は指摘しています。

今回研究者は、LLMが生成した解答を自ら「批評」し、繰り返し自己改善するというアプローチについても触れています。これまで、いくつかの論文では、LLMにはこのような「自己改善」能力があると主張されています。しかしこのアプローチを信じる人は、LLMは検証するのが得意だという思い込みに基づいているとのことです。

確かに、人間は時として自己批判によって自らの誤りを修正することができます。しかし、LLMにそのような能力があると仮定する根拠はありません。なぜなら、LLMは問題を解決しようとしているのではなく、単に推測を生成（おおよそ検索）しているだけだからです。  
実際、LLMが「自己検証」を行うことでパフォーマンスが悪化するケースもあります。

## LLMの計画・推論タスクにおける役割

研究者は、LLMが計画や推論をできるといった論調に疑問を投げかけつつも、LLMがそれらのタスクに貢献すること自体は否定していません。なぜなら、LLMがアイデアや潜在的な解決策を生成する驚くべき能力を持っていることは事実だと考えているからです。

そこで、モデルベースの検証器やドメインの専門家と組み合わせる枠組みを活用することで、LLMの能力を有効に生かせることに注目しています（裏を返せば、LLMが生成した解答は、常に外部の検証器でチェックする必要があるというのが研究者の主張です）

なお研究者が詳しく調べたところ、LLMの計画能力を主張する論文の多くは、LLMから抽出した「一般的な計画知識」を「実行可能な計画」と混同しているそうです。また人間が繰り返しプロンプトを与えることで計画を「修正」していることもしばしばあります。

そのためLLMのアウトプットが合理的に見えても、実行時にはエラーが発生する可能性があることに注意したほうがいいとのことです。

## LLMを計画知識の抽出に活用する

研究者によると、LLMは、世界のダイナミクス、ユーザーの好みを近似的にモデル化するための豊富な情報源になり得るそうです。そのためには、人間や専門家がモデルを検証・洗練し、モデルベースのソルバーに渡す必要があると述べています。

もし知識の「正確性」に対する要件を緩めることができれば、LLMは人間の負担を軽減する知識の情報源になり得ると言います。

はじめから正確性が求められないタイプの製品、例えばエンタメ作品などにおいてはメリットとデメリットがそのまま合致するかもしれません。

## まとめ

本記事ではLLMの推論・計画能力について詳細に議論を行なっている論文を紹介しました。研究者は、自身の研究や検証の結果から、LLMが通常の意味での推論や計画を行っているとは考えにくいと述べています。LLMが行っているのは、超大規模の学習データに基づく高度な近似検索であり、それが時として推論能力と混同されているというのが著者の主張です。

ただし、LLMが推論・計画タスクの解決に貢献すること自体は否定しておらず、むしろ本質的な能力に目を向けることで正しいアプローチがとれることを主張しています。LLMはアイデア生成に優れているため、外部の検証器や専門家による検証と組み合わせたフレームワークが最も妥当とのことです。

LLMの能力を見極めて適切に活用することが重要だと再認識させる内容の報告でした。
