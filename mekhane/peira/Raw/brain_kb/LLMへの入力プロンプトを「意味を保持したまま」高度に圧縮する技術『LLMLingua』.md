---
created: 2026-01-01T09:37:45 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/60431
author: AIDB Research
---

# LLMへの入力プロンプトを「意味を保持したまま」高度に圧縮する技術『LLMLingua』 - AIDB

> ## Excerpt
> Microsoftの研究チームは、従来の大規模言語モデル（LLM）への入力プロンプトを効率的に圧縮し、意味そのものは維持する技術『LLMLingua』を開発しました。 本技術は、長いプロンプトによってLLMの応答速度が遅延したりコストが高くなったりしてしまう問題に対処するものです。 実施された実験では、LLMLinguaが他の手法に比べて優れた性能を示し、さまざまなタスクにおいてもその効果が確認さ…

---
Microsoftの研究チームは、従来の大規模言語モデル（LLM）への入力プロンプトを効率的に圧縮し、意味そのものは維持する技術『LLMLingua』を開発しました。

本技術は、長いプロンプトによってLLMの応答速度が遅延したりコストが高くなったりしてしまう問題に対処するものです。

実施された実験では、LLMLinguaが他の手法に比べて優れた性能を示し、さまざまなタスクにおいてもその効果が確認されました。

本記事では詳細を見ていきます。

![[LLMへの入力プロンプトを「意味を保持したまま」高度に圧縮する技術『LLMLingua』 - AIDB/AIDB_60431-1024x576.jpg]]

**参照論文情報**

-   タイトル：LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models
-   著者：Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu
-   所属：Microsoft
-   URL：[https://doi.org/10.48550/arXiv.2310.05736](https://doi.org/10.48550/arXiv.2310.05736)
-   GitHub：[https://github.com/microsoft/LLMLingua](https://github.com/microsoft/LLMLingua)

**本記事の関連研究**：[LLMZip：大規模言語モデルがテキスト圧縮の新境地を開く](https://ai-data-base.com/archives/52758)

## 研究の背景

LLMに与えられるプロンプトは、長くなりがちです。時には数千、場合によっては数万のトークンにまで及ぶことがあります。プロンプトが長いと、モデルの推論速度の遅延を引き起こし、コストが増加する原因にもなります。

チェーンオブソート（CoT）やインコンテキスト学習（ICL）など、プロンプト手法が探求されるにつれて、ますます多くの情報をプロンプトに含める状況になっているのも理由の一つです。

このような問題を解決するため、世の研究者たちや技術者たちは、プロンプトを効率的に圧縮する方法を探っています。今回、Microsoftの研究者らによって開発された「LLMLingua」は、そんなプロンプト圧縮の最新手法です。

**本記事の関連研究**：[ChatGPTの効果的なプロンプト手法における「基本のキ」を理論とテンプレート両方で紹介](https://ai-data-base.com/archives/58361)

以下にLLMLinguaの主な特徴を並べます。

### 1\. 重要な情報を保持する

LLMLinguaは、プロンプト圧縮時に重要な情報を損なうことなく、セマンティックな一貫性を維持することに重点を置いています。後述する「予算コントローラー」というコンポーネントが重要な役割を果たしています。

### 2\. LLMの推論速度が向上する

小規模モデルを使用して効率よく情報を圧縮し、大規模モデルとの間にある分布のギャップに対処することで、LLMの推論速度の向上を実現するとのことです。  
計算プロセスを増大させることなく、効果的な情報の維持を行います。

### 3\. **計算コストが削減される**

圧縮されたプロンプトは、元のプロンプトに含まれる推論情報を効果的に保持し、たとえ圧縮率が14倍や20倍に達しても、パフォーマンスのわずかな低下のみで計算コストを抑えることができるとされています。

**本記事の関連研究**：[基盤モデル（GPT-4）はプロンプトの工夫で専門特化モデルに匹敵するほど性能が向上することが「医学分野」で示唆される](https://ai-data-base.com/archives/59798)

## LLMLinguaのフレームワーク

![[LLMへの入力プロンプトを「意味を保持したまま」高度に圧縮する技術『LLMLingua』 - AIDB/AIDB_60431_1-1024x653.jpg]]

### 1\. 大雑把な圧縮とトークンレベルでの圧縮の組み合わせ

圧縮プロセスには、2つの異なるステージがあります。

まず、

ここから限定コンテンツ

大雑把な圧縮という形で、デモンストレーションレベルの内容を圧縮し、同時にセマンティックな整合性を高い圧縮率で維持します。

次に、プロンプトから知識を保持しながら圧縮を行うために設計された「反復的なプロンプトアルゴリズム」が導入されます。

### 2\. プロンプトの各部分に適切な圧縮率を割り当てる

「予算コントローラー」と呼ばれるメカニズムを介して、プロンプトの異なるコンポーネントに対して動的に異なる圧縮率を割り当てます。指示、デモンストレーション、質問などプロンプトの各要素が適切に圧縮され、セマンティックな一貫性を保ちながら、プロンプト全体のサイズを削減します。

### 3\. プロンプトから重要な情報を保ちながら圧縮する

圧縮プロセス中には、条件依存性を考慮しながら、プロンプト内の重要情報をより良く保持します。単に情報を落とすだけではなく、複雑な推論や論理を必要とするタスクにおいても、圧縮されたプロンプトがオリジナルの意図を維持することが可能とのことです。

### 4\. Instruction Tuningによってモデル間の差異を調整する

フレームワークの最終段階では、小規模なモデルと大きなブラックボックス型LLM間の分布のギャップに対処するためのアライメントが導入されます。異なるサイズのモデル間での一貫性を確保し、圧縮されたプロンプトがLLMによって効果的に処理されるようにするためです。

下の表は、GSM8Kデータセットにおける1ショット制約でのアブレーション（除去）研究を示しています。様々なコンポーネント（イテレーティブトークンレベルのプロンプト圧縮、予算コントローラーなど）を取り除いた場合の影響を評価し、それぞれの機能が全体の性能に与える重要性を検証しています。結果から、LLMLingua手法の各要素が性能に対してどのように影響しているかが分かります。

![[LLMへの入力プロンプトを「意味を保持したまま」高度に圧縮する技術『LLMLingua』 - AIDB/AIDB_60431_4.png]]

**本記事の関連研究**：[プロンプトを調整しないLLMのプロンプトエンジニアリング新手法『ControlPE』](https://ai-data-base.com/archives/59618)

## 性能の検証実験

LLMLinguaの有効性は、下記のような条件のもとで広範に検証されています。

### 1\. 異なるシナリオの4つのデータセットで実験

下記のように異なるタイプのタスクをカバーする4つのデータセットで行われました。

・推論とインコンテキスト学習（ICL）に関連するGSM8KとBBH  
・会話を扱うShareGPT  
・要約のタスクを扱うArxiv-March23

下の表は、会話（ShareGPT）と要約（Arxiv-March23）タスクにおける圧縮効果の異なる手法を評価しています。品質を評価するためのBLEUやROUGEのスコアと、圧縮後のトークン数が示されており、「Ours」（LLMLingua）が高圧縮率と出力品質のバランスを維持する優れた性能を持っていることを示しています。

![[LLMへの入力プロンプトを「意味を保持したまま」高度に圧縮する技術『LLMLingua』 - AIDB/AIDB_60431_2-1024x329.png]]

下のグラフは、異なるデータセットに対するプロンプト圧縮の効果を、生成されたトークンの長さで示しています。圧縮率が上がるにつれて生成トークンの長さが減少する傾向があり、これはプロンプト圧縮が効率的に行われていることを意味します。特にGSM8Kデータセットでの圧縮効果が顕著です。

![[LLMへの入力プロンプトを「意味を保持したまま」高度に圧縮する技術『LLMLingua』 - AIDB/AIDB_60431_6-1.png]]

### 2\. 他手法よりも高性能

LLMLinguaは、従来の手法と比較して、全体的に優れた性能を示しました。

なおプロンプトの効率的な圧縮を通じて、従来のLLMの推論とICL能力を評価する初めての試みとのことです。

下のグラフは、異なるプロンプト圧縮方法がGSM8Kデータセットにおいて達成した正確なマッチ（Exact Match）スコアを示しています。圧縮率が高くなるにつれて、ほとんどの手法のスコアが下がりますが、「Ours」（研究で提案された方法）は比較的安定した高スコアを維持していることが見て取れます。

![[LLMへの入力プロンプトを「意味を保持したまま」高度に圧縮する技術『LLMLingua』 - AIDB/AIDB_60431_7.png]]

**本記事の関連研究**：[ユーザープロンプトをLLMが言い換えて、LLM自身が理解しやすくする手法『RaR』](https://ai-data-base.com/archives/51160)

## 競合技術（先行研究）への優位性詳細

### プロンプトの意味を高品質に保持

複雑な推論やインコンテキスト学習（ICL）タスクにおいても、高いBERTScore F1を達成しています。これは、初期プロンプトのセマンティック情報を効果的に保持していることを示しており、プロンプト圧縮後も意味の深さと文脈の完全性を損なうことなく、推論機能を維持しています。

### 圧縮率が高くても、パフォーマンスの低下が少ない

GSM8KおよびBBHデータセットにおいて、フルショットアプローチよりも優れた、またはそれに匹敵する結果を提供しています。圧縮プロセスを最適化することで、高い圧縮率を達成しつつも、出力品質の低下を最小限に抑えていることを示しています。

下の表は、GSM8Kデータセットを使用し、Claude-v1.3モデルでの「Ours」（LLMLingua手法）の性能を示しています。1ショットとハーフショット制約の下での正確度（EMスコア）と使用トークン数を比較し、単純なプロンプトと比べた圧縮効率（1/τ）を評価しています。結果として、プロンプト圧縮を適用することで、トークン数を削減しつつ、高い精度を維持していることが分かります。

![[LLMへの入力プロンプトを「意味を保持したまま」高度に圧縮する技術『LLMLingua』 - AIDB/AIDB_60431_5.png]]

### 異なるタイプのLLMタスクに対して広範囲にわたる適用可能性

様々なシナリオをカバーする4つのデータセットにおいて、最先端の性能を発揮し、最大20倍の圧縮を少ないパフォーマンス損失で実現しています。LLMの応用範囲を大幅に広げ、異なるタスクに対して高い汎用性と柔軟性を持つことを示す優位性です。

下の表は、GSM8KとBBHにおいて、様々な方法の性能を示しており、「Ours」（LLMLingua）も含まれています。正確な一致（EM）のスコアと使用されたトークン数を比較し、より少ないトークンで高い精度を維持するLLMLinguaの効率性を強調しています。LLMの出力品質を損なうことなく入力プロンプトを大幅に圧縮していることを示しています。

![[LLMへの入力プロンプトを「意味を保持したまま」高度に圧縮する技術『LLMLingua』 - AIDB/AIDB_60431_3-775x1024.png]]

**本記事の関連研究**：[「入力プロンプト」を最新情報で自動アップデート＆最適化する手法『FRESHPROMPT』がLLMの出力精度](https://ai-data-base.com/archives/58986)

## まとめ

本記事では、Microsoftの研究者が開発したLLMLinguaという新技術を紹介しました。

大規模言語モデルへの長いプロンプトを効率的に圧縮し、高い精度を保ちながら推論速度を向上させることを可能にするとされています。実験結果は、この技術が既存の手法よりも優れた性能を示し、様々なタスクにおける応用可能性を確認しています。

LLMLinguaのコードはGitHubで公開されており、RAG（Retrieval-Augmented Generation）を使用するシナリオでも有効性が示されています。

ただし、実験にはGPT-3.5、Claude-v1.3、Alpaca-7Bなどのモデルが使用されましたが、他のモデルでも同様の結果が得られるかどうかは、さらなる検証が必要とされています。
