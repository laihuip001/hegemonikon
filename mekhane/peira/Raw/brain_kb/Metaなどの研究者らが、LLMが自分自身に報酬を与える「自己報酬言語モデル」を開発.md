---
created: 2026-01-01T09:37:26 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/62797
author: AIDB Research
---

# Metaなどの研究者らが、LLMが自分自身に報酬を与える「自己報酬言語モデル」を開発 - AIDB

> ## Excerpt
> Metaとニューヨーク大学は、LLMが自ら自分自身に報酬を与える「自己報酬言語モデル」を開発したと報告しています。 実験では、同社が開発したオープンソースモデルLlama 2 70Bに自己報酬フレームワークを適用し、クローズドの優秀なモデルであるClaude 2、Gemini Pro、GPT-4などをある側面から凌駕する結果が得られているとのことです。 本記事では研究背景、フレームワークの内容、実…

---
Metaとニューヨーク大学は、LLMが自ら自分自身に報酬を与える「自己報酬言語モデル」を開発したと報告しています。

実験では、同社が開発したオープンソースモデルLlama 2 70Bに自己報酬フレームワークを適用し、クローズドの優秀なモデルであるClaude 2、Gemini Pro、GPT-4などをある側面から凌駕する結果が得られているとのことです。

本記事では研究背景、フレームワークの内容、実験と結果、そして最後に結論と重要な注意点を紹介します。

![[Metaなどの研究者らが、LLMが自分自身に報酬を与える「自己報酬言語モデル」を開発 - AIDB/AIDB_62797-1024x576.png]]

**参照論文情報**

-   タイトル：Self-Rewarding Language Models
-   著者：Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston
-   所属：Meta, NYU
-   URL：[https://arxiv.org/abs/2401.10020](https://arxiv.org/abs/2401.10020)

****本記事の関連研究**：**

-   [従来の小さなニューラルネットワークでも「メタ学習」でChatGPTを凌駕するほど高度な生成AIができるとの報告、Nature誌](https://ai-data-base.com/archives/57838)
-   [DeepMindの研究者らが有効性を検証した、LLMに自ら高品質な訓練データを生成させる「自己学習」](https://ai-data-base.com/archives/60538)
-   [LLMに「自分自身との対戦」で能力を向上させる手法『Self-Play Fine-Tuning（SPIN）』](https://ai-data-base.com/archives/61996)
-   [OpenAIが開発中の「人間を超えたAIを制御する」方法](https://ai-data-base.com/archives/61116)
-   [AGI（汎用人工知能）の原則6箇条とレベル5段階](https://ai-data-base.com/archives/58565)

## 研究背景

現在の主要なLLMは、人間のフィードバックに基づいて訓練されています。Reinforcement Learning from Human Feedback (RLHF) と呼ばれる手法が主流です。

RLHFは、人間の好みに基づいて固定された報酬モデルを訓練し、強化学習（例えばPPO）を使ってモデルを訓練する手法です。なお、直接嗜好最適化（DPO）など、報酬モデルの訓練を避け、直接人間の好みを使うアプローチもあります。

このアプローチは安全かつ有効にモデルを学習できる一方で、人間の好みによって制限される可能性があり、報酬モデルの品質も問題となることがあります。つまり、モデルの能力が人間の理解や判断の範囲内に留まる恐れがあります。

また報酬モデル自体は訓練後には通常「凍結」され、その後は改善や更新が行われません。このことが、モデルの進化や、新しい情報に対して障壁になります。

これらの課題から、Metaとニューヨーク大学の研究者らは自動学習能力を持つLLMを開発する必要があると考えました。

以下ではフレームワークと実験結果、また注意点を詳しく紹介します。

## 自己報酬言語モデルのフレームワーク

研究者たちは、

ここから限定コンテンツ

新しいフレームワーク「自己報酬言語モデル（Self-Rewarding Language Models）」を考案しました。

このフレームワークの重要な点は、LLMが自分自身の報酬モデル（自分の回答がどれだけ良いかを判断するモデル）として機能し、その結果、外部からの報酬モデルを使わなくても良くなることです。  
本フレームワークにおいて重要な能力は以下2点です。

**指示に従う能力**: ユーザーからのリクエストを受けて、LLMが有用で安全な回答を生成する能力です。

**自己指示の作成**: モデルが自分で問題を作り、それに答え、その答えがどれだけ良いかを自分で判断する能力です。

![[Metaなどの研究者らが、LLMが自分自身に報酬を与える「自己報酬言語モデル」を開発 - AIDB/AIDB_62797_1-1024x434.png]]

### データセット

自己報酬言語モデルを訓練するための基本的なデータセットには、初期段階において主に二つのタイプのデータが使われます。

**（１）指示に従うデータ（Instruction Fine-Tuning, IFTデータ）**

人間が作成した指示（リクエスト）とそれに対する応答（レスポンス）の例です。事前学習済みLLMをさらに細かく訓練するために使われます。

**（２）評価に従うデータ（Evaluation Fine-Tuning, EFTデータ）**

ある指示に対する応答の質を評価するための例を示すデータです。モデルに応答の質を評価させる指示と、その評価結果が含まれます。評価結果は、考えの流れを示す正当化（ジャスティフィケーション）と最終スコア（例えば5点満点で）から成り立っています。モデルが報酬モデルとしての役割を果たすための訓練データとして機能します。

### 自己指示の作成

自己報酬型言語モデルの訓練過程では、モデルが自分自身で新しい訓練データを作り出し、それを使ってさらに学習します。細かくステップを分けると以下のようになります。

**（１）新しい指示（プロンプト）を生成**

既存のデータ（IFTデータ）から、新しい問題や指示を生成します。

**（２）候補応答を生成**

作成した新しい指示に対して、モデルが複数の異なる応答を生成します。

**（３）候補応答を評価**

最後に、モデル自身が「LLM-as-a-Judge」（モデルが審査員として機能する）を使って、自分が生成した応答を評価します。この評価は、0から5のスコアで行われます。

### LLM-as-a-Judgeについて

「LLM-as-a-Judge」は、言語モデルがユーザーの質問とその対応する応答を評価するために使用されるものです。この機能はプロンプトで実現されるもので、5点の加算式スコアリングシステムで、各基準に基づいて点数が累積されます。以下はその詳細です。

**1点**: 応答が関連性があり、ユーザーの問いに関連する情報を提供しているが、不完全であったり無関係な内容を含んでいる場合。

**2点**: 応答がユーザーの質問のかなりの部分に対処しているが、問いを完全に解決していない、または直接的な回答を提供していない場合。

**3点**: 応答がユーザーの質問の基本要素に対する有用な回答であり、AIアシスタントによって書かれたか、ブログや検索結果に一般的に見られる要素が含まれている場合。

**4点**: 応答がAIアシスタントの観点から明確に書かれており、ユーザーの質問に直接かつ包括的に対処し、整理されており、役に立つ内容であるが、明確さ、簡潔さ、焦点にわずかな改善の余地がある場合。

**5点**: 応答がユーザーの質問に完璧に対応し、余計な情報がなく、専門知識を反映し、高品質で魅力的かつ洞察に富むAIアシスタントによる回答である場合。

モデルは、上記の基準に基づいてユーザーの指示と応答を検討した後、合計スコアを100語以内で簡潔に説明し、スコアを「Score: <合計点数>」の形式で結論付けることが求められます。

![[Metaなどの研究者らが、LLMが自分自身に報酬を与える「自己報酬言語モデル」を開発 - AIDB/AIDB_62797_2.png]]

### 指示に従うための訓練

モデルがより良い応答を生成する能力を高めるためにAIフィードバック訓練（AIFT）という方式が採用されています。前のステップでモデルが作成した自己指示のデータを使って、初期のIFT（指示に従う訓練）とEFT（評価に従う訓練）データを拡張します。

以下の2つの方法があります。

**好みのペアを使用する方法**: 最も良い応答と最も悪い応答をペアにして、訓練データを作ります。  
**良い例のみの追加**: 完璧なスコア（例えば5点満点）を得た応答のみを追加します。

### 自己改善の流れ

モデルが段階的に自分自身を改善するプロセスの流れは以下の通りです。

複数のモデルが一連のステップを通じて訓練されます。

**M0**: 最初のモデルは、事前に訓練された基本言語モデルです。この段階ではまだ細かな調整は行われていません。

**M1**: M0を基にして、IFT（指示に従う訓練）とEFT（評価に従う訓練）データを使って細かい調整（ファインチューニング）が行われます。

**M2**: M1を基にして、M1が作成したAIフィードバック訓練（AIFT）データを使って訓練されます。

**M3**: M2を基にして、M2が作成したAIFTデータを使って訓練されます。

それぞれのモデル（M1、M2、M3）は、前のモデルが作成した訓練データを基にして訓練され、各モデルは前のモデルよりも改善されていきます。

## 実験と結果

### 実験条件

まず、研究者らが行った実験の条件は以下の通りです。

**基本モデル**：「Llama 2 70B」（大きな事前訓練済みのモデル）を使用しました。

**訓練データ**：指示に従うための訓練（IFT）では、高品質な人間が作成した例（「Open Assistant」データセット）を3200例使用しました。評価に従うための訓練（EFT）では、同じく「Open Assistant」データセットから複数のランク付けされた応答例を使用しました。

**評価方法**：GPT-4を使って、さまざまなモデルの性能を比較しました。256のテストプロンプトを使い、異なるソースから得た応答を評価しました。また、人間のランキングとモデルの評価の一致を調べました。各指示に対する平均2.85の応答があり、これらのランキングの順序がモデルの評価とどれだけ一致するかを測定しました。

**訓練**：さまざまな学習率、バッチサイズ、ドロップアウト率を使って訓練しました。また新しいプロンプトを生成するために、固定されたモデルを使用し、応答の生成と評価には訓練中のモデルを使用しました。

### 実験結果

**指示に従う能力の結果**

評価に従う訓練（EFT）と指示に従う訓練（IFT）の組み合わせは、IFTデータのみの使用とほぼ同じ性能を示しました。モデルが自己報酬の能力を向上させても、他のスキルに影響を与えないことを意味します。

また、2回目の反復訓練（M2）では、最初の反復訓練（M1）や基本モデル（SFTベースライン）よりも優れた性能を示しました。3回目の反復訓練（M3）では、さらに性能が向上しました。

![[Metaなどの研究者らが、LLMが自分自身に報酬を与える「自己報酬言語モデル」を開発 - AIDB/AIDB_62797_5.png]]

### AlpacaEval 2.0リーダーボードで競合モデルと比較

実験によって微調整されたモデルは、AlpacaEval 2.0リーダーボードを用いて評価されました。AlpacaEval 2.0リーダーボードは、言語モデルの性能を広範囲にわたって評価するツールであり、多くの異なるタスクを通じてモデルの能力を検証します。

結果、モデルはAlpacaEval 2.0リーダーボードで良好な結果を示しました。反復訓練を重ねるごとに、GPT-4 Turboモデルに対する勝率が向上しました。

この実験により微調整されたLlama 2 70Bモデルは、他の先進的なモデルであるClaude 2、Gemini Pro、およびGPT-4の0613モデルを上回る性能を示しました。（なお、GPT-4 0314モデルはその上を行きます）

下の表は自己報酬言語モデルが反復を経た後の勝率を示しており、他の主な言語モデルと比較しています。本モデルは各反復後、勝率が向上しており、3回目の反復では20.44%となっています。

![[Metaなどの研究者らが、LLMが自分自身に報酬を与える「自己報酬言語モデル」を開発 - AIDB/AIDB_62797_4.png]]

### 報酬モデルに対する評価

**EFTデータの追加が報酬モデリングに与える影響**

EFT（評価に従う訓練）データを追加することで、モデルにおけるLLM-as-a-Judge（モデルが審査員として機能する）タスクの性能が改善しました。IFT（指示に従う訓練）データだけを使うよりも、EFTデータを追加することで、モデルの応答評価能力が向上しました。

**自己報酬訓練の影響**

自己報酬訓練を行うことで、モデルが自分自身で報酬を与える能力が向上しました。2回目の反復訓練（M2）では、1回目の反復訓練（M1）の報酬モデルを使いつつも、全ての測定指標でM1よりも性能が向上しました。例えば、人間とのペアワイズ精度は78.7%から80.4%に向上しました。3回目の反復訓練（M3）では、さらにいくつかの指標でM2よりも性能が向上しました。

![[Metaなどの研究者らが、LLMが自分自身に報酬を与える「自己報酬言語モデル」を開発 - AIDB/AIDB_62797_3.png]]

**LLM-as-a-Judgeプロンプトの重要性**

実験では特定のLLM-as-Judgeプロンプト形式を使用しました。さまざまなプロンプトを試し、最も効果的なものを選んだ結果、SFTベースラインでのペアワイズ精度が大幅に異なりました（今回のプロンプトでは65.1%、他のプロンプトでは26.6%）。

## 結論と注意点

### 結論

自己報酬型言語モデルは自分の生成したデータに対して報酬を与え、それに基づいて訓練することができることが示されました。本フレームワークは、モデルの指示に従う能力と報酬モデリング能力を向上させます。

将来的にはモデルがより良い報酬を割り当て、より良い指示に従う能力を持つことが目指されます。

### 注意点

今回得られた結果は有望ですが、研究はまだ初期段階であり、安全性評価や反復訓練の限界など、検証すべきことはたくさんあるとのことです。

今回の実験では3回の反復訓練しか行われていません。さらに多くの反復や異なる言語モデルで試すことで手法の効果がより理解される必要があります。

また、GPT-4とAlpacaEval 2リーダーボードを使って評価されていますが、他の自動評価基準も使ってさらに実験を進めるべきとされています。

また安全性の観点から、「報酬ハッキング」が起こり得るかどうか、また、どのような状況で起こるかを理解することも重要です。

## まとめ

本記事では、自己報酬型言語モデル（Self-Rewarding Language Models）の開発とその機能について紹介しました。自分自身で生成した応答に対して報酬を割り当て、そのデータを使用して自己を訓練する能力を持つようにするフレームワークです。プロセスは反復的に行われ、各ステージでモデルは自己生成した訓練データを利用して指示に従う能力と報酬モデリング能力を向上させます。

研究の結果、モデルが自己報酬によって自己改善を行うことに成功しており、反復ごとに性能が向上していることが示されています。

しかし、研究はまだ初期段階です。安全性の評価（気になるところです）や反復訓練の限界など、今後の研究に期待したいですね。
