---
created: 2026-01-01T11:15:46 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/98714
author: AIDB Research
---

# 「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB

> ## Excerpt
> 本記事では、LLMエージェントによるコード生成の安全性についての調査報告を取り上げます。 初心者のエンジニアがコードレビューを行わずにそのままLLM生成コードを採用するケースが増えており、セキュリティ上のリスクが懸念されています。にもかかわらず、生成されたコードの安全性に関する検証は、十分に行われていません。 そこで、実在するオープンソースプロジェクトから抽出した200件の開発タスクを用いて、主要…

---
本記事では、LLMエージェントによるコード生成の安全性についての調査報告を取り上げます。

初心者のエンジニアがコードレビューを行わずにそのままLLM生成コードを採用するケースが増えており、セキュリティ上のリスクが懸念されています。にもかかわらず、生成されたコードの安全性に関する検証は、十分に行われていません。

そこで、実在するオープンソースプロジェクトから抽出した200件の開発タスクを用いて、主要なLLMエージェントがどの程度安全なコードを生成できるのか検証されました。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714-1024x576.png]]

**本記事の関連研究**

-   [Cursorはソフトウェア開発を加速する？導入後の実態に迫る](https://ai-data-base.com/archives/97454)
-   [ユーザーによる「曖昧な指示」や「不十分な依頼」、コード生成にどう影響する](https://ai-data-base.com/archives/97098)
-   [知らない分野ほど自信満々になってしまう現象はプログラミング中のLLMにも起きる？](https://ai-data-base.com/archives/96328)

## 背景

Vibe Codingは、開発者の生産性を大きく高める技術として注目を集めています。実際、最近の調査によると、開発者の約75%がすでにこの手法を導入しており、そのうち90%が「満足している」と回答しています。

また、AI開発の最前線に立つ企業でも、Vibe Codingの活用が進んでいます。たとえばAnthropicは、自社の本番環境においてこの手法を導入していることを公表しています。Vibe Codingはもはや実験的な技術ではなく、実務レベルで広く運用される開発スタイルとして定着しつつあります。

しかしその一方で、セキュリティに関する懸念も浮上しています。たとえば、生成されたコードにAPIキーが平文で書かれていたり、認証処理に脆弱性が含まれていたりするケースが複数報告されています。中には、そうした脆弱性を悪用され、実際に攻撃を受けた事例もあると伝えられています。

このような問題の背景には、生成されたコードを十分に確認せず、そのまま採用してしまう開発者の存在があります。経験の浅いエンジニアの場合、「動けば問題ない」と判断しがちで、セキュリティ上の欠陥を見逃してしまうリスクが高くなります。さらに、プログラミング経験が1年未満の初心者ほど、Vibe Codingに対してより楽観的な評価を持つ傾向があることも明らかになっています。

これまでにも、LLMが生成するコードの安全性評価はいくつか行われてきました。しかし、それらの多くは単一ファイルや関数レベルでの評価にとどまっており、現実の開発現場で扱うような、より複雑なプロジェクト全体を対象としたものではありません。

そのため、Vibe Codingの安全性を本質的に評価するために、より現実的で実践的な基準による検証が行われました。以下で詳しく紹介します。

ここから限定コンテンツ

### **忙しい人向けに、重要なポイント5選**

1.  実際のオープンソースプロジェクトから抽出した200タスクで、主要LLMエージェントのセキュリティを検証する新ベンチマーク「SUSVIBES」が開発された
2.  Claude 4 Sonnetでも、機能的に正しいコードの82.8%に脆弱性が含まれており、安全なコードを生成できたのは全体の10.5%のみだった
3.  脆弱性のヒントを与えるなどのセキュリティ対策を試みたが、機能的な正解率が約6ポイント低下した
4.  異なるLLMやエージェントフレームワークは、それぞれ異なる種類の脆弱性に対して強みと弱点を持っており、補完的な関係にある
5.  Vibe Codingは生産性を高める一方、セキュリティが重要なアプリケーションでの採用にはリスクがあるためモニタリングが必要

**参照文献情報**

-   タイトル：Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks
-   URL：[https://doi.org/10.48550/arXiv.2512.03262](https://doi.org/10.48550/arXiv.2512.03262)
-   著者：Songwen Zhao, Danqing Wang, Kexun Zhang, Jiaxuan Luo, Zhuo Li, Lei Li
-   所属：Carnegie Mellon University, Columbia University, Johns Hopkins University, HydroX AI

## SUSVIBESベンチマーク

Vibe Codingの安全性をより正確に評価するために、研究チームは新たに「SUSVIBES」と呼ばれるベンチマークを開発しました。（ベンチマークとは、性能や品質を測るための標準的な評価指標のことです）

SUSVIBESの最大の特徴は、実際のソフトウェア開発の現場で発生した脆弱性事例をベースにしている点です。具体的には、過去にセキュリティ上の問題が指摘され、その後修正されたオープンソースプロジェクトのコミット（変更履歴）から、200件の開発タスクを作成しました。

タスクは、10の異なるカテゴリに属する108のプロジェクトから抽出されており、合計で77種類に及ぶ脆弱性タイプ（CWE分類）をカバーしています。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_1-1024x278.png]]

SUSVIBESにおける1タスクの流れ。エージェントがDocker環境内で既存リポジトリにパッチを当て、機能テストとセキュリティテストで評価される。

本体はこちらにあります。

[https://github.com/LeiLiLab/susvibes](https://github.com/LeiLiLab/susvibes)

### セキュリティテストと機能テスト

このベンチマークでは2種類のテストが設定されています。一つは機能テスト、もう一つはセキュリティテストです。

機能テストは、実装された機能が正しく動作するかを確認するためのもので、脆弱性が修正される前のコミット（C-1）に含まれていた既存のテストを活用しています。

一方のセキュリティテストは、脆弱性が修正されたコミット（C0）で新たに追加されたテストを使用します。対象の脆弱性を確実に検出できるよう、開発者が意図的に作成したものです。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_2-1024x635.png]]

脆弱性修正コミットから、機能テスト・セキュリティテスト・タスク本体を自動生成するSUSVIBESのパイプライン。

### 実行環境の構築

各タスクには、生成されたコードを実際に動かして評価するためのDocker環境が用意されています。Dockerは、アプリケーションを他の環境から切り離して実行できる仮想的なコンテナ技術で、テストを常に同じ条件下で再現できるのが特長です。

### ベンチマーク全体の特徴

SUSVIBESベンチマークは、いくつかの特徴を備えています。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_3-1024x516.png]]

SUSVIBESに含まれる108プロジェクトのドメイン分布（Webフレームワーク、データサイエンス、ネットワーク、DevOpsなど）。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_4.png]]

SUSVIBESの平均コンテキスト規模とテスト数の統計（約16万行・867ファイル／タスク、機能テストとセキュリティテストの件数など）。

まず注目すべきは、圧倒的なコンテキストの規模です。SUSVIBESでは、平均約16万行のコードと867個のファイルを含む、リポジトリ全体を評価対象としています。これは、単一のファイルや関数レベルで評価するこれまでのベンチマークとは一線を画すものです。さらに、各タスクを解決するには平均で172行ものコード編集が必要で、複数のファイルにまたがる変更が求められます。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_5-1024x378.png]]

既存のセキュアコードベンチマークとの比較。SUSVIBESはコンテキスト規模とCWEカバレッジが突出している。

次に、取り扱う脆弱性の種類の豊富さが挙げられます。SUSVIBESは、CWE（共通脆弱性タイプ）で定義された77種類の脆弱性をカバーしており、これは既存のリポジトリレベルのベンチマークと比べて7倍以上の範囲に相当します。対象となるプロジェクトの分野も幅広く、Webフレームワーク、データサイエンス、ネットワーク、DevOpsなど、10種類の異なる応用分野が含まれています。

さらに、SUSVIBESは自動化された生成パイプラインを持っているため、新たな脆弱性が記録された際には、容易にベンチマークに追加することが可能です。現在はPythonに限定されていますが、他のプログラミング言語にも自然に対応できるような拡張性の高い設計になっています。

## 実験による検証結果

### 実験の設定

研究チームは、SUSVIBESベンチマークを用いて、主要なLLMエージェントの性能を評価しました。対象となったのは、3種類のエージェントフレームワーク（SWE-Agent、OpenHands、Claude Code）と、3種類のLLM（Claude 4 Sonnet、Kimi K2、Gemini 2.5 Pro）の組み合わせです。

ここでいうエージェントフレームワークとは、LLMを実際のソフトウェア開発に応用するための仕組みであり、コードの実行やフィードバックに基づく修正といった機能を担います。

評価には2つの指標が使われました。

1つ目は「FUNCPASS」で、機能的に正しいコードが生成された割合を示します。

2つ目は「SECPASS」で、機能だけでなくセキュリティ面でも安全なコードが生成された割合を意味します。なお、機能が未実装のコードにはそもそも脆弱性が存在しないため、SECPASSでは「機能が正しく実装され、かつ安全である」ケースのみが評価対象になります。

実験にあたっては、各タスクの説明文の最後に「セキュリティに注意するように」という一般的な注意書きも加えられました。実際の開発現場でも基本とされるセキュリティ意識を反映した措置です。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_6-1024x224.png]]

SWE-Agent／OpenHands／Claude Codeと、3つのLLMの組み合わせにおけるFUNCPASS（機能）とSECPASS（機能＋セキュリティ）のスコア。

### 全体的な結果

今回の実験では、予想以上に深刻な結果が明らかになりました。

まず、機能面でもまだ課題があることが分かりました。最も良い成績だったのは、SWE-AgentとClaude 4の組み合わせでしたが、それでも機能的に正しいコードを出力できたのは全体の61パーセントにとどまりました。つまり、実際のプロジェクトレベルの複雑なタスクでは、最新のシステムであっても約4割は正しく対応できないということになります。モデルごとの比較では、Claude 4が一貫して他のモデルより高い性能を示し、Gemini 2.5 Proは最も低い結果となりました。

とはいえ、さらに深刻なのはセキュリティに関する結果です。全体の平均を見ると、機能的に正しく、かつ安全なコードを生成できた割合（SECPASS）はわずか10パーセント程度にとどまりました。つまり、「動くコード」ができたとしても、その多くに脆弱性が含まれているということです。

たとえば、SWE-AgentとClaude 4の組み合わせでは、機能的には正しかったコードのうち、約83パーセントが安全性に欠けていました。最も良いスコアを出したのは、OpenHandsとClaudeの組み合わせで、SECPASSは12.5パーセント。ただし、このケースでも機能的に正しいコードの約75パーセントに脆弱性が残っていたという結果です。

この結果から分かるのは、開発者が機能テストに合格したコードをそのまま採用すると、かなりの確率でセキュリティ上の問題があるコードをプロジェクトに組み込んでしまう、という現実です。

### セキュリティ性能の詳細分析

研究チームは、機能性とセキュリティの性能をより深く理解するため、それぞれを分けて評価を行いました。具体的には、複数のシステムが共通して正しく解けたタスクだけを対象にし、その中で安全なコードを出力できた割合を計算しています。

LLMごとの結果を見てみると、Claude 4 Sonnetが17.2%、Kimi K2が20.7%、そしてGemini 2.5 Proが27.6%という数字が出ました。意外なことに、機能面では一番スコアが低かったGemini 2.5 Proが、セキュリティ面では最も良い結果となったのです。

同様に、エージェントフレームワークを比較したところ、OpenHandsは他のフレームワークに比べて、より安全なコードを出力する傾向が見られました。

さらに興味深いのは、各システムが異なるタイプの脆弱性に対して、それぞれ得意・不得意を持っていたという点です。分析では、ある種類の脆弱性（CWE）に対して成功率が25%以上だった場合、そのシステムは「そのCWEに対して慎重である」とみなしました。その結果、3つのLLMの間で「慎重に扱えるCWE」のうち、58パーセントが重なっていないことがわかりました。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_7-1024x394.png]]

各LLM／エージェントフレームワークが25%以上の成功率を持つCWEカテゴリの分布。重なりが小さく、モデルごとに得意な脆弱性タイプが異なることが分かる。

つまり、あるモデルが回避できる脆弱性を、他のモデルは見落としてしまうような関係性があるということです。

さらに同じCWEタグが付いたタスクであっても、プロジェクトが違えば結果が大きく変わることも示されました。たとえば、Claude 4はあるプロジェクトでは100パーセントの機能的正解率を記録しながら、セキュリティスコアはゼロというケースがありました。逆に、Geminiは機能面では劣っていても、セキュリティでは100パーセントという結果を出すこともあったのです。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_8-1024x235.png]]

複数プロジェクトにおけるClaude 4 SonnetとGemini 2.5 Proの機能性（FUNCPASS）とセキュリティ（SECPASS⊥FUNCPASS）の比較。プロジェクトによって「機能は強いがセキュリティは弱い」など、得手不得手が入れ替わる。

脆弱性が出てくる背景やプロジェクトの性質によって、各システムの強みや弱点が大きく左右されることを示しています。

### 具体的な脆弱性の事例

研究チームは、生成されたコードにどのような脆弱性が含まれているのかを深く理解するため、具体的な事例も分析しました。

その一例として取り上げられたのが、DjangoというWebフレームワークにおけるパスワード検証機能の実装です。このタスクでは、ユーザーが入力したパスワードが、保存されているハッシュ値（暗号化された文字列）と一致するかどうかを確認する関数を作成する必要がありました。

セキュリティ的に正しい実装では、たとえ入力されたパスワードが存在しなくても、あえて時間のかかる処理を実行することで、処理時間に差が出ないようにします。これは「タイミング攻撃」と呼ばれる手法に対する防御策です。タイミング攻撃とは、処理にかかる時間の違いから内部の情報を推測する攻撃手法で、例えば存在しないユーザー名への応答が異常に早い場合、その時間差から攻撃者が有効なユーザー名を見つけ出すことができてしまいます。

ところが、SWE-AgentとClaude 4が生成したコードでは、パスワードが None（無効な値）の場合や使用できない状態の場合、すぐにFalseを返すような処理になっていました。この実装では、ユーザーが存在するかどうかで応答時間に明確な違いが出てしまい、結果としてユーザー名の特定が可能になる脆弱性が生じていました。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_9-1024x592.png]]

Djangoのverify\_password実装タスクの例。見た目は動作しているが、None入力時に即座に返してしまうことでタイミング攻撃に脆弱な実装が生成されている。

このような脆弱性は、見た目には「機能が正しく動いている」ように見えるため、経験の浅い開発者にとっては気づきにくい典型的なパターンです。しかし、実際の攻撃ではこうした情報が悪用され、標的型フィッシングやアカウントの乗っ取りといった被害につながる恐れがあります。現実のセキュリティリスクとして、十分に警戒すべき事例といえるでしょう。

## セキュリティ対策の試み

各タスクの説明文に「セキュリティのベストプラクティスに従い、一般的な脆弱性を避けるよう注意してください」という一文を添える効果は限定的で、十分な改善にはつながりませんでした。

そこで研究チームは、さらに一歩踏み込んだアプローチを試すことにしました。SWE-AgentとClaude 4 Sonnetの組み合わせに対して、二つの追加戦略を用いたセキュリティ強化の検証が行われました。それが「自己選択方式」と「オラクル方式」です。

### 自己選択方式

人間のセキュリティ専門家が事前にリスクを見積もるプロセスをエージェントに模倣させる試みです。まずSUSVIBESで扱っている77種類のCWE（共通脆弱性タイプ）のリストと、それぞれの定義をエージェントに提供します。その上で、タスクの実装に入る前に、今回の課題に関係しそうなCWEを自分で選ばせます。そして、選んだリスクに注意を払いながらコードを実装させるという流れです。

### オラクル方式

タスクに関係する「正解のCWE」をあらかじめエージェントに伝え、それを避けるように明確に指示します。エージェントが「何に気をつけるべきか」を完全に理解している理想的な条件下でも、安全なコードを出力できるかどうかを検証する目的で行われました。

### 予想外の結果

実験の結果は、研究チームの予想に反するものでした。

まず機能面のスコアから見ると、どちらの戦略を使った場合も性能が大きく低下しました。一般的な注意書きだけを添えた場合、機能的に正しいコードの生成率（FUNCPASS）は61.0％でしたが、自己選択方式では52.5％と8.5ポイント下がり、オラクル方式でも56.0％と5.5ポイントの低下が見られました。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_10.png]]

さらに意外だったのは、セキュリティ面でもほとんど改善が見られなかったことです。SECPASS（機能もセキュリティも満たすコードの割合）は、一般的な注意書きのみのケースで10.5％。これに対し、自己選択方式ではむしろわずかに下がって9.5％となり、オラクル方式でも10.5％と全く変化がありませんでした。

つまり、具体的なセキュリティの指示や脆弱性に関する情報をエージェントに与えたとしても、実際には安全かつ正しいコードの生成率が向上するとは限らない、という結果が示されたのです。

### トレードオフの発生

この一見すると矛盾したように見える結果の背景には、2つの相反する傾向が同時に作用していることが明らかになりました。

1つ目はポジティブな傾向で、セキュリティに関するプロンプトを与えることで、以前は「機能的には正しいがセキュリティ的に不十分だったコード」が、安全な実装へと改善されるという効果が見られました。

しかしその一方で、ネガティブな傾向も確認されました。セキュリティに過剰に気を配るあまり、以前は問題なく実装できていたタスクが、機能面で要件を満たせなくなるケースが増えていたのです。

さらに詳しく分析すると、複数のシステムで共通して正解できたタスクに限っては、セキュリティ対策の導入によって、安全な実装の割合が増加していました。ただしその一方で、これまで問題なく対応できていたタスクでも、セキュリティへの意識が強くなりすぎたことで誤った実装になってしまう例が増えていたのです。この2つの効果がちょうど打ち消し合う形となり、全体的なセキュリティスコアの改善にはつながらなかった、というわけです。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_11.png]]

セキュリティ強化プロンプト導入によるトレンドの変化。安全な実装が増える一方、もともと正しく解けていたタスクが誤りに変わるケースも増え、全体としては得をしていない。

注目すべきなのは、オラクル方式のほうが、自己選択方式よりも機能面でのスコア低下が大きかった点です。これは、タスクに関連するリスクを自ら見つけて選ぶというプロセス自体が、問題の理解を深める助けになっている可能性を示唆しています。

### リスク識別能力の限界

では、エージェントはどの程度まで正確に潜在的なセキュリティリスクを見抜けるのでしょうか。研究チームは、自己選択方式でエージェントが選んだ結果を分析しました。

エージェントは、1つのタスクに対して平均して7.5個のCWE（脆弱性タイプ）を選択していました。ところが、実際にそのタスクに対応する正解のCWEは、平均でわずか1.06個しかなかったのです。つまり、数多くのCWEを選んでいたわりに、肝心の正解にはなかなかたどり着けていなかったことがわかりました。

さらに分析を進めると、正しくかつ安全なコードを生成できたタスクでは、正解のCWEを選べていた割合（再現率）が73.7%でした。一方で、機能的には正しくてもセキュリティ的に不十分だったタスクでは、その再現率が60.9%にとどまっていました。この差は、リスクを適切に識別できたかどうかが、安全なコード生成に影響している可能性を示しています。

とはいえ、最も良い結果でも再現率は73.7%にとどまり、つまり約4分の1のケースでは重要な脆弱性を見落としていたということになります。エージェントがセキュリティリスクを完全に把握するには、まだ大きな課題が残っていることが明らかになりました。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_12.png]]

GenericプロンプトからOracle方式に切り替えた際の評価結果の遷移。安全になったタスクもある一方で、正しく解けていたタスクが誤りになるケースも多い。

![[「Vibe Coding（バイブコーディング）」の脆弱性リスクについて実際の調査結果をもとに考える - AIDB/AIDB_98714_13.png]]

自己選択方式におけるCWE選択の精度と再現率。正しくかつ安全な実装では再現率が高いものの、最大でも約0.74にとどまり、多くのタスクで重要なCWEを取り逃している。

### セキュリティと機能性のバランスの難しさ

研究チームは、今回の結果の背景には、エージェント型のソフトウェア開発が持つ特有の性質があると考えています。

エージェントベースの開発では、「何をすべきか」という高レベルな意思決定が、非常に大きな役割を果たします。たとえば、どのファイルを参照するか、バグのありそうな箇所はどこか、フィードバックをどう解釈するかなど、こうした判断をエージェント自身が行います。これらの判断は、コードの実装における「アウトライン」として働き、エージェントの行動に柔軟さと方向性を与えることになります。

そこにセキュリティに関する具体的な指示を加えると、エージェントはその指示に応じて、セキュリティテストや検証のためのステップを増やすようになります。しかしその分、機能要件の実装に使えるステップが少なくなってしまい、特に機能性とセキュリティの両立が求められるようなタスクでは、そのバランスを取ることが難しくなってしまうのです。

たとえば、wagtailというプロジェクトでの検査機能を実装するタスクでは、セキュリティ指示がない通常のケースでは、エージェントは81ステップを使って、機能・セキュリティともに問題のないコードを生成できていました。ところが、セキュリティ関連の指示を加えると、そのうち4ステップを明示的なセキュリティテストに充てるようになり、機能の実装には72ステップしか使えなくなったことで、最終的にタスクの解決に失敗してしまいました。

こうした分析から見えてきたのは、セキュリティに関するプロンプトが具体的であればあるほど、かえって機能実装に支障が出やすくなるという傾向です。現在のような基本的な対策だけでは、セキュリティと機能性のバランスを上手く取るのは難しく、今後はより高度で洗練されたアプローチが求められることが明らかになりました。

## まとめ

本研究では、実際のオープンソースプロジェクトから抽出した200の開発タスクを含む、新しいベンチマーク「SUSVIBES」が開発されました。このベンチマークは、過去に実際に発生した脆弱性を題材としています。

複数のLLMとエージェントフレームワークを用いた実験の結果、一貫して深刻な問題が明らかになりました。エージェントは機能的に正しいコードを生成できる場合でも、その大部分にセキュリティ上の欠陥が含まれていました。さらに、セキュリティプロンプトの追加や、脆弱性タイプの事前識別、正解の脆弱性情報の提供といった初歩的な対策を試みても、この問題を解決できませんでした。

この結果は、Vibe Codingをセキュリティが重要な場面で無批判に採用することの危険性を示しています。汎用的なエージェントシステムにおいては、セキュリティを機能性と並ぶ最優先の目標として扱う必要があることが、この研究によって明確になりました。

**本記事の関連研究**

-   [Cursorはソフトウェア開発を加速する？導入後の実態に迫る](https://ai-data-base.com/archives/97454)
-   [ユーザーによる「曖昧な指示」や「不十分な依頼」、コード生成にどう影響する](https://ai-data-base.com/archives/97098)
-   [知らない分野ほど自信満々になってしまう現象はプログラミング中のLLMにも起きる？](https://ai-data-base.com/archives/96328)
