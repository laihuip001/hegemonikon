---
created: 2026-01-01T11:16:11 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/98441
author: AIDB Research
---

# 「検証してから答える」ことでLLMの推論精度を向上させる手法 - AIDB

> ## Excerpt
> LLMは複雑な推論において、流暢でも誤った答えを出すことがあります。 しかし「検証してから答える」だけで既存の高度な推論強化手法を上回る成果も報告されています。これは数学、プログラミング、エージェントの制御など、幅広い分野に応用が期待されています。 具体的にどういうことなのか詳しく紹介します。 本記事の関連研究 背景 業務でLLMを活用する企業が増えています。そこで、複雑な問題を扱う際には、「ステ…

---
LLMは複雑な推論において、流暢でも誤った答えを出すことがあります。

しかし「検証してから答える」だけで既存の高度な推論強化手法を上回る成果も報告されています。これは数学、プログラミング、エージェントの制御など、幅広い分野に応用が期待されています。

具体的にどういうことなのか詳しく紹介します。

![[「検証してから答える」ことでLLMの推論精度を向上させる手法 - AIDB/AIDB_98441-1024x576.png]]

## 背景

業務でLLMを活用する企業が増えています。そこで、複雑な問題を扱う際には、「ステップバイステップで考えて」といった指示がよく使われます。これは、問題を分解して段階的に考えさせる方法で、一般に高い精度が期待されるとされています。

ところが、実際に試してみると気づく課題があります。LLMは非常に自然で流れるような文章を出力する一方で、その内容が論理的に誤っていることが少なくありません。

こうした現象は「ハルシネーション」とも呼ばれます。原因は、LLMの出力が「もっともらしい単語の並び」を予測して生成されている点にあります。流暢さを優先する仕組みのため、筋の通った誤答が平然と出力されてしまうのです。

この問題に対処するため、さまざまな工夫が提案されてきました。詳細な指示文で誘導したり、同じ問題に複数回回答させて多数決をとったり、モデル自体を再学習させたりする方法です。

ただし、こうした手法には共通の悩みがあります。いずれも高いコストがかかるのです。特化プロンプトの作成には時間と知識が必要ですし、多数回の生成には大きな計算資源が必要です。追加学習となると、さらに高品質なデータと計算能力が求められます。

これまでLLMの推論を強化するには「コストがかかるのは当然」と考えられてきました。その前提に対し、今回まったく異なる視点から新たな方法を提示しています。

ここから限定コンテンツ

### **忙しい人向けに、重要なポイント5選**

1.  LLMに候補の答え（ランダムでも可）を先に検証させてから正解を導かせるだけで、推論精度が向上する
2.  追加学習や大量の計算は不要で、出力トークン数が20〜50%増える程度の最小限のコストで実現できる
3.  数学問題では標準的なCoTプロンプトと比較して最大10ポイント以上の精度向上を確認
4.  数学的推論だけでなく、プログラミング、API制御、エージェントタスクなど幅広い分野で有効性を実証
5.  Self-CorrectionやBest-of-Nなどの既存のテスト時スケーリング手法を、より少ない計算コストで上回る結果を達成

**参照文献情報**

-   タイトル：Asking LLMs to Verify First is Almost Free Lunch
-   URL：[https://doi.org/10.48550/arXiv.2511.21734](https://doi.org/10.48550/arXiv.2511.21734)
-   著者：Shiguang Wu, Quanming Yao
-   所属：Tsinghua University

## 「Verification-First」手法

提案されているアプローチは、驚くほどシンプルです。

一般的な指示では「ステップバイステップで考えて答えを出してください」と促します。しかし、ここでは流れを少し変えます。

「この問題の答えは○○かもしれません。まず、この答えが正しいかどうかを確かめてください。そのうえで、ステップバイステップで正しい答えを導いてください。」

基本はこれだけです。

ポイントは、提示する答えが正しくなくても問題ないところにあります。ランダムに選んだ答えでも、明らかに間違った短い答えでも効果があることが確認されています。

![[「検証してから答える」ことでLLMの推論精度を向上させる手法 - AIDB/AIDB_98441_1.png]]

本手法と、ふつうのCoTとの違いにおけるイメージの違い（赤がふつうのCoT）

![[「検証してから答える」ことでLLMの推論精度を向上させる手法 - AIDB/AIDB_98441_2-1024x728.png]]

本手法と、ふつうのCoTとの違いを実際の入力・出力で比較

### ランダムな答えでも精度が上がる

なぜ、正しいかどうかもわからない答えを検証させるだけで、LLMの精度が上がるのでしょうか。研究チームはその理由を2つ挙げています。

1つ目は、検証という行為そのものが推論を深めるという点です。認知科学では「答えを考えるより、与えられた答えを評価する方が負担が小さい」とされています。LLMも「答えは1です」といった候補を与えられると、「それが正しいなら、どういう条件を満たすはずか」という逆向きの推論を始めます。これは通常の“前向き”な推論とは異なる視点をもたらし、より多くの情報を引き出す助けになります。

たとえば「バットとボールで合計1.10ドル。バットはボールより1ドル高い。ボールの値段は？」という有名な問題。従来のプロンプトでは「バット1ドル、ボール0.10ドル」と誤って導きがちですが、「答えは1かもしれない」と検証させると「合計が3ドルになる」と気づき、正しく0.05ドルと導けるようになります。

2つ目は、外から与えられた答えを検証させることで、より客観的な思考を引き出せる点です。人間にも「自分の答えを正しいと思い込みやすい」傾向がありますが、LLMも自ら出した答えに引きずられて誤った推論を続けてしまうことがあります。外部から与えられた答えを評価する立場に置くことで、その流れを断ち切り、慎重な判断が促されるのです。

もう一つの例として、「28日ある月はいくつ？」という問題があります。直感的には「2月だけ」と答えがちですが、「答えは1」と提示して検証させると、「28日が“含まれる”月という意味なら、すべての月に28日はある」と気づき、正しく「12」と答えられるようになります。

直感に頼らず、外からの視点で問い直す。その姿勢が、LLMの推論にも有効に働くということです。

### 複雑なタスクにも対応できるよう拡張するには

ランダムな答えを検証させる手法は、数学の文章題や選択問題では効果を発揮します。しかし、実際のビジネス現場ではより複雑なケースに直面します。

たとえばコード生成のようなタスクでは、「答えは1です」や「print(‘Hello World’)」といった単純な出力を検証しても、問題の本質には迫れません。

そこで研究チームは、この手法をより広く使えるよう拡張しました。

やることはシンプルで、LLMを2回使うだけです。

まず1回目で、通常のプロンプトを用いて答えを生成します。次に、その出力を「仮の答え」として2回目の入力に組み込み、検証と再生成を促します。

たとえば、「リスト内のペア間の最大差を求める関数を書け」という問題が例として挙げられています。1回目の出力は、リスト全体の最大値と最小値の差を取るという誤ったものでした。しかし2回目の検証フェーズでは、ペアごとの差を考える必要があると気づき、正しいコードへと修正されました。

このように「仮の答え→検証→再出力」の流れを繰り返すことで、精度をさらに高めることができます。この反復型の戦略は、計算コストの許す限り繰り返せます。

初回の出力はランダムでも、通常のプロンプトでも構いません。出力を検証材料として活用するという発想が大事です。

![[「検証してから答える」ことでLLMの推論精度を向上させる手法 - AIDB/AIDB_98441_3.png]]

LLMを2回使うパターン

### 既存の反復手法とは決定的に異なる

今回の手法は、Self-CorrectionやReflexionなど、これまで提案されてきた自己改善プロンプトと似た構造を持っています。しかし、根本的な違いがあります。

従来の手法では、前回の推論の流れ全体をすべて保持したまま、どこに誤りがあったかを振り返り、修正を試みます。つまり、前のプロセス全体を引き継いで再推論する仕組みです。

それに対し、今回の手法は前回の「答え」だけを取り出し、それが正しいかを検証することから再スタートします。過去の思考プロセスには依存せず、毎回ゼロから考え直します。

この違いは、統計の分野でいう「マルコフ性」に関係します。マルコフ性とは「次の判断が、直前の結果だけに基づいて決まる」性質のことです。この手法はマルコフ的であるため、LLMが長すぎる文脈を処理できずに混乱するリスクを避けることができます。過去の誤りが影響し続けることもありません。

実際、LLMは文脈が長くなるほど誤り（ハルシネーション）が増えやすくなります。既存のSelf-Correctionは、繰り返すほど精度が落ちる傾向がありますが、今回の手法はその弱点を構造的に避けられる設計になっています。

## 実験で効果を検証

研究チームは、この戦略が本当に有効かどうかを確かめるため、さまざまなタスクで検証を行いました。数学の推論問題やコード生成だけでなく、実際のビジネスを想定したタスクにも対応しています。

実験は、主に4つの観点から設計されています。

### ランダムな答えでも有効か

まずは最もシンプルな形として、ランダムな答えを使ったプロンプトの効果を検証しました。対象は数学の推論タスクで、広く使われている3つの標準ベンチマークが使われています。

使用されたのは、小中学校レベルのGSM8K、大学レベルのMATH500、そして大学院レベルのGPQA-Diamondです。GSM8KとMATH500では、すべての問題に対して「答えは1です」という単純な候補を提示。GPQA-Diamondでは、選択肢をシャッフルし、ランダムに1つを候補として提示しました。

モデルはQwen2.5とLlama3から選ばれ、1B（10億）から72B（720億）パラメータまで、さまざまな規模のものが使われました。一般にモデルは大きいほど高性能ですが、そのぶん計算コストも上がります。

結果は明確でした。プロンプトの形式を変えただけで、モデルのサイズにかかわらず、従来のステップバイステップ型の出力（CoT）より高い精度が得られました。GSM8KやMATH500では、正解率が10ポイント以上向上するケースも確認されています。

![[「検証してから答える」ことでLLMの推論精度を向上させる手法 - AIDB/AIDB_98441_4.png]]

![[「検証してから答える」ことでLLMの推論精度を向上させる手法 - AIDB/AIDB_98441_5.png]]

一方、GPQA-Diamondでは改善の幅がやや小さくとどまりました。研究チームは、問題の性質が「知識寄り」であるため、推論プロセスだけでは限界があると分析しています。

コストについても検証が行われました。検証のプロセスが入るため、出力されるテキスト量（トークン数）は、従来のプロンプトに比べて20〜50%ほど多くなります。LLMの利用料金はトークン数に比例するため、一見するとコスト増に思えるかもしれません。

しかし、他のテスト時スケーリング手法は数倍から数十倍のトークンを必要とするため、それに比べると非常に小さな増加です。性能向上とのバランスを考えれば、十分に費用対効果が高い手法といえます。

さらに、候補として与える答えの違いが結果にどう影響するかも調べられました。GSM8KとMATH500では「1」「2025」「正解」の3種、GPQA-Diamondでは「ランダムな選択肢」「誤答」「正解」で比較しています。

当然ながら、正しい答えを検証させた場合（ただし正しいと明示はしない）には、大きな精度向上が見られました。これは、LLMが検証に強みを持つこと、そして「検証は生成よりもやさしい」ことを示しています。

![[「検証してから答える」ことでLLMの推論精度を向上させる手法 - AIDB/AIDB_98441_6-1024x300.png]]

注目すべきは、ランダムな答えや明らかな誤答でも、最終的な性能に大きな差が出なかった点です。つまり、どんな答えを与えるかよりも、「検証する」行為そのものが重要だということです。

実務上は、正しいかどうかを気にせず、単純なランダム回答を与えるだけで十分に効果があり、複雑な表現はかえって避けたほうがよいという指針が得られます。

### 反復的検証によるスケーリング効果

次に、今回の手法が他のテスト時スケーリング手法と比べてどれだけ有効かを検証した実験を紹介します。  
テスト時スケーリングとは、推論時に計算リソースを多めに使うことで精度を高める手法群のことです。

今回の手法は、追加学習や専門知識を使わずに利用できるため、同じ条件で使える代表的な手法と比較されました。比較対象は次のとおりです。

-   Self-Correction（前回の推論内容すべてを保持し、改善点を反省させる逐次型手法。）
-   PHP（過去の答えを「ヒント」として提示するが、指示は行わない。）
-   Self-Consistency（複数の推論結果を並列に出し、多数決で答えを決める。）
-   Best-of-N（複数の答えを生成し、LLM自身に評価させて最良の答えを選ぶ。）

今回の手法には2つのバリエーションがあります。初回に「1」という些細な答えから始める「VF起点」と、通常のCoTで始めて2回目から検証に入る「CoT起点」です。

MATH500での実験では、どちらのパターンでも、限られた反復回数で他の手法より高い精度を達成しています。

![[「検証してから答える」ことでLLMの推論精度を向上させる手法 - AIDB/AIDB_98441_7-1024x440.png]]

興味深いのは、VF起点とCoT起点で初期性能に差はあるものの、数回の反復でほぼ同じ結果に収束する点です。これは、初回の答えよりも「検証というプロセス自体」が鍵であることを示しています。

PHPとの比較では、ヒントだけを与えるPHPに対し、明確に「検証せよ」と指示するこの手法の方が高性能でした。LLMが自動的にヒントを活用できるわけではなく、明示的な指示が必要ということです。

Self-Correctionとの違いはさらに顕著です。Self-Correctionは、繰り返すほど性能が悪化する傾向があります。過去の出力をすべて引き継ぐことで文脈が肥大化し、誤りが蓄積してしまうためです。  
対照的に、今回の手法は「前回の答えだけを使って再スタート」する仕組みのため、こうした劣化を回避できます。

一方、Self-ConsistencyやBest-of-Nは計算量を増やすほど安定的に精度が上がります。並列処理できる点では有利ですが、改善の幅はそれほど大きくありません。

ただし、研究チームは今回の手法にも限界があると述べています。あまりに多く反復すると、毎回文脈をリセットする性質上、性能が頭打ちになる可能性があります。

そこで今後の方向性として、複数の検証経路を並列に走らせたり、すべての出力に対して多数決を取ったりする設計も提案されています。反復と並列処理の強みを組み合わせることで、さらなる性能向上が期待されます。

### 実際の業務に近いタスクでの検証

研究チームは、学術ベンチマークにとどまらず、実際のビジネス現場を想定したタスクでも手法の有効性を検証しました。評価は2つの観点から行われています。

#### 形式が複雑なタスクでの評価

1つ目の観点は、答えの形式が複雑で、ランダムな候補をあらかじめ定めるのが難しいタスクです。対象は、コード生成とAPI呼び出しの2種類です。

コード生成では、HumanEvalとMBPPという標準ベンチマークが使われました。出題は「リスト内の数値ペアが一定の条件を満たすか確認する関数」や「球の体積を求める関数」などです。こうした問題では、「return 1」や「print(‘Hello World’)」のような答えを検証しても意味がないため、VFプロンプトには前回のコード出力をそのまま使う方式が取られました。

API呼び出しでは、API-BankのLevel-1とLevel-2が使われました。これは対話型のタスクで、ユーザーの発言を理解した上で、ツール関数のドキュメントを参照し、正しい関数と引数でAPIを呼び出す必要があります。この場面でも、関数名や構文を事前にランダムで用意するのは難しいため、同様に前回の出力を使う方式が採用されました。

比較対象として、同じく2回分の生成コストで使える手法が選ばれました。

-   CoT×2（pass@2）(2回独立に生成し、どちらかが正解なら成功とみなす並列型。）
-   Self-Correction（1回目の出力と推論内容をそのまま2回目に渡し、改善を促す逐次型。）

評価には「pass@1」と「pass@2」という2つの指標が使われています。pass@1は最終出力の正答率、pass@2は1回目と2回目のいずれかが正解なら成功と判定します。最終的な性能比較では、pass@2が重視されます。

結果は明快でした。pass@2での評価では、今回の手法が一貫して最高の性能を示しました。

![[「検証してから答える」ことでLLMの推論精度を向上させる手法 - AIDB/AIDB_98441_8-1024x392.png]]

APIタスクでは、Self-Correctionとほぼ同等の精度になるケースもありましたが、研究チームはその理由を「推論が非常に短く、答えの処理が中心になるタスクだから」と説明しています。こうしたケースでは、プロンプト設計の違いがあまり影響しません。

またpass@1を見ると、VFもSelf-CorrectionもCoT（1回生成）を安定して上回っており、テスト時に計算を増やす価値が確認されました。

さらに注目すべきは、VFのpass@1が、CoT×2のpass@2より高いケースもあったことです。これは、単に答えを複数生成するよりも、検証によるフィードバックがある方が有効な場合があることを示しています。反復と検証を組み合わせたアプローチが、特定の実用タスクでより強力に働くことが明らかになりました。

#### 内部の思考プロセスが見えないLLMでも有効か

もう一つの重要な検証対象は、商用の先端LLMサービスでの有効性です。GPT-4やGemini、Claudeといったモデルは、推論の過程をユーザーに公開せず、入力に対して最終的な答えだけを返す仕組みになっています。

このような環境では、多くのプロンプト設計やスケーリング手法が使えません。たとえば、「ステップバイステップで考えて」といった指示も、モデル内部にすでに推論機構が備わっていれば意味を持たない可能性があります。Self-Correctionのように、前回の推論内容を参照して改善を促す手法も、そもそも思考内容が見えない以上、使えません。

その点で、今回の戦略はシンプルな利点があります。必要なのは、入力に「まず検証してから答えよ」と明示するだけ。内部の設計に関係なく、別の思考経路を強制できるため、ブラックボックスなモデルに対しても効果が期待できます。

研究チームは、OpenAIの商用モデルであるGPT-4o-miniとo1-mini（論文中では仮名としてGPT-5 Nano/Miniと記載）で実験を行いました。どちらも思考プロセスを非公開とする設計です。

いずれも、明示的な検証指示だけで精度が向上しています。

![[「検証してから答える」ことでLLMの推論精度を向上させる手法 - AIDB/AIDB_98441_9.png]]

注目すべきは、出力トークンの増加が少ない点です。MATH500において、GPT-4o-miniでは約38%、o1-miniでは約29%の増加にとどまりました。これは、すでに強力な内部推論を備えたモデルであっても、「検証してから答える」という明示的な流れを加えるだけで、追加の価値が生まれることを示しています。

## まとめ

本記事では、清華大学の研究チームが提案した「まず検証してから考える」戦略を紹介しました。

LLMに対して候補の答えを提示し、その正しさを先に検証させるというシンプルな発想です。候補はランダムなものでも構いません。この指示によって、通常の順方向とは異なる“逆向きの推論”が働き、LLMの論理的な誤りが減ることがわかっています。

実験では、数学、コーディング、API呼び出し、エージェント制御といった幅広いタスクで、標準的なステップバイステップ指示（CoT）を一貫して上回る性能が確認されました。追加学習や大規模な計算は不要で、コスト増はトークン数が2〜5割増える程度に抑えられます。

さらに、この検証プロセスを繰り返す方式（Iter-VF）は、従来のスケーリング手法より少ない計算量で高い性能を実現しました。毎回の推論をリセットする“マルコフ性”により、文脈の肥大化や誤りの蓄積を防げる点が大きな特徴です。

商用の高度なLLMでも効果が確認されており、思考過程が見えないモデルに対しても有効です。現実的で応用範囲の広い手法として注目されます。
