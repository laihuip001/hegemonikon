---
created: 2026-01-01T09:37:52 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/60678
author: AIDB Research
---

# わずか2行のプロンプトでも実効性のある新しいアライメント手法『URIAL』 - AIDB

> ## Excerpt
> 「LLMのアライメントは、実はプロンプトで少し指示を与えるだけでも実現できるのではないか？」と考えたワシントン大学とAI2の研究者らは、最低2行のプロンプトから実効性のある新しいアライメント手法『URIAL』を考案しました。 なおアライメントとは、AIが持つべき道徳的価値観や行動指針を定めて、人間社会の倫理に合わせるプロセスです。 本記事では、『URIAL』の手法や効果などを見ていきます。 参照論…

---
「LLMのアライメントは、実はプロンプトで少し指示を与えるだけでも実現できるのではないか？」と考えたワシントン大学とAI2の研究者らは、最低2行のプロンプトから実効性のある新しいアライメント手法『URIAL』を考案しました。

なおアライメントとは、AIが持つべき道徳的価値観や行動指針を定めて、人間社会の倫理に合わせるプロセスです。

本記事では、『URIAL』の手法や効果などを見ていきます。

![[わずか2行のプロンプトでも実効性のある新しいアライメント手法『URIAL』 - AIDB/AIDB_60678_thum-1024x576.jpg]]

**参照論文情報**

-   タイトル：The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning
-   著者：Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, Yejin Choi
-   所属：Allen Institute for Artificial Intelligence, University of Washington
-   URL：[https://doi.org/10.48550/arXiv.2312.01552](https://doi.org/10.48550/arXiv.2312.01552)
-   GitHub：[https://allenai.github.io/re-align/](https://allenai.github.io/re-align/)

**本記事の関連研究**：[AGI（汎用人工知能）の原則6箇条とレベル5段階](https://ai-data-base.com/archives/58565)

## 研究の背景

### 従来のアライメント手法

これまで大規模言語モデル（LLM）のアライメント調整（人間の倫理観や道徳観に沿った行動を取るようにする手法）には、主に2つの方法が用いられていました。

1つ目は人間のフィードバックに基づく強化学習（RLHF）、2つ目は監督付きファインチューニング（SFT）です。

しかし、従来のアライメント手法は下記のような課題を抱えています。

**（１）大量の計算リソースが必要**

RLHFやSFTは膨大な計算能力を必要とし、コストが高くなる傾向があります。

**（２）長期間にわたる学習が必要**

長時間の学習プロセスを要するため、モデルの迅速な開発やイテレーションが困難になります。

**（３）特定のタスクやデータセットに過度に適合**

調整しすぎた結果、多様なシナリオでの一般化性能が低下することがあります。

### 研究者の疑問

研究者たちは「より簡単な方法で同じアライメントを実現できないか？」という疑問を持ちました。

そして、従来の手法はLLMの表面的な部分にのみ影響を及ぼすことに着目しました。そもそもLLMは人間の指示に正確に従うことは困難だとも認識していました。

そこで、無調整モデルで同様のアライメントを達成する方法として、プロンプトだけを使ってLLMのアライメントを改善する新しい手法「URIAL」の開発に着手しました。

なお下の画像は、アライメントを調整したLLMと調整していないLLMがどのように応答を生成するかの違いを分析しています。また、アライメント調整によってどのようなトークンが選ばれる傾向にあるかを示しています。

![[わずか2行のプロンプトでも実効性のある新しいアライメント手法『URIAL』 - AIDB/AIDB_60678_2-1024x580.jpg]]

**本記事の関連研究**：[大規模言語モデルのセーフガードを故意に突破する「脱獄プロンプト」とは](https://ai-data-base.com/archives/54468)

研究者らは、LLMの重みを調整することなくアライメントする手法を、『URIAL』（Untuned LLMs with Restyled In-context ALignment）と名付けました。

インコンテキスト学習（ICL）、つまりシステムプロンプトを用いて実現されます。

下の図は、『URIAL』で調整したLLMの応答例を示しています。

![[わずか2行のプロンプトでも実効性のある新しいアライメント手法『URIAL』 - AIDB/AIDB_60678_5-1024x696.jpg]]

### 『URIAL』のポイント

**（１）大規模データや複雑な学習を必要としない**

大量のデータセットや時間を要する複雑な学習プロセスを必要としません。代わりに、わずか数行のインコンテキスト学習を使用します。

****（２）**インコンテキスト学習を用いた細かい調整**

学習させたいテーマに関する具体的な質問とその答えの例をいくつかモデルに示します。すると似たような新しい質問に対しても、適切な応答を生成できるようになります。

**（３）様々なタスクや応答スタイルに柔軟に適応**

さまざまなタスクや応答スタイルに柔軟に対応できるように設計されています。ユーザーのニーズに合わせてモデルを調整することが可能です。

### 技術的フレームワーク

『URIAL』は、下記のように既存のアライメント手法と比較して、よりシンプルで効率的なアプローチです。

**（１）ベースLLMの使用**

まず、基礎となるLLMを選択します。性能や学習済みのデータ、使用環境要件、計算コストやリソースなどを考慮します。

**（２）特定のプロンプトの導入**

次に、モデルにプロンプトで特定の指示や例を与えます。特定のタイプの応答やスタイルを教えるために必要なだけの簡単な文章や質問です。

**（３）インコンテキスト学習の活用**

モデルは、与えられたプロンプトを元に、そのスタイルや内容に合わせた応答を学びます。

なお、「従来の手法であるRLHFやSFTはLLMの表面的な部分しか改善していないのではないか」と研究者らが考えたのには背景があります。  
ベースとなるLLMと調整済みLLMの間のトークン分布のシフトに関する詳細な分析を行った結果、アライメントがトークン選択のわずかな部分（スタイルや安全に関する方針など）にのみ主に影響を与えると考察されたのです。

下の図は、異なるLLMの間でトークン分布の変化を示しており、アライメント調整前後でトークンの選択がどのように変わるかを比較しています。

![[わずか2行のプロンプトでも実効性のある新しいアライメント手法『URIAL』 - AIDB/AIDB_60678_3-1024x351.png]]

また下のグラフは、言語モデルのアライメント調整によるトークンの分布の変化を数値的に分析した結果を示しています。

![[わずか2行のプロンプトでも実効性のある新しいアライメント手法『URIAL』 - AIDB/AIDB_60678_4-1024x323.jpg]]

**本記事の関連研究**：[現時点でのLLMに対する網羅的な評価分析が行われました。](https://ai-data-base.com/archives/58149)

## 実験と結果

### 実験内容

『URIAL』の効果を検証するため、研究者たちは様々なLLMに対して、

ここから限定コンテンツ

異なる数のインコンテキスト例（K=1, K=3, K=8など）を使用し、その性能を従来のアライメント手法（SFTおよびRLHF）と比較しました。

なお『URIAL』の評価を行う際には、さまざまなタイプのタスクやトピックにわたる性能を検証することが不可欠でした。下のグラフは、『URIAL』を用いたモデルの評価結果の統計データです。データセットのサブセット名、タスクのタイプ、そしてトピックカテゴリーの分布が示されています。

![[わずか2行のプロンプトでも実効性のある新しいアライメント手法『URIAL』 - AIDB/AIDB_60678_6-1024x472.jpg]]

### 効果の検証結果

**RLHFやSFTと比較して同等またはそれ以上の性能**:

『URIAL』は、Llama-2-7bモデルでのSFT/RLHFの結果と同等、またはそれを超えるレベルの性能を達成しました。

さらに、Mistral-7Bモデルをベースに使用した場合、公式のSFTモデル（Mistral-7B-Instruct）を上回る結果を示し、7BレベルのLLMの中で最高のパフォーマンスを達成しました。

なお、従来の手法で調整されたLLMは、知識を忘れたり、過敏になったりする可能性があります。たとえばSFTで調整されたMistral-7Bは「Facebookは社名を変更していない」と誤って回答する可能性があります。一方で同モデルに『URIAL』を適用した場合は、「Meta Platform Inc.」と正しく回答できます。

**少ない計算リソースと短時間での目的達成**:

より多くの例を使用することが必ずしも全体的な性能向上につながらないことがわかりました。平均してK=3の定数インコンテキスト例を使用し、合計で約1011トークン（671語）を使用しました。

**さまざまなタイプのLLMに適用可能**:

異なるタイプのLLMに対して一貫した効果を示しました。異なるインコンテキスト例に対する頑健性も示し、その結果、便益性、明確性、事実性、深さ、エンゲージメント、安全性などのさまざまな指標で高いスコアを達成しました。

下の図は、その比較結果をまとめたもので、『URIAL』を含む各手法がどれだけ効果的であるかを示しています。

![[わずか2行のプロンプトでも実効性のある新しいアライメント手法『URIAL』 - AIDB/AIDB_60678_1.png]]

上記の結果から、従来のアライメント手法に代わる効果的で効率的な代替手段としての潜在力を持っていることが示されました。ただし『URIAL』のアプローチは、LLMの基本モデル自体の会話能力に大きく依存していることも示唆されています。

下の表は、異なるアライメント手法を使ったモデルの性能を複数の側面で数値化して比較しています。

![[わずか2行のプロンプトでも実効性のある新しいアライメント手法『URIAL』 - AIDB/AIDB_60678_7-1024x662.png]]

**本記事の関連研究**：[LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ](https://ai-data-base.com/archives/58767)

## 注意点と展望

『URIAL』は、SFT（監督付きファインチューニング）やRLHF（人間のフィードバックからの強化学習）と同等の性能を持ちながら、これら従来の手法に比べて実装が簡単で、完全に再現可能です。非常に大きなモデル（例：Llama-2-70b、Falcon-180b）を最小限の労力でアラインメントする点も魅力です。

ただし、URIALがすべてのシナリオでSFTやRLHFを完全に置き換えるとは言われていません。

コーディング、数学、対話エージェントなどのタスクでは、モデル自体のチューニングが依然として必要とされる場合があります。

下のレーダーチャートは、異なるタスクやトピックに対するモデルのアライメント性能を視覚化しています。

![[わずか2行のプロンプトでも実効性のある新しいアライメント手法『URIAL』 - AIDB/AIDB_60678_8-1024x621.jpg]]

**本記事の関連研究**：[大規模言語モデルの毒に用心を　データポイズニングのリスク](https://ai-data-base.com/archives/52217)

## まとめ

本記事では、手軽なアライメント手法として開発された手法『URIAL』について紹介しました。

従来の大規模言語モデル（LLM）アライメント手法（RLHFやSFT）は、高いリソース要求や長期間の学習プロセスなどの課題がありました。一方で『URIAL』は、モデルの重み調整なしでLLMのアライメントを達成する新手法です。インコンテキスト学習（ICL）を用いたプロンプトに基づいており、大規模データや複雑な学習を必要としません。

実験の結果、従来のSFTやRLHFと比較して同等またはそれ以上の性能を示し、少ない計算リソースで多様なLLMに適用可能です。  
ただし『URIAL』はタスクや分野によっては限界があります。この手法の理解にはさらなる長期的な研究が必要と考えられています。
