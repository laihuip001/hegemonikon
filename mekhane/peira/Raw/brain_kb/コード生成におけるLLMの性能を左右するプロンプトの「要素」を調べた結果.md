---
created: 2026-01-01T11:17:58 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/94072
author: AIDB Research
---

# コード生成におけるLLMの性能を左右するプロンプトの「要素」を調べた結果 - AIDB

> ## Excerpt
> 本記事では、コード生成におけるLLMの性能に影響を与えるプロンプト内の要素を調べた研究を紹介します。 LLMは例示されたコードから新たな問題を解く能力を持ちますが、その性能はどのような情報を与えるかによって大きく変わります。この研究は、変数名やコードの書式など、例として与えるコードの細かな構成要素がモデルに与える影響を系統的に検証しています。プロンプト設計やベンチマーク作成に携わる実務者にとって、…

---
本記事では、コード生成におけるLLMの性能に影響を与えるプロンプト内の要素を調べた研究を紹介します。

LLMは例示されたコードから新たな問題を解く能力を持ちますが、その性能はどのような情報を与えるかによって大きく変わります。この研究は、変数名やコードの書式など、例として与えるコードの細かな構成要素がモデルに与える影響を系統的に検証しています。  
プロンプト設計やベンチマーク作成に携わる実務者にとって、実用的なヒントが得られる内容です。

![[コード生成におけるLLMの性能を左右するプロンプトの「要素」を調べた結果 - AIDB/AIDB_94072-1024x576.png]]

## 背景

ソフトウェア開発におけるLLM活用の中でも、コード生成はとくに注目されている分野です。

LLMによるコード生成では、追加の学習やパラメータ更新を行わずとも、入力文に例を組み込むだけで出力を改善できることがテクニックとして知られています。

あらかじめ用意した例をプロンプトに加えることで、モデルは人間の書いたコードのパターンを学び取り、新しい問題にも対応します。

しかし、一つの大きな疑問が残っています。それは「与えた例の中のどの要素が性能向上に貢献しているのか」という点です。たとえば、変数名や関数名の付け方など、いくつもの要素がありますが、それぞれがどの程度モデルの理解に影響を与えているのかは、はっきりしていません。

こうした細かな要素の影響を理解することができれば、より効果的なコード例の作り方が見えてきます。そこで本記事では、例の変化がモデルの出力にどう影響するかを調べるという実験の結果を取り上げます。

ここから限定コンテンツ

## コード生成の鍵は「例の出し方」？

### 例を見せるだけで学ぶ

In-Context Learning（文脈内学習）は、LLMが少数の例を参考にして学ぶ仕組み（そうした能力を備えていること）です。モデル全体を再訓練しなくても、プロンプトにいくつかの例を加えるだけで、新しい問題に対応する現象を指します。

たとえばコード生成の場面では、「プログラミングの問題」と「その正解コード」をセットにした例を用意します。モデルは、その中から関係の深いものをいくつか選び、新しい問題と一緒にプロンプトに組み込みます。これだけで、追加の学習なしに解答を出せるようになります。

モデルの中身をいじる必要がないため、計算リソースの節約にもなり、現場での使いやすさという点でも注目されています。

### 「例の中の何が効いているのか」という素朴な疑問

文脈内学習はなぜうまくいくのかを探る研究が盛んに行われています。これまでの研究では、コード例と新しい問題との「意味的な近さ」が重要だと考えられてきました。

けれども、多くの研究ではコード例をひとまとまりとして扱っていて、その中のどの要素が本当に効果をもたらしているのかまでは深掘りされていません。変数名のつけ方、コードの整え方、あるいは実装の書き方など、要素はいろいろありますが、それぞれの影響度はよく分かっていません。

背景でも述べたように、もしそれぞれの要素がどれくらい効果に関わっているかを把握できれば、もっと的確なコード例を選んだり作ったりできるはずです。そこで今回研究者たちは、正解コードから特定の要素をわざと取り除き、その変化がモデルの出力にどう影響するのかを丁寧に検証するという、かなり実験的なアプローチを試みました。

## 実験の進め方

### 全体の流れ

研究チームは、コードのどの要素がLLMの出力に影響しているかを明らかにするために、段階的な実験を設計しました。

基本的には、正解コードの一部を意図的に取り除いて、その変化がモデルの生成結果にどう表れるかを観察するという内容です。

実験は三つの課題に分かれています。

最初の二つでは、LeetCodeから収集した正解コードをもとに、特定の情報を削る加工を施し、モデルの挙動を比較しました。たとえば、変数名を記号のような名前に置き換えたり、インデントをすべて削除したりすることで、どの情報が効いているのかを検証しました。

三つ目の課題では少し異なるアプローチがとられました。正解コードではなく、似たような問題のコードを例として与えることで、モデルがそこからヒントを得られるかどうかを確かめました。これは、現実的な応用に近い設定です。

![[コード生成におけるLLMの性能を左右するプロンプトの「要素」を調べた結果 - AIDB/AIDB_94072_1.png]]

実験の全体的な流れ

では課題を一つずつ見ていきます。

### 課題1 コードのどこが性能に影響しているのか

#### コードの構成要素を三つに分類

検証の対象となるコード要素は、命名（意味）、書式（見た目）、実装（構造）の三つに分けて整理されました。

命名に関わる要素は、変数名や関数名、クラス名などです。どんな名前が使われているかによって、コードの意味がどれだけ伝わるかが左右されます。

書式に関する要素は、インデントや改行など、コードの視認性を支える見た目のルールを指します。

実装に関する要素は、制御構文や予約語、アルゴリズムの記述スタイルといった、コードの論理的な構造に関わる部分です。

![[コード生成におけるLLMの性能を左右するプロンプトの「要素」を調べた結果 - AIDB/AIDB_94072_2.png]]

文脈内学習に用いたプロンプトの構造

#### 九つのパターンでコードを意図的にいじる

それぞれの要素がどの程度影響しているのかを調べるため、研究チームは九つの変異パターンを準備しました。対象となる情報を意図的に加工したバージョンのコードを作ることで、その違いを観察するのです。

1.  関数名を記号のような名前に置き換える
2.  変数名を記号化する
3.  関数名と変数名の両方を同時に記号化する
4.  インデントをすべて削除する
5.  改行を削除してコードを一続きにする
6.  コードをトークンの並びに変換する
7.  プログラミング構造を自然言語で説明する形に変換する
8.  for文とwhile文を入れ替える
9.  言語固有の構文を取り除き、疑似コードに変換する

あわせて、正解コードをそのまま使ったバージョンと、例をまったく与えないゼロショットのパターンも用意されました。

### 課題2 変数名のスタイルがモデルに与える影響

#### 命名ルールや情報量に注目

変数名は、書式、情報量、文字数の長さという三つの観点から整理されました。

書式は、Pythonならsnake\_case、JavaならcamelCaseのように、言語ごとの命名ルールに従っているかどうかを指します。

情報量は、変数名にどのくらい意味や役割が含まれているかという点です。

文字数の長さは、変数名が短いものと長いものでは、モデルの理解に差があるかを確認するための指標です。

#### 新たに四つの変異スタイルを導入

変数名のスタイルを検証するために、以下の四つのパターンが用意されました。

1.  モデルを使って変数名を標準的な命名に書き換える
2.  意味を詳しく説明するような長めの変数名に置き換える
3.  よく使われるが意味を持たない変数名に差し替える
4.  頻出する三つの単語を組み合わせ、言語ごとのスタイルで命名する

### 課題3 似たコードからどこまで学べるのか

#### 正解コードなしでも通用するのかを検証

実際の利用シーンでは、問題に対する正解コードが手に入らないことも多くあります。そこで三つ目の課題では、正解ではないけれど内容が似ているコード例を与えたときに、モデルがそこからどれだけ学べるかを検証しました。

#### 類似問題を見つけるための検索システム

この検証では、検索機能を備えた生成システムが使われました。対象の問題に関連するコードを探し、プロンプトに組み込むという仕組みです。

検索には以下のように複数の方法が使われました。

-   単語の出現頻度に基づいた古典的な検索方法
-   意味の近さを数値ベクトルで測る埋め込みモデルによる検索
-   問題に付けられたアルゴリズム分類を使った検索
-   比較のために用意されたランダム検索

検索の結果として使うコード例の数も、ひとつの場合と五つの場合を比較し、単一の例と複数の例で効果に違いが出るかを見ました。

![[コード生成におけるLLMの性能を左右するプロンプトの「要素」を調べた結果 - AIDB/AIDB_94072_3.png]]

類似問題を検索してプロンプトに組み込む仕組み

### 実験に使ったモデルと評価のしくみ

#### 評価に使った四つのLLM

検証に使われたのは、次の四つのモデルです。

-   GPT-4o-mini（OpenAI）
-   Qwen2.5-7B-Instruct（Qwen）
-   Qwen2.5-32B-Instruct（Qwen）
-   DeepSeek-Coder-V2-Lite-Instruct（DeepSeek）

すべてのモデルで、出力のばらつきをなくすためにtemperatureはゼロに固定され、最大出力長も共通で設定されました。プロンプトの形式は、LiveCodeBenchの公式テンプレートに統一されています。

#### LeetCode問題を用いた現実的な検証

実験では、LeetCode（コーディング面接準備のためのオンラインプラットフォーム）の中から362問を選びました。自己完結しており、外部ライブラリに依存せず、明確なアルゴリズム問題として成り立っています。さらに、各問題にはタイムスタンプが付いているため、学習データとの重なりを避けやすくなっています。

使用言語はPythonだけでなく、実際の開発や教育でも広く使われているJavaも含まれました。Javaについては、Pythonの実装を参考に、専用の実行環境とプロンプトが用意されました。

性能の評価には、pass@1が使われました。これは一回の試行で正しく動作するコードを生成できたかどうかを見る指標で、コード生成タスクでは一般的に用いられています。

## 実験結果と分析

### 課題1 コードのどの部分が効いているのか

#### 変数名の影響がとくに大きい

実験では、コード内のさまざまな要素のうち、変数名が特に大きな役割を果たしていることが分かりました。

変数名をすべて記号に置き換える（VOE）、あるいは関数名と合わせて記号化する（FVE）と、モデルの性能が大きく低下しました。とくに関数名と合わせて記号化する（FVE）場合では、Qwen-7BによるPythonコードの生成で、スコアが30ポイントも下がるケースがありました。

一方で、インデントや改行の削除、トークン化といった書式の変化や、自然言語への変換、ループ構造の書き換え、疑似コード化といった実装部分の変更は、それほど大きな影響を与えませんでした。平均的な性能の低下はおおよそ20ポイントほどで、変数名の「意味」がモデルの理解に与える影響の大きさが際立つ結果となりました。

興味深いのは、関数名だけを記号化した場合（FOE）には、性能の低下がそれほど大きくなかった点です。関数名はプロンプト中で直接指定されることも多いため、モデルはコード本体から意図を補えると考えられます。

#### プログラミング言語ごとの違い

PythonとJavaでは、モデルの反応に違いが見られました。

正解コードをそのまま例として使った場合、多くのモデルでJavaの方が高い性能を示しました（ただしQwen-32Bのような高性能モデルでは両者に差は見られませんでした）。

とくに注目すべきなのは、書式の変更に対するモデルの対応力です。Javaでは、実行上は書式の乱れが問題にならないはずですが、モデルはその崩れた見た目に影響され、正確に理解することが難しかったようです。

逆にPythonでは、インデントなどの書式がコードの意味に直結しているにも関わらず、モデルはうまく対応していました。たとえば、コードをトークン列に変換した実験では、GPT-4o-miniはJavaでの性能を12ポイント落としましたが、Pythonでは大きな崩れは見られませんでした。

#### モデルの性能による差

モデルの能力によって、情報の欠落に対する耐性も変わってきます。

Qwen-32Bのような大規模モデルでは、どの変異パターンでも87％以上のスコアを維持し（Javaの疑似コード化を除く）、非常に安定した結果を示しました。これは、強力なモデルほど特定のコード要素に依存せず、柔軟に対応できることを意味します。

一方、GPT-4o-miniやQwen-7Bのような比較的小型のモデルでは、情報を削るにつれて性能が大きく低下しました。

また、コード生成に特化したDS-Coderには独特の傾向が見られました。正解コードをそのまま使った場合の性能は他モデルに劣る一方、書式を崩したコード（インデント・改行削除、トークン化）では逆に性能が向上する場合もありました。これは、モデルがそもそも簡略化された表現で学習されている可能性を示唆しています。

### 課題2 変数名の「中身」が成否を分ける

#### 意味を持つ名前が圧倒的に有利

変数名のスタイルを細かく分析した結果、名前にどれだけ「意味」が含まれているかが、モデルの性能に強く影響することが分かりました。

精密な命名（PN）や冗長な命名（VN）のように、変数の機能や役割が分かる名前を与えると、記号化（VOE）より大きく性能が改善し、正解コードと近いスコアになりました。

逆に、高頻度の置換（VHR）や複数語の置換（VMR）のように、見た目はそれらしいが意味がかみ合わない名前を与えた場合は、記号化よりも性能が悪化するという結果になりました。意味を取り違えることで、かえって混乱を招いてしまったと考えられます。

#### 短く正確な名前が最も効果的

精密な命名（PN）と冗長な命名（VN）を比較すると、Pythonではすべてのモデルが、短く簡潔で要点を押さえた名前を好む傾向を示しました。長く説明的な名前よりも、的確な一語がモデルの理解を助けたと考えられます。

Javaではモデルによって差がありました。Qwenファミリーでは精密な命名（PN）と冗長な命名（VN）の間に大きな違いはなかったのに対し、GPT-4o-miniやDS-Coderでは、精密な命名（PN）のほうが性能が高くなりました。モデルの学習データやスタイルの違いが表れている可能性があります。

#### 見た目より意味が大切

命名規則に従った変数名でも、意味が不自然だったり誤解を招くようなものだったりすると、性能は下がってしまいました。見た目を整えるよりも、名前の意味が正しく伝わることの方が重要であることが示されています。

|     モデル     |   言語   | Ground Truth | 類似問題コード (SQ) |  bm25  | Zero Shot |
|-------------|--------|--------------|--------------|--------|-----------|
| GPT-4o-mini | Python |    76.26%    |    38.38%    | 42.42% |  37.37%   |
| GPT-4o-mini |  Java  |    86.17%    |    42.02%    | 37.23% |  39.89%   |
|   Qwen-7B   | Python |    85.86%    |    29.80%    | 33.33% |  34.85%   |
|   Qwen-7B   |  Java  |    91.49%    |    31.91%    | 29.79% |  31.38%   |
|  Qwen-32B   | Python |    98.48%    |    54.04%    | 57.07% |  53.54%   |
|  Qwen-32B   |  Java  |    99.47%    |    48.40%    | 50.53% |  51.06%   |
|  DS-Coder   | Python |    60.61%    |    28.28%    | 27.27% |  29.80%   |
|  DS-Coder   |  Java  |    72.34%    |    27.13%    | 27.13% |  32.45%   |

### 課題3 似た問題を使っても性能は上がらない

#### 検索で見つけたコードは効果なし

似た問題のコード例を使えばモデルの出力がよくなるのでは？そんな仮説を検証しましたが、結果は期待外れでした。

BM25や埋め込みモデル（gist-large、jina-v2-code、codesage-small）、アルゴリズム分類を使った検索はいずれも、ランダム検索とほぼ同じ結果になりました。ゼロショットよりも性能が下がることも多く、似ていても直接役立たないコードはむしろモデルの注意を散らす原因になってしまうことが分かりました。

#### モデルの能力によっても差が出る

性能の低いモデルでは、この影響がより顕著でした。たとえばQwen-7Bなどでは、文脈内学習によってかえって性能が落ちることが多く、ノイズに惑わされやすいことがうかがえます。

一方で、Qwen-32Bのような高性能モデルは、与えられたコードと対象タスクの違いをうまく見極められたようで、大きな性能低下は見られませんでした。ただ、それでも性能の向上にはつながりませんでした。

#### 人の手による類似コードでも結果は同じ

検索の精度が原因かどうかを確かめるため、人間が「似ている」とラベル付けしたLeetCodeのペア問題を使っても試しました。しかし結果は変わらず、モデルの性能は改善しませんでした。

さらに、最も類似したコードをCodeBLEUスコアで選んで与えるという極端なケースでも、結果は同様でした。現時点では、LLMが「似ているけれど違うコード」から本質的な解法を見出すのは難しいということが明らかになりました。

|     モデル     |   言語   | 正解コード (GT) | CodeBLEU (k=1) | CodeBLEU (k=5) | Zero Shot (ZS) |
|-------------|--------|------------|----------------|----------------|----------------|
| GPT-4o-mini | Python |   78.45%   |     38.95%     |     38.67%     |     39.78%     |
| GPT-4o-mini |  Java  |   84.81%   |     42.27%     |     40.61%     |     41.71%     |
|   Qwen-7B   | Python |   86.46%   |     30.66%     |     31.77%     |     38.40%     |
|   Qwen-7B   |  Java  |   90.06%   |     30.94%     |     31.22%     |     33.43%     |
|  Qwen-32B   | Python |   98.62%   |     51.93%     |     52.76%     |     55.25%     |
|  Qwen-32B   |  Java  |   97.24%   |     51.66%     |     50.28%     |     54.97%     |
|  DS-Coder   | Python |   63.54%   |     27.35%     |     29.28%     |     30.94%     |
|  DS-Coder   |  Java  |   69.06%   |     30.94%     |     28.18%     |     34.81%     |

### 成功と失敗の実例

#### 意味のない名前は誤解のもと

変数名を記号にしたコード例を与えられたGPT-4o-miniは、元の条件文「if sum(grid\[i\]) == n – 1:」を、「if sum(grid\[i\]) == 0:」と書き換えてしまいました。意味を持たない名前が、モデルの理解を妨げたと見られます。

また、冗長すぎる変数名を使ったQwen-7Bも混乱を招き、コード例の意図を理解できずに、まったく別の解決策を出してしまいました。

![[コード生成におけるLLMの性能を左右するプロンプトの「要素」を調べた結果 - AIDB/AIDB_94072_4.png]]

変数名を記号に変えたコード例（左）を参照した結果、LLMが誤った条件文を生成した例（右）

![[コード生成におけるLLMの性能を左右するプロンプトの「要素」を調べた結果 - AIDB/AIDB_94072_5.png]]

変数名を記号化した例（左）を与えられた結果、LLMが条件文を誤って生成したケース

#### 書式が乱れていても理解できる場合もある

一方で、書式を完全に取り除いたトークン列のコードを与えられたGPT-4o-miniは、正しい解法を出すことができました。書式が崩れていても、意味のある変数名さえ残っていれば、モデルは論理構造を正しく捉える力を持っていることが分かります。

この例は、コード例を選ぶ際に「きれいに整っていること」よりも「正しい意味を伝えること」の方が大切であるという事実を実感させてくれます。

![[コード生成におけるLLMの性能を左右するプロンプトの「要素」を調べた結果 - AIDB/AIDB_94072_6.png]]

コードをトークン列に変換した例（左）を与えても、LLMは正しい解法を生成できたケース

## 考察と示唆

### 成功と失敗から見えてくること

#### 意味のない変数名は、理解を妨げる

変数名をすべて記号に変えたコードを渡されたGPT-4o-miniは、本来の条件文「合計がn-1かどうか」を、「合計が0かどうか」と誤って生成しました。変数名の意味が消えることで、モデルは意図を読み取れなくなってしまったのです。

また、Qwen-7Bは、長くて詳しすぎる変数名を与えられた結果、情報が多すぎて混乱し、コード例を無視して独自の解法を出してしまいました。「多ければよい」とは限らないことが分かります。

#### 書式が崩れても、伝わるものはある

一方、見た目がバラバラなトークン列に変換されたコードでも、GPT-4o-miniは正しい解答を導きました。インデントや改行といった“きれいさ”よりも、変数名などの意味的な情報のほうが、モデルにとっては重要だということが分かります。

### プロンプト設計と活用へのヒント

#### 伝わる名前をつけることが何より大切

今回の実験から、プロンプトに使うコード例では「変数名や関数名が適切に付けられているか」が非常に重要であることが明らかになりました。インデントや改行の整い具合よりも、名前の持つ意味がしっかり伝わっているかを優先すべきです。

ベンチマークを作る研究者にとっても、この結果は示唆に富んでいます。もし変数名が無意味だったり、抽象的すぎるコードばかりを集めたベンチマークを使ってしまえば、LLMの能力を正確に測れなくなる可能性があります。実際の現場に近い、意味の通るコードを使うべきでしょう。

さらに、既存のコードに意味の伝わらない名前が使われている場合は、モデルが理解しやすいようにリファクタリングを検討するのも有効です。

#### 検索補強型にも応用できる

最近では、コード提案や自動補完を行うAIツールで、過去のコードを検索してプロンプトに組み込むRAG（検索補強生成）システムが使われることが増えています。こうした環境でも、変数名や命名の一貫性が非常に重要になります。

モデルにとって理解しやすいコードを用意することで、AIの提案精度は向上することが期待されます。

### 今回の研究における弱点と補足

#### 加工パターンには限りがある

今回使われた「変異演算子」は、意味・書式・構造という三つの観点から要素を削るものでした。カバー範囲は広いですが、コードには他にも影響する要素があるかもしれません。その点で、まだ余地は残されています。

とはいえ、既存研究を参考にしたうえでの設計であり、代表的なコードの特徴をカバーしている点は評価できます。

#### 実験の舞台はLeetCode

検証にはLiveCodeBench経由で選ばれたLeetCode問題が使われました。これはLLMの性能評価によく使われる形式ですが、実際の開発はもっと複雑で大規模です。今回の結果がそのまま現場に当てはまるかどうかは、今後の検証が必要です。

研究チームは、人が注釈をつけた類似問題も使って追加実験を行い、PythonとJavaという二つの代表的な言語で検証を進めました。今後はさらに他の言語にも広げていくことで、より汎用的な知見が得られそうです。

#### モデルの選定には幅が持たされた

使用されたのは複数のモデルですが、それぞれ異なるファミリー、サイズ、学習設計を持っています。結果として、一貫した傾向が複数モデルに共通して見られたことで、今回の発見が特定モデルに依存しないことも示されました。

ただし、学習方法や構造の異なるモデルで試すと、また違った結果が出る可能性もあり、今後の比較研究が待たれます。

## まとめ

本記事では、LLMによるコード生成の仕組みを分析した研究を紹介しました。

実験からは、コード例の中でも変数名や関数名といった「名前のつけ方」がとくに重要であることが示されました。  
一方で、インデントや改行といった書式の変化や、似たコードを与える工夫は、思ったほど効果がない場合もあると分かりました。  
こうした知見は、プロンプト設計やコード補完ツールを使う際の判断材料になります。自分が書いたコードや提示する例に、意味の通る名前が使われているかを意識してみてはいかがでしょうか。

**参照文献情報**

-   タイトル：What Builds Effective In-Context Examples for Code Generation?
-   URL：[https://doi.org/10.48550/arXiv.2508.06414](https://doi.org/10.48550/arXiv.2508.06414)
-   著者：Dongze Li, Songqiang Chen, Jialun Cao, Shing-Chi Cheung
-   所属：The Hong Kong University of Science and Technology
