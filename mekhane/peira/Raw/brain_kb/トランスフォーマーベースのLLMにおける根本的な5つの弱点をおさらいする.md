---
created: 2026-01-01T11:16:27 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/98011
author: AIDB Research
---

# トランスフォーマーベースのLLMにおける根本的な5つの弱点をおさらいする - AIDB

> ## Excerpt
> 本記事では、LLMの5つの根本的な弱点についての分析を取り上げます。 LLMは急速に進化し、文章生成や質問応答、コード作成などに幅広く使われています。モデルの規模を大きくすれば性能も上がるという「スケーリング則」も知られ、GPTシリーズは数年で1000倍以上の規模に拡大しました。それでもLLMには、いくつか弱点があります。 本記事の関連研究 背景 過去5年間で、LLMは急速に進化してきました。Op…

---
本記事では、LLMの5つの根本的な弱点についての分析を取り上げます。

LLMは急速に進化し、文章生成や質問応答、コード作成などに幅広く使われています。モデルの規模を大きくすれば性能も上がるという「スケーリング則」も知られ、GPTシリーズは数年で1000倍以上の規模に拡大しました。それでもLLMには、いくつか弱点があります。

![[トランスフォーマーベースのLLMにおける根本的な5つの弱点をおさらいする - AIDB/AIDB_98011-1024x576.png]]

## 背景

過去5年間で、LLMは急速に進化してきました。OpenAIが開発した初期のGPT-1は約1億のパラメータしか持っていませんでしたが、今では1兆を超えるモデルも存在します。数年で1万倍もの拡大が起きたことになります。

この急成長を支えたのが「スケーリング則」と呼ばれる法則です。モデルのサイズや学習データ、計算資源を増やすほど、性能も向上するという経験則です。実際、GPT-3.5からGPT-4への進化では、ベンチマークのスコアが大きく伸びました。

こうした成果から、より大きなモデルに、より多くのデータを与えれば課題は解決できるという楽観論が広がりました。スケーリングさえ進めれば知能も高まり、現在の問題は技術的な調整で解決できると考えられてきたのです。

しかしモデルが1兆パラメータ規模になっても、依然として課題は残っています。事実と異なる内容を自然に語る「幻覚」、論理の破綻、長い文脈を覚えていられない、検索結果を活用できない、画像とテキストの整合性が取れないなどの問題は、今のスケール拡大では解消されず、体系的に現れています。

こうした問題は偶然ではなく、計算理論・情報理論・学習理論といった数学的な限界に根ざしたものである可能性が高い、という話を紹介します。

ここから限定コンテンツ

### **忙しい人向けに、重要なポイント5選**

1.  モデルを大きくしても、計算理論と情報理論の制約により性能向上は頭打ちになる可能性が高い
2.  事実でない情報の生成は、データ不足の問題ではなく計算可能性の限界に起因する
3.  コンテキストウィンドウが広くても、学習の偏りや注意機構の制約で実際に使える範囲は狭い
4.  統計的な暗記と論理的推論は別物であり、この違いを理解することが重要
5.  画像や音声を加えたときに起こる新しい問題にも注意を払うべき

![[トランスフォーマーベースのLLMにおける根本的な5つの弱点をおさらいする - AIDB/AIDB_98011_1-1024x790.png]]

５つのポイントについてそれぞれ説明したイラスト

**参照文献情報**

-   タイトル：On the Fundamental Limits of LLMs at Scale
-   URL：[https://doi.org/10.48550/arXiv.2511.12869](https://doi.org/10.48550/arXiv.2511.12869)
-   著者：Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Zeeshan Memon, Muhammad Ibtsaam Qadir, Sagnik Bhattacharya, Hassan Rizwan, Abhiram R. Gorle, Maahe Zehra Kazmi, Ayesha Mohsin, Muhammad Usman Rafique, Zihao He, Pulkit Mehta, Muhammad Ali Jamshed, John M. Cioffi
-   所属：Stanford University, The University of Oklahoma, Emory University, Purdue University, University of California, Riverside, University of California, Berkeley, National University of Sciences and Technology, Zoox, Meta, Google DeepMind, University of Glasgow

## 幻覚のような誤りは、仕組み上どうしても起こる

LLMが事実と違う内容を自然に作ってしまう「ハルシネーション」は、学習が足りないだけの問題と見られがちです。しかし研究者ら（上記記載）は、この現象には4つの要因が重なっていると述べています。計算の仕組みそのものにある限界、データに含まれる欠けや偏り、評価のしかたの不一致、そして創造性と正確さの両立がむずかしいという点です。

### 計算のしくみから見て、誤りは避けられないとされている

まず押さえておきたいのは、ハルシネーションは技術の不備ではなく、数学的に避けられない性質だということです。この考えは、次の3つの理論によって支えられています。

1つ目は「対角化論法」と呼ばれる古典的な証明です。どんなに多くのモデルを並べても、それらすべてが正しく答えられない入力が必ず存在するという内容です。しかも、各モデルにとってそうした失敗は1つではなく、無限にあることが示されています。

2つ目は「計算できない問題」があるという事実です。1936年にチューリングが証明した「停止性問題」では、あるプログラムが止まるかどうかを事前に判定することはできないとされました。LLMがこのような問題に直面すると、答えを拒めば不完全に見え、答えようとすれば誤った情報を出してしまうという板挟みになります。実際、多くの応用タスクがこのタイプの問題を含んでいます。

3つ目は「情報と学習の限界」です。モデルが持つ情報量には限りがあるため、複雑な知識すべてを正確に記憶することはできません。これは「コルモゴロフ複雑性」という概念で説明されます。また、統計学習理論では、学ぶべき事実が多くなるほど、それに必要なデータも膨大になることが分かっています。たとえば、100万人の誕生日のようにパターンのない知識は、現実のデータ量では到底カバーしきれません。

これら3つの限界は階段状に重なっています。対角化と停止性問題は「そもそも解けない問題がある」ことを示し、情報理論の制約は「たとえ解ける問題でも、完璧に覚えることはできない」ことを意味しています。

![[トランスフォーマーベースのLLMにおける根本的な5つの弱点をおさらいする - AIDB/AIDB_98011_2_2-1024x590.jpg]]

### 学習データに抜けがあると、誤りがさらに目立つようになる

数学的な限界が前提にある一方で、学習データにも問題があり、それが状況をさらに悪化させます。

まず「データの不完全さ」です。どれほど大規模なデータを使っても、世界中の知識をすべて含めることはできません。知らない内容を聞かれると、モデルは既知のパターンから推測するしかなく、それが誤答につながります。

次に「ノイズや誤りの混入」です。学習データの多くはウェブから取得されますが、そこには検証されていない情報や古い内容、風刺やフェイクも含まれています。こうした誤情報は全体の2〜3％にのぼるとされ、モデルは正誤を区別せずにそのまま学習してしまいます。

「情報の偏り」も問題です。世の中の知識は一部に集中しています。有名な人物や話題はよく登場しますが、マイナーな情報はほとんど出てきません。たとえばWikipediaの閲覧が少ない人物に関する質問では、GPT-4の正確性が40％を下回る一方、有名人では90％以上に達しています。モデルの容量には限りがあるため、出現頻度の低い知識はうまく保持できません。

「情報の古さ」も無視できません。モデルはある時点のデータだけを学習しているため、世界の変化に追いつけません。政治や科学、経済などの情報は日々更新されますが、学習時点から半年もすれば古くなる内容が半数以上あると論文は述べています。それでもモデルはすべてを同時代の情報として扱ってしまい、現実とズレた回答を出すことがあります。

「矛盾する情報」も影響します。学習データには立場や文化の違いから異なる主張が混在しています。モデルはこれらを解決せずそのまま覚えるため、質問の仕方次第で出力される「事実」が変わってしまいます。

最後に「露出バイアス」という技術的な問題もあります。モデルは訓練中には正しい文をもとに次の語を予測しますが、実際の利用では自分が出力した文に基づいて次を予測します。初期に小さな誤りがあると、それが連鎖してどんどん崩れていくという現象が起きやすくなります。

![[トランスフォーマーベースのLLMにおける根本的な5つの弱点をおさらいする - AIDB/AIDB_98011_3_2-1024x590.jpg]]

### 評価の方法が、あいまいな答えを促してしまうことがある

理論やデータによって基本的なハルシネーションの確率は決まりますが、評価や訓練の方法がそれをさらに悪化させています。

現在のLLM評価の多くは「正解か不正解か」の2択で、「わからない」は不正解として扱われます。この形式だと、迷ったときに素直に「わからない」と答えるより、あてずっぽうでも答えたほうが有利になります。確率的に見ても、少しでも正解の可能性があるなら、推測する方が得点が高くなるからです。

この傾向は、人の評価に基づく強化学習（RLHF）でさらに強まります。評価する側は、慎重な回答よりも、自信ありげで流ちょうな回答を好む傾向があります。その結果、モデルは事実の正しさよりも、自信や言い回しのうまさを優先するように学習します。LLM自身を評価者に使う場合も同じで、長く丁寧な回答を好む傾向があり、細かい誤りを見逃しがちです。

さらに「過信」の問題もあります。理想的には、90％の確信を持つ答えが実際に90％の確率で正しいべきですが、現実のLLMは確信のわりに誤ることが多いです。とくに珍しい質問や学習外の内容では、このズレが顕著になります。

![[トランスフォーマーベースのLLMにおける根本的な5つの弱点をおさらいする - AIDB/AIDB_98011_4_2-1024x583.jpg]]

### 想像力と正確さは、同時に満たしにくいものになっている

最後に、LLMには避けがたいジレンマがあります。テキスト生成は確率に基づいており、「温度」と呼ばれる設定でその幅を調整します。温度を低くすると、もっとも可能性の高い単語だけを選ぶため、正確ですが単調な出力になります。逆に温度を上げると、あえて確率の低い選択肢も選ぶようになり、創造性は増しますが誤りも多くなります。

モデルの容量に限りがある以上、正確性を高めれば創造性は下がり、創造性を高めれば正確性が落ちるという関係が生まれます。たとえば小説やアイデア出しには高温度が向いていますが、医療や法律の用途では低温度が必須です。

興味深いのは、この視点をさらに推し進めると、

> ハルシネーションは創造性に不可欠

だということです。もしモデルが学習データにある情報だけしか出せなければ、新しい文章は生み出せません。創造性とは、学習内容の外側に踏み出すことですが、それこそがハルシネーションが起きる領域なのです。小説では創造性として評価され、質問応答では誤りとされる現象は、実は同じしくみから生まれています。

![[トランスフォーマーベースのLLMにおける根本的な5つの弱点をおさらいする - AIDB/AIDB_98011_5_3-1024x583.jpg]]

## 実際には文脈長の全体は使えない

最近のLLMは、とても長い文脈を扱えるとされています。たとえば128,000トークン、場合によっては100万トークンにも対応すると宣伝されています。トークンはテキストを処理する最小単位で、英語ならだいたい単語の4分の3ほどに相当します。つまり128,000トークンは約10万語、日本語なら文庫本数冊分の情報量になります。

しかし、こうした表示と、実際にモデルがうまく使える長さとの間には大きな差があるのではないかと考えられています。

この差が生まれる原因として、3つの根本的な要因があるとされています。それぞれを順に見ていきましょう。

### 学習データが手前の情報を重視し、後ろの情報をうまく扱えない

最初の要因は、学習データにおける「位置」の偏りが非常に大きいことです。訓練では、長い文章より短い文章が多く使われ、長文でも前半の部分ばかりが頻繁に登場します。そのため、文章の後半に出てくるトークン同士の関係は、学習データではほとんど見られません。

例えば、2,048トークンの範囲で学習したモデルでは、範囲の後半にあるトークン同士の組み合わせは全体の20％未満しかなく、末尾に近いほど5％以下にまで減ります。

![[トランスフォーマーベースのLLMにおける根本的な5つの弱点をおさらいする - AIDB/AIDB_98011_6-1024x919.jpg]]

これが問題なのは、モデルが「見たことのある例」に対してしかパラメータを調整できないからです。後ろのほうにあるトークン同士の関係が学習でほとんど登場しないということは、長い距離の依存関係を学習する機会がほぼないという意味です。

そしてこれは、単純に学習データを増やしても根本的には改善しません。そもそも自然言語の文章は短文が圧倒的に多いためです。意図的に長文データを増やすこともできますが、コストがかかるうえ、短文での性能を下げてしまう可能性もあります。

### 位置情報の仕組みが長い距離になると効かなくなる

2つ目の要因は、位置情報の表し方そのものに限界があることです。LLMの基盤となるTransformerという仕組みは、本来、単語の順番を認識できません。そこで「位置エンコーディング」という技術で、各トークンに位置の情報を追加しています。もっとも基本的なのが「正弦波エンコーディング」です。これは、異なる波の形（正弦波や余弦波）を組み合わせて、各位置ごとに固有のベクトルを与える方法です。一見するとどの位置も区別できるように見えます。

しかし数学的に見ると問題があります。たとえば、位置iとjの類似度を計算すると、それは距離が離れるほど振動しながら弱まっていきます。つまり、位置20,000と位置1では、位置的にはほとんど関係がないと判断されてしまいます。内容に関連があっても、位置情報ではそれが伝わらないのです。

![[トランスフォーマーベースのLLMにおける根本的な5つの弱点をおさらいする - AIDB/AIDB_98011_7-1024x416.jpg]]

### 長い文では重要な情報に注意が向かず、埋もれてしまいやすい

3つ目の要因は、LLMの中心的な仕組みである「注意機構（Attention）」の数学的な特徴にあります。注意機構とは、ある単語が他のどの単語にどれくらい注目すべきかを決めるしくみです。

文脈内にトークンがN個あるとき、ある単語にしっかり注意を向けるには、そのスコアが他のN−1個の平均より log(N) だけ高くなければなりません。その薄まりを防ぐには、スコアをlog(N)分だけ引き上げる必要があります。

つまり文脈が10倍になれば、目立つために必要なスコア差も対数的に増えます。

実際には、モデルは数千トークンまでの文脈で訓練されているため、log(数千)程度のスコア差しか学習していません。ところが、10万トークンなどの長文脈で同じ仕組みを使うと、log(100,000)に相当する大きなスコア差が必要になります。訓練で見たことのない値を求められるため、モデルには負荷がかかります。

この結果、注意が拡散してしまいます。本来重要な情報に集中できず、関係ない情報にも注意が分散し、必要な内容が埋もれてしまいます。

さらに、注意機構は「計算量とメモリ」がトークン数の二乗に比例して増えます。これを「二次計算量」といい、10万トークンを処理するには非常に大きなリソースが必要になります。最近はFlashAttentionのような工夫で効率化はされていますが、基本的な計算の重さは変わりません。

最近のLLMは流ちょうな文章を作り、ときには非常に賢そうな答えを返します。しかし本当にモデルは「推論」しているのか、それとも覚えたパターンをなぞっているだけではないのか。

この問いは、実用上とても重要です。というのも、「推論」ができるなら未知の問題にも対応できますが、「暗記」に近いなら、学習データに似たものしか解けないからです。どちらかで適用できる場面は大きく変わってきます。

### 次の単語を予測するだけなら、推論とはいえないかもしれない

いまのLLMが学習している目標はとても単純です。与えられた文章の続きを予測する、つまり「次に来る単語」を当てることです。

この仕組みは言葉のパターンを学ぶには適していますが、論理的な推論を求めるものではありません。文法が正しく、文脈に合っていればよく、内容が論理的に正しいかどうかは問われません。

また、今のLLMには人間のような「思考のための言語」がないという指摘もあります。人間は言葉の前に、記号を使った抽象的な思考構造を持っていて、それで推論するという分析がされています。一方で、LLMはすべてを連続的なベクトルで処理しています。この違いが、既存の知識を組み合わせて新しい解決策を生み出す「構成的汎化」の弱さにつながっているという見方があります。

### 推論がうまくいかないときの4つの原因

研究者らは、LLMの推論がなぜ不安定なのかを、4つの失敗のパターンにまとめています。

1つ目は「目的とのズレ」です。たとえばChain-of-Thought（CoT）という手法では、モデルに思考の流れを出力させますが、学習では最終的な答えが合っているかどうかしか見られません。途中の推論が正しいかどうかは評価されないのです。

2つ目は「知識の使い方の誤り」です。外部から知識を与えても、モデルがそれを使って本当に考えているとは限りません。表面的には引用していても、答えと知識のパターンが一緒に現れる頻度に頼って出力しているだけかもしれません。

3つ目は「探索のむずかしさと計算の使い方の偏り」です。毎回もっとも確率の高い単語だけを選ぶと、視野が狭くなってすぐ行き詰まります。逆にいくつも候補を同時に探すと、計算が重くなりすぎます。

4つ目は「インターフェースや評価の弱さ」です。たとえば、思考の始まりや終わりを示す記号の位置がずれると、途中の思考がうまく出せません。また、Pass@kのような指標は答えの正しさだけを見るので、思考の流れが改善されたかどうかを評価できません。

### 推論を「制約付きの最適化問題」としてとらえ直す

こうした問題を解決するため、推論を「単に答えを出す作業」ではなく、「制約を満たしながら最適な答えを選ぶ問題」として再定義することが提案されています。

この問題に対しては以下の3つのアプローチが対策になります。

1つ目は「ソルバーベース手法」です。これは自然言語の問題を論理式に変換し、専用のソルバーで解いたあと、また自然言語に戻します。

2つ目は「プロンプトベース手法」です。これはモデル自身に推論させる方法です。たとえばCoTは順を追って考えさせ、Tree-of-Thoughtsは複数の展開を探索し、Cumulative Reasoningはグラフ構造で問題を分解します。

3つ目は「ファインチューニング手法」です。モデルの中身そのものを、論理的に考えるように訓練します。確かに誤解釈のリスクは減りますが、学習にかかるコストは高くなります。

### 答えの正しさだけでなく、全体の一貫性も大事

1つの質問に正しく答えるだけでは足りません。関連する複数の質問に対して、答え同士が矛盾しないことも重要です。これが「推論の一貫性」の問題です。

一貫性にはさまざまなタイプがあり、以下のように制約として表すことができます。

| 一貫性の種類 |            意味            |
|--------|--------------------------|
| 否定一貫性  | 命題 `p` と `¬p` が同時に真になってはいけない |
| 含意一貫性  | `p → q` と `p` が真なら `q` も真であるべき |
| 推移一貫性  |  `p→q` と `q→r` があれば `p→r` を保つ  |
| 事実一貫性  |   出力が外部の知識ベースと矛盾しないこと    |

このような一貫性を保つ方法のひとつに、MaxSATソルバーの活用があります。MaxSATは、すべての制約を満たすのが難しいときでも、できるだけ多く満たすように調整する仕組みです。

### 実践向けの推論パターン

理論を現場で活かすため、いくつか方法が提案されています。

|                 手法                 |                         概要                         |
|------------------------------------|----------------------------------------------------|
| Program-Aided Language Models（PAL） | 計算を外部の実行エンジンに任せます。モデルはPythonコードを生成し、インタープリタが実行します。 |
|    Chain-of-Verification（CoVe）     |  初期の答えを仮説とし、検証質問を生成して検証結果で答えを修正する自己検証ループを組み込みます。   |
|            ニューロシンボリック統合            |     LLMが候補を出し、記号的システムが検証したり、記号ルールで生成を制約したりします。     |
|       マルチモーダルグラウンディング・ツール連携        |     視覚や音声などの追加情報や、電卓・検索・データベースなどの外部ツールを活用します。      |

上記の方法に共通するのは、推論を「検証できる制約を守りながら進める構造的プロセス」へと変えようとしている点です。

![[トランスフォーマーベースのLLMにおける根本的な5つの弱点をおさらいする - AIDB/AIDB_98011_8-1024x338.jpg]]

## 検索のしくみを使っても避けられない部分がある

RAGは、LLMに外部の知識を与える有力な方法として注目されています。仕組みはシンプルで、ユーザーの質問に対してまず関連する文書を検索し、それをLLMの入力に追加します。こうすることで、モデルは検索結果を参考にしながら答えを出せるようになります。

理論的には、このしくみは理想的に見えます。あらかじめ知識をすべて詰め込む代わりに、必要なときだけ情報を取り出せるので、常に最新で正確な答えが得られるように思えます。

しかし現実には、RAGには検索の段階と生成の段階の両方に弱点があります。そのため、意図した通りに動かないケースが多く、逆に新しい種類の失敗を生み出してしまうことがあります。

### 検索の質が答えの限界を決める

RAGの性能は、検索の精度に大きく左右されます。どれほど優れたLLMでも、必要な情報が検索されなければ正しい答えは出せません。検索が失敗する原因をいくつか見ていきます。

**関連性とカバレッジは両立しにくい**

最初の問題は、LLMが扱える文書の長さに限りがあることです。つまり、検索で読み込めるトークン数には上限（B）があり、その中で「関連性」と「カバレッジ」のどちらを優先するかというトレードオフが避けられません。

関連性とは、検索結果が質問にどれだけ直接関係しているか、カバレッジは、答えに必要な情報をどれだけ広く含んでいるかを指します。

関連性を重視すると、確かに直接関係のある文書だけを選べますが、周辺知識や推論に必要な補足情報を取りこぼすことがあります。逆にカバレッジを重視すると、多くの情報を拾えるものの、関係のない文書も混ざり、貴重なトークン枠を使い果たしてしまいます。

限られた予算の中では、理想的な性能には届かず、ある程度妥協した点が最適解になります。

この問題に対処する方法として、グラフベースの検索や階層的な検索が提案されています。関連情報を圧縮して少ないトークンで伝えようという工夫です。  
ただしグラフ構造の抜け、構造を線形に直す際の負荷、移動経路の計算コストといった新たな課題も生じます。

### チャンクに分けると文脈が壊れる

2つ目の問題は、情報を細かく分けることで意味のつながりが壊れてしまう点です。文書は通常、数百トークンずつの「チャンク」に分割され、それぞれが独立して検索されます。しかし、自然な文章の意味はこうした人工的な区切りをまたいで存在することが多いのです。

たとえば「もし条件Aなら結果B」という内容があったとして、AとBが別々のチャンクに分かれていたらどうなるでしょうか。どちらのチャンクも単独では関連性が低く見えて、検索対象から外れてしまう可能性があります。けれど、AとBはセットで意味を持つのです。

このような「意味の断片化」は、最適化の問題として表すことができます。全体で使えるトークン数がB以下という制限のもとで、どのチャンクの組み合わせがもっともスコアが高くなるかを選ぶ問題です。しかしこのとき、各チャンクが独立していると仮定していることに無理があります。意味がつながっているチャンク同士がバラバラに扱われると、重要な情報が失われてしまうのです。

対策として、階層的にチャンクを作る方法や、チャンクを使わずに検索する方法も提案されています。ただし、どちらもトークン数の上限という現実的な制約からは逃れられません。情報を直列に並べてコンテキストに収める以上、何かを削らざるを得ないのです。

### ランキングのミスで重要な情報が見落とされる

3つ目の問題は、検索結果の並べ方と、LLMの使い方の両方に原因があります。

検索システムは、関連性の高い上位k件だけを返すのが一般的です。ところが、本当に必要な情報がk+1位以降にあると、モデルには届きません。このような「トップkの打ち切り」は、Recall@kという指標で評価されます。Recall@kとは、必要な文書のうち、何割がトップk件に入っているかを示します。

また、複数の文書に情報が分かれている場合、一部だけが検索されると推論が不完全になります。これは「散らばった証拠の取りこぼし」と呼ばれています。

LLM側にも課題があります。LLMは入力の先頭や末尾にある情報に強く反応し、中間の内容を軽視する傾向があるとわかっています。これは「中間で迷子になる（lost in the middle）」現象と呼ばれます。つまり、どの文書が検索されたかだけでなく、それがどの順番で渡されたかによっても、モデルの出力は変わります。

### 情報の混入が、知識ベースを乗っ取る

4つ目の、そしておそらく最も深刻な問題は、知識ベースへの敵対的な攻撃です。

RAGは外部のデータベースに頼る仕組みなので、その中身が汚染されると、検索結果も汚れてしまいます。

たった5件の悪意ある文書を、100万件規模の知識ベースに紛れ込ませるだけで、約90％の高い攻撃成功率を示した事例もあります。攻撃者は、特定の質問に対して検索スコアが非常に高くなるように文書を細工します。同時に、その文書は他の正常な文書とは無関係になるように調整されます。その結果、検索時に毒入り文書だけが上位に入り、正しい情報は押し出されてしまいます。

さらに問題なのは、この攻撃がパラフレーズ（質問の言い換え）にも有効な点です。埋め込み空間では、似た意味の質問は近い位置に配置されるため、言い方を変えても同じ毒入り文書が検索されてしまうのです。

対策もいくつか提案されています。LLMの内部の活性化パターンを見て汚染を検出し、約98％の精度で正しく検出しつつ、誤検出も2％未満に抑えるアプローチが発見されています。また、ある文書を除いたときに答えが変わるかを調べることで、どの文書が出力に影響を与えているかを特定する「追跡」手法もあります。

それでも根本的な問題は残ります。検索はあくまで「似ているかどうか」に基づいて行われるため、たとえ中身が間違っていても、類似度が高ければ選ばれてしまいます。そして、一度毒入り文書がトップに入れば、それが出力を大きく左右してしまうという構造は変わりません。

### LLMは検索結果をそのまま信じてしまう傾向にある

**無関係な情報にも注意が向いてしまう**

LLMの「注意機構」は、本来、関係のある情報に注目するための仕組みです。しかし実際には、関係のない情報にも引っ張られてしまうことがあります。

その原因のひとつは、どの文書にどれだけ注意を向けるか（注意重み αi）が、検索スコアではなく、モデル内部でのベクトルの近さ（クエリとキーの内積）で決まってしまう点にあります。検索システムが「この文書は関係ない」と判断しても、埋め込み空間で似ていれば、モデルは強く反応してしまうのです。

実験では、関係ない文書をひとつ追加するだけで、答えの精度が最大30％も落ちることが示されています。とくに、その無関係な文書が質問とベクトル的に似ている場合、影響が大きくなります。

なぜこうなるのでしょうか。LLMは、事前学習とファインチューニングの過程で、すべてのトークンから情報を集めるように最適化されています。そのため「どれを無視するか」という能力は十分に育っていません。注意重みは「この情報を信じてよいか」を表すものではなく、単に統計的に近いかどうかを示しているだけです。

対策としては、無関係な文書を含むデータでモデルを訓練し、不要な注意にはペナルティを与える方法が提案されています。ただしそのためには、よく管理された大規模なデータセットが必要で、計算負荷も高くなります。

### モデルの知識と検索情報がぶつかる

LLMは2つの知識を使って答えを作ります。1つはモデルのパラメータに埋め込まれている「パラメトリック知識」、もう1つは検索で得た「コンテキスト知識」です。この2つが食い違ったとき、どちらが優先されるかは予測が難しくなります。

ベンチマークでは、次の4つのケースすべてが確認されています。モデルだけ正しい、コンテキストだけ正しい、両方間違い、両方正しい。モデルの種類やタスクによって、いずれも起こりうるのです。

なぜこんな不安定さが生まれるのでしょうか。注意機構は「似ているトークン」に注目するしくみですが、その情報が正しいかどうかを判断する機能はありません。もともと強い事前知識があると、検索結果が無視されてしまうこともあれば、逆に間違った検索情報に上書きされてしまうこともあります。

この問題をやわらげる方法として、3つの方向が考えられています。1つ目は、訓練時に意図的に衝突するデータを入れて重み λ(q) を調整する方法。2つ目は、推論時に両方の答えを比べて矛盾がないかチェックする方法。3つ目は、検索結果をグラフや主張単位に構造化して、論理的な矛盾を検出する方法です。ただし、これらはすべて「正しさ」を直接扱っているわけではありません。

### 質問があいまいだと情報の統合がうまくいかない

最後の問題は、ユーザーの質問自体があいまいな場合です。

質問は、ユーザーの本当の意図を正確に表しているとは限りません。理想的には、検索システムはすべての可能な解釈に対して、必要な情報をまんべんなくカバーすべきです。しかし現実の多くのシステムは、たった1つの解釈に絞って検索してしまいます。

この問題を解決する方法として、質問を複数の言い換えに展開する技術もあります。ただしこの方法にも課題があります。言い換えの質がばらついたり、どれを採用してどう統合するかが難しく、計算コストも増えてしまいます。

複数の検索結果を1つにまとめるとき、LLMは注意機構を使って各文書に重みをつけます。けれどこの重みは「どの解釈が正しいか」の確率に基づくわけではなく、単にトークン同士の類似度から決まります。

本来あるべき動作は、意図のゆらぎをすべて考慮しながら、最も期待できる答えを出すことです。

## 視覚に対する期待と誤解

画像や音声も扱えるマルチモーダルLLMは、これまで言語モデルが抱えてきた「グラウンディング問題」つまり、記号と言葉だけでは現実世界のモノや感覚とつながらないという課題を解決する手段として期待されてきました。

こうした取り組みは直感的には理にかなっています。テキストだけでは知覚の裏づけがないなら、画像や音声と組み合わせれば、視覚が言語にリアリティを与えて、もっと信頼できる推論ができるようになるはずです。

しかし、こうした期待とは逆の事実も存在します。

### 言語の表現が視覚の情報をゆがめてしまう場合がある

現在のマルチモーダルLLMの多くは、あらかじめ学習された画像処理モデル（視覚エンコーダ）と言語モデルを、「アダプター」と呼ばれる中間の層でつなぐ構造を取っています。この設計には、本質的なバランスの悪さが組み込まれています。

**言語が表現の中心になってしまう**

画像から得られる情報（視覚トークン）は、言語モデルが理解できるように、言語トークンと同じ形のベクトル空間に変換されます。この変換には、線形変換やMLPなどのシンプルなニューラルネットワークが使われます。

しかしモデルの学習目標はあくまでテキストを正しく生成することです。そのため、視覚情報は言語の生成に役立つように調整されるだけで、視覚としての意味の正しさはあまり重視されません。

**アライメントのずれが視覚の意味をゆがめる**

多くのマルチモーダルモデルは、CLIP（Contrastive Language-Image Pretraining）という手法で学習された視覚エンコーダを使っています。

このしくみは画像検索には役立ちますが、別の問題も引き起こします。それが「意味のずれ（意味的ドリフト）」です。CLIPで得られる視覚表現は、目に見える特徴そのものではなく、ウェブ上の説明文と一緒に現れる統計パターンに強く影響されてしまいます。

たとえば「犬」という概念を学んだモデルは、実際の犬の姿そのものではなく、ネット上で「犬」と一緒に語られやすい特徴や表現を学んでしまいます。その中には文化的な偏りや、偶然の関係（偽相関）も含まれており、モデルの視覚理解は現実とは少し違ったものになってしまうのです。

**モダリティを統合すると情報が圧縮されて失われる**

画像とテキストの情報（視覚トークンとテキストトークン）を統合するとき、一般的には2つの方法が使われます。1つは単純に並べて1つの系列にする方法、もう1つは注意機構で重みをかけて平均を取る方法です。

けれど、この統合の過程で「情報のボトルネック」が起きます。Transformerの注意機構は、すべてのトークンに重みをつけて平均を計算するため、細かい空間的な位置関係や時間的な動きがなだらかになり、失われやすくなります。

実際、視覚トークンをプーリングすると、空間的・時間的な構造が崩れます。その後にクロスアテンションをかけても、トークン同士の細かいやり取りは失われます。

一部のモデルでは「ボトルネック設計」といって、少数の潜在トークンに情報を通す方法を採っていますが、このボトルネックを小さくすればするほど、視覚や関係の細かい情報は削ぎ落とされてしまいます。これは精度とのトレードオフになります。

### トークン化が視覚の情報を大きく削る

言語と視覚は、もともと情報のかたちが大きく異なります。言語は単語や文などのように、離散的で構造化された記号の世界です。一方、視覚は連続的で高次元な情報であり、無限に変化する細かな違いを持ちます。

さらに、学習中に一部のコードばかりが使われ、他のコードが使われない「コードブック崩壊」も起きます。これにより、表現できる情報の幅は実質的に狭まってしまいます。

つまり、視覚の豊かさを言語と同じ形式に落とし込む過程で、多くの情報が圧縮され、失われているのです。

### 指示チューニングは表面的なやり取りを学ぶにとどまっている可能性

多くのマルチモーダルLLMは、事前学習のあとに「指示チューニング」と呼ばれる追加訓練を受けます。これは、画像・指示・応答の3つをセットにしたデータを使って、モデルに視覚に関する対話のしかたを教える工程です。この点には2つの懸念があります。

1つ目は、指示チューニングが教えているのは期待される答え方の統計的な傾向であり、モデルがもともと持っている言語バイアスや誤った関連づけ（偽相関）を根本から解決するものではないこと。

2つ目は、そもそも合成された指示データ自体が、使った生成モデルの思考パターンや偏りをそのまま反映しているため、それを学習すると既存の偏りをさらに強めてしまうおそれがあることです。

### グランディングしているわけではない

マルチモーダルLLMは画像を「見ている」ように振る舞いますが、実際に処理しているのは視覚エンコーダが抽出した特徴ベクトルであり、生のピクセル情報にはアクセスしていません。そのため、画像に写っていてもキャプションに記述されていない情報は、モデルの内部表現には現れません。

たとえば、物体を回転させたときに見える面を問うようなタスクでは、キャプションだけでは足りず、実際の知覚的特徴が必要になります。こうした視覚理解は、現状のマルチモーダルLLMには困難です。

モデルは、行動として正しい文を出す能力（もっともらしさ）は持っていますが、内部的に視覚を正確に再構成する能力はありません。そのため、空間認識や因果推論が必要な場面で限界が見えます。

視覚とテキストの対応関係も、本来の構造ではなく、単なる共起パターンに基づいて学習されがちです。「ソファの上の猫」には対応できても、「猫の上のソファ」のような未知の組み合わせには弱く、出力が破綻する例もあります。

さらに、異なるモダリティ間で誤った情報が影響し合う「クロスモーダル・ハルシネーション」も起きます。曖昧な画像が「もっともらしい言葉」を引き出し、逆に言語の偏りが視覚の解釈をねじ曲げてしまう構造です。

一部の手法では、視覚モダリティの重み付けや画像領域との対応付けでこれを緩和しようとしていますが、根本的な構造の弱さは残ります。

加えて、学習に使われるデータ自体にも偏りがあります。ウェブ由来の画像・キャプションや、別のモデルによる自動生成キャプションが主な情報源であり、人間の主観や過去のモデルの誤りを受け継いでしまうのです。

その結果、マルチモーダルLLMは頑健な視覚理解を身につけているわけではなく、訓練データに現れる「それっぽさ」を再生産しているに過ぎない可能性があります。

### スケーリングが成り立ちづらくなる

単一モダリティのLLMでは、モデルやデータを増やすと、損失が安定して減っていく「スケーリング則」が成り立ちます。しかし、マルチモーダルではこの法則が崩れます。

テキストと画像では、損失の減り方（スケーリング指数）が異なります。マルチモーダルモデルは両者を単純に足すわけではなく、モダリティ間の複雑な相互作用が入るため、全体の性能向上は遅くなります。どちらかが足を引っぱる構造です。

さらに、一方のデータを増やすことで逆にバランスが崩れ、全体の性能が落ちる「反スケーリング」も起こります。

また、視覚とテキストの統合には注意機構が使われ、処理量は視覚・テキストのトークン数の二乗に比例します。高解像度画像や長文では計算負荷が急激に上がり、実用上の限界が出ます。これを避ける工夫（スパース注意など）は、モダリティ間の情報共有を弱める結果になりがちです。

大規模データでは、画像とテキストのずれ（ミスアライメント）が少しでもあると、それが積み重なってノイズになります。小規模では無視できても、何十億ものデータを扱うとその影響が支配的になります。こうして、視覚と言語の対応が曖昧になり、モデルが本質的な結びつきを学びにくくなります。

### 実用への示唆と今後の方向性

こうした限界は、LLMを現場で使う際の信頼性や説明可能性、展開のしやすさに直結します。

一方で、前向きな展望もあります。たとえば、マルチモーダルの埋め込みに記号ベースの推論を組み合わせる「ニューロシンボリック統合」は、関係性や因果性を明示的に扱う道を開きます。

また、ロボットやセンサーを使った実世界のデータを通じた訓練では、現実とのつながりが強い学習信号が得られます。こうした環境では、行動の結果が感覚として返ってくるため、モデルの表現がより確かなものになります。

重要なのは、「視覚を加えるだけ」では十分でないということです。真に意味のあるマルチモーダル理解には、実体のある身体性（具現化）と、視覚・行動・言語の循環的な学習が必要です。

## 評価の難しさ

### ベンチマークハッキングが起きる

ここで自然に湧いてくる疑問があります。もしこれらが本質的な限界なら、なぜ最近のモデルはベンチマークで高得点を出し続けているのでしょうか。

スコアの向上はモデルの本当の進化を意味しているのではなく、「評価を攻略する力」が高まっているだけかもしれません。この現象は「ベンチマークハッキング」と呼ばれています。

LLMはインターネット上の膨大な文章で学習します。そのため、ベンチマークの問題が訓練データに含まれる可能性が高くなります。この重なりの割合は、ベンチマークデータ汚染と呼ばれます。  
汚染が問題なのは、モデルが問題を理解して答えているのではなく、単に覚えているだけだからです。人で言えば、試験前に問題と答えを丸暗記しているような状態です。点数は取れますが、実力を測っているとは言えません。  
ただし、汚染を見つけるのは簡単ではありません。言い回しが少し変わっているだけでも、単純な一致検索では見抜けません。根本的な解決策は、汚染の影響を受けにくいベンチマークを使うことです。

### 「LLM-as-a-judge」の良さと懸念

最近、LLMの出力を別のLLMに評価させる「LLM-as-a-judge」という方法が広まっています。人間による評価は手間もコストも大きいため、LLMによる自動評価は便利です。しかし論文は、この手法にいくつか深刻なバイアスがあると警告しています。

|        バイアス        |           何が起きるか           |
|--------------------|----------------------------|
|      自己選好バイアス      |   評価モデルが自分や同系列モデルの出力を好む    |
|       位置バイアス       |     選択肢の順序やラベルで評価が変わる      |
|      冗長性バイアス       |      長い回答やリストを高評価しがち       |
| クロスモデル不一致（評価者ドリフト） |  評価者やプロンプトを変えるとランキングが変わる   |
|      プロトコル感度       | プロンプト表現や出力フォーマットで結果が大きく変わる |

極端な例では、同じ情報でもプレーンテキストかJSONかで、スコアが最大40ポイントも変わることが確認されています。

### 計算量も考慮してランキング表示すべき

多くのベンチマークは、正解率などの最終スコアだけを重視し、その結果を出すためにどれだけの計算資源が使われたかは考慮されません。しかし、実際の運用ではコストが非常に重要です。

最近のモデルの中には、複数の候補からの再選択や長い思考ステップ（Chain-of-Thought）、外部ツールの使用によって、非常に多くの計算を使うものがあります。

現在のリーダーボードでは、精度だけが主な評価指標になっており、推論時間やトークン数、エネルギー消費といった現実的に重要な情報はほとんど共有されていません。

精度だけでなく、推論時間やコストも併せて評価するリーダーボードがあれば、現実的な運用に近い視点から、よりバランスの取れた評価が可能になるはずです。

### 再現性はあるのか？という問題

多くのベンチマークは、結果のブレが大きく、同じ条件で再現することが難しいという問題を抱えています。理由は、テストセットが小さいこと、生成にランダム性があること、そしてプロンプトのわずかな違いで結果が変わってしまうことです。

つまり、多くのリーダーボードで見られる「改善」は、実は統計的に不安定かもしれません。複数回の実行や異なるシードでの平均スコアを標準化すべきだ、という指摘が出ています。

不安定さは、モデルのバージョン変更によっても起こります。APIモデルは、予告なくアップデートされるため、同じ評価を後日やり直しても同じ結果が得られません。オープンソースモデルでも、使っているハードウェアやライブラリの違いで並列処理が変わり、結果にブレが生じることがあります。評価環境をしっかり管理する必要があります。

### 公平性や安全性の評価は十分か

多言語や専門分野向けのベンチマークは、主に英語などのリソースが豊富な言語で作られ、他言語へ翻訳されています。しかし、この翻訳プロセスで表現のニュアンスや意味が変わることがあります。

安全性の評価も偏っています。多くは事前に用意された「レッドチーム」プロンプトリストに頼っており、それに特化した対応だけが強化されてしまいます。

## 欠点を受け入れてシステムを設計するための考え方

LLMに「完璧なシステム」は存在しないという前提が重要です。どれだけモデルを大きくしても、必ず失敗する場面があることは、計算理論や情報理論の観点からも証明されています。大切なのは、その失敗が技術的なものか、それとも原理的に避けられないものかを見極めることです。技術的な問題なら改善できますが、原理的な壁なら、別の方法を考える必要があります。

そのため、LLMシステムの設計では、「失敗をゼロにする」のではなく、「どんな失敗が起こりうるかを予測して、うまく扱う」ことが目標になります。たとえば、モデルが自信を持てないときに「わからない」と答えるしくみや、タスクの特性に合わせて確実性と創造性のバランスを調整する仕組みが求められます。検索を使う場合も、情報を大量に取り込むのではなく、必要な証拠だけを絞って取り入れる戦略が重要です。

長文脈の扱いも同様です。たとえ128Kトークンまで対応していても、実際に有効に活用できる範囲は限られます。重要な情報は文の最初や最後に配置し、途中に埋もれさせない工夫が有効です。推論では、LLMは統計的パターンには強いものの、論理的な推論には弱いので、外部の検証システムに計算を任せたり、出力の正しさを再確認する質問ループを入れたりするのが有効です。

評価についても、データの汚染や評価の偏りを避けるために、毎回新しい問題を自動生成したり、複数の評価方法を組み合わせたりする必要があります。精度だけでなく、計算コストや処理時間もあわせて評価し、実用面から判断することが大切です。

最後に重要なのは、「スケールすればすべて解決する」という楽観ではなく、「理論的な限界を理解し、その中で最適な方法を考える」という現実的な見方です。これは悲観ではなく、正しい期待値を持つための姿勢です。LLMの得意なこと・不得意なことを見極め、できる部分に集中し、できない部分は別の手段で補う。この考え方が、実用的で信頼できるシステムをつくるための基本になります。

## まとめ

本記事では、LLMが直面する5つの限界（ハルシネーション、文脈圧縮、推論の弱さ、検索の不安定さ、マルチモーダルの不整合）についての詳しい分析を取り上げました。

これらは一時的な技術の問題ではなく、計算や情報の理論上、避けられない性質だと考えられています。現状、モデルには弱点があることを前提にして先に進む必要があります。

完璧を目指すのではなく、失敗を予測し管理するように考え方を切り替えるのが得策のようです。たとえば「わからない」と答える仕組みや、正確さと柔軟さのバランス、外部検証の導入などが現実的な設計方針となります。
