---
created: 2026-01-01T09:29:21 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/59618
author: AIDB Research
---

# プロンプトを調整しないLLMのプロンプトエンジニアリング新手法『ControlPE』 - AIDB

> ## Excerpt
> これまでのLLMの使用法は、モデルの動きを観察し、その反応に基づいてプロンプトを調整するという方法に重点が置かれてきました。しかし、プロンプトの効果を微細に制御するには不十分といえます。 この問題に対処するため、センスタイム社を含む研究者たちが開発したのが『ControlPE』（Continuously Controllable Prompt Engineering）です。ControlPEは、プ…

---
これまでのLLMの使用法は、モデルの動きを観察し、その反応に基づいてプロンプトを調整するという方法に重点が置かれてきました。しかし、プロンプトの効果を微細に制御するには不十分といえます。

この問題に対処するため、センスタイム社を含む研究者たちが開発したのが『ControlPE』（Continuously Controllable Prompt Engineering）です。ControlPEは、プロンプトによるLLMの動きを直接調整する手法です。モデルの挙動を細かく直接的に制御することを目指しています。

ControlPEはモデルを直接編集することなく実現します。そのため、開発者や研究者はLLMをカスタマイズする際に大きなリスクやリソースを必要としません。本記事では背景、ポイント、実装について、性能評価の結果について見ていきます。

![[プロンプトを調整しないLLMのプロンプトエンジニアリング新手法『ControlPE』 - AIDB/AIDB_59618_thum-1024x576.jpg]]

**参照論文情報**

-   タイトル：To be or not to be? an exploration of continuously controllable prompt engineering
-   著者：Yuhan Sun, Mukai Li, Yixin Cao, Kun Wang, Wenxiao Wang, Xingyu Zeng, Rui Zhao
-   所属：浙江大学、センスタイム、香港大学、シンガポール経営大学
-   URL：[https://doi.org/10.48550/arXiv.2311.09773](https://doi.org/10.48550/arXiv.2311.09773)

**本記事の関連研究**：[「入力プロンプト」を最新情報で自動アップデート＆最適化する手法『FRESHPROMPT』がLLMの出力精度を飛躍的に上げる](https://ai-data-base.com/archives/58986)

プロンプトの制御は、大規模言語モデル（LLM）において重要な役割を果たしています。LLMがどのように情報を処理し、応答を生成するかを導くための鍵となる要素です。LLMのプロンプト制御には数多くの手法が存在し、テキスト生成タスクを容易にし、タスク特有のトレーニングを不要にするなど幅広い効能の目的があります。

**関連研究：**[ChatGPTの効果的なプロンプト手法における「基本のキ」を理論とテンプレート両方で紹介](https://ai-data-base.com/archives/58361)

しかし、適切なプロンプトを設計することは簡単な作業ではありません。微妙なプロンプトの変更が大きなパフォーマンスの変化をもたらすことがあり、無限に近い自然言語プロンプトの中から最適なものを見つけるのは困難です。

従来のプロンプト最適化技術は、プロンプトエンジニアリング手順を自動化することに焦点を当てていますが、離散的なプロンプトを連続的な空間でさらに制御または組み合わせる作業に焦点を当てた研究は少ないです。

**関連研究**：[プロンプトを遺伝的アルゴリズムで自動最適化するプロンプトエンジニアリング手法『Promptbreeder（プロンプトブリーダー）』](https://ai-data-base.com/archives/56319)

この課題に対処するため、研究者らはプロンプト側の微調整ではなくLLM側の動作を変更する手法を研究しました。

## 『ControlPE』のポイント

![[プロンプトを調整しないLLMのプロンプトエンジニアリング新手法『ControlPE』 - AIDB/AIDB_59618_1-1024x536.jpg]]

短い応答の効果を示す簡潔な回答

『ControlPE』（Continuously Controllable Prompt Engineering）は、LLMのプロンプト制御における新しい切り口の取り組みです。LoRA（Low-Rank Adaptation）の力を利用し、プロンプトの影響を重み付けのように微調整できるように設計されています。

ControlPEは、既存のプロンプトエンジニアリングを補完する形で、連続的なターゲットの制御を実現します。  
短い応答、拒否応答、チェーン・オブ・ソート（CoT）プロンプトなど、様々なプロンプトの制御が可能になります。

要するに、LLMのカスタマイズがより柔軟かつ精密に行えるように、プロンプトがモデルに与える影響をよりダイナミックに変更するということです。

**関連研究**：[ユーザープロンプトをLLMが言い換えて、LLM自身が理解しやすくする手法『RaR』](https://ai-data-base.com/archives/51160)

![[プロンプトを調整しないLLMのプロンプトエンジニアリング新手法『ControlPE』 - AIDB/AIDB_59618_2-1024x700.jpg]]

「既知の情報に参照がない場合、”関連する情報はありません”と回答し、事実を捏造しない」ControlPEの効果を示す図

## そもそもLoRAとは

LoRA（Low-Rank Adaptation）は、大規模なニューラルネットワークモデル、トランスフォーマーベースのモデルを中心とした微調整手法です。モデルの重みを直接更新するのではなく、より単純で小規模な行列を用いて、既存の重みに変更を加える手法です。

#### モデルの変換の仕組み

LoRAでは、トランスフォーマーの重み行列の出力を調整することにより、入力 ( x ) の次元から別の次元 ( h ) への変換を微調整します。

二つの小さな投影行列 ( A ) と ( B ) の積を ( x ) に加えることによって行われます。これらの行列は元の行列の次元 ( d ) と ( k ) よりもはるかに小さいランク ( r ) を持ちます。

#### トランスフォーマー内の適用

LoRAの変更は、トランスフォーマーの注意機構内のクエリと値の投影行列に主に影響します。

LoRAチューニングを開始するとき、行列 ( A ) はランダムなガウス値で埋められ、行列 ( B ) はゼロで始まります。元の事前訓練済みモデルの挙動が最初に保持されるというのが重要なポイントです。

#### 料理に例えると

LoRAを料理に例えると、完成された豪華な料理（元の事前訓練済みモデル）に対して、追加のソースやスパイス（LoRAによる小さな行列）を用いて味付けを調整するような行為と言えるかもしれません。

料理全体の構造や品質を変えずに、特定の風味や特徴を微調整します。そのようにLoRAは、大規模言語モデルの挙動を細かく調整し、特定のタスクや要件に合わせて最適化するための効率的な手段として知られています。

## ControlPEのフレームワーク

『ControlPE』の実装は、三段階の方法論が提案されています。以下に、ControlPEを実装するための主なステップを論文から紹介します。まず下図は、ControlPEのプロセスを三段階で図解するものです。

ここから限定コンテンツ

![[プロンプトを調整しないLLMのプロンプトエンジニアリング新手法『ControlPE』 - AIDB/AIDB_59618_3-1024x366.jpg]]

三段階のプロセスを図解

#### 1\. 特化データセットの生成

最初のステップは、ターゲットプロンプト蒸留用の特化データセットを生成することです。

データセットは、プロンプトをLoRAモデルのパラメータに蒸留するために使用されます。

#### 2\. LoRAモデルにプロンプトを蒸留

生成されたデータセットを使用して、LoRAモデルをトレーニングします。

トレーニングの目的は、プロンプトをLoRAのパラメータに蒸留することです。

#### 3\. LoRAのマージング重みの調整

最後のステップは、LoRAのマージングにおける重みを調整し、プロンプトの影響を適切に重み付けすることです。

重み調整により、短くて簡潔な回答の提供や拒否プロンプトの処理など、モデルの応答を最適化します。

## 性能評価結果

![[プロンプトを調整しないLLMのプロンプトエンジニアリング新手法『ControlPE』 - AIDB/AIDB_59618_4-1024x324.png]]

LoRA重み調整による効果をグラフ化

『ControlPE』におけるLLMの挙動を微細に制御する性能は、複数の実験を通じて検証されています。以下に、主な性能評価結果を紹介します。

#### 1\. モデルの応答長の効果的制御

特定のLoRA行列にのみ重みを適用することにより、モデルの応答長の線形調整が可能だと示されています。

つまりLoRAの重みを調整することで、モデルの応答長を効果的に制御できることが確認されました。

#### 2\. 不適切なプロンプトへの拒否応答の最適化

DocQAタスクにおいて、必要に応じて「関連する情報がない」という応答を生成することができることが示されました。

![[プロンプトを調整しないLLMのプロンプトエンジニアリング新手法『ControlPE』 - AIDB/AIDB_59618_5-1024x425.png]]

「既知の情報に参照がない場合、”関連する情報はありません”と回答し、事実を捏造しない」ControlPEの効果を示すグラフ

このことから、ControlPEは、不適切なプロンプトへの拒否応答の最適化にも効果的だと確認されました。

#### 3\. チェーン・オブ・ソート推論（CoT）の最適なパフォーマンス

![[プロンプトを調整しないLLMのプロンプトエンジニアリング新手法『ControlPE』 - AIDB/AIDB_59618_6-1024x432.png]]

CoT使用時の問題解決精度

「一歩一歩考えてみよう」というプロンプトを追加することで、GSM8Kデータセット上でのモデルの正確性が12.6%から14.9%に向上しました。CoT推論においても、ControlPEの効果が確認されました。

**関連研究**：[LLMが巡回セールスマン問題などの最適化問題を解く〜自分自身で優れたプロンプトを作成＆活用〜](https://ai-data-base.com/archives/55087)

## 注意点

ControlPEの実装には、LoRAの適用、プロンプトの設計、データセットの準備、パラメータの調整などの技術的な操作が含まれるため、ユーザーに一定の知識が求められます。

アプリケーションレベルでの実装においては、その性能の検証が推奨されています。実際の使用環境におけるControlPEの有効性を確認することが重要です。

## まとめ

『ControlPE』（Continuously Controllable Prompt Engineering）は、大規模言語モデル（LLM）のプロンプト制御において、新たなアプローチを提供します。

従来の方法では達成できなかったプロンプトの微細な調整を可能にし、LoRA（Low-Rank Adaptation）技術を駆使してモデルの挙動を直接制御します。

ControlPEの実装は、特化データセットの生成、LoRAモデルへのプロンプトの蒸留、LoRAのマージング重みの調整という三段階のプロセスを経ることで、モデルの応答長の制御、拒否応答の最適化、チェーン・オブ・ソート推論の改善を実現することが実験で確認されています。

LLMが、より柔軟で状況に応じた応答を生成することを助ける技術フレームワークとなる可能性があります。ただし、その実装には専門的な知識と技術的な操作が必要であり、アプリケーションレベルでの性能検証も重要です。
