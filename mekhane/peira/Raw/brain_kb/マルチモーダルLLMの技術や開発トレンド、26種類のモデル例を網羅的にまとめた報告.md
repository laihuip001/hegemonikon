---
created: 2026-01-01T09:37:42 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/63257
author: AIDB Research
---

# マルチモーダルLLMの技術や開発トレンド、26種類のモデル例を網羅的にまとめた報告 - AIDB

> ## Excerpt
> 本記事はマルチモーダル大規模言語モデルについての調査報告を紹介します。 「マルチモーダル」とは、異なる種類のデータ（例えば、テキスト、画像、音声など）を組み合わせて扱うことを意味します。元々は言葉だけを扱っていたLLMが、複数の種類のデータの入力や出力に対応できるようになってきたのが現状です。 今回Tencentや京都大学などの研究者らは、マルチモーダルLLMに関する広範な調査を行った結果を報告し…

---
本記事はマルチモーダル大規模言語モデルについての調査報告を紹介します。

「マルチモーダル」とは、異なる種類のデータ（例えば、テキスト、画像、音声など）を組み合わせて扱うことを意味します。元々は言葉だけを扱っていたLLMが、複数の種類のデータの入力や出力に対応できるようになってきたのが現状です。

今回Tencentや京都大学などの研究者らは、マルチモーダルLLMに関する広範な調査を行った結果を報告しています。設計や訓練方法、26種類の既存モデルなどに言及しています。

![[マルチモーダルLLMの技術や開発トレンド、26種類のモデル例を網羅的にまとめた報告 - AIDB/AIDB_63257-1024x576.jpg]]

**参照論文情報**

-   タイトル：MM-LLMs: Recent Advances in MultiModal Large Language Models
-   著者：Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, Dong Yu
-   所属：Tencent AI Lab, 京都大学, Mohamed Bin Zayed University of Artificial Intelligence
-   URL：[https://doi.org/10.48550/arXiv.2401.13601](https://doi.org/10.48550/arXiv.2401.13601)

**本記事の関連研究**：

1.  [Geminiの「常識を推論する能力」を網羅的に調査した結果　間違えやすいタイプの問題も明らかに](https://ai-data-base.com/archives/61597)
2.  [AGIを見据えて専門家レベルの問題を集めたベンチマーク「MMMU」、GPT-4VやGemini Ultraでも正解率6割未満](https://ai-data-base.com/archives/61463)
3.  [Gemini Pro 対 GPT-4V、画像認識能力でどちらが優秀なのか](https://ai-data-base.com/archives/61286)
4.  [Googleが「人間の専門家レベルを超える最初のモデル」とする『Gemini』発表、GPT-4を凌駕](https://ai-data-base.com/archives/60035)
5.  [画像分析機能を持つオープンソースLLM『LLaVA-1.5』登場。手持ちの画像を分析可能。GPT-4Vとの違い](https://ai-data-base.com/archives/56440)

## 研究背景

LLMは、大量のテキストデータから学び、人間の言葉を理解し、生成する技術です。そんなLLMが、多様な形式のデータを扱えるように発展した「マルチモーダルLLM」と呼ばれるモデルが注目されるようになってきました。

LLMがもともと持つ強力な言語生成能力や、新しいタスクにもゼロショットですぐに対応できる能力などが他のモダリティに活かされ強化されている状況です。研究者たちは異なる種類のデータを統合して協力し解析する方法を模索しています。例えば、画像からテキストを生成したり、音声を生成したりするなど、さまざまなデータ間での変換が可能になってきています。

研究が非常に進んできたため、今回研究者らは現況を下記の軸でまとめました。

1.  マルチモーダルLLMの設計や訓練方法
2.  26種類の最先端モデルの特徴や開発トレンド
3.  タスク性能
4.  より効果的なモデルのための方向性

なお下記の図はマルチモーダル大規模言語モデル（MM-LLMs）の発展を示しています。2022年初頭から2023年末まで、短期間に多くの新しいモデルが導入されていることがわかります。

![[マルチモーダルLLMの技術や開発トレンド、26種類のモデル例を網羅的にまとめた報告 - AIDB/AIDB_63257_1.png]]

以下では、マルチモーダルLLMの仕組み、訓練のステップ、モデル事例（26種類、リンク付き）とベンチマーク、今後の展望について紹介します。

## 仕組み

マルチモーダルLLMは、主に、

ここから限定コンテンツ

以下のような5つのコンポーネントで構成されています。

1.  **モダリティエンコーダー**：様々な種類のデータを「エンコード」（つまり、コンピューターが理解できる形に変換）する部分です。たとえば、画像や音声データを受け取り、それぞれを特徴に変換します。
2.  **インプットプロジェクター**：エンコードされたデータをテキストの特徴空間に合わせる役割を持ちます。異なる種類のデータがLLMで処理できるようになります。
3.  **LLMバックボーン**：ここが中核部分で、テキストを含む様々なデータを処理し、意味を理解し、推論や決定を行います。言語を生成したり、特定のタスクを遂行したりします。
4.  **アウトプットプロジェクター**：LLMバックボーンからの出力を、次に処理するモダリティジェネレーターが理解できる形に変換します。
5.  **モダリティジェネレーター**：最後のステップとして、画像や音声など、異なる形式で出力を生成します。

下記の図はそれぞれのコンポーネントの繋がりを示しています。様々な種類のデータを統合して処理するために連携して機能しています。

![[マルチモーダルLLMの技術や開発トレンド、26種類のモデル例を網羅的にまとめた報告 - AIDB/AIDB_63257_2-1024x402.png]]

## 訓練のステップ

トレーニングには主に2つの段階があり、「マルチモーダル事前学習」と「マルチモーダル指示チューニング」に分けられます。

**（１）マルチモーダル事前学習**

事前学習では、異なる種類のデータ（モダリティ）間の整合性を実現します。たとえば、画像の内容とそれに関連するテキストがうまく結びつくように、画像とテキストを含むデータセットを用意します。他にもビデオとテキスト、音声とテキストなど、何らかのデータ形式とテキストを合わせたデータセットを使用します。

先ほどの仕組みで紹介した「インプットプロジェクター」と「アウトプットプロジェクター」が主に訓練されます。異なるモダリティ間でのデータの変換を助ける部分です。

****（２）**マルチモーダル指示チューニング**

次に、事前訓練されたマルチモーダルLLMをさらに微調整します。この段階の目的は、新しい指示に基づいて、まだ見ぬタスクに対応できるようにすることです。

指示チューニングには、教師ありファインチューニング（SFT）と人間のフィードバックからの強化学習（RLHF）が含まれます。SFTでは、データを命令に従う形式に変換し、RLHFでは、モデルが生成する応答に対する人間のフィードバックに基づいてさらに微調整を行います。

## モデル事例

下記に、26種類の主流なマルチモーダルLLMを示します。

**(1) Flamingo**

画像とテキストを組み合わせて処理し、自由形式のテキストを生成するモデルです。視覚と言語を統合して理解する力に重点を置いています。

論文：[https://doi.org/10.48550/arXiv.2204.14198](https://doi.org/10.48550/arXiv.2204.14198)

**(2) BLIP-2**

より効率的な枠組みを導入し、軽量な「Q-Former」という技術を使って異なるモダリティ間のギャップを埋めることに注力しています。

論文：[https://doi.org/10.48550/arXiv.2301.12597](https://doi.org/10.48550/arXiv.2301.12597)

**(3) LLaVA**

指示チューニング技術をマルチモーダル領域に適用し、データ不足に対応するために新しいオープンソースのデータセットを作成しています。

論文：[https://doi.org/10.48550/arXiv.2304.08485](https://doi.org/10.48550/arXiv.2304.08485)

**(4) MiniGPT-4**

GPT-4の能力を模倣するために、より単純化されたアプローチを採用しています。事前訓練された視覚エンコーダーをLLMと整合させることに焦点を当てています。

論文：[https://doi.org/10.48550/arXiv.2304.10592](https://doi.org/10.48550/arXiv.2304.10592)

****(5)** mPLUG-Owl**

視覚的な文脈を取り入れた新しいモジュラー訓練フレームワークを提示しています。異なるモデルの性能を評価するための指導的評価データセット「OwlEval」を含んでいます。

論文：[https://doi.org/10.48550/arXiv.2304.14178](https://doi.org/10.48550/arXiv.2304.14178)

****(6)** X-LLM**

様々なモダリティに拡張されており、特に中国語などの言語への適用が成功しています。

論文：[https://doi.org/10.48550/arXiv.2305.04160](https://doi.org/10.48550/arXiv.2305.04160)

******(7)**** VideoChat**

ビデオを理解するための効率的なチャット中心のマルチモーダルLLMで、将来の研究の基準を設定しています。

論文：[https://doi.org/10.48550/arXiv.2305.06355](https://doi.org/10.48550/arXiv.2305.06355)

******(8)**** InstructBLIP**

指示に基づく視覚的特徴抽出を可能にし、多様な特徴を抽出する能力を持っています。

論文：[https://doi.org/10.48550/arXiv.2305.06500](https://doi.org/10.48550/arXiv.2305.06500)

******(9)**** PandaGPT**

テキスト、画像/ビデオ、音声、熱、深度、慣性計測ユニットなど、6つの異なるモダリティにまたがる指示に基づいて行動する能力を持っています。

論文：[https://doi.org/10.48550/arXiv.2305.16355](https://doi.org/10.48550/arXiv.2305.16355)

******(10)**** PaLI-X**

混合された視覚言語目的と単一モダリティ目的を使用して訓練され、より効果的な微調整の結果を達成しています。

論文：[https://doi.org/10.48550/arXiv.2305.18565](https://doi.org/10.48550/arXiv.2305.18565)

**(11) Video-LLaMA**

ビデオの視覚と音声の内容を同時に処理し、人間との会話に参加するマルチブランチクロスモーダルPTフレームワークを紹介しています。このフレームワークは視覚を言語と、音声を言語と整列させます。

論文：[https://doi.org/10.48550/arXiv.2306.02858](https://doi.org/10.48550/arXiv.2306.02858)

**(12) Video-ChatGPT**

ビデオ会話に特化したモデルで、時空間の視覚表現を統合することにより、ビデオについての議論を生成する能力があります。

論文：[https://doi.org/10.48550/arXiv.2306.05424](https://doi.org/10.48550/arXiv.2306.05424)

**(13) Shikra**

画像内の領域や物体に関する議論を含む参照対話に特化した、シンプルで統一された事前学習されたMM-LLMです。このモデルは未見の設定に効果的に対応する一般化能力を示しています。

論文：[https://doi.org/10.48550/arXiv.2306.15195](https://doi.org/10.48550/arXiv.2306.15195)

**(14) DPL**

単一モーダルの文章のデータセットでトレーニングされたP-Formerを用いて、理想的なプロンプトを予測することを目的としています。これは単一モーダルのトレーニングがMM学習を強化する可能性を示しています。

論文：[https://doi.org/10.48550/arXiv.2308.10061](https://doi.org/10.48550/arXiv.2308.10061)

**(15) BuboGPT**

画像、テキスト、音声など異なるモダリティ間の細かい関係を探求するために共有された意味空間を学習することで、MMコンテンツを包括的に理解するモデルです。

論文：[https://doi.org/10.48550/arXiv.2307.08581](https://doi.org/10.48550/arXiv.2307.08581)

**(16) ChatSpot**

精密な指示を微調整するシンプルながらも強力な手法を導入し、画像や領域レベルの指示を含む正確な指示の組み込みにより、多粒度の視覚言語タスクの記述を強化します。

論文：[https://doi.org/10.48550/arXiv.2307.09474](https://doi.org/10.48550/arXiv.2307.09474)

**(17) Qwen-VL**

英語と中国語の両方をサポートする多言語MM-LLMで、トレーニングフェーズ中に複数の画像を入力として受け付けることで、視覚コンテキストの理解を改善します。

論文：[https://doi.org/10.48550/arXiv.2308.12966](https://doi.org/10.48550/arXiv.2308.12966)

**(18) NExT-GPT**

画像、ビデオ、音声、テキストの自由な入出力をサポートするエンドツーエンドの汎用MM-LLMです。これは、エンコーディングフェーズでLLM中心の整列を、デコーディングフェーズで指示に従った整列を利用する軽量な整列戦略を採用しています。

論文：[https://doi.org/10.48550/arXiv.2309.05519](https://doi.org/10.48550/arXiv.2309.05519)

**(19) MiniGPT-5**

生成的トークン（vokens）への逆変換とStable Diffusionとの統合を特徴とするMM-LLMです。マルチモーダル生成での交互出力において優れた性能を発揮し、トレーニングフェーズでの分類器フリーのガイダンスを含むことで生成品質を向上させています。

論文：[https://doi.org/10.48550/arXiv.2310.02239](https://doi.org/10.48550/arXiv.2310.02239)

**(20) LLaVA-1.5**

LLaVAシリーズの最新版であり、改良された命令チューニングデータセットとベンチマークにより、言語モデルの性能をさらに向上させています。

論文：[https://doi.org/10.48550/arXiv.2310.03744](https://doi.org/10.48550/arXiv.2310.03744)

紹介記事：[画像分析機能を持つオープンソースLLM『LLaVA-1.5』登場。手持ちの画像を分析可能。GPT-4Vとの違い](https://ai-data-base.com/archives/56440)

**(21) MiniGPT-v2**

LLaMA-2の進化版であり、より洗練されたプロンプトと調整機能を備え、幅広いマルチモーダルタスクに対応しています。

論文：[https://doi.org/10.48550/arXiv.2310.09478](https://doi.org/10.48550/arXiv.2310.09478)

**(22) CogVLM**

認知的視覚言語モデリングを目指し、深い意味理解と推論能力を備えています。

論文：[https://doi.org/10.48550/arXiv.2311.03079](https://doi.org/10.48550/arXiv.2311.03079)

**(23) DRESS**

特にドキュメント理解や要約に焦点を当てたMM-LLMで、情報抽出と処理の精度を高めています。

論文：[https://doi.org/10.48550/arXiv.2311.10081](https://doi.org/10.48550/arXiv.2311.10081)

**(24) X-InstructBLIP**

異なるモダリティ間での指示に基づく学習に特化し、よりダイナミックなマルチモーダル対話を可能にしています。

論文：[https://doi.org/10.48550/arXiv.2311.18799](https://doi.org/10.48550/arXiv.2311.18799)

**(25) CoDi-2**

マルチモーダル入力に基づくコンテキスト依存の対話生成に特化したモデルで、より複雑な対話シナリオに対応しています。

論文：[https://doi.org/10.48550/arXiv.2311.18775](https://doi.org/10.48550/arXiv.2311.18775)

**(26) VILA**

ビジュアル言語タスクにおいて、高度な意味理解と生成を行うことを目指しており、複雑な視覚的シーンにおける言語モデルの適用可能性を拡張しています。

論文：[https://doi.org/10.48550/arXiv.2312.07533](https://doi.org/10.48550/arXiv.2312.07533)

下記はこれらマルチモーダルLLMの概要をまとめた表です。

各モデルが、画像、ビデオ、オーディオ、テキストなどのモダリティをどのように処理するか、どのようなエンコーダーやプロジェクターを使用しているか、そしてどのようなLLMバックボーンが使われているかについての詳細が示されています。

![[マルチモーダルLLMの技術や開発トレンド、26種類のモデル例を網羅的にまとめた報告 - AIDB/AIDB_63257_3-1024x437.png]]

## ベンチマークとパフォーマンス

※ベンチマークとは、モデルの性能を評価するための一連のテストや基準のことです。

18種類の視覚言語（VL）ベンチマークを使用して、さまざまなマルチモーダルLLMにおける性能の比較をまとめたものが下記の表です。どれが最も高いパフォーマンスを示したか（赤色）、そして二番目に高いパフォーマンスを示したか（青色）が記されています。

![[マルチモーダルLLMの技術や開発トレンド、26種類のモデル例を網羅的にまとめた報告 - AIDB/AIDB_63257_4-1024x414.png]]

### 性能を上げるためのトレーニングデータ

#### 高品質な訓練データ

まずは事前訓練におけるトレーニングデータの品質が重要です。高解像度の画像を使うことで、モデルがより細かい視覚的な詳細を捉えられるようになります。細部に注目が必要なタスクに特に有効です。

ただし、高解像度はトークン列の長さを増やし、追加の訓練と推論コストがかかります。そこで、例えばMiniGPT-v2のようなモデルでは、埋め込み空間で隣接する視覚トークンを結合して長さを短縮する工夫が行われています。

また他に、Monkeyというモデルでは、低解像度の視覚エンコーダーだけを使って、入力画像の解像度を向上させる工夫が施されています。

#### 高品質な教師ありファインチューニングデータ

教師ありファインチューニングにおいても、高品質なデータを使用することで特定のタスクの性能が大幅に向上することが示されています。

例えばVILAでは、教師ありファインチューニングのデータとして、画像とテキストが交互に配置されたデータが有益であり、画像とテキストがペアになっているだけでは最適ではないことが分かっています。

また、テキストだけの指示データを画像/テキストペアデータと再結合させることで、視覚言語タスクの精度を向上させることができるとされています。

## 今後の展望

#### より広範なデータを利用し、強力なモデルを開発

さらに多くのモダリティ（例：ウェブページ、熱マップ、図表など）を取り入れることで、モデルの応用範囲がさらに広がっていくことが期待されています。

また、異なる種類やサイズのモデル同士を組み合わせることで、さまざまなニーズに応える柔軟性が生まれるとも考えられます。

**関連記事：**[既存のLLMを融合させて強力なモデルを作る手法「知識融合」](https://ai-data-base.com/archives/63153)

さらに、マルチモーダル指示チューニングデータセットの品質を向上させることや、より高度なモデルの開発も重要です。

#### より厳しいベンチマーク

既存のベンチマークでは、マルチモーダルLLMの能力を十分に試すにはまだ不十分であり、もっと厳しい、多様なモダリティを含む大規模なベンチマークの開発が必要と考えられています。

#### **軽量化**

リソースに制約のあるプラットフォーム（例：モバイル、IoTデバイス）での効率的な展開のために、軽量な実装が重要です。（既存の研究では、MobileVLMはより少ないパラメータで高速な計算を実現しています）

#### **エンボディメント**

環境を効果的に理解し、対話する能力を備えたロボットの開発も将来の課題です。より自律的なロボットの開発が含まれ、まだ多くの課題があります。

#### 継続的な指示チューニング

実用的なアプリケーションにおいては、モデルは新しいタスクに適応していく必要があります。今のマルチモーダルLLMは静的であり、新たに出現する要求に完全に対応できるようにはなっていません。継続的に新しいデータを利用しながら、既存のタスクの性能を維持する方法の開発が求められています。

## 注意点

本論文の著者は、この分野の発展が速すぎるがゆえに、今回の調査結果が果たして網羅し切れているのかはわからないと言います。そのため、専用のウェブサイトを開いて情報をアップデートしていきたいとしています。

## まとめ

本記事では、マルチモーダルLLMの進化に焦点を当てた論文を紹介しました。

研究者らは、アーキテクチャ、トレーニングプロセス、および最先端モデルの特徴についてまとめています。今後のマルチモーダルLLMが、テキスト、画像、音声などの複数のモダリティを処理する能力を持ち、これらを統合して新しいアプリケーションを生み出す可能性を示しています。

発展が速い分野では、このように状況を整理する調査も重要ですね。
