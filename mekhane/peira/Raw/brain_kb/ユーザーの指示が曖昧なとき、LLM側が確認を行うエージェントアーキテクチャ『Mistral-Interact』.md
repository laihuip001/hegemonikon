---
created: 2026-01-01T09:37:03 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/64628
author: AIDB Research
---

# ユーザーの指示が曖昧なとき、LLM側が確認を行うエージェントアーキテクチャ『Mistral-Interact』 - AIDB

> ## Excerpt
> 現行のLLMエージェントは、ユーザー指示の不明確な点を質問したり、ユーザーの意図を正確に把握することが苦手です。 そこで今回研究者らは、ユーザーの意図を探るためのベンチマークを開発しました。そしてタスクの曖昧さを見極め、ユーザーの意図を聞き取り、実行可能な目標に絞り込んでから下流のエージェントによるタスク実行を行うモデルMistral-Interactを開発しました。 参照論文情報 本記事の関連研…

---
現行のLLMエージェントは、ユーザー指示の不明確な点を質問したり、ユーザーの意図を正確に把握することが苦手です。

そこで今回研究者らは、ユーザーの意図を探るためのベンチマークを開発しました。そしてタスクの曖昧さを見極め、ユーザーの意図を聞き取り、実行可能な目標に絞り込んでから下流のエージェントによるタスク実行を行うモデルMistral-Interactを開発しました。

![[ユーザーの指示が曖昧なとき、LLM側が確認を行うエージェントアーキテクチャ『Mistral-Interact』 - AIDB/AIDB_64628-1024x576.jpg]]

**参照論文情報**

-   タイトル：Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents
-   著者：Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong, Zhong Zhang, Jie Zhou, Yankai Lin, Zhiyuan Liu, Maosong Sun
-   所属：Tsinghua University, Renmin University of China, WeChat AI, Tencent Inc.
-   URL：[https://doi.org/10.48550/arXiv.2402.09205](https://doi.org/10.48550/arXiv.2402.09205)
-   GitHub：[https://github.com/HBX-hbx/Mistral-Interact](https://github.com/HBX-hbx/Mistral-Interact)

**本記事の関連研究**：

-   [ユーザープロンプトをLLMが言い換えて、LLM自身が理解しやすくする手法『RaR』](https://ai-data-base.com/archives/51160)
-   [「わたしの話」を体系的に覚えてもらいながらLLMと会話する技術MemoChat登場](https://ai-data-base.com/archives/54560)
-   [自信がないときにLLMに発言を控えさせる手法](https://ai-data-base.com/archives/63749)
-   [LLMへの入力プロンプトを「意味を保持したまま」高度に圧縮する技術『LLMLingua』](https://ai-data-base.com/archives/60431)

## 背景

大規模言語モデルはAIエージェントとしてユーザーを支援することにも使われ始めています。また、エージェント開発に特化した多くのオープンソースフレームワークが登場しています。

しかし、現在のLLMエージェントには次のような課題があります。

-   ユーザーがエージェントに与える指示は曖昧で簡潔すぎる場合が多い
-   一見明確な指示でも意図が異なる可能性があるため、明示的な質問によって探る必要がある

要するに、エージェントは表面的な目標を達成したように見えても、実はユーザーの真の意図とかけ離れてしまう「見せかけの成功」に陥りがちです。そのため、エージェントはユーザーとのやり取りを通じて隠れた意図を理解することが重要になります。

既存のエージェント設計とベンチマークは、タスクが明確であることを前提としており、意図の汲み取りは評価対象に含まれていません。そこで研究者らは、タスクの曖昧さ判断や真の意図の理解を通じたエージェントの対話能力をテストするベンチマークを作成することにしました。

なお、これまでに行われてきた研究では、エージェント間のコミュニケーション・連携・評価のためのマルチエージェントフレームワークについて探求されてきました。しかし、エージェント設計におけるユーザーの役割についてはあまり重要視されてきていません。

下記では、新しいベンチマークの開発と、モデル「Mistral-Interact」の開発、そして性能評価結果について紹介します。まず、

ここから限定コンテンツ

ベンチマークについて取り上げます。

## タスク指示の明確さを測るベンチマーク

従来のエージェント用ベンチマークの多くは、タスクが明確であることを前提としていましたが、実際にはユーザーから与えられる指示は曖昧であることが少なくありません。例えば、下記の図にある「私の街で最高のヨガ教室を見つける」というタスクの場合、「私の街」がどこなのか、「最高」の基準が何なのかが不明確です。エージェントの実行効率を高めるためには、ユーザーの真の意図を明確に把握することが必要になります。

![[ユーザーの指示が曖昧なとき、LLM側が確認を行うエージェントアーキテクチャ『Mistral-Interact』 - AIDB/AIDB_64628_2-1024x749.jpg]]

そこで、エージェントが不足している情報を積極的に質問し、ユーザーの隠れた意図を理解する必要があります。エージェントのこの能力を定量的に評価して強化するため、研究者らはIntention-in-Interaction (IN3) というベンチマークを導入します。

### ベンチマークの構築

IN3は、何百ものカテゴリー（料理、芸術、プログラミングなど）にわたる多様なエージェント用タスクをカバーします。タスクの曖昧さ、不足している情報（もしあれば）、各不足情報の重要度レベル（3段階）、各不足情報に対する選択肢候補、といった情報で注釈をつけます。上述の健康アドバイス（ヨガ教室を見つける）タスクでは、ユーザーの都市とベストの基準に関する不足情報と、それらの候補例についてIN3が注釈をつけて、ユーザーの真の意図を反映します。都市はヨガ教室検索に不可欠なので重要度が高く（Lv 3）、好みのマッチングに役立つだけのベストの基準（Lv 2 ）よりも重要になっています。

タスクの説明とそのカテゴライズは、GPT-4で行っています。200以上のカテゴリを考慮し、1300以上の多様なタスクを作成しました。以下のステップで作成されています。

-   まず人間が作成した最初のタスクを元に、モデルがデータセットを拡充しながら新たなタスクを生成します。
-   そして生成されたタスクから例題となるものをランダムに選び、再び生成プロセスを実行します。
-   次に、GPT-4の提案を参考にしながら、人間が各タスクの曖昧さ、不足情報、各情報の重要度と候補を注釈していきます。

なお下記の表でわかるように、IN3は訓練用とテスト用に分割されています。

![[ユーザーの指示が曖昧なとき、LLM側が確認を行うエージェントアーキテクチャ『Mistral-Interact』 - AIDB/AIDB_64628_3-1024x862.jpg]]

### ベンチマーク自体の評価

IN3は予備調査も行われました。データセットから10件のタスクを抽出してLLaMA-2-7B-Chat1、Mistral-7B-Instruct-v0.22、GPT-4に①タスクの曖昧さを判断、②タスクが曖昧な場合は不足情報を質問、③詳細なユーザーのタスク目標を要約 してもらいます。

定量分析では、まずモデルによるタスク曖昧さの判断がIN3の人間注釈と一致する数をカウントします。次に、曖昧なタスクで生成された質問の数と、そのうちIN3と一致する質問（本当に意味・必要・重要な不足情報についてのもの）を数えます。合計24個の人間注釈済みの不足情報があります。定性分析では、やり取りを3つの段階（曖昧さの判断、不足情報の質問、詳細な目標の要約）に分け、様々な不完全もしくは失敗したやり取りパターンを集め、深刻度で評価します。

結果の概要は下記の表に示されています。

![[ユーザーの指示が曖昧なとき、LLM側が確認を行うエージェントアーキテクチャ『Mistral-Interact』 - AIDB/AIDB_64628_4-1024x513.jpg]]

3つのモデルすべてが課題を抱えていますが、失敗傾向に違いがあります。

-   LLaMA-2は最も精度が低く、明確な目標でも識別できず、不必要な質問が多いです。
-   MistralはLLaMA-2より良好ですが、人間の意図への理解力が低いです。
-   GPT-4は、タスクの曖昧さ、重要な不足情報に関する判断で、人間の意図に最も近い結果でした。

## 方法論

研究者らは、エージェントの意図理解能力をさらに高めるため、ユーザーとの対話を通してそれに特化した専門モデルを訓練し、エージェントに上流モジュールとして組み込むことを考案しています。

具体的には、専門モジュールはユーザーとの対話を通して、以下の情報を取得します。

-   ユーザーがタスクで達成したい具体的な目標
-   ユーザーがタスク実行に使える時間やリソース
-   ユーザーのタスクに対する優先順位

例として、ユーザーが「旅行に行きたい」という曖昧なタスクを提示した場合に必要な情報は以下になります。

-   ユーザーが行きたい場所
-   ユーザーが旅行に行ける期間
-   ユーザーの旅行の予算

このように、モジュールはユーザーとの対話を通して曖昧なタスクを具体化することで、エージェントによるタスク実行の精度とユーザー満足度を向上させます。

実現するためには、まずはIN3を使って学習用の会話記録を作成します。そして会話記録を用いて、Mistral-7Bを以下の機能を持つMistral-Interactへと変化させます。

-   ユーザー指示の曖昧さ判断
-   提案を交えた不足情報の能動的な質問
-   詳細なユーザー意図の明確な要約

以下、手法を詳しく説明します。

### 学習データの作成

会話を通して曖昧タスクの不足情報を聞き出す方法を学習する必要があります。IN3には注釈つきの多様なタスクが用意されているので、学習用の会話記録をそこから作ります。上述の予備実験より、ユーザー役としてGPT-4を使うことは妥当だと示されました。そこで、2つのGPT-4を用いて自動的・効率的に会話をシミュレーションすることにします。 1つは特定のタスクを完了したいユーザー(User-GPT)役、もう1つはIN3の注釈を参考にしながらユーザーの意図を明確に理解することを目標としたアシスタント(Assistant-GPT)役です。

![[ユーザーの指示が曖昧なとき、LLM側が確認を行うエージェントアーキテクチャ『Mistral-Interact』 - AIDB/AIDB_64628_5-1024x585.jpg]]

**最初の考えの明示**

IN3にあるUser-GPTのタスクを受け、最初にAssistant-GPTの初期の思考を、IN3に基づいたタスク曖昧さ、不足情報、選択肢を含む判断で構成します。その場で理由を考えながら任意の質問をするのではなく、コアとなる細部について質問するようモデルを導きます。

**選択肢付きで質問**

会話の各ラウンドでは、Assistant-GPTに考えを示させつつ、選択肢つきの質問を1度に1件のみさせます。質問は初期思考でリストアップされていたものを参考にします。モデルの質問が強引にならないようにしつつ、合理的な選択肢を見せることで深い考えや隠れた意図の明示をユーザー(この場合はUser-GPT)に促すためです。その結果として、モデルとの対話がユーザーフレンドリーになることが期待されます。

**多様なユーザーの口調**

応答には、「簡潔(短い返答)」「熱心(新しい情報を多く含めて長い返答をする)」などのパターンに合わせてUser-GPTに返答させます。学習モデルの幅広い適用能力や堅牢性を高めます。

**ユーザー意図の要約**

Assistant-GPTに初期の段階で、質問への応答内容と新しい情報の全てをまとめさせます。

なお、IN3のタスクが明確な場合は、すべての会話シミュレーションがスキップされます。初期の思考における曖昧さ判断(明確)、とまとめの思考のみ作成します。また、異なるユーザーを模倣しているため、出来上がる訓練データ数はIN3の約2倍になります。

### 学習の詳細

作成した会話記録をもとに、Mistral-7BをMistral-Interactに適応させます。モデルはユーザーの意図をより良く理解し、曖昧なタスクをエージェントが実行するための明確な目標に変換できるようになります。

最初のデータでモデルに初期思考を作らせ、タスクの曖昧さ判断や質問をさせます。続くデータでは、モデルに考えに基づいた不足情報の質問を繰り返させます。最後に、ユーザーの返答を考えと共に要約させる練習をさせます。明確なタスクの場合、思考・判断・最終的な要約の作成を直接1つのデータインスタンスで練習させます。

## 実験

エージェントの意図理解能力は、ユーザーとのやりとりから直接的にも、それを踏まえたタスク実行結果を介して間接的にも評価できます。前者は意図理解そのものに、後者はその最終目標であるエージェント設計の効率化に着目します。

そこで、対話可能なエージェント設計の有効性を総合的に評価するため、実験を以下の2つに分けます。

-   **指示の理解:** ユーザーとのやり取りの中での意図理解能力を評価し、上流処理モジュールとしての性能を直接見ます。
-   **指示の実行:** 上流の対話モデルと連携した場合のタスク実行性能を評価し、間接的に有効性を測定します。

### 「指示の理解」の評価

エージェントのリアルタイム実行は含まず、その代わり、対話中に言語モデル自体の能力を見て上流モジュールとして活用できるかを判断します。

#### **実験設定**

**（１）データと条件**

IN3のテスト用タスクセットを用います。各タスクで、当該モデルを使ってユーザーとの自由な対話をさせ、積極的に意図を聞き出させます。多様なバックグラウンドを持つ大学生に参加してもらい、応答してもらいます。 会話全体を記録し、IN3の正解を参照して評価します。

****（２）**比較モデル**

Mistral-Interact とLLaMA-2-7B-Chat、Mistral-7B-Instruct-v0.2、GPT-4を比較します。公平のため、全モデルにタスクの曖昧さ判断、不足情報の質問、ユーザー目標の要約を指示します。

**（３）評価指標**

以下の新規指標を使用します。主観的なユーザー意図を客観的な数値に変換し、分析や比較を簡単にします。

-   **曖昧さ判断の正確性:** モデルによるタスクの曖昧さ（曖昧 vs 明確）判断のうち、正解と一致した割合を算出。
-   **不足情報発見率:** 正解として示された不足情報のうち、対話を通じてモデルがいくつ見つけられた（明示的に質問された）かを重要度レベル別に算出。
-   **要約での意図網羅率:** 最終的にモデルが要約したユーザーの意図のうち、実際にユーザーが伝えた意図との一致率を算出。

また、ユーザーの隠れた意図を理解するモデルの能力を直接反映する（上記）3つの主要指標に加え、次の項目も分析し、より包括的な評価を行います。

-   **選択肢提示率:** モデルが質問した不足情報のうち、選択肢つきで質問された割合。
-   **選択肢妥当率:** モデルが提供した選択肢のうち、ユーザーが妥当と感じた割合。
-   **選択肢の平均数:** 不足情報一つにつきモデルが平均でいくつ選択肢を出したか。
-   **質問された不足情報の平均数:** タスク一つにつきモデルが質問した不足情報の平均数。
-   **会話ラウンド数の平均:** タスク一つにつきモデルがユーザーと行った会話の平均ラウンド数。
-   **ラウンドごとの不足情報質問数の平均:** 会話1ラウンドあたりモデルがいくつ質問するか。

#### 結果

全体的な結果（下記）から、Mistral-Interactに関する知見を以下にまとめます。

![[ユーザーの指示が曖昧なとき、LLM側が確認を行うエージェントアーキテクチャ『Mistral-Interact』 - AIDB/AIDB_64628_6-1-1024x426.png]]

全てのオープンソースモデルの中で、Mistral-Interactはユーザーが重視するタスクの曖昧さを見抜き、必要な不足情報（特にレベル3と2）を取り出すことに最も優れています。判断の正確さと重要な抜け情報の70%を超える検出能力において、LLaMA-2-7B と Mistral-7Bを大きく上回り、GPT-4に匹敵する性能です。

また、ユーザー意図の要約生成にもMistral-Interactは優れています。平均約4.5と他のオープンソースモデルに比べ会話ラウンド数が多く、そのためユーザーからより多くの情報が提供されていますが、それでもMistral-Interactの要約は漏れ無く最も網羅度が高く、ユーザー意図の96%以上をカバーしています。

さらに他のオープンソースモデルと比較して、Mistral-Interactは適切かつユーザーフレンドリーな方法で不足している分の情報を聞くことができます。ラウンド当たり約1件と質問数が少なく、かつ高い補充率の維持や妥当な選択肢を多く提示できており、ユーザーが意欲的に内面を明かしやすい状態を作っています。

会話シナリオにおけるMistral-Interactの3つの事例を見てみましょう。

![[ユーザーの指示が曖昧なとき、LLM側が確認を行うエージェントアーキテクチャ『Mistral-Interact』 - AIDB/AIDB_64628_7-1-1024x762.jpg]]

-   ケースA：ユーザーの返信が簡潔でも詳細でも、熱心でも気乗りしなくても、誤字を含んでいても、Mistral-Interactは正確に理解して適切な返答ができています。
-   ケースB：ユーザーに消極的・非協力的な態度がみられた場合でも、Mistral-Interactが会話を続けて軌道に戻させることができるかを検証しています。結果、ユーザーが質問を回避した場合でも、会話から外れないよううまく方向転換できることを示しています。
-   ケースC：Mistral-Interactが、必ずしも質問していない付加的な情報を要約に組み込む様子を観察します。つまり、モデル側が完全に細部まで聞き出せなかった場合や、ユーザー側が特定の要件を持っている場合でも、全ての意図を合理的に要約してくれることがわかります。

### 「指示の実行」に対する評価

タスクにおける意図の理解を評価するため、Mistral-Interactを対話モジュールとしてXagentフレームワーク（複雑なタスク解決のための自律型エージェントシステム）に組み込みました。以下、実験と結果です。

#### 実験設定

**（１）データ**

IN3のテストセットから以下に当てはまる10件のタスクをランダムに抽出しました。①正解とMistral-Interactのどちらもが曖昧と判断したもの。②エージェントの能力範囲内であること。

次に、曖昧な初期段階のタスク表現、そして対話で得られた明確な意図を含む要約文を取り出し、それぞれエージェント実行評価と比較に回します。

****（２）**設定**

XAgentのスケジュール機能と実行エンジンを使用します。実行環境は、ウェブ検索、コード実行、コマンドライン、ファイルシステムなどを含みます。タスク計画と実行の両方でデフォルトモデルとしてGPT-4を使用します。

**（３）評価指標**

XAgentはタスクを効率的に実行するために、サブタスクとツール呼び出しチェーンで構成されるタスク計画を作成します。ユーザーの意図を明確にするために、以下の指標で評価します。

-   不要なサブタスク/マイルストーンの割合
-   一般的なサブタスク/マイルストーンの割合
-   サブタスク/マイルストーンあたりのツール呼び出し回数

#### 結果

定量的な評価結果を以下に示します。

![[ユーザーの指示が曖昧なとき、LLM側が確認を行うエージェントアーキテクチャ『Mistral-Interact』 - AIDB/AIDB_64628_8-1024x219.png]]

①実行時の不必要な目標設定回避、②ユーザーの細かな意図との合わせ込み、③エージェントの実行効率化ができています。

**事例分析**

Mistral-Interactの効果をわかりやすくするために、ケーススタディを紹介します。

![[ユーザーの指示が曖昧なとき、LLM側が確認を行うエージェントアーキテクチャ『Mistral-Interact』 - AIDB/AIDB_64628_9-1-1024x512.jpg]]

赤字の部分から、ユーザーの目標が曖昧だと、XAgentはより一般的なサブタスクとマイルストーンを設定しがちです。さらに、紫字で示すように、タスクが曖昧すぎて実行できないときXAgentは不必要なサブタスクとマイルストーンを設定してしまうことがわかります。

一方、対話を通じた後では、明確になった目標に応じてより具体的で細かいサブタスクとマイルストーンが生成されています。これは、緑字部分で示す一致度の上昇として見られます。同時にタスク実行の流れがシンプルになり、使用するツールも絞られてツール起動回数が大幅に減少しました。

## 今後の展望

今のエージェントは、あまりユーザー中心の設計ではありません。しかし、タスクを始める前に、ユーザーが本当に何をしたいのか（暗黙の意図）をしっかりと理解するような仕組みを使えば、エージェントがもっとうまく働くようになります。

例えば、選択肢がいくつかある時や、何をすればいいのかよくわからない時、またはエラーが出た時などに、その場でユーザーに聞いて確認する方法もあります。さらに、エージェントを使っている途中で、ユーザーが操作を止めたい時や、ちょっとリスクがある行動をする前に許可を取ることで、ユーザーがもっと積極的に関わることも大切です。

ただ専門知識があるモデルを組み込むだけではなく、エージェントが複雑なやり取りをする必要があります。今後はこのような研究が進むことが期待されます。

またIN3データセットを作るとき、GPT-4のプロンプトを工夫して、色々な役割を持ったユーザーを模倣させました。怒っている様子や熱心な様子、簡潔な返答や詳細な返答など、様々な口調やスタイルを使い分けることができます。これの意味は二つあります。一つは、モデルが生成したユーザーの会話を使って、アシスタントモデルを教えることができる点です。もう一つは、実際のユーザーが参加して時間がかかるテストを減らし、モデルだけで評価することができる可能性がある点です。

上記は個人の好みに基づいていませんが、もし個人の会話履歴から好みを知ることができれば、もっと一貫性があり信頼性の高い模倣が可能になるかもしれません。

## まとめ

本記事では、エージェント設計における暗黙的な意図の理解とその評価に焦点を当てた研究を紹介しました。研究者らは、エージェントがユーザーの意図をどれだけ正確に理解できるかを測るための新しいベンチマーク「Intention-in-Interaction (IN3)」を発表しました。さらに、「Mistral-Interact」という強力なオープンソースモデルの訓練を行いました。

実験により、「Mistral-Interact」がユーザーのタスクの不明瞭さを効率的に判断し、必要な情報を引き出しながら適切な選択肢を提示し、ユーザーの意図を漏れなく要約する能力を持つことを示しました。またXAgentフレームワーク内でMistral-Interactを利用することにより、エージェントの処理能力がどのように向上するかを、タスク理解と実行の両面から評価し、この手法の有効性を証明しました。

「ユーザーの指示は必ずしも明確ではない」という課題に対して「プロンプトは明確に書きましょう」と言うのではなく、モデル側で曖昧さを解析して解決する話でした。実装ロジックはやや複雑ですが、ぜひ参考にしてみてください。
