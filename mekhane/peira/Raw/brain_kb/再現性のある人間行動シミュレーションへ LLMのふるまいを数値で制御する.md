---
created: 2026-01-01T11:17:35 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/95343
author: AIDB Research
---

# 再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB

> ## Excerpt
> 本記事では、LLMのふるまいを数値で整えて、人間らしい行動を安定して再現する手法を紹介します。 まずは、自然言語だけでは思いどおりの反応が得られにくいという課題にふれていきます。そのうえで、数値を使ってモデルのふるまいを調整する考え方が、どんな可能性をひらくのかを見ていきます。 実務につながりそうな視点でまとめていきます。 本記事の関連研究 背景 LLMを使ったシミュレーションが注目されています。…

---
本記事では、LLMのふるまいを数値で整えて、人間らしい行動を安定して再現する手法を紹介します。

まずは、自然言語だけでは思いどおりの反応が得られにくいという課題にふれていきます。そのうえで、数値を使ってモデルのふるまいを調整する考え方が、どんな可能性をひらくのかを見ていきます。

実務につながりそうな視点でまとめていきます。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_95343_thum2-1024x576.png]]

**本記事の関連研究**

-   [キャラクターなりきりLLM対話を実現するための設計とステップ　微調整は使わない](https://ai-data-base.com/archives/92995)
-   [個人の深い価値観にもとづく「その人らしい答え」をAIで再現する手法](https://ai-data-base.com/archives/90734)
-   [プロンプトによるLLM応答のパーソナライゼーション　仮説を活用して文体を調整](https://ai-data-base.com/archives/89384)

## 背景

LLMを使ったシミュレーションが注目されています。LLMを使えば、現実では難しい実験や、倫理的に扱いづらい行動も安全に検証できます。複雑な人間行動の分析が、これまでよりずっと手軽かつスピーディに進むようになってきました。

ただ、エージェントの行動をどう制御するかが、重要な問題です。

たとえば「ティナは35歳の経済学者で、サンフランシスコ在住。趣味はハイキング」といった自然言語の説明を使って特徴を決める方法には、いくつかの問題があります。

1つは、同じ説明でもモデルが変わるとエージェントの行動が変わってしまうことです。

また、自然言語での設定には曖昧さもあります。「権威に従いやすい」「集団に同調しやすい」と書いても、どのくらいの強さで、どんな場面でそうなるかまでは明確に伝わりません。

さらに、モデルのバージョンが変わったり、アクセスできる環境が変わったりすると、同じ設定でも再現が難しくなることがあります。

そこで本記事では異なる切り口からLLMのふるまいをコントロールする術を紹介します。

数値で「プログラム可能な特性」として扱う方法です。

実験心理学の知見をもとに、定量的で再現しやすいエージェント制御の仕組みを設計し、社会シミュレーションの精度と信頼性を高めることを目指した取り組みとなっています。

ここから限定コンテンツ

## モデルのふるまいは曖昧な指示では本当に安定しないのか

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_1-1024x207.png]]

自然言語だけの指定はモデル間で解釈が分かれやすい一方、数値でバイアスを指定すると役割ごとの傾向が安定して表れることを示すイメージ

研究チームは、自然言語でエージェントを設定する方法に本当に問題があるのかを確かめるために実験を行いました。

### 人間らしい設定を2通りで検証

実験では、実際の研究でも使われてきた2つの設定方法を使いました。

1つ目は、年齢や職業、家族構成、趣味などを細かく書いて人物像を作るやり方です。エージェントに人間らしさを持たせる目的で、これまで多くの研究に採用されてきました。

2つ目は、教師やプログラマーのように役割を与える方法です。その役割に応じた知識や行動を期待するものです。

### 判断のゆらぎを測るために心理実験を活用

行動の違いを測るために、心理学の有名な課題である「アジア病問題」を使いました。同じ内容でも言い方が変わると判断が変わるという、フレーミング効果を測る実験です。

たとえば600人が危機にあるときに、「200人が助かる」と示す場合と「400人が死ぬ」と示す場合では、意味は同じでも判断が変わることがあります。

### モデルごとの違いを比較するためのテスト設計

実験では4つのLLMを使い、それぞれに同じ設定を与えて150回ずつテストを行いました。結果のばらつきを抑えるため、出力の温度は0.25に統一しています。

### 同じ設定でもモデルが変わるとふるまいが変わる

もっとも大きな発見は、同じエージェント設定でもモデルによって反応がまったく変わることでした。

たとえば、経済学者として設定されたエージェントが、あるモデルではポジティブな選択を強く好み、別のモデルでは中立的な判断を示すなど、傾向に大きな違いが見られました。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_3-1024x559.png]]

フレーミング効果に対する選好の分布（経済学者／一般／空欄、複数モデル）。同一の自然言語設定でもモデルごとに傾向が大きく異なる

### 専門知識の設定が効果的に働かない

専門性を持たせた設定でも、思ったようなふるまいが再現されるとは限りませんでした。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_4-1024x428.png]]

教師のロールを与えても、公平な判断の期待は安定して再現されない。自然言語の役割指定だけでは一貫した効果が出にくい

経済学者であれば、一般の人よりもフレーミング効果の影響を受けにくいと考えられますが、実験ではむしろ強く影響を受けるケースもありました。教師として設定した場合も、公平な判断が期待される立場でありながら、表現の仕方によって評価の内容が変わる傾向が見られました。

### モデルの解釈に委ねる設定では限界がある

自然言語によるエージェント設定には柔軟さがありますが、モデルがそれをどう解釈するかは制御できません。「権威に従いやすい」と書いたとしても、その程度や発現の条件はモデルごとに異なります。

人によって「経済学者らしさ」の捉え方が違うように、LLMもモデルごとに異なる解釈をすることがあります。同じ設定でも、一貫したふるまいが得られない原因となっていました。

信頼できるシミュレーションを行うには、こうした曖昧さを減らす仕組みが必要です。

## 数値でふるまいを整えると、モデルはもっと思いどおりになる？

上述のように、自然言語による設定には曖昧さがつきまといます。そこで研究チームは、より明確にエージェントを制御できる仕組みの設計に取り組みました。

### 再現しやすく、調整もしやすいことを目指す

目標は2つあります。どんなモデルを使っても似たふるまいを再現できること。そして、研究者の意図にあわせて行動を調整できることです。

#### モデルや条件が違っても結果が安定する

まず大切なのは、再現性です。同じ設定なら、使うモデルが違っても、似たような結果が出る状態を目指します。

たとえば、

-   モデルが違っても傾向が揃う
-   温度を0.2や0.8に変えても行動が大きくぶれない
-   回答方法が変わっても結果が安定する

といった点を重視しました。

#### 意図にあわせてふるまいを細かく調整できる

もう1つのポイントは制御のしやすさです。指示の強さに応じて、エージェントの反応もなめらかに変化するよう設計されています。

-   強めに設定すれば行動も大きく変わる
-   小さな調整では反応もゆるやかに変わる
-   バイアスを強める・弱める両方に対応できる

### 数値で「指定」と「調整」を両立させる

この目標をかなえるために、考案された手法には2つの仕組みが組み込まれています。

#### バイアスの強さを数値で指定する

1つ目は「認知バイアス指標（CBI）」です。エージェントの持つバイアスの強さを、数値で明示的に指定できます。

たとえば「権威に従いやすい」ではなく、「権威バイアス2.6ポイント」と書くような形です。数値は心理学の古典実験に基づいて決められます。

#### 数値どおりに動きを調整する

2つ目は「行動調整エンジン」です。指定された数値に従って、エージェントの出力がコントロールされます。

調整方法は3通りあります。

-   入力の段階で指示を入れる
-   モデルの中間処理に働きかける
-   モデル本体を軽くチューニングする

### なぜ古典実験を使うのか

古典的な心理実験は、長年にわたって繰り返し検証されてきました。人間の判断がどんな条件でどう揺れるかが、すでに数値で把握されています。古典実験を基準にすれば、LLMのふるまいも人間の傾向と照らし合わせて調整しやすくなります。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_2-1024x262.png]]

自然言語の説明とCBIの数値指定を組み合わせ、古典課題で測定し、行動調整エンジンが自動で目標値へ近づけるループを回す

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_5-1024x242.png]]

古典的な課題と行動パターンをテンプレート化し、回答確率を重み付きで集計してCBIを算出。算出した指標を目標にして行動調整エンジンで制御

### 曖昧な記述より、数値で伝える方が安定する

本手法の特徴は、あいまいな記述ではなく、数値を使ってモデルに指示する点にあります。

「少し権威に従いやすく」ではなく、「権威バイアス3.0ポイント」と伝えることで、研究者の意図がそのまま反映されやすくなります。

## モデルのふるまいをうまく整える3つの方法

モデルの出力を変えさせるための数値をどう反映させるかが重要です。研究チームは、モデルの処理過程に応じて3つの制御方法を使い分ける設計をとりました。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_6-1024x233.png]]

入力、内部表現、パラメータという三つの段階で制御。各方法はCBIを目標に設計され、単調性、滑らかさ、表現力の向上を狙う。

### モデルに働きかけられるポイント

LLMの出力は、大きく分けて「入力」「内部処理」「パラメータ」の3段階で形づくられます。それぞれの段階に合わせた制御方法があります。

#### 入力に数値を含めて指示する

最も手軽なのが、プロンプトで数値を直接伝える方法です。

たとえば「権威バイアスを75パーセントに設定する。0は全く従わない、100は完全に従う。5パーセント刻みで調整できる」といった具体的な記述にします。

この方法の強みは、どんなモデルでも使えることです。API経由でしか使えないモデルにも対応できます。

#### 内部処理の途中でバイアスをかける

2つ目は、モデルの中で行われる計算処理に直接手を入れる方法です。

出力が生成される途中に、「バイアス方向」と呼ばれる信号を加えます。たとえば、権威に従うような出力を強め、反対の出力は弱めるように調整します。

文脈に応じた細かい制御ができる一方で、モデルの内部にアクセスできる環境が必要です。

#### モデル自体を軽くチューニングする

3つ目は、パラメータを調整してバイアスを組み込む方法です。

バイアスあり・なしの出力を大量に用意し、それを使ってモデルを軽く再学習させます。その差分だけを元のモデルに適用し、特性を加えます。

一度設定すれば安定して動作しますが、再調整には再学習が必要になります。

### 3つの方法にはそれぞれ得意分野がある

#### プロンプト制御

-   すぐに使える
-   どのモデルでも動く
-   ただし、解釈にばらつきが出やすい

#### 内部状態制御

-   細かい調整がしやすい
-   文脈に合わせた出力が期待できる
-   モデル内部にアクセスできる必要がある

#### パラメータ制御

-   一度設定すれば安定する
-   長期の実験に向いている
-   再調整には手間がかかる

### どう使い分けるべきか

APIしか使えないモデルであれば、プロンプト制御が現実的です。細かいふるまいを調整したい場合は、内部状態に働きかける方法が適しています。同じ条件で多数の実験を繰り返すなら、パラメータ制御が安定します。

環境や目的に応じて方法を選べば、モデルのふるまいをより思いどおりに整えられます。

## 狙いどおりに動くか検証

研究チームは、実際の動作で効果を検証しました。

### 評価の基準はどう決めたか

性能は次の3つの視点で評価しました。

-   単調性（設定を強めるほど、ふるまいもその方向に変わるか）
-   滑らかさ（少しだけ設定を変えたとき、出力が急に飛ばないか）
-   表現力（弱くも強くも、広い範囲で調整が効くか）

### 効果は他の実験でも再現されるか

1つの課題で効いた設定が、別の課題でも効くかを確認しました。

たとえば、投資や保険に関する判断でフレーミング効果を強く示したエージェントが、病気対策を扱う課題でも同じように反応するかをテストしました。

結果は良好でした。調整を強くすれば、そのぶん一貫してバイアスも強まる傾向が見られました。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_7-1024x524.png]]

投資の課題で設定したCBIとアジア病の課題で測定したCBIの関係を示す。線形加算と射影のいずれでも単調な対応が確認できる

### モデルや条件が変わっても安定するか

同じ設定を複数のLLMに適用して比較しました。

以前の自然言語設定では、モデルが変わると出力に大きなばらつきが出ていましたが、新しい手法ではふるまいがよく揃うようになりました。

たとえば「権威バイアス2.5」と指定したとき、異なるモデルでも近い従順度が得られました。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_12-1024x474.png]]

複数のバイアスと課題で、異なるモデルに同じ制御を適用した場合のCBIを比較。全体として単調な制御曲線が得られ、モデル間の挙動が近づく様子が確認できる

#### 温度や推論方法を変えても崩れないか

温度（出力のランダム性）を0.1から1.0まで動かしても、基本的な傾向は安定していました。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_9-1024x563.png]]

温度を0.1から1.0まで変えても、制御係数に対するCBIの動きは安定して単調に変化。RepE Projectionはしきい値付近で急に立ち上がり、数値プロンプトは緩やかに増加

また、直接回答と段階的推論のどちらを使っても、同じ設定なら同じような反応が再現されました。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_10-1024x512.png]]

直接回答と段階的推論のいずれでも、課題間のCBIの対応関係が保たれる。推論様式が変わっても制御の方向づけがずれにくいことを示す

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_11-1024x513.png]]

各制御方法で、制御係数に対するCBIの曲線が直接回答と段階的推論で近い形にそろう。細部の差はあるものの、全体の傾向は一貫

### どの制御方法が使いやすいか

#### 三つの方法を比べてみた

それぞれの制御方法には得意な領域があります。

-   内部状態制御は調整の反応が素直で、滑らかさも高く、細かい制御に向いています。
-   パラメータ制御は一度設定すると動きが安定し、長期的な利用に向いています。
-   プロンプト制御は調整幅が広く、ふるまいを大きく変えやすいですが、細かなコントロールはやや苦手です。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_8-1024x733.png]]

バンドワゴン、確証、フレーミング、権威の各バイアスで目標との差を比較。いずれも無制御より差が小さくなる傾向があり、状況によって適した制御が変わる

#### APIモデルでも効果は出る

内部にアクセスできないモデルでも、プロンプト制御だけで十分にふるまいを整えられました。

実験では、クローズドなLLMでも期待どおりのバイアスレベルを再現できており、多くの実務用途で使えることが確かめられました。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_13-1024x618.png]]

APIのみで使うモデル群でも、数値プロンプトの制御係数を上げるとCBIが単調に増加。モデルが異なっても大枠の傾向はそろう

### 現場で使えるツールとしての実力は？

検証を通じて、今回の手法が以下を満たすことが確認されました。

-   モデルが変わっても、似たふるまいになる
-   条件（温度や推論方法）が変わっても崩れにくい
-   意図にあわせた細かな調整が効く
-   幅広いモデルで応用できる

従来の曖昧な設定に比べて、信頼性とコントロール性が大きく向上していることが、数値ではっきりと示されました。

## 実社会の現象を再現できるか検証

研究チームは、SNSによる「感情の伝染」現象を使って、新しい手法の実用性を試しました。

### 感情の伝染とはどんな現象か

感情の伝染とは、他人の投稿を見た影響で、自分の気分も同じ方向に引っ張られる現象です。ネガティブな投稿を多く見ると、自分の投稿もネガティブになりやすくなる。そうした傾向が、2014年のFacebook実験でも確認されています。

また、「周囲に合わせてしまう」心理としてバンドワゴン効果が知られています。バンドワゴンが強い人ほど、感情の伝染も起きやすいと考えられます。

この関係を利用して、新しい手法が社会現象の再現に役立つかを確かめました。

### 実験はどう設計されたか

#### 数値でバンドワゴン効果を設定

強さの異なる5種類のエージェントを用意しました。バンドワゴン効果を、2.55〜3.13の範囲で段階的に設定しています。

比較のため、自然言語でも同じように「なし」から「強い」までの5段階で設定し、効果を比べました。

#### ネガティブ投稿を段階的に見せる

各エージェントに対して、ネガティブなSNS投稿を0〜15件まで段階的に見せました。投稿内容はLLMで生成し、1,000通りのバリエーションを使ってリアルなSNS環境を再現しています。

そのあと、エージェント自身に投稿を作らせ、どれだけネガティブな傾向があるかを数値で測りました。

### 自然言語の設定では差が出にくい

自然言語で指定した5段階のバンドワゴン効果は、感情の伝染パターンにほとんど違いを生みませんでした。

グラフ上では線が重なってしまい、「強い」設定と「弱い」設定を区別できない結果となりました。モデルが曖昧な指示を安定して解釈できていないことが分かります。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_14-1024x734.png]]

自然言語で“なし〜強い”の五段階を指定しても、ネガティブ投稿の増加にともなう曲線の差は小さく、強弱の区別がつきにくくなる

### 数値で指定すれば反応は明確に変わる

#### 数値が高いほど影響も強くなる

一方、数値で設定したエージェントでは、ネガティブ投稿への反応に明確な差が出ました。

バンドワゴン効果が弱いエージェントは影響を受けにくく、強いエージェントははっきりと反応を強める。5本のグラフの線も段階的に分かれ、意図した強さの順に整然と並びました。

![[再現性のある人間行動シミュレーションへ LLMのふるまいを数値で制御する - AIDB/AIDB_AIDB_95343_15-1024x718.png]]

CBIで段階設定したエージェントは、ネガティブ投稿数に対して用量反応的に変化し、曲線がきれいに分離。数値指定が感情の伝染に与える強さを明確に示す

#### 社会心理学の理論とも一致

この傾向は、実際の社会心理学の知見とも合致します。周囲に同調しやすい人ほど感情の伝染を受けやすいという理論が、LLMエージェントでも再現されました。

### 回答だけでなく、認知の傾きも制御できている

#### 古典実験で調整した特性が、別の現象でも効いた

バンドワゴン効果は、過去の心理実験（アッシュの同調実験やホテルのタオル実験など）を基に調整されたものです。

今回の検証では、その数値設定がSNSのような現代的な場面でも効果を発揮しました。表面的な出力ではなく、根本的な認知傾向そのものがコントロールできていたと考えられます。

### 社会のふるまいを探るツールとしても使えそう

この実験は、新しい手法がただの研究用ツールではなく、実社会のふるまいを分析する手がかりにもなりうることを示しました。

たとえば、SNSマーケティングの設計や、政策決定の支援など。モデルのふるまいを信頼して使える場面が広がっていきそうです。

曖昧な指示に頼らず、数値でしっかり調整することで、より確かなシミュレーションが可能になると示されました。

## どこまで役立つ？何に気をつけるべき？

「親しみやすく話して」と言っても、モデルによって解釈はまちまちです。今回の手法は、こうした曖昧な指示を数値に変換し、動作として安定させる道筋を示しました。

将来的には、研究者が自然言語で入力するだけで、内部では数値による細かな制御が行われるようになるかもしれません。

たとえば「少し保守的で、権威を重んじる人」と書くだけで、「保守性3.2」「権威バイアス2.8」といった設定に自動で変換されるイメージです。

これまでバイアスは「修正すべき問題」と見なされがちでしたが、人との対話では、適度なバイアスがある方が自然に感じられる場面もあります。むしろ、完全に合理的なふるまいは冷たく見えることもあります。

バイアスを細かく調整できれば、相手の文化や期待に合わせて、適度な同調や敬意を示すことも可能になります。LLMのふるまいが、より人に寄り添ったものに近づきます。

### リスクへの備えも必要

一方で、バイアスを操作して意見を誘導したり、偏見を強めたりする悪用の可能性もあります。たとえばマーケティングや政治の現場で、不適切に使われると、人々の判断に大きな影響を与えるかもしれません。

こうしたリスクに備えるには、以下のような対策が必要です。

-   エージェントのバイアス設定を明示する
-   不適切な使い方を検出する監査の仕組みを整える
-   研究倫理や社会的責任に関するルールを設ける

逆に、この技術を使えば、有害なバイアスの仕組みを安全に再現し、その抑え方を検証できます。差別や偏見の抑制に役立つ知見が得られるかもしれません。

### 社会や実務への応用も広がる

こうした手法を使えば、より精密で再現性の高い実験が可能になります。現実では倫理的に扱いづらいテーマも、安全な環境で検証できます。

教育では、学習者の性格や傾向に応じて支援のレベルを調整するチューター設計に役立つ可能性があります。政策では、価値観の異なる人々の反応を事前にシミュレーションして、政策の設計に活かせるかもしれません。

可能性を伸ばすだけでなく、影響をしっかり見極めながら、バランスの取れた活用を進めていくことが求められます。

## まとめ

自然言語だけでエージェントを設定すると、モデルごとにふるまいがばらつきやすいことが分かっています。それに対して、バイアスを数値で指定し、3つの方法で調整することで、再現性や操作のしやすさが大きく改善されました。

ただし、使い方によっては意図しない誘導や偏見の強化につながるおそれもあるため、透明性や監査の仕組みを備えて使うことが求められます。  
実務で試すなら、まずはプロンプト制御から小さく始めて、目的に応じて段階的に調整していくのがよさそうです。

**参照文献情報**

-   タイトル：Programmable Cognitive Bias in Social Agents
-   URL：[https://doi.org/10.48550/arXiv.2509.13588](https://doi.org/10.48550/arXiv.2509.13588)
-   著者：Xuan Liu, Haoyang Shang, Haojian Jin
-   所属：University of California, San Diego
