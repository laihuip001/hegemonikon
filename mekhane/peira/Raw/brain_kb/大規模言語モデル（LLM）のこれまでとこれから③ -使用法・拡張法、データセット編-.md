---
created: 2026-01-01T09:36:56 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/64398
author: AIDB Research
---

# 大規模言語モデル（LLM）のこれまでとこれから③ -使用法・拡張法、データセット編- - AIDB

> ## Excerpt
> 本記事では、LLM研究全体の背景と現状、そして将来展望を網羅的に整理する調査論文をもとに、LLMの基礎を振り返ります。初回は代表的なモデルについて深掘りし、前回はモデルの構築法について深掘りしました。 初回の記事：大規模言語モデル（LLM）のこれまでとこれから①　-代表的なモデル編- 前回の記事：大規模言語モデル（LLM）のこれまでとこれから②　-モデル構築編- 今回は、モデルの使用法・拡張法、そ…

---
本記事では、LLM研究全体の背景と現状、そして将来展望を網羅的に整理する調査論文をもとに、LLMの基礎を振り返ります。[初回](https://ai-data-base.com/archives/64232)は代表的なモデルについて深掘りし、[前回](https://ai-data-base.com/archives/64331)はモデルの構築法について深掘りしました。

初回の記事：[大規模言語モデル（LLM）のこれまでとこれから①　-代表的なモデル編-](https://ai-data-base.com/archives/64232)

前回の記事：[大規模言語モデル（LLM）のこれまでとこれから②　-モデル構築編-](https://ai-data-base.com/archives/64331)

今回は、モデルの使用法・拡張法、そして主なデータセットについて深掘りします。

![[大規模言語モデル（LLM）のこれまでとこれから③ -使用法・拡張法、データセット編- - AIDB/AIDB_64398_thum-1024x576.jpg]]

**参照論文情報**

-   タイトル：Large Language Models: A Survey
-   著者：Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao
-   所属：論文には所属機関が示されていないため各機関から有志の研究グループが結成されたことが推測されます。
-   URL：[https://doi.org/10.48550/arXiv.2402.06196](https://doi.org/10.48550/arXiv.2402.06196)

**LLM関連のサーベイ論文事例：**

-   [ナレッジグラフ（知識グラフ）とLLMを掛け合わせる方法のロードマップ](https://ai-data-base.com/archives/63808)
-   [マルチモーダルLLMの技術やトレンド、26種類のモデル例を網羅的にまとめた報告](https://ai-data-base.com/archives/63257)
-   [LLMの知識を狙い撃ちして変更・修正する「知識編集（Knowledge Editing）」](https://ai-data-base.com/archives/61831)
-   [LLMにおける情報抽出（文章から必要な事柄を読み取る）タスクについての調査](https://ai-data-base.com/archives/61703)
-   [LLMのRAG（外部知識検索による強化）をまとめた調査報告](https://ai-data-base.com/archives/61367)

## 前回のおさらい

前回は、以下の項目に沿ってLLMの構築方法について触れました。

-   主流となっているLLMの構造
-   データクリーニング
-   トークン化
-   位置エンコーディング
-   モデルの事前学習
-   ファインチューニングと指示チューニング
-   アライメント
-   デコーディングの戦略
-   コスト効率に優れた開発と運用

本記事では、モデルの使用方法や拡張（強化）方法や主なデータセットにフォーカスします。

LLMは基本的なプロンプト(指示)だけでも使用可能ですが、その能力を最大限に活用したり、欠点を補うためには、さらなる工夫が必要になります。

今回はまず、

ここから限定コンテンツ

LLMの主な欠点に触れます。続いて、プロンプトや拡張手法で幻覚に対処すること、そしてLLMを外部世界とやり取りできる本格的なAIエージェントに進化させる手段を紹介します。また、LLMの研究開発によく使用される主なデータセットを紹介します。

下記は本記事の中でLLMの使用方法と拡張方法に関わる部分の概要です。

![[大規模言語モデル（LLM）のこれまでとこれから③ -使用法・拡張法、データセット編- - AIDB/AIDB_64398_1-1024x763.jpg]]

まず、LLMはあくまで次の単語を予測するように訓練されていることを念頭に置く必要があります。ファインチューニングやアライメント（調整）により性能は向上しますが、それでも以下のような限界があります。

-   LLMは通常、過去のプロンプト内容を記憶できません。状態管理を必要とするユースケースではこれが制約になります。
-   同一のプロンプトでも毎回異なる応答を提供します。 制限するパラメータ（temperatureなど）はありますが、完全に応答を固定することはほとんどできません。
-   LLMは現在時刻や日付を認識せず、またトレーニングデータに含まれていない情報には基本的にアクセスできません。
-   トレーンングと利用に高価なGPUを多数必要とします。大規模なモデルではマシンスペック不足が処理の遅延を引き起こすことがあります。
-   LLMに「真実」の概念はなく、良い情報も悪い情報も混在したデータで訓練されるため、「それっぽいが事実に反する」応答をすることがあります。幻覚（ハルシネーション）と呼ばれています。

上記の欠点の中でも幻覚については最近大きな関心を集め、軽減を目的としたさまざまな拡張手法の開発につながっています。

### 幻覚（ハルシネーション）について

LLMの「幻覚」とは、元の情報に反していたり、意味不明な出力を生成することを指します。大きく２種類あります。

1.  元の情報と直接矛盾し、事実誤認や論理的破綻を含む
2.  元の情報との矛盾はないものの、検証が不可能な推論や憶測の要素を含む

LLMにおける「元の情報」の定義はタスクによって異なります。例えば対話系タスクでは「世界知識」、要約タスクでは要約対象のテキストが「元の情報」に該当します。

そして幻覚が与える影響は状況に大きく依存します。例えば詩などクリエイティブな文脈では幻覚はある程度許容されたり、有益とさえみなされるケースもあります。要するに元の情報に対して正確であることのみを重視すると、創造的な内容にならないという考え方です。

LLMはインターネット・書籍・Wikipediaなど多様なデータで訓練され、あくまで確率モデルに基づいてテキストを生成するため、「真実か否か」の本質的理解はありません。最近は事実性に沿った出力を促す試みはされていますが、確率的生成という基本が覆らない限りは幻覚はなくならないと言われています。

LLMでの幻覚の自動検出では、統計ベースとモデルベースの指標の併用が効果的です。

**統計ベースの指標**

-   ROUGE, BLEUなどのテキスト類似度評価指標は、本質的な幻覚（直接矛盾する内容）の検出に用いられます。
-   PARENT, PARENT-T, Knowledge F1などの高度な指標は構造化された知識ソースとの比較が可能です。

**モデルベースの指標**

-   情報抽出モデルによる知識の単純化と比較
-   質問応答システムを通じた元の情報との整合性評価
-   自然言語推論による真偽の判定
-   タスクに応じたデータセットによる詳細評価

なお、上記の指標も万能ではありません。

**幻覚の評価と対策**

幻覚の評価は、最終的には人間の判断が重要です。一般的な手法は以下の2つです。

1.  **スコアリング:** 定義済みの尺度で幻覚のレベルを人間の評価者が判定します。
2.  **比較分析:** 生成された内容を基準や正解データと比較することで、主観的な評価を加えます。

人間とモデル両方での評価に利用可能な指標の例がFactScoreです。FactScoreはLLMの出力を「基本的な事実」に分割し、各事実が元情報に裏付けられているかを2値で判定し、合計してスコア化します。

LLMの幻覚を軽減するためには用途に応じた対策が必要です。代表的なアプローチは次の通りです。

-   製品設計を工夫し、ユーザーフィードバックを取り入れる仕組みを整える
-   幻覚の事例を蓄積し分析することで継続的な改善に繋げる
-   検索結果との統合など、高度なプロンプト手法を使用する
-   出力のランダム性を抑える
-   RLHF（人間からのフィードバックによる強化学習） や分野特化型のファインチューニングを施す

## LLMの活用

モデルの出力を導くためのユーザーからのテキスト入力をプロンプトと呼びます。プロンプトの内容は、簡単な質問から詳細な説明、具体的なタスク指示まで多岐にわたります。一般的には下記で構成されます。

-   指示
-   質問
-   入力データ
-   例　など

モデルから望み通りの応答を得るには、指示または質問のどちらかが必須で、その他の要素は任意です。高度なプロンプトとしては、論理的思考プロセスに従うようモデルを導く「連鎖的思考」などの複雑な構造があります。

LLM（など生成AI）とのやり取りや出力を形作るためにプロンプトを工夫するテクニックは急速に発展している技術分野であり、プロンプト・エンジニアリングと呼ばれています。その本質は、ある目的を達成するために最適なプロンプトを作成することです。単にモデルへの指示だけでなく、モデルの能力と限界、それが動作する環境についての理解も伴います。ひとつのケースで通用した手法が、別のケースでも全く同じように通用するとは限りません。

さらに、プロンプト・エンジニアリングは、モデルの評価やハイパーパラメータのチューニングなどの従来の機械学習の手法同様、繰り返しと試行を伴うプロセスです。バージョン管理や回帰テストなどの従来のエンジニアリング手法は、他の機械学習アプローチに適合されたように、この新しい方法にも適合させる必要があります。

### 高度なプロンプト手法

LLM の出力をより高度に制御するプロンプト手法がいくつか開発されています。

**連鎖的思考 (CoT : Chain of Thought)**

LLMは単語予測が得意ではあっても、論理的思考向けに設計されてはいません。そこで思考手順をモデルに示すことで解決を図るのがCoTです。例えば「まず〇〇について考え、次に××について考えよう」「ステップバイステップで取り組もう」などと指示することで、情報検索やパターン認識だけでなく論理的な出力を導きます。CoT には「ゼロショット」と「フューショット」の2種類があります。前者が一般的ですが、後者のように例を交えて教える方が効果的です。ただし、手動で例を作成するのは大変な作業です。「まず〇〇について考え、次に××について考えよう」の〇〇や××の部分を人間が考えてあげるということです。

**思考の木 (ToT : Tree of Thought)**

複数の解決策や思考プロセスを検討してから最もそれらしいものを選ぶという思考法に基づく手法です。LLMに各枝となる「思考の木」に沿って異なる推理を試させます。なお、推論経路の評価が重要です。LMが生成した思考の分岐のそれぞれについて妥当性や質問との関連性を検証し、最も整合性が高く論理的な結論を選びます。単一の推論では不十分な複雑な問題解決に役立ちます。

**自己整合性 (Self-Consistency)**

これはLLMに対して同じ質問への応答を複数生成させ、応答の一貫性を精度と信頼性の指標とする手法です。同じプロンプトに対して複数回、類似した応答をするほど、それが正確である可能性が高くなります。整合性（応答同士の類似性）は、内容の重複、意味的類似度の比較、高度な分析手法などを組み合わせて測定します。情報が正確であることが重要な分野でよく利用されます。

**内省 (Reflection)**

LLMに自身の出力について推論させ、修正を加えられるように促す手法です。最初に解答を作成した後、その内容が事実的に正確で論理的に一貫しているかなどをモデル自身に考えさせます。

**専門家によるプロンプト (Expert Prompting)**

LLM に特定分野の専門家の回答を模倣させる手法です。Role-play promptingの一種でもあり、専門知識に基づいた高品質な解答が可能になります。より効果的なアプローチとして、複数の専門家の視点からの回答を同時に検討させ、それらを組み合わせて包括的な回答を作成する方法もあります。

**連鎖 (Chains)**

複雑なタスクを処理するため、LLMを使って処理の流れを段階的に構築する手法です。異なる構成要素を接続したワークフローのように、前の処理の結果が次の処理の入力になるような流れを作成します。各段階は特定の機能を持ち、複雑で微妙な処理が可能になります。Chain の設計ツールもあります。

**Rails (制約)**

ルールやテンプレートによって出力をコントロールする手法です。応答が基準を満たすようにすることで、関連性・安全性・正確性を向上させます。Railsに沿わせるためのガイドラインは通常モデリング言語などを使って定義されます。目的は用途によって様々で、たとえば、特定のトピックに留まらせたり、誤った情報の生成を最小限に抑えるといった使い方があります。

**自動プロンプト生成 (APE : Automatic Prompt Engineering)**

プロンプト作成を自動化する手法です。モデル自身にプロンプトを生成させたり、評価の自動化によって効率的に良いプロンプトを設計します。 の手順は次の通りです。

1.  タスクに応じてモデルに複数のプロンプト候補を考えさせます。
2.  生成されたプロンプトを、明瞭さや適切さなどの基準で評価します。
3.  評価結果を元にプロンプトを改善し、さらに良いものを目指します。

## 外部知識を使ったLLMの拡張（RAG）

LLMは常に最新の知識を持っているわけではなく、通常は外部の情報にアクセスできません。それを解決するのがRAG（Retrieval Augmented Generation）です。

RAGは入力プロンプトから質問部分を抽出し、検索エンジンなど外部の知識源から関連情報を取得します。取得した知識は元のプロンプトに加えられ、その後、モデルから最終的な応答が生成されます。RAGシステムには、「検索」「生成」「拡張」の3つの重要な構成要素があります。

![[大規模言語モデル（LLM）のこれまでとこれから③ -使用法・拡張法、データセット編- - AIDB/AIDB_64398_2-1024x546.jpg]]

![[大規模言語モデル（LLM）のこれまでとこれから③ -使用法・拡張法、データセット編- - AIDB/AIDB_64398_3-1024x543.jpg]]

### RAGのためのプロンプト

RAGを意識したプロンプト設計手法も開発されています。そのひとつがFLARE（Forward-looking Active Retrieval Augmented Generation )で、モデルによる文章生成と情報検索を繰り返し組み合わせることでLLMの能力を強化する手法です。従来型のRAGは通常、検索を1回のみ行いますが、FLAREでは動的に継続することで応答の正確性と関連性を向上させます。

流れを時系列で追うと以下のようになります。

1.  生成された各文章について確信度を評価する
2.  一定のしきい値を下回る場合、その文章を検索クエリとして関連情報を取得する
3.  再生成したり文章を洗練させる
4.  応答の各部分が常に最新の利用可能な情報によって補強されます。

## 外部ツールの活用

LLMの機能を拡張するには、RAGのように外部の知識源を使用する以外にも方法があります。たとえばAPIを通じてさまざまなサービスと連携させることで、単純な情報検索から複雑なデータベースの操作までできることが広がります。

論文「[Toolformer: Language Models Can Teach Themselvesto Use Tools (ツールを用いることを自ら学習できる言語モデル)](https://arxiv.org/abs/2302.04761)」では、モデル自身にどのツールをいつ、どのようなパラメータで使用するかを判断させる手法が示されています。また、より最近では、Berkeleyの研究者が、API利用能力でGPT-4を凌ぐ[Gorilla](https://arxiv.org/abs/2305.15334)というモデルを発表しました。

### ツール利用のためのプロンプト技法

RAGと同様に、外部ツール利用を円滑に行うためのプロンプト手法も開発されています。そのひとつがART(Automatic Multi-step Reasoning and Tool-use)です。連鎖的思考による推論とツール利用を自動的に組み合わせる手法で、複雑なタスクに対しモデルによる分析結果を元に適切なツールを呼び出して処理させるよう促します。最初にライブラリから類似タスクを探し出してプロンプトに例として追加することで、モデルに実行手順を教えることができます。

## LLMエージェント

AI分野においてエージェントとは、周囲の環境を認識し、状況を判断し、利用可能なアクションを選んで実行する自律的なシステムを指します。

そしてLLMエージェントは、（ツールなどを組み合わせた）LLM をベースに、特定のタスクを自発的に行うよう設計したものです。ユーザーによる指示や対話から目的などを推測し、判断を下します。

以下はLLMエージェントの研究事例で、エージェントベースでツールの使用を判断するHuggingGPTというシステムです。

![[大規模言語モデル（LLM）のこれまでとこれから③ -使用法・拡張法、データセット編- - AIDB/AIDB_64398_4-1024x486.jpg]]

一般的なLLMエージェントの機能は次の通りです。

-   ツールへのアクセスと利用
-   意思決定

たとえば、天気APIにアクセスできるLLMなら、特定の場所の天気に関する質問に答えることができます。他にもショッピング用のAPIを使うことができるなら、情報取得だけでなく購入処理を行うエージェントを構築することも理論的には可能です。

下記は、会話を通じた情報取得を担うLLMエージェントの例です。

![[大規模言語モデル（LLM）のこれまでとこれから③ -使用法・拡張法、データセット編- - AIDB/AIDB_64398_5-1024x699.jpg]]

この事例では、以下の機能が備えられています。

-   作業メモリ：対話の状態を保持する
-   ポリシー：タスク実行計画の立案とアクションの選択を行う
-   アクション実行：ポリシーで選ばれたアクション（外部知識の統合、応答生成指示など）を実行する
-   ユーティリティ: モデルの応答が妥当か評価を行い、エージェントの性能向上にフィードバックする

またRAGや外部ツールと同様に、LLMエージェントのニーズに特化したプロンプト手法が開発されています。以下が事例です。

**ReWOO (Reasoning without Observation)**

外部データに即座に頼らず、推論計画（メタプラン）を事前に生成する手法。エージェントがあらかじめ解決するための手順を考え、データが取得できればそれをもとに実行に移します。データへのアクセスが困難な場合に役立ちます。

**ReAct (Reason and Act)**

言葉による推論だけでなく、実行可能な手順も生成させ、問題解決能力を強化する手法です。理由の説明とアクションを交互に挟むことで、状況に対応しつつ問題の解決を進めていきます。

**DERA (Dialog-Enabled Resolving Agents)**

対話を通じて疑問を解決し、意思決定を行うエージェントシステムです。情報収集・分析を行う「研究者」と、それを元に最終判断をする「決定者」という複数のエージェントで構成され、役割分担することで効率よく問題を解決します。医療診断や顧客サービスなど、複雑な意思決定を伴うタスクに適しています。

## LLM用のデータセット

![[大規模言語モデル（LLM）のこれまでとこれから③ -使用法・拡張法、データセット編- - AIDB/AIDB_64398_6-988x1024.jpg]]

LLMの特定のタスク能力評価にはデータセットが使用されています。初期は翻訳・要約・質問応答などの一般的なNLPタスクが探求されていましたが、コード生成や金融など幅広い分野で進展があります。さらに最近では公平性・バイアス、事実確認、推論力なども問われます。

下記で、よく使われるベンチマークを紹介します。

![[大規模言語モデル（LLM）のこれまでとこれから③ -使用法・拡張法、データセット編- - AIDB/AIDB_64487_1-1-1024x1002.png]]

### 基本的な能力を評価するベンチマーク

**Natural Questions**

Google検索での実際の質問を集めたQAデータセットです。Wikipedia上のページと質問内容から、正解となる段落や短い答え（実体）などを特定しています。

**MMLU (Massive Multitask Language Understanding)**

理系から文系まで57科目について、ゼロショット・フューショット学習での知識量を評価します。一般的な知識と問題解決能力の両方を測れます。難易度は初級からプロ向けまでと幅広いのが特徴です。GPT-4やGeminiなど最先端モデルの専門的能力を測る上でよく注目されます。

**MBPP (Mostly Basic Python Problems)**

コード生成向けベンチマークです。基本プログラミングや標準ライブラリの利用など、幅広い974問のPythonプログラムで構成されています。課題の説明、コード解答、自動テストケースなどでモデルの性能を評価します。

### 新たな能力を測るベンチマーク

LLMが新たに出現させた能力を評価するものです。

**GSM8K**

7500問の学習用データと1000問のテスト用データに分けられた、小学校レベルの算数の文章問題集です。 2～8ステップで解けるように設計されており、基本的な四則演算を組み合わせる必要があります。LLMの複数ステップに渡る数学的推論を評価します。

**MATH**

高校レベルの数学で使われるような12500問の問題集です。問題ごとに段階を踏んだ解き方と最終的な答えが示されています。複数の分野の問題があり、難易度は5段階で用意されています。

**HellaSwag**

7万問の常識に関する多肢選択問題です。ActivityNetとWikiHowという分野から出題されます。ある状況が起きた際に、次に何が起こりそうか4つの選択肢から回答させ、LLMの常識力を調べます。

なお以下も常識推論のデータセットです。

**AI2 Reasoning Challenge (ARC)**

英語の問題で構成された7787問の理科テスト問題集です。 難易度の高いチャレンジセット(2590問) と簡単なイージーセット(5197問) に分けられています。

**PIQA**

日常生活の中で役立つ物理常識に関するデータセットです。質問に対し2つの選択肢があり、より正しい常識的な解決法を選ぶ能力が問われます。

**SIQA**

3万8千問の多肢選択問題で、日常の社会的な状況での常識を問うデータセットです。感情や社会的な知能についての回答も含まれ、 ソーシャルスキルの評価に使えます。

**OpenBookQA (OBQA)**

単なる読解力だけでなく、常識も必要とされる新しい質問応答データセットです。約6000問の多肢選択問題があり、答えの根拠となる重要な事実の他に6000以上の関連情報が提示されます。質問はクラウドソーシングで作成されており、専門家により厳選されています。背景知識が限られた中で複数段階の推論を必要とするため難易度が高いのが特徴です。

**TruthfulQA**

全部で817問からなる、真実性の高い回答を出せるかを評価するためのデータセットです。健康、法律、金融、政治など38分野のトピックがあり、よくある誤解に基づく不正確な答えに繋がりやすいよう注意して作られています。

**OPT-IML Bench**

LLMに与える指示（インストラクション）を柔軟に解釈・実行する能力を測定するベンチマークです。既存ベンチマークから選んだ2000種類の自然言語処理タスクが用意されています。1790万の学習データ、14万5千の開発用データ、そして32万1千のテスト用データで構成されます。

### 外部ツールを活用する能力を評価するためのベンチマーク

LLMが外部知識やツールを使って機能を拡張する能力を測定するベンチマークもあります。

**HotpotQA**

Wikipedia上の2つの記事（とそこから引用された段落）と質問文から成る11万3千問のデータセットです。回答するには記事の内容を複数段階に渡って推論する必要があります。 どの文章が回答の根拠となっているかを人間が示しているのも特徴です。

**ToolQA**

外部のツールを活用するLLMの能力を評価する質問応答ベンチマークです。具体的なツールを使うことでどのような質問文に答えられるかを調べます。

**GPT4Tools**

ツールの使い方を解説する教師用データセットです。画像、ツールの説明などの情報を元に「教師役」（例：ChatGPTなど、より洗練されたAI）がツールの使い方を説明することで作成されています。学習用（7万1千項目）、検証用、そして学習データに使われなかったツールも含むテスト用と、段階を踏んだ３種類のデータセットが存在します。

なお、さまざまなベンチマークはどのようなライセンスで公開されているのかを調べた結果のグラフが下記です。Apache-2.0は商用利用可能なライセンスで、これが最も多くなっています。

![[大規模言語モデル（LLM）のこれまでとこれから③ -使用法・拡張法、データセット編- - AIDB/AIDB_64398_7-988x1024.jpg]]

## まとめ

本記事は、LLM研究全体の背景と現状、そして将来展望を網羅的に整理する調査論文をもとに、「LLMの使用法・拡張法、主なベンチマーク」にフォーカスして紹介しました。

広く知られている手法やツールを中心に網羅的にまとまっているかと思います。プロンプト手法は特に今回取り上げたもの以外も非常に多く研究事例が上がっているので、詳しく知りたい方は調べてみてください。

次回は主要なベンチマーク別のLLMの能力比較と、将来課題についてまとめます。本調査論文のシリーズ記事も、いよいよ終盤です。
