---
created: 2026-01-01T11:15:36 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/99123
author: AIDB Research
---

# 学習なしでLLMを強くするための「文脈を育てる」という発想 - AIDB

> ## Excerpt
> 本記事では、LLMの性能を、追加の学習なしで向上させる新しいアプローチとして「文脈を育てる」という考え方を紹介します。 最近では、モデルの性能を高めるためには追加学習や微調整が定番となっていますが、これには多くの時間やコストがかかります。そこで、失敗した回答の例を集め、それらに共通するパターンを整理しておくというRAGの手法が提案されています。 本記事の関連研究 背景 仕事でLLMを使っていると、…

---
本記事では、LLMの性能を、追加の学習なしで向上させる新しいアプローチとして「文脈を育てる」という考え方を紹介します。

最近では、モデルの性能を高めるためには追加学習や微調整が定番となっていますが、これには多くの時間やコストがかかります。そこで、失敗した回答の例を集め、それらに共通するパターンを整理しておくというRAGの手法が提案されています。

![[学習なしでLLMを強くするための「文脈を育てる」という発想 - AIDB/AIDB_99123-1024x576.png]]

**本記事の関連研究**

-   [LLMを新しいタスクに順応させる「文脈内学習」における効率的なコンテキストの作り方](https://ai-data-base.com/archives/96070)
-   [文脈内学習は「少数事例からの単純な学習だけでなく、言語モデルが持つ幅広い適応能力」](https://ai-data-base.com/archives/80765)
-   [LLMにプロンプトのみで仮想的な強化学習を発生させる方法](https://ai-data-base.com/archives/91141)

## 背景

仕事でLLMを使っていると、同じモデルでも「少し指示を変えるだけで」結果が大きく変わることがあります。うまくいった理由はまだ振り返りやすいですが、うまくいかなかった原因は見過ごされがちです。

そもそも、LLMを業務に合わせて強化する方法には大きく分けて二つあります。一つはモデルそのものを調整する追加学習の方法で、「教師あり微調整」と呼ばれています。これは効果が出やすい反面、モデルの重みに手を加える必要があるため計算コストが高く、もともと備わっていた知識や得意分野が失われるリスクもあります。

もう一つの方法は、モデル自体を変更せずに、入力文の工夫によって出力をコントロールするやり方です。これは「文脈内学習」と呼ばれ、プロンプトに例やルールを書き込むことで、その場でやり方を覚えたように動かすものです。ただしこの方法には、プロンプト設計に大きく左右されやすく、幅広く効果を出すのが難しいという課題があります。

最近では文脈そのものを自動的に生成・更新する研究が活発になっています。しかし、既存のアプローチには二つの課題があります。一つは、個々の失敗例に引っ張られてノイズが混じりやすいこと。もう一つは、改善案を追加する際に「本当に良くなったか」を確かめずに情報を積み上げてしまい、結局、役に立たないどころか逆効果の内容が蓄積してしまうという点です。

そこで本記事では、「文脈は完成されたものではなく、少しずつ育てていくもの」と捉えた手法を取り上げます。単に成功例を増やすのではなく、失敗から共通点を見つけてメモにし、それを慎重に選んで文脈に残す。以下で詳しく見ていきましょう。

ここから限定コンテンツ

### **忙しい人向けに、重要なポイント5選**

1.  Mistake Notebook Learning は、追加学習をせずに、失敗パターンを抽象化して蓄える「失敗ノート」を使って文脈を更新していく枠組み
2.  失敗を一件ずつ扱うのではなく、関連する失敗をバッチでまとめて分析し、ノイズを減らしつつ汎用的な指針に落とし込む
3.  ノートは「正しい例」「正しい進め方」「ミスの要約」「再利用できる戦略」に加えて「やってはいけない条件（アンチパターン）」まで含む形で整理
4.  文脈の更新は無条件に採用せず、同じバッチ上で改善が確認できたときだけ反映するため、悪化を避ける品質ゲートになる
5.  実行時は質問に近いノートを検索してプロンプトに差し込み、さらに「この指針が今の質問に当てはまるか」を判断させて過剰適用を抑える

**参照文献情報**

-   タイトル：Mistake Notebook Learning: Selective Batch-Wise Context Optimization for In-Context Learning
-   URL：[https://doi.org/10.48550/arXiv.2512.11485](https://doi.org/10.48550/arXiv.2512.11485)
-   著者：Xuanbo Su, Yingfang Zhang, Hao Luo, Xiaoteng Liu, Leo Huang
-   所属：Bairong Inc, Harbin Institute of Technology, Jilin University

## 反復改善とはどんな考え方か

よく使われるようになってきた「反復的な改善」という考え方を整理しています。モデル本体を再学習させるのではなく、プロンプトや外部メモリの内容を少しずつ書き換えていくことで、何度も試行錯誤を繰り返しながら精度を高めていく方法です。ここでいう外部メモリとは、モデルの外側にある“メモ帳”のようなもので、過去の失敗や有用なアドバイスを記録しておき、それを次回の回答時に参照するという仕組みだと考えるとイメージしやすいでしょう。

基本的な流れはとてもシンプルです。  
まずモデルに一度タスクを解かせてみて、うまくいかなかった点を振り返ります。その反省をもとに、プロンプトや外部メモリに情報を加えて、再び同じタスクに取り組ませます。  
こうして更新を繰り返すことで、少しずつ精度が上がっていくのではないかという直感的な期待があります。

こうした考え方には一定の合理性がありつつも、見過ごせない落とし穴もあります。

問題視されているのは、「更新するかどうかの判断基準があいまいなことがある」という点です。ここでの基準とは、更新を加えた結果、本当に改善されたのかを確認したうえで反映するかどうか、という判断の枠組みです。

一部の手法では、モデルが出力した助言や経験を、検証もせずにそのまま取り込んでしまう傾向があると述べられています。言い換えると、モデルが「これは改善策だ」と言った内容を、あまり疑うことなく信じて反映してしまうということです。

![[学習なしでLLMを強くするための「文脈を育てる」という発想 - AIDB/AIDB_99123_1-921x1024.png]]

検証なしのメモ更新は、誤った当てはめを生む

こうした無条件の更新を続けていくと、外部メモリの中身が役に立たない情報や、場合によっては有害な情報で埋まってしまう危険があります。その結果、参照するメモの量が多くなりすぎたり、誤ったアドバイスが混じったりして、モデルの挙動が不安定になります。

さらにやっかいなのは、あとから間違いに気づいても修正が難しくなる点です。更新をどんどん重ねていくばかりでは、途中に紛れ込んだ悪い更新が原因で性能が落ちたとしても、どこで問題が生じたのかを特定し、元に戻すのが難しくなってしまいます。その結果としてモデルの性能が早い段階で頭打ちになり、環境の変化やノイズの影響にも弱くなると指摘されています。

そのため、反復的な改善を行う際には「検証」と「巻き戻し」の仕組みが不可欠です。検証とは、更新を加えたあとで本当に性能が良くなったかどうかを確認することです。巻き戻しは、もし結果が悪化していた場合に、以前の良好な状態に戻すことを指します。

![[学習なしでLLMを強くするための「文脈を育てる」という発想 - AIDB/AIDB_99123_2-1024x559.png]]

提案手法の流れ  
更新は検証して良ければ採用、悪ければ巻き戻す

## 方法

### この手法で更新するのは RAG の参照先

ここで育てるのは、RAGで参照する知識ベースそのものです。

扱うのは「事実の資料」というより、失敗から作った「解き方のメモ」です。バッチごとに失敗を集めてメモを更新し、同じバッチで良くなったと確認できたときだけ採用します。

### まず作るデータ構造

最初に、知識ベースの1エントリを次の3点セットとして持ちます。難しそうに見えますが、どれも実装のための箱です。

|    項目     |          役割          |
|-----------|----------------------|
|  subject  | メモの見出しで、検索しやすい粒度のタグ  |
| guidance  |    メモ本文で、決まった型で書く    |
| embedding | 見出しを意味で検索するための数値ベクトル |

### guidanceを実践向けにするための型

guidanceは「自由記述メモ」だとブレます。そこで本文を5つのパーツに固定します。やることと同じくらい、やらないことを書いておくのがポイントです。

たとえば次のような構成です。

|           内容            |        役割        |
|-------------------------|------------------|
|           修正例           |  望ましい出力の見本を見せる   |
|         正しい進め方          |    解き方の手順を示す     |
|          ミスの要約          |  何がズレていたかを言語化する  |
|        再利用できる戦略         |  汎用的に使えるコツにまとめる  |
| 当てはめ禁止条件（anti-patterns） | 適用してはいけない条件を明示する |

### 実装に入る前に用意するもの

この手法は「良くなったか」を毎回判定できないと回りません。まずは勝ち負けがはっきりする仕事から始めるのが安全です。

| 事前に決めること |                     目安                      |
|----------|---------------------------------------------|
|  採点ルール   | 良くなったかを機械的に判定できる形にする（例としてText to SQLなら完全一致） |
|  バッチサイズ  |            最初は小さめで回して、安定してから増やす             |
|   役割分担   |   回答するLLMとノートを作るLLMを分ける（同じモデルでも別モデルでもよい）    |

### 手順

#### ステップ1 一回解いて基準を作る

まず各質問について、その質問文とsubjectの意味のちかさを比べ、上位からいくつか選びます。比べる方法は、文章の意味を数字に変えて、それぞれのちかさを測る仕組みです。あらかじめ決めた基準より遠いものは選びません。選ばれたguidanceはプロンプトに入れますが、そのまま使わせるのではなく、「このメモは今回の質問に合っているか？」を事前に判断させます。似ているけれど違う問題に間違って適用してしまうのを防ぐためです。

#### ステップ2 バッチ全体をまとめてsubjectを付ける

次に、ひとまとまりの質問（バッチ）をまとめて見せ、それぞれにsubjectを付けます。まとめて分類することで、似た質問には似た名前がつきやすくなり、あとで失敗を整理するのが楽になります。subjectの一覧は最初から用意しておかなくてもかまいません。新しい問題が出てくれば、その都度追加していけばいい設計になっています。

#### ステップ3 失敗だけを残す

subjectごとに、基準として出した答えと正解とを採点ルールで比べます。ここで残すのは「正解のほうが明らかに良い」と判断できるケースだけです。どちらが良いか微妙なケースまで含めてしまうと、メモがノイズでふくらんでしまいます。もし失敗が一つもなければ、そのバッチ全体はスキップされます。

#### ステップ4 同じsubjectの失敗をまとめて失敗パターンにする

残った失敗は、1件ずつ別々に処理するのではなく、同じsubjectに属するものをまとめて眺めます。そして、共通するつまずき方をひとつのパターンとして整理します。たとえば、いくつかのSQLの失敗が「不等号の種類を取り違えた」というひとつのパターンにまとまる、という具合です。こうしておくと、個別の事情に左右されにくく、応用しやすいメモになります。

#### ステップ5 パターンからguidanceを生成する

抽象化された失敗パターンをもとに、五つの要素でguidanceを書き起こします。ここで大事なのは、「やってはいけない条件」まできちんと書いておくことです。これがないと、検索で引っかかったときに、関係のない問題にまで適用されてしまう恐れがあります。

#### ステップ6 似たsubjectのノートは統合する

新しく書いたguidanceをそのまま追加していくと、内容が似ているノートが増えてしまい、検索の精度が下がります。そこで、subject同士の意味的なちかさを調べて、よく似たものがあればひとつにまとめます。似たものがなければ新規追加です。回答時には「質問からsubjectを探す」、統合のときには「subjectからsubjectを探す」という向きの違いが、整理を助けるポイントになります。

#### ステップ7 更新版の知識ベースで同じバッチを解き直して採用か却下を決める

最後に、更新を反映させた知識ベースを使って、同じバッチをもう一度解き直します。確認するのは失敗だけではなく、バッチ全体の出来です。更新後にうまくいった回数と、更新前にうまくいっていた回数を比べ、更新後のほうが多ければその更新を採用します。いくつかの失敗を直せたとしても、他の部分を壊してしまうような更新は、ここで不採用になります。

### ここまでを回せば、実装として形になる

この流れでやることは一貫していて、RAGの参照先を更新しながら、更新にも品質ゲートを付けるだけです。最初はナレッジベースを空から始めて、バッチを回すたびに、統合されて整理されたメモだけが残っていきます。

## 本手法の有効性はどうか

### 実験設定

実験では、数学推論とText-to-SQLの2種類の課題が使われました。数学には、難易度の高いAIME 2024・2025と、小学生向けのGSM8Kを使用。Text-to-SQLには、SpiderとKaggleDBQAのデータセットを使います。

AIMEとKaggleDBQAは問題数が少ない小規模データ、GSM8KとSpiderは問題が多い大規模データです。前者では通常の学習が過学習しやすいため、MNLのように追加学習なしで使える工夫が有効かを見ています。後者では、教師あり学習にどこまで近づけるかを評価しています。

使用モデルはQwen3-8B、DeepSeekV3.2-Exp、Qwen3-Maxの3種類。軽量な公開モデルから大規模な最先端モデルまで幅を持たせています。オープンウェイトとは、モデルの中身が公開され手元で使えるもののことです。

評価では出力の安定性を保つため、温度0の貪欲デコードを採用。Pass@32という、複数回試して1回でも正解すればOKとする指標で正解率を見ます。Text-to-SQLでは実際にSQLを実行して結果の一致を確認。数学では式の表記ゆれをならして完全一致かを見ます。

実験条件はバッチサイズを16に固定し、乱数の種も統一。学習は基本1回だけにして、繰り返すと悪化しやすいことが後の検証で判明しています。多くの実験では、助言と回答の両方を同じモデルが行う自己チューニングを採用。生成の長さも制限して公平に評価しています。比較対象のTFGOやMementoも同じ設定でそろえています。

### 実験結果

数学の分野では、特に難易度が高いAIME 2025のような問題で、Qwen3-8BにMNLを組み合わせたことで成績が向上したと報告されています。GSM8KにおいてもMNLは高い正解率を出しており、すでに性能が頭打ちに近い強力なモデルに対しても、性能を落とさずに維持できた点が強調されています。

|  データセット   |     モデル      | ベースライン | TFGO  |  MNL  |
|-----------|--------------|--------|-------|-------|
| AIME 2024 |   Qwen3-8B   |  0.30  | 0.23  | 0.33  |
| AIME 2024 | DeepSeekV3.2 |  0.87  | 0.93  | 0.90  |
| AIME 2024 |  Qwen3-Max   |  0.93  | 0.90  | 0.93  |
| AIME 2025 |   Qwen3-8B   |  0.23  | 0.23  | 0.30  |
| AIME 2025 | DeepSeekV3.2 |  0.80  | 0.90  | 0.83  |
| AIME 2025 |  Qwen3-Max   |  0.96  | 0.90  | 0.96  |
|   GSM8K   |   Qwen3-8B   | 0.918  | 0.912 | 0.939 |

数学推論の結果 MNL（本手法）で上向き

Text-to-SQLについても、KaggleDBQAのように例題が少ないケースで、Qwen3-8BやDeepSeekV3.2を使った際に大きな改善が見られたと述べています。Spiderにおいても、Qwen3-8Bが元の状態より成績を伸ばし、他の手法よりも良い結果を出しています。著者らは、こうした結果から「例題が少ない状況ほど、失敗パターンをまとめたアドバイスが特に効きやすい」と述べています。

|      データセット      |     モデル      | ベースライン | Memento | TFGO  |  MNL  |
|------------------|--------------|--------|---------|-------|-------|
| KaggleDBQA（DBQA） |   Qwen3-8B   | 0.190  |  0.151  | 0.221 | 0.280 |
| KaggleDBQA（DBQA） | DeepSeekV3.2 | 0.238  |  0.194  | 0.243 | 0.314 |
| KaggleDBQA（DBQA） |  Qwen3-Max   | 0.400  |  0.470  | 0.475 | 0.459 |
|      Spider      |   Qwen3-8B   | 0.689  |  0.673  | 0.701 | 0.717 |

Text-to-SQLの結果 小規模データほど効く

### ファインチューニングとの比較

教師ありの微調整と本手法を直接比較した結果も出ています。

教師あり微調整とは、正解がついた大量のデータを使って、モデルの中身そのものを学習し直す方法です。ここではQwen3-8Bを対象に、GSM8KとSpiderのデータで両者の違いを比べています。微調整の実験にはH20というGPUを用い、エポック1回、バッチサイズ16、学習率は5×10のマイナス6乗という設定で行われました。

その結果、GSM8Kでは本手法が93.9%、微調整が94.3%となり、その差はわずか0.4ポイントです。一方でSpiderでは、微調整が79.0%、MNLは71.7%と差が開きました。ただし、本手法はもとの成績である68.9%から2.8ポイント向上しており、モデルの中身をいじらずに得られる実用的な改善として十分意味があると述べています。

![[学習なしでLLMを強くするための「文脈を育てる」という発想 - AIDB/AIDB_99123_3-1024x675.png]]

微調整と比べたときの本手法  
GSM8Kはほぼ同等、Spiderは差あり

### どの部分がどれだけ効いているのかを確かめる

まずはバッチサイズを変えたときの変化を見ています。

![[学習なしでLLMを強くするための「文脈を育てる」という発想 - AIDB/AIDB_99123_4-1024x701.png]]

バッチで抽象化すると精度が上がってメモが減る

バッチサイズ1は、ほとんど1問ずつ処理する形なので、個別の失敗に影響されやすくなります。これを16にすると、KaggleDBQAでの正解率が24.0%から28.0%に上がったとされています。  
さらに面白いのは、知識ベースに残されたメモの数が69件から23件に減ったことです。いくつかの失敗をまとめて見ることで、似たパターンをひとつに整理できて、少ないメモでも広く使えるアドバイスになった、というわけです。  
ただし、バッチサイズを32にすると正解率は変わらず、メモの数が少し増えたので、大きくしすぎると細かい違いが見えにくくなるかもしれないとも示唆されています。

次は、同じデータを何周学習させるかによる違いです。著者たちは意外な結果として、何度も学習を繰り返すと過学習がはっきり出たと述べています。過学習というのは、練習には強くなるけれど、本番ではうまくいかなくなる現象のことです。  
実験では、1回だけ学習したときがテストの結果としては一番よく、2回目には成績が落ち、学習中のスコアだけがどんどん上がっていきます。あわせて、知識ベースに記録される内容も、周回するほどどんどん増えてしまい、練習データに偏ったアドバイスを覚えすぎていると解釈されています。そのため、著者たちはエポックは1回で止める方針を取り、いわば早めに切り上げる運用がちょうどよいと判断しています。

![[学習なしでLLMを強くするための「文脈を育てる」という発想 - AIDB/AIDB_99123_5-1024x740.png]]

回しすぎ注意。テストは落ち、メモは増える

### 自己チューニングと別モデルチューニングの比較

さらに、実用的な観点から「助言を出す役割に、必ず強い別のモデルが必要なのか」を確かめています。具体的には、助言を作るのにより高性能なDeepSeekV3.2を使い、実際に問題を解くのはQwen3-8Bのままにする設定が「別モデルチューニング」。一方で、助言も回答もQwen3-8B自身が担当するのが「自己チューニング」です。

結果を見ると、KaggleDBQAでは別モデルチューニングが31.0%、自己チューニングは28.0%でした。つまり、強いモデルに助言をまかせることで成果が上がる場合もあるということです。ただし、自己チューニングでも十分な改善が得られていて、「手元にあるモデルだけでもちゃんと良くなる」という実用的な意味合いがあるとも言えます。

### コスト分析

次はコストの比較です。性能が良くても、費用がかかりすぎては使いにくいので、どれくらいの計算資源や推論コストで動かせるかも見ています。たとえばKaggleDBQAでは、本手法は0.19ドルで0.459の精度に達しました。一方、Mementoは0.43ドルかけてもわずかにしか改善しておらず、TFGOにいたってはもっと高コストなのに精度は伸びませんでした。

GSM8KやSpiderでも同じ傾向があり、本手法は微調整とほぼ同じ成績を、より短い時間と低いコストで実現しています。たとえばGSM8Kでは、MNLが0.99ドルで0.939のスコアを出し、微調整は0.943ですが1.98ドルかかっています。Spiderでは本手法が1.98ドルで0.717、微調整は0.79を出していますが、費用は3.32ドルと高めです。

![[学習なしでLLMを強くするための「文脈を育てる」という発想 - AIDB/AIDB_99123_6.png]]

コスト対精度。本手法は低コストで到達

本手法はコストを約半分に抑えながら、性能面でも十分に追いつけることを示していると言えそうです。

## まとめ

本記事で紹介したのは、LLMをより賢く使うために、「モデルの中身を学習で変える」のではなく、「文脈や知識ベースを丁寧に整える」ことに重きを置く考え方です。

手順を改めて整理します。  
まず失敗した例をまとめて見直し、そこから再利用しやすいパターンを作ります。そして、同じ条件で比べて良くなったと確認できたものだけを知識ベースに追加していくことで、無駄なく整理された内容に育てていきます。

このやり方なら、余計な計算コストや、覚えすぎて逆に大事なことを忘れてしまうような問題も避けられます。そのうえで、モデルの微調整に近いレベルの効果を出せる可能性もあります。文脈をうまく整えることが、現実的な選択肢になりうることが示唆された研究でした。

**本記事の関連研究**

-   [LLMを新しいタスクに順応させる「文脈内学習」における効率的なコンテキストの作り方](https://ai-data-base.com/archives/96070)
-   [文脈内学習は「少数事例からの単純な学習だけでなく、言語モデルが持つ幅広い適応能力」](https://ai-data-base.com/archives/80765)
-   [LLMにプロンプトのみで仮想的な強化学習を発生させる方法](https://ai-data-base.com/archives/91141)
