---
created: 2026-01-01T11:18:31 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/92608
author: AIDB Research
---

# 指示が増えるとLLMはどうなるのかを限界まで検証した結果 - AIDB

> ## Excerpt
> 本記事では、LLMに多数の指示を与えたときにどのような挙動を示すのかを検証した研究を紹介します。 最近のLLMはコンテキストウィンドウ（入力文字数）の拡大により、一度に多くの条件を詰め込んだプロンプトが現実的になってきましたが、その限界は明確ではありません。 そんな中、最大500個の指示を用いて複数のモデルの性能劣化やエラー傾向が詳細に分析されています。実運用に向けて、どのモデルがどんな場面に適し…

---
本記事では、LLMに多数の指示を与えたときにどのような挙動を示すのかを検証した研究を紹介します。

最近のLLMはコンテキストウィンドウ（入力文字数）の拡大により、一度に多くの条件を詰め込んだプロンプトが現実的になってきましたが、その限界は明確ではありません。

そんな中、最大500個の指示を用いて複数のモデルの性能劣化やエラー傾向が詳細に分析されています。  
実運用に向けて、どのモデルがどんな場面に適しているかを見極めるための参考になりそうです。

![[指示が増えるとLLMはどうなるのかを限界まで検証した結果 - AIDB/AIDB_92608-1024x576.png]]

## 背景

LLMは一度にいくつまでの指示に従えるのか、気になりませんか？  
プロンプトの工夫で複雑なタスクをこなしてくれるLLMですが、指示が多くなると徐々に精度が落ちるのではないかという不安を感じたことがある方も多いはずです。

業務では同時に処理すべき指示が非常に多くなります。コンテンツ生成のガイドラインや事実確認の要件、自動化フローに含まれる多様なビジネスルール、エージェントに求められるメモリ管理やツールの使い分けなど。そのため、モデルが複数の制約を正確に守れるかどうかは、実運用における関心ごとでもあります。

LLMは最大数百万トークンを処理できる大きなコンテキストウィンドウを備えており、理想的には多くの指示を与えても全てに応えてほしいところです。しかし、実際には限界があるはずです。とはいえ、何個までなら「ちゃんと従える」のでしょうか？これは、今まであまり調べられていませんでした。既存のベンチマークの多くは、1〜数個の指示しか含まれておらず、指示が100個、200個と増えたときにどうなるかまでは見ていないのです。

そこで本記事では、指示の密度が上がるにつれてモデルの振る舞いがどう変わるのか調査した事例を取り上げます。大量の指示を詰め込んだプロンプトを使い、モデルがどこまで対応できるのかが調べられています。扱っているタスク自体は単純ですが、その先にあるのは「LLMの限界とは何か？」という問いです。

ここから限定コンテンツ

## これまで何が調べられてきたか

LLMに多くの指示を守らせたいとき、どこに限界があるのか。これまでの研究では何が分かってきたでしょうか。

### 単純な指示を守れるかどうかからスタート

初期は少数の指示に対する整合性や汎化力がチェックされ、LLMが人の意図にどこまで応えられるかを測る取り組みが進められました。

この頃は比較的シンプルなタスクを対象とし、たとえば「1つや2つのルールなら守れるが、それが10個、50個となったときにどうなるか」といった視点はあまり含まれていませんでした。

### 複数の条件が重なったタスクへのチャレンジへ

最近は、より現実に近い複雑な条件を扱うベンチマークも増えてきています。たとえば、「敬語を使い、300字以内で、専門用語を避ける」といった複数の制約つきの文章生成を課して、モデルの指示理解が試されたりしています。

あるいは「Aの処理をして、その結果を使ってBを行い、最後にCでチェック」といった手順の流れに対応できるかどうかも見られています。そして、構造的なタスクに対しては指示の工夫次第で精度が上がるケースも確認されています。

ただし、それでも評価されている指示の数はそこまで多くありません。数百におよぶ大量の指示が同時に出されたときに何が起こるかまでは、十分に検証されていない状況です。

### 否定の指示や並び順が、モデルの混乱を引き起こす？

「〜しないでください」といった否定形の指示や、関係のない話題が混じっている場合にモデルが混乱しやすい可能性も示唆されています。

また、指示の並び順にも影響があることがわかっています。リストの前の方にある指示の方が、後ろの指示よりも守られやすいという傾向があり、人間の「初頭効果」に近い性質がLLMにも見られるようです。

とはいえ、これらの研究も基本的には少数の指示を対象にしています。大量の指示を詰め込んだときにどうなるか、全体としてどんな崩れ方をするのかは、まだ見えていません。

## どれだけ指示を増やせるのかを試すテストを開発

LLMに一度にたくさんの指示を守らせたいとき実際にどこまで対応できるのかを調べるためにベンチマーク「IFScale」が開発されました。

仕組みはとてもシンプルです。LLMにビジネスレポートを書かせながら、「この単語を必ず含めてください」といった形で複数のキーワードを指定します。その数を10個から500個まで少しずつ増やしていき、どこでパフォーマンスが崩れるのかを観察します。

たとえば、「収益を入れてください」「戦略を入れてください」といったような単語指定を何十、何百と与えていきます。含まれているかどうかは正規表現で自動チェックできるため、人手を介さずに広範囲の評価が可能です。

### 評価に使うビジネス用語の選び方

まず必要なのは、指示に使うキーワードの準備です。研究者たちは、米国企業が提出する年次報告書（SECの10-K書類）をもとに、ビジネスに関係する単語を収集しました。

各書類から抽出された上位500語をもとに、重複を除き、文書内に実際に現れるかどうかを確認。さらに、英語としての一般性や意味の近さなども考慮して、不適切な語を取り除いていきました。

最終的には、モデルにとって生成が難しい単語のスコアを推定し、その難易度に基づいて500語を選定。簡単なものから難しいものまで、偏りのない語彙セットが整えられました。

### 実験の流れと評価の工夫

実験では、10個から500個までのキーワード数でプロンプトを構成し、各条件につき5回ずつ評価を行います。指示リストは、「この単語を必ず正確に使ってください」といった文で構成され、あわせて「複数セクションからなるプロフェッショナルなビジネスレポートを書いてください」と依頼します。

評価では、生成されたレポートに指示された単語が正しく含まれているかを自動でチェックします。明らかに指示を無視したような出力があった場合は、再生成も行うようになっています。

全体としてはシンプルな構成ですが、指示数を段階的に増やしながら評価することで、LLMがどこでつまずくかを丁寧に探る設計です。そして、人手による評価が不要なため、多数のモデルを効率よく比較できるのが大きな特徴です。

## どんなモデルがどこまで指示に耐えられるのか

実際に、IFScaleを使って20種類のモデルをテスト。どのような傾向が見えてくるのでしょうか。OpenAI、Anthropic、Google、Meta、xAIなど、主要なプロバイダーのモデルが対象になっています。

### 検証対象になったモデル一覧

以下が実験で検証対象となったモデル一覧です。

-   **llama‑4‑maverick**
-   **llama‑4‑scout**
-   **gpt‑4.1**
-   **gpt‑4.1‑mini**
-   **gpt‑4.1‑nano**
-   **gpt‑4o**
-   **gpt‑4o‑mini**
-   **gpt‑4.5‑preview**
-   **claude‑3.5‑haiku**
-   **claude‑3.7‑sonnet**
-   **claude‑opus‑4**
-   **claude‑opus‑4 (reasoning)**
-   **claude‑sonnet‑4**
-   **claude‑sonnet‑4 (reasoning)**
-   **deepseek‑r1‑0528**
-   **gemini‑2.5‑flash‑preview**
-   **gemini‑2.5‑pro‑preview**
-   **o3 (medium)**
-   **o3 (high)**
-   **o4‑mini (medium)**
-   **qwen3‑235b‑a22b**
-   **grok‑3‑beta**
-   **grok‑3‑mini‑beta**

### テストの進め方

それぞれのモデルに対して、10個から500個までの指示を10個刻みで与え、各条件を5回ずつ評価しました。推論力のあるモデルでは「考える設定（high reasoning）」をオンにし、通常設定のまま自然な生成がどれくらい保たれるかを見ています。

評価は、指定されたキーワードが文章の中に入っているかどうかを正規表現でチェックする仕組みです。たとえば「戦略」が「戦略的」と変化していた場合は「変更エラー」、そもそも含まれていなければ「省略エラー」としてカウントされます。

加えて、指示リストの前半・中間・後半で守られ方に違いがあるかも調べ、人間にも見られる「最初の指示ほど覚えている」傾向（初頭効果）がLLMにもあるのかを確認しています。

### 実験結果

![[指示が増えるとLLMはどうなるのかを限界まで検証した結果 - AIDB/AIDB_92608_2.png]]

#### 頼りになるのは推論モデル

まず全体の傾向として、推論力を持つモデル（たとえばo3やgemini-2.5-pro）が他のモデルよりも安定して高い精度を示しました。100〜250個の指示まではほぼ完璧にこなすことができ、そこから徐々に崩れていくような動きです。

新しい世代や大規模なモデルほど良い結果を出す傾向が見られた一方で、意外な例もいくつかありました。たとえばgrok-3は推論設定なしでも、500個の指示で61.9%という高水準を記録しています。逆に、deepseek-r1やqwen3のように、性能のわりに期待を下回る結果にとどまったモデルもありました。

#### 劣化の仕方には3つの型がある

指示が増えるにつれて性能が落ちていく過程を詳しく見ると、モデルによって崩れ方に違いがあることが分かりました。

![[指示が増えるとLLMはどうなるのかを限界まで検証した結果 - AIDB/AIDB_92608_1-1024x610.png]]

**急に崩れるタイプ（閾値型）**  
ある程度まではほぼ完璧に対応できるのに、閾値を超えた瞬間から急に性能が落ちるタイプです。o3やgemini-2.5-proなど、推論モデルがこの傾向を示しました。じっくり考える力があることで、指示を丁寧に追えるようです。

**少しずつ落ちていくタイプ（線形型）**  
指示数が増えるにつれて、着実に性能が下がっていくタイプです。gpt-4.1やclaude-3.7-sonnetがこのパターンに当てはまります。崩れ方が予測しやすいという特徴があります。

**最初から落ち込むタイプ（指数型）**  
わずかな指示でも対応しきれず、早い段階で性能が急激に落ち、そのまま低水準で安定してしまうタイプです。claude-3.5-haikuやllama-4-scoutなどがそうで、だいたい7〜15%の精度に張りつくような動きを見せました。

#### テスト結果のバラつきにもパターンがある

同じ条件で5回試したときの精度のバラつきにも、モデルごとの傾向がありました。

高性能なモデル（o3やgrok-3など）は、指示数が増えるほどばらつきも増えていきます。これは限界に近づくと出力が不安定になることを示しています。

中くらいのモデルでは、150〜300個あたりでバラつきが一時的に大きくなり、その後は落ち着きます。いわば「崖の縁」がそのあたりにあるようです。

一方で、最初から対応が難しいモデルは、少ない指示の段階で早々にばらつきが収束します。もう無理だとあきらめているような状態です。

#### 指示の順番にも影響がある

どのモデルでも共通して見られたのが、指示の順番による違いです。リストの最初にある指示のほうが、後ろにあるものより守られやすい傾向がありました。

ただし、この「初頭効果」はずっと強いわけではありません。指示が少ないときは目立たず、150〜200個くらいでピークになり、それ以上の数になるとまた弱まります。

![[指示が増えるとLLMはどうなるのかを限界まで検証した結果 - AIDB/AIDB_92608_3-1024x308.png]]

あまりに多くの指示を詰め込むと、モデルは選別どころか全体的に追いきれなくなるようです。

#### 精度だけでなく速度も重要

実用面では、指示に正しく従えるかだけでなく、どれくらいの時間で結果が返ってくるかも重要です。

高性能な推論モデルは時間がかかります。たとえばo4-miniは250個の指示で7分以上かかっていますし、o3も220秒ほどかかっています。それに対し、一般的なモデルは10〜15秒で処理を終えるケースもあります。

興味深いのは、精度と速度のバランスを取った「効率性」で見ると、必ずしも高精度モデルがベストとは言い切れない点です。grok-3-miniやgemini-2.5-flashのように、小型で高速なモデルが、時間の制約がある場面では現実的な選択肢になります。

#### 指示を守れなかったときの傾向

指示に従えなかったときのエラーにもパターンがありました。

**省略エラー**  
指示された単語がまったく出てこないケースです。

**変更エラー**  
「戦略」→「戦略的」のように似た単語で代用してしまうケースです。

指示の数が増えてくると、ほとんどのモデルで省略エラーが急増します。たとえばllama-4-scoutでは、変更エラーの35倍もの省略エラーが観測されました。

ただし、o3やo4-miniのような推論モデルは、ぎりぎりの状態でもなんとか指示に応えようとする動きが見られます。完全にあきらめずに、少しでも意味の近い語を入れてくるあたりに、粘り強さのようなものを感じます。

#### レポートの文章そのものはどうだったか

最後に、生成されたビジネスレポート自体の品質も見ておきましょう。多くの指示を処理することで、文章の整合性が失われるのではないかという懸念がありました。

実際には、ほとんどのモデルで大きな問題は起きていません。ただし、o3やo4-miniは例外で、指示数が増えるにつれてレポートの一貫性が下がる傾向がありました。

理由のひとつは、これらのモデルが出力するトークン数が少ないことにあります。500個ものキーワードを盛り込もうとすると、3語に1つが指示語になるような状態になってしまい、自然な文章を組み立てるのが難しくなるのです。

ところが、同じように短めの出力だったgrok-3は、それでも高い品質を維持していました。短い文章のなかでも効率よく指示を消化できる構造になっていると考えられます。

## 実験から見えてきたこと

### 劣化パターンごとの使い分けができる

性能の落ち方に3つの型があったことは、モデル選びに新しい視点を与えてくれます。

たとえば、大量の指示を正確に守らせたい場合は、閾値型のモデル（o3、gemini-2.5-proなど）が適している可能性があります。ある程度の量まではきちんと対応してくれるので、ルールの多い業務にも安心して使えます。

一方で、「このくらいの指示なら、これくらいの精度だろう」と予測を立てたい場合は、線形型のモデルが向いています。徐々に精度が落ちていくため、見通しを立てながら使えます。

もし指示数が少なめで済むようなシンプルな用途であれば、指数型のモデルでも十分です。早い段階で性能は落ちますが、短くて簡単なタスクなら問題ありません。

ただし、あくまで今回の実験内容から推測される範囲でのモデルの特徴ということです。

### 指示の順番に頼れるのは中程度まで

初頭効果の観点からも、プロンプト設計に少し工夫を加える価値があることが分かりました。重要な指示を先頭に置くという定番のやり方は、150〜200個くらいまでの指示数なら有効です。

ただ、300個を超えるとその効果はほとんどなくなります。あまりに多くの指示が詰め込まれると、順番の工夫だけでは対応しきれなくなるようです。プロンプトの工夫だけでは限界があるという、他の場面でもよく見られる現象とも通じます。

タスクの分割や指示数の抑制など、構造的な見直しも視野に入れる必要がありそうです。

### 安定性を見ることの大切さ

精度だけでなく、出力のばらつきにも注目したいところです。平均が高くても結果が毎回ばらばらでは、信頼して使うのが難しくなります。

中程度のモデルが150〜300個の指示で見せる不安定さも興味深いポイントです。このゾーンがそのモデルの「限界近く」である可能性があるため、業務で使う際には注意が必要です。逆に、少し精度は劣っても安定して出力してくれるモデルの方が、安心して使えるケースもあります。

### 精度と速度のバランスをどう考えるか

性能が高ければいい、というわけではありません。実務では時間も大事な要素です。

たとえばo4-miniやo3のような推論モデルは、高精度ですが処理に数分かかることもあります。対して、grok-3-miniやgemini-2.5-flashのような軽量モデルは高速で結果を返してくれます。

処理時間と精度を合わせた「効率性」で見ると、小さなモデルが有利になる場面も多くありそうです。とくに、カスタマー対応のように即時性が求められるケースでは、こうしたモデルが現実的な選択肢になります。

### エラーパターンから見えるモデルの個性

エラーの出方も、モデルの「振る舞い方」の違いを教えてくれます。

省略エラーが多いモデルは、難しくなると指示そのものを無視しがちです。対して、変更エラーが多いモデルは、なんとか似た形で応えようとする傾向があります。これは「諦めやすさ」の違いとも言えます。

o3やo4-miniのような推論モデルは、限界に近づいてもできるだけ指示に近い語を使おうとします。これは単に高精度というだけでなく、対応の姿勢に誠実さがあるようにも見えます。

ただ、状況によっては完全に失敗してくれた方が、逆に分かりやすいということもあります。用途に応じて、どちらの性格が合っているかを見極める必要があります。

### 本来の出力品質にも影響が出るか

最後に、指示の多さがメインの出力にどう影響するかも見ておきたいところです。

多くのモデルでは、指示数が増えても文章の一貫性には大きな変化は見られませんでした。これは、指示を守る処理と文章を作る処理がある程度は独立して動いていることを示しています。

ただ、o3やo4-miniでは品質の低下が見られました。これらのモデルは出力量が少なめなので、500個ものキーワードを入れようとすると、自然な文章にする余地が足りなくなってしまうのです。

それでも、同じくらい短めの出力だったgrok-3は品質を保っており、効率的な処理の仕組みがあることをうかがわせます。モデルの設計や学習方法の違いが、こうした結果の差につながっているのかもしれません。今後の開発でも注目されそうなポイントです。

## まとめ

本記事では、LLMが同時にどれだけの指示に対応できるかを評価する研究を紹介しました。

実験を通じて、モデルごとに異なる性能劣化のパターンや初頭効果、処理時間とのトレードオフが明らかになりました。また、指示の並び順やエラー傾向からは、プロンプト設計やモデル選択に関する実践的な示唆も得られました。

一部のモデルでは指示数の増加が文章品質に影響を与えることも確認され、限界を踏まえた運用が求められます。

**参照文献情報**

-   タイトル：How Many Instructions Can LLMs Follow at Once?
-   URL：[https://doi.org/10.48550/arXiv.2507.11538](https://doi.org/10.48550/arXiv.2507.11538)
-   著者：Daniel Jaroslawicz, Brendan Whiting, Parth Shah, Karime Maamari
-   所属：Distyl AI
