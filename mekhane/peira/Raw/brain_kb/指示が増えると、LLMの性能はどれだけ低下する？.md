---
created: 2026-01-01T11:17:23 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/96029
author: AIDB Research
---

# 指示が増えると、LLMの性能はどれだけ低下する？ - AIDB

> ## Excerpt
> 本記事では指示が増えたときにLLMがどれだけ性能を保てるのかについての体系的な評価を紹介します。 文章生成とプログラミングの二つの領域で、10種類の主要なLLMを調べています。どのようなテストが行われたのか、どんな結果が得られたのか、そして実務での示唆について順を追って見ていきます。 背景 LLMを使っていて、こんな経験はないでしょうか。 「この文章を要約して。ただし箇条書きで、500文字以内で、…

---
本記事では指示が増えたときにLLMがどれだけ性能を保てるのかについての体系的な評価を紹介します。

文章生成とプログラミングの二つの領域で、10種類の主要なLLMを調べています。どのようなテストが行われたのか、どんな結果が得られたのか、そして実務での示唆について順を追って見ていきます。

![[指示が増えると、LLMの性能はどれだけ低下する？ - AIDB/AIDB_96029-1024x576.png]]

## 背景

LLMを使っていて、こんな経験はないでしょうか。

「この文章を要約して。ただし箇条書きで、500文字以内で、専門用語は避けて、ですます調で書いて」と指示したら、一部の条件が無視されていた。

あるいはプログラミングで「この機能を実装して。インデントは2スペースで、コメントは英語で、変数名はキャメルケースで、行は80文字以内に」と頼んだのに、途中から指示がごちゃ混ぜになっていた。

実は、この「複数の指示を同時に守る」というのは、LLMにとって思いのほか難しい課題なのです。

普段の使い方を思い返してみると、単純な指示だけでAIを使うことはほとんどありません。「コードを書いて」だけでは実務では使えず、チームのコーディング規約、ドキュメント要件、パフォーマンス条件など、複数の制約を同時に満たす必要があります。ビジネス文書も同じです。「プレゼン資料を作って」の後ろには、文字数制限、トーン指定、フォーマット要件、禁止ワードなど、実際には5個も6個も条件が付いてきます。

そこで本記事では、複数の指示を同時に守る必要のある課題で、LLMはどれほど精度を保てるのかを実験した取り組みを紹介します。[以前の記事](https://ai-data-base.com/archives/92608)よりもさらに明確に「多様な指示」を増やしているのが特徴です。

ここから限定コンテンツ

**参照文献情報**

-   タイトル：When Instructions Multiply: Measuring and Estimating LLM Capabilities of Multiple Instructions Following
-   URL：[https://arxiv.org/abs/2509.21051](https://arxiv.org/abs/2509.21051)
-   著者：Keno Harada, Yudai Yamazaki, Masachika Taniguchi, Edison Marrese-Taylor, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo
-   所属：The University of Tokyo, Kyoto University, University of the Ryukyus

## 何を測定したのか

それでは、この研究で実際に何が測られたのか、結果を理解するために必要な前提を整理していきましょう。

研究チームは二つのテストを用意しました。一つは普通の文章を書くタスク、もう一つはプログラムを書くタスクです。どちらも実務でよくある使い方を想定しています。

### 文章生成テスト

最初のテストは、日常的な文章生成の場面を想定しています。例えば、

「日本への旅行についてブログ記事を書いて」  
とか  
「ある会社の広告文を作って」  
といった課題があります。こうした課題記述が216種類用意されました。

重要なのは、これらの課題に対して様々な追加条件を付けるという点です。例えば次のような指示が組み合わされます。  
「タイトルを二重山括弧で囲んで」「特定のキーワードを必ず3回以上使って」「500文字以上で書いて」「箇条書きを2つ入れて」「全て大文字は使わないで」といった具合です。  
これらの指示は6つのグループに分類できます。

1.  キーワードに関する指示
2.  文字数や段落数といった長さに関する指示
3.  箇条書きなどのフォーマット指示
4.  大文字小文字の使い分け
5.  文章の始まりと終わりの形式
6.  句読点のルール

ポイントは、同じ「日本旅行のブログを書く」という課題に対して、指示を1個だけ付けたバージョン、2個付けたバージョン、3個、4個と、最大10個まで付けたバージョンを作ったことです。課題の本質的な難しさは変わらず、単純に「同時に気を付けないといけないこと」だけが増えていくわけです。

最終的に全部で2,160個のテストケースができ、指示の数ごとにバランスよくサンプルが分散されました。

### プログラミングのテスト

二つ目のテストは、実務のプログラミングを想定しています。

「配列をソートする関数を書いて」  
「文字列から特定の文字を削除する関数を書いて」  
といった基本的なPythonプログラミングの課題が500種類あります。

これらの課題には、動作確認用のテストケースが付いています。  
例えば「remove\_Occ(“hello”,”l”)を実行したら”heo”が返ってくるべき」といった具合です。  
つまり、生成されたコードが機能的に正しいかどうかは、テストを実行すれば客観的に分かります。

機能面の正しさに加えて、コーディングスタイルも評価します。実務では「動けばいい」だけでなく、チームのコーディング規約に従うことが求められるからです。

具体的には次のような指示が追加されます。  
「MITライセンスの表記を入れて」  
「インデントはスペース2個で統一して」  
「各関数にドキュメント文字列を付けて」  
「1行は79文字以内に収めて」  
「変数名は3文字以上にして」といったものです。

これらも実際の開発現場でよく求められるルールばかりです。そして、Pylintという実際に使われているコード解析ツールを使って、機械的に判定します。

文章生成のテストと同様に、同じプログラミング課題に対して指示を1個から6個まで増やしたバージョンが用意されています。全部で3,000個のテストケースです。

## 実験と結果

それでは、上記二つのテストを使って、実際にどのようなLLMが評価され、どんな結果が出たのかを見ていきましょう。

### どんなモデルが評価されたのか

研究チームは10種類のLLMを評価しました。最新とは言えませんが多様なラインナップです。

これらは大きく二つのグループに分けられます。

一つ目は、API経由でアクセスするタイプのモデル。つまり、企業がサーバー上で運用していて、ユーザーはインターネット経由で使うタイプのものです。

-   GPT-4o
-   Claude 3.5 Sonnet
-   Gemini 1.5 Pro
-   o3-mini

二つ目は、オープンモデルです。

-   Gemma 2の9Bと2Bの二つのサイズ
-   Llama 3.1の8B
-   Qwen2.5の72B
-   DeepSeek-V3
-   DeepSeek-R1

が評価されました。

（ここで出てくる「8B」とか「72B」という数字は、モデルのパラメータ数を表していて、基本的には大きいほど高性能ですが、その分計算リソースも必要になります。「B」はビリオン、つまり10億という単位です）

全てのモデルに対して「ゼロショットプロンプティング」が使われました。事前に例を見せることなく、いきなり本番の課題を与えるというやり方です。実務でLLMを使う時も、毎回例を用意するわけではないので、より現実的な評価方法と言えます。

### 評価の指標

結果を見る前に、どのように性能が測定されたのかを理解しておく必要があります。この研究では二つの指標が使われました。

一つ目は「プロンプトレベル正解率」と呼ばれるものです。  
一つの課題に対して、与えられた全ての指示を同時に満たせたかどうかを測ります。  
例えば5個の指示があったら、その5個全てを守れて初めて正解とカウントされます。4個守れても1個漏れていたら不正解です。この指標は「複数の条件を本当に同時に管理できているか」を厳しく評価します。

二つ目は「指示レベル正解率」です。  
個々の指示が守られたかどうかを別々に評価します。  
5個の指示があって4個守れたら、正解率は80%となります。こちらは「個別の指示にどれくらい対応できているか」を見る指標です。

### 文章生成での結果

では実際の結果を見ていきましょう。まず文章生成のテストからです。

結果を整理した図を見ると、非常に明確なパターンが現れています。どのモデルも、指示の数が増えるほどプロンプトレベル正解率が低下していきます。つまり「全ての条件を同時に満たす」ことが、指示が増えるにつれてどんどん難しくなっているのです。

![[指示が増えると、LLMの性能はどれだけ低下する？ - AIDB/AIDB_96029_1-1024x545.png]]

例えばClaude 3.5 Sonnetを見ると、指示が1個の時は95%という高い正解率ですが、10個になると48%まで落ちます。GPT-4oはより顕著で、1個で94%だったのが、10個では21%まで低下します。約5分の1です。Gemini 1.5 Proも同様の傾向を示しています。

一方、指示レベル正解率を見ると、こちらはそれほど大きく下がっていません。  
つまり、個々の指示は比較的高い確率で守れているのです。  
しかし、それらを全て同時に満たすとなると途端に難しくなる。

これは人間でも経験があるかもしれません。一つ一つは簡単なことでも、複数のことを同時に気にしながら作業すると、どれかが抜け落ちてしまう。

興味深いのは、モデル間の性能順位が指示の数によらず比較的一貫していることです。指示レベル正解率で優れているモデルは、プロンプトレベル正解率でも優れている傾向があります。つまり、個別の指示を守る能力と、複数の指示を同時に管理する能力には、ある程度の相関があるということです。

小規模なオープンソースモデルの結果も示唆的です。Gemma 2の2B版は、指示が10個になるとプロンプトレベル正解率がほぼゼロになります。Llama 3.1の8B版も同様です。これらのモデルは、単純な課題では使えますが、複数の条件を同時に満たす必要がある実務的な使い方では、大きな制約があることが分かります。

### プログラミングでの結果

次にプログラミングのテストを見てみましょう。結果のグラフを見ると、こちらも文章生成と同様の傾向が現れており、指示が増えるほど全ての条件を満たせる確率が下がっていきます。

![[指示が増えると、LLMの性能はどれだけ低下する？ - AIDB/AIDB_96029_2-1024x535.png]]

ただし、モデル間の差がより顕著です。GPT-4oは指示が6個でも68%という比較的高い成功率を維持していますが、Claude 3.5 SonnetとGemini 1.5 Proは6個で1%と13%まで落ち込みます。これは実用上、非常に大きな違いです。

この差の原因も分析されています。Claude 3.5 SonnetとGemini 1.5 Proは、「1行あたり79文字以内」という指示を単独では97%から99%守れるのですが、他の5個の指示と組み合わせると2%から20%まで急落するのです。つまり、特定の指示が複数の条件下で極端に難しくなるという現象が起きています。

小規模モデルでも特徴的なパターンが見られます。Gemma 2とLlama 3.1は「インデント」の指示を守るのが非常に苦手で、これが含まれると全体の成功率が大きく下がります。

さらに重要な発見が下の図に示されています。これは二段階の評価結果です。左側のグラフは「テストケースが通るか」つまり機能的に正しいコードが書けているかを示しています。右側は「テストケースが通り、かつ全てのスタイル指示も守っているか」を示しています。

![[指示が増えると、LLMの性能はどれだけ低下する？ - AIDB/AIDB_96029_3-1024x529.png]]

左側を見ると、指示が増えてもテストケースの通過率はほとんど変わりません。つまりLLMは、複数のスタイル指示が加わっても、基本的な機能を実装する能力は維持できているのです。

しかし右側を見ると、指示が増えるにつれて成功率が大きく下がっています。これは何を意味するかというと、「動くけれど規約違反」というコードが大量に生成されているということです。これは実務で非常によくある状況です。機能的には正しく動くコードを書けるのに、コードレビューでスタイルの指摘を受ける、まさにその現象がデータとして可視化されているのです。

### 推論型モデルの優位性

興味深い結果の一つとして、推論機能を持つモデルの性能向上があります。DeepSeek-R1やo3-miniといったモデルは、答えを出す前に「考える」プロセスを持っています。

実験では、推論の深さを最も深く設定した時に最も良い結果が出ました。文章生成のテストで指示が10個の場合、通常のGPT-4oが21%なのに対し、o3-miniの高設定では78%という大幅な改善を示しています。

DeepSeek-R1も同様の改善を示しています。以下の「思考の痕跡」では、各指示を一つ一つ明示的に確認しながらアプローチの計画を立てている様子が分かります。この「考えながら作業する」アプローチが、複数指示への対応力を向上させているのです。

![[指示が増えると、LLMの性能はどれだけ低下する？ - AIDB/AIDB_96029_4-922x1024.png]]

## まとめ

指示が増えるほど性能が低下するという現象は、どのモデルでも共通して観察されました。

また、機能面では正しくてもスタイル指示が守られないというプログラミングでの発見は、コードレビューの負担軽減を期待してLLMを導入する場合に考慮すべき点です。

現時点では指示が10個程度でも完璧に対応できるモデルはまだありません。実務では、出力を確認する手間は依然として必要だと言えるでしょう。
