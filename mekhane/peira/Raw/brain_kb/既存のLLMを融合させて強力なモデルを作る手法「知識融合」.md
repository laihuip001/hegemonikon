---
created: 2026-01-01T09:37:33 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/63153
author: AIDB Research
---

# 既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB

> ## Excerpt
> LLMを一から作るには膨大な時間とコストがかかる上に、似たようなモデルができてしまうリスクもあります。 そこで今回研究者が提案しているのが「知識融合（knowledge fusion）」という手法です。すでに存在している事前学習済みLLMを組み合わせ、もっと強力なモデルを作るアプローチです。 実験では、さまざまなタスクで成功が確認されています。 参照論文情報 本記事の関連研究： 背景 仮に十分な性…

---
LLMを一から作るには膨大な時間とコストがかかる上に、似たようなモデルができてしまうリスクもあります。

そこで今回研究者が提案しているのが「知識融合（knowledge fusion）」という手法です。すでに存在している事前学習済みLLMを組み合わせ、もっと強力なモデルを作るアプローチです。

実験では、さまざまなタスクで成功が確認されています。

![[既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB/AIDB_63153-1024x576.jpg]]

**参照論文情報**

-   タイトル：Knowledge Fusion of Large Language Models
-   著者：Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, Shuming Shi
-   所属：Sun Yat-sen University, Tencent AI Lab
-   URL：[https://doi.org/10.48550/arXiv.2401.10491](https://doi.org/10.48550/arXiv.2401.10491)
-   GitHub：[https://github.com/fanqiwan/FuseLLM](https://github.com/fanqiwan/FuseLLM)

**本記事の関連研究**：

-   [オープンソースLLMのMixtral 8x7B　GPT-3.5に匹敵する性能を示す高効率モデル](https://ai-data-base.com/archives/62480)
-   [LLMの知識を狙い撃ちして変更・修正する「知識編集（Knowledge Editing）」](https://ai-data-base.com/archives/61831)
-   [1.1Bパラメータの小さなモデルを巨大データ（約3兆トークン）で訓練したモデル『TinyLlama』が、比較的優秀な性能を発揮](https://ai-data-base.com/archives/61914)
-   [あらゆるLLMを「使い心地」基準でバトルさせる便利なプラットフォーム『Chatbot Arena：チャットボットアリーナ』](https://ai-data-base.com/archives/61080)
-   [「ChatGPTの1周年を記念して」、オープンソースLLMがChatGPTにどこまで追いついているか体系的調査報告](https://ai-data-base.com/archives/59713)

## 背景

仮に十分な性能を持つLLMを一から作ろうと考えた場合、膨大な量のデータや高度な技術と知識、そして大量の計算資源（GPUなど）が必要になります。開発の過程でエネルギー消費や環境への影響も甚大になってしまいます。

また既存のLLMはさまざまなタスクで似たような能力を持っていることも報告されています。そのため研究者らは今回、新しいモデルを作る時には、すでに存在するLLMを融合させたほうが効率的に強いモデルを開発できるのではないかと考えました。

なお、これまでにもニューラルネットワークモデルを組み合わせる研究はいくつか行われてきました。例えば「アンサンブル法」という、複数のモデルによる出力を合わせる手法や、複数のネットワークを一つに合わせる「マージング（重みのマージ）」が試されてきました。しかしこれら既存の手法はLLMには向いていないとされています。サイズが大きく、メモリや処理時間の要求が高いためだと言われています。

今回研究者らは、複数の異なるLLMが生成する確率分布（入力に対してどんな出力を行うかの確率）を混ぜることで、各LLMの知識や強みを単一のLLMに移すことを目指す手法「知識融合」を考案しました。理論通りにいけば、混ぜ合わせられる前の各モデルよりも強くなるアプローチです。

![[既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB/AIDB_63153_1-1024x438.png]]

なお本手法は、一般的な知識蒸留とは違って、対象となるモデルのサイズに制約はないそうです。知識蒸留とは、大きなモデルから学んだ知識を小さなモデルに伝える手法で、もとのモデルの性能を超えることはあまりないと言われています。実験では、知識蒸留との直接的な比較も行われています。

以下では、知識融合の方法論と実験結果などを紹介します。

## 知識融合の方法論

研究者らは、

ここから限定コンテンツ

複数のLLMから集めた知識を一つのLLMに統合することを本研究の主な目的としました。それぞれの異なる構造を持つLLMは各々のデータセットで訓練されているため、統合するためのフレームワークが必要です。

知識融合の基本的な方法としては、各LLMが次にくる言葉を同時に予測させ、その結果から確率分布を評価して、一番正確な出力を使用して目標となる融合モデルを訓練します。なお、このようにモデルが次のトークンをどれだけうまく予測できるかを測って訓練するアプローチは「Causal Language Modeling Objective」と呼ばれています。

訓練の際には、目標モデルの予測分布と融合分布の差異を最小化することに注意が必要とのことです。簡単に言うと、新しく作られるLLM（目標モデル）が、複数のモデルから得られた情報（融合分布）をどれだけ正確に反映できているかを確認し、その精度を最高にしたいというわけです。

アプローチをアルゴリズムに落とし込んだものが下記になります。

![[既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB/AIDB_63153_10-1024x336.png]]

まず初めに新しいモデルを初期化し、次に訓練用のテキストデータセット（C）からテキスト（t）を一つずつ取り出して処理します。そして各LLMを使って、テキストに基づく確率分布（予測結果の分布）を計算します。  
さらに計算された確率分布を「最小編集距離（MinED）」という方法を使って整合させ、整合された確率分布を一つの表現（P\_t）に融合させます。  
最後に全体の損失関数を最小化させながら、新しい表現を使って、新しいLLMのパラメータを更新して終了です。

## 実験

研究者らは、アイデアの有効性を検証すべく、Llama-2、OpenLLaMA、MPTという3つのLLMの融合を実験しました。

### セットアップ

**訓練**：目標モデル（LLM）の訓練には、多様でありながらもコンパクトな「MiniPile」というデータセットが使用されました。またNVIDIA A100 GPUを搭載したマシンで約33時間かかって（多くの工夫で高速化した結果の時間）訓練しました。

**融合手法**：融合には「最小クロスエントロピー（MinCE）」という、出力の精度を高めるための手法が使用されました。

### 評価方法

実験では、論理（reasoning）、常識（commonsense）、コード生成（code generation）の能力を評価するため、3つの異なるベンチマークが使用されました。

### 実験結果

**論理能力**

FUSELLM（融合モデル）は、一般的な論理能力を評価する「Big-Bench Hard（BBH）」というベンチマークで、他のモデルと比べて良い結果を示しました。中でもとりわけHyperbatonタスクでは大幅な改善が見られました。

![[既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB/AIDB_63153_2-1024x737.jpg]]

**常識能力**

常識能力を評価するベンチマークでも、融合モデルは他のモデルを上回る結果を示しました。ARC-challengeやOpenBookQAタスクでは顕著な改善がありました。

![[既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB/AIDB_63153_3-1024x285.png]]

**コード生成能力**

コード生成能力を評価する「MultiPL-E（ME）」ベンチマークでは、融合モデルは10のタスク中9つでLlama-2を上回りました。なお、R言語でのスコアが4.97から5.84に向上しており顕著でした。

![[既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB/AIDB_63153_4-1024x395.png]]

### 融合された確率分布の効果

さらに、研究者らは訓練中の性能向上の傾向を追って複数のLLMから得られた融合確率分布の効果を調査しました。

例えば、Big-Bench Hard（BBH）というベンチマークでのLlama-2 CLMとFUSELLMの性能を比較すると、融合モデルはLlama-2 CLMに比べて、2.5%の精度向上を達成しています。Llama-2 CLMが必要とするトークン量の約3.9倍の削減に相当します。

![[既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB/AIDB_63153_5.png]]

### 実装プロセスに関する分析

**ソースとなるLLMの数**：3つを混ぜる場合以外も、異なる数のモデルを融合した場合の性能が比較されています。結果、3つのモデルを融合した時に最も良い性能が出ることがわかったそうです。

![[既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB/AIDB_63153_6.png]]

**トークン整合性の基準**：複数のモデルからのトークンと語彙を合わせることが重要とのことです。なお最小編集距離（MinED）を基準にした方法が、正確な一致（EM）を基準にした方法よりも良い結果を出してることが分かりました。要するにトークン列の小さな違いを許容して、より多くの有用な情報を保持しつつ、予測分布を合わせるほうが大事との結果です。

![[既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB/AIDB_63153_7.png]]

**融合方法**：融合の際の基準として最小クロスエントロピー（MinCE）を使う方法と、クロスエントロピーに基づいた重み付け平均（AvgCE）を使う方法を比較したところ、MinCEを使った方が、AvgCEよりも全てのベンチマークで性能が良いことがわかりました。

### 知識融合 vs 知識蒸留

冒頭で述べたように、知識蒸留は、大きなモデル（教師モデル）の知識を小さなモデル（生徒モデル）に移す技術です。今回は知識蒸留も使用して、Llama-2の13Bモデルから7Bモデルに能力を移す実験をしています。

その結果、知識蒸留されたモデル（Llama-2 KD）は、もちろん元のLlama-2 7Bよりも全てのベンチマークで性能が良くなっています。しかし、知識融合の場合と比べると、改善が小さかったことが示されました。

![[既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB/AIDB_63153_8-1.png]]

### 知識融合 vs アンサンブル法、重みマージ

こちらも冒頭で述べたように、アンサンブル法は、複数のモデルからの確率を平均する技術です。そして重みのマーズ（マージング）は、複数のモデルを一つのパラメータ空間内で統合する技術です。

実験では、同じ基本モデルから派生した、異なるコーパスで訓練された3つのモデル（Pythia 1B）を使って、それぞれの融合技術の効果が比較されました。

その結果、それぞれのドメイン固有のLLMは、他のドメインで性能が低下しているが、知識融合は他の融合技術よりも、全てのドメインで平均的なパープレキシティ（perplexity、モデルの混乱度）が低かったとのことです。

![[既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB/AIDB_63153_9-1.png]]

これら一連の実験を通じて、知識融合で生まれた融合モデル（FUSELLM）が個々のソースとなったLLMよりも優れた性能を持ったことが明らかになりました。そして構造が同じLLMを使った実験では、アンサンブル法やマージングの方法と比較しても効果的であることがわかり、大きな可能性を示しています。

## まとめ

本記事では、複数のLLMを組み合わせ、その集合知を活用した新しいLLMを開発する方法に関する研究を紹介しました。

各モデルの独自の強みを活かしつつ、互いの知識を統合することで、個々のモデルよりも高いパフォーマンスを発揮する目標モデルを作り上げる手法です。

実験よりさまざまなタスクで個々のモデルを超える能力を持つ新しいLLMを構築できたことが示されました。

今後、本手法を実用した事例が多く現れることに期待したいですね。
