---
created: 2026-01-01T11:17:55 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/94246
author: AIDB Research
---

# 自信過剰になるLLM 長く考えさせることの副作用と検索機能が果たす役割 - AIDB

> ## Excerpt
> 本記事では、LLMの「自信」の正確さと、推論にかける時間との関係を調べた研究を紹介します。 LLMにじっくり考えさせれば、より正確な答えが得られるという期待は多くの現場で共有されています。しかし、推論時間を増やすほどかえって自信過剰になり、判断の精度が下がるという可能性も懸念されています。 本記事の関連研究 背景 いまやLLMは、質問に答えたり、エージェントの判断を支えたりと、さまざまな場面で活躍…

---
本記事では、LLMの「自信」の正確さと、推論にかける時間との関係を調べた研究を紹介します。

LLMにじっくり考えさせれば、より正確な答えが得られるという期待は多くの現場で共有されています。しかし、推論時間を増やすほどかえって自信過剰になり、判断の精度が下がるという可能性も懸念されています。

![[自信過剰になるLLM 長く考えさせることの副作用と検索機能が果たす役割 - AIDB/AIDB_94246-1024x576.png]]

## 背景

いまやLLMは、質問に答えたり、エージェントの判断を支えたりと、さまざまな場面で活躍するようになっています。こうしたモデルを安心して使うには、どのくらい自信を持って答えているのかをうまく調整できるかどうかが重要になります。ただ、出力に明確な正解がないときには、その自信の扱いがとても難しくなります。

とくに専門的な情報を扱う場面では、この問題が深刻です。専門の分野でLLMに相談する人が増えている今、信頼できる応答が求められる場面は確実に増えています。

こうした背景のもとで注目されているのが、LLMの「推論」です。いきなり答えを出すのではなく、いったん長い思考の流れを組み立ててから結論にたどり着くというアプローチです。最近では、この推論にかける時間や計算量を増やすことで精度を高めようとする動きが広がっています。

本記事では、「LLMは専門家が文章に付けた“どのくらい信頼できるか”という評価を、どれくらい正しく読み取れるのか」、さらに「推論に使う計算を増やせば、その精度は高まるのか」を確かめようとした研究を紹介します。

ここから限定コンテンツ

LLMは、あたかも確信をもって語っているかのような出力をすることがあります。しかしその「自信」が本物なのか、ただ言葉遣いがそう見せているだけなのかは、使う側にとって気になるポイントです。この問題に向き合ってきた、いくつかの試みを紹介します。

### 言い回しひとつで答えが変わる

「たぶんこうだと思う」と言うのと、「絶対にそうだ」と言い切るのとでは、受け手の印象がまったく異なります。人間なら当たり前のように感じ取れるこの違いですが、LLMにとってはそう単純ではありません。

ある調査では、質問応答のプロンプトに「確信していますか」といった強い表現を追加した場合、かえって正答率が下がることがわかっています。逆に、「思いますか」といった控えめな言い回しでは、より高い正答率を示しました。つまり、LLMは言葉に含まれる“自信のサイン”を、そのまま事実の確かさとして処理してしまうことがあるのです。

「強く言えば正しい」というわけではないということです。

### モデル自身に“確信度”を言わせる試み

この課題を踏まえ、「LLM自身に、自分の答えへの自信を言語で表現させる」取り組みも進められてきました。たとえば、回答とあわせて「高い自信があります」や「90％の確信があります」と出力させるようにする方法です。

こうした自己申告型の出力には一定の効果も見られましたが、一方で訓練データに強く引っ張られすぎるなどの弱点も明らかになっています。モデルが“学習した通りの言い方”をしているだけで、本当にそう思っている（=判断している）とは限らない、という問題が浮き彫りになっています。

### 自分が「知っていること」を自覚できるのか

LLMは大量のテキストから知識を獲得していますが、どの情報が“しっかり理解している内容”で、どれが“なんとなく見覚えがある”程度なのかを、自分で区別することはできるのでしょうか。

この点を探るため、モデルに「この答えはどれくらい正確か？」と自己評価させるタスクが行われた実験もあります。興味深いことに、例をいくつか見せてお手本を示すことで、モデルの自己評価の精度が大きく向上することが確認されています。

つまり、まったく見当外れな自信を持ってしまうこともある一方で、適切な指導があれば「正しさを見極める力」もある程度引き出せることが示唆されました。

### 学習データの中身がパフォーマンスを左右する

モデルの判断の背景には、学習時に取り込まれた情報が大きく影響しています。特定の話題について、どの資料を多く読んでいるかによって、得意・不得意が分かれることもあります。

たとえば、文章中の名前を隠してモデルに当てさせるようなタスクでは、訓練データに含まれている資料の頻度と正答率に強い関係が見られました。つまり、モデルが“よく目にした情報”には自信を持ちやすく、そうでない分野では判断も不安定になりがちです。

この知見は、「専門家のように信頼度をつける」タスクの難しさを物語っています。とくに科学的な内容のように、データとしての出現頻度が少ない情報では、LLMが正確な判断をするのは簡単ではありません。

### 専門の話題は、どこまで任せられる？

LLMに専門的な分野の情報を扱わせる際、モデルの出力がどれだけ正確か、どれだけ文脈に沿っていて、どれだけ不確かさをうまく伝えられているか、という観点での評価が行われます。「断言すること」よりも「不確かさを認めること」のほうが大切な場合もあるからです。

### 考える時間を増やせば、もっと正確になる？

最近注目されているアプローチに、「テストタイム・スケーリング」があります。これは、モデルが出力を出す際により多くの計算リソースや思考ステップを使うことで、答えの正確さを高めようとする方法です。

実際にこの手法を取り入れた最新モデルのなかには、科学的な質問に対して非常に高い精度で答えられるものも出てきています。モデルに時間をかけて考えさせることで、人間に近い判断ができるようになるのではないかと期待されているのです。

ただし、ここには一つの疑問が残ります。「時間をかけて考えた答えは、本当により自信をもって判断されたものなのか？」  
その疑問こそが、今回の研究が出発点としたテーマでした。

## 専門家の感覚をモデルに教える方法論

LLMに「これは自信を持って答えられる」といった判断をさせるには、人間の専門家がどのように確信を形成しているのかを、まずモデルに学ばせる必要があります。そのためには、専門家が実際に「どのくらい確かだと考えているか」を丁寧に記録したデータが欠かせません。

そうした専門家の判断を定量化した事例として、科学の分野が非常に参考になります。科学は、限られた証拠をもとに仮説を立て、不確実性を含みながらも「これは確からしい」と判断していく営みです。

この研究では、まず専門家が長年積み上げてきた「確信の物差し」をLLMに学習させ、そのうえで他の領域にも応用できるかを探っています。

### 気候科学から学ぶ、確信度という考え方

ひとつめに使われたのは、気候変動に関する国際的な報告書から作られたデータセットです。専門家が「この情報はどれくらい信頼できるか」を明示的に示した文章が集められています。

#### 「確信度」の表現ルール

気候変動の予測は、高い不確実性をともないます。そのため、IPCC（気候変動に関する政府間パネル）は、科学的な確信度をどのように表現するかをルールとして定めています。

確信度は5段階に整理されており、証拠の量や質、専門家の合意の強さなどをふまえて、次のように記述されます。

-   very low（非常に確信度が低い）
-   low（確信度が低い）
-   medium（中程度の確信度）
-   high（確信度が高い）
-   very high（非常に確信度が高い）

興味深いのは、「very low」と判断された内容は、そもそも最終的な報告書に載らないこともあるという点です。つまり、私たちが報道などで目にする情報の多くは、ある程度の確信が担保されたものになっているという背景があります。

#### 8,000件超の評価付き文書

こうした確信度ラベルを含む文章を集めて「CLIMATEX」というデータセットが作られました。IPCC第6次評価報告書の中から、合計8,094件の文章を抽出し、文末に確信度が明示されているものだけを選び出しています。

報告書のPDFを加工し、テキストを一文ずつ抽出・整形して、括弧付きで確信度が記載されたものだけをピックアップ。最終的に300件をテスト用に、残り7,794件を学習用として活用しました。

### 医療分野でも、専門家の確信を分類している

気候科学のような自然科学だけでなく、人間の健康に関わる分野でも、専門家による判断が明文化されています。そこで、がん研究の分野で使われてきた「発がん性のリスク評価」を使い、同様の仕組みがLLMに学習可能かを検証しました。

#### リスク分類

国際がん研究機関（IARC）は、物質や環境要因の発がんリスクを、次のように5段階で分類しています。

-   グループ1（人に対して発がん性がある）
-   グループ2A（おそらく発がん性がある）
-   グループ2B（発がん性の可能性がある）
-   グループ3（分類が難しい）
-   グループ4（おそらく発がん性はない）

この分類は1971年から一貫して続いており、長年にわたって積み上げられた知見が含まれています。研究では、IARCモノグラフから1,053件の評価を抽出し、CLIMATEXと同様に「ラベルを隠して予測させる」ベンチマークとして利用しました。

### モデルが「確信の感覚」を学ぶための土台が整った

こうして、気候と医療という異なる領域から、専門家の判断を数値的に捉えたデータが集まりました。どちらも[HuggingFaceで公開](https://huggingface.co/datasets/rlacombe/ClimateX)されており、研究で使われたコードも[GitHubで入手](https://github.com/rlacombe/LLM-Calibration)できます。

## LLMは専門家の確信度をどこまで読み取れるか

専門家が文章の中で示す「どれくらい確かな話なのか」という感覚を、LLMはどれほど正確に再現できるのでしょうか。この問いを掘り下げるために、研究チームは3つの視点から実験を設計しています。

### 検証の目的

実験では、次の3点を確認することが目的でした。

-   LLMは専門家のように確信度の表現を読み取れるのか
-   処理に使う時間や計算量を増やせば、確信度の判断がより正確になるのか
-   医療など他の専門領域でも同じような再現性があるのか

まず、気候変動に関する文章から「high confidence」などの確信度表現を伏せ、LLMにその部分を予測させるタスクを実施しました。次に、処理にかけるリソースを変えて、出力の精度にどう影響するかを観察しました。さらに、がんリスクに関する医学的な文章を使い、気候分野以外でも同様の効果が見られるかを確認しました。

### 確信度ラベルを選ばせるタスクを設定

LLMは、確信度の記述が省かれた文をもとに、あてはまるラベルを4つの中から選びます。

たとえば、元の文では「海面上昇は今世紀末まで続くと予想される（high confidence）」と書かれていましたが、モデルには「（？？？？）」の状態で与えられます。そこから「low」「medium」「high」「very high」のどれかを選ぶという形式です。

このタスクは、DSPyというフレームワークを用いて、さまざまなLLMに対して実行されました。

以下に本研究で用いた実験ワークフローを示します。

![[自信過剰になるLLM 長く考えさせることの副作用と検索機能が果たす役割 - AIDB/AIDB_94246_1-1024x242.png]]

実験ワークフロー LLMに対するマスク付き確信度予測タスクの処理手順

### 評価には3種類の数値指標を使用

モデルの出力を評価するために、次の3つの指標が用いられました。

-   正答率は、専門家の判断とどれだけ一致したかを示す
-   コーエンのカッパは、偶然の一致を除いたうえでの一貫性を測る
-   平均確信度は、モデルが選んだラベルの数値（0〜3）を平均し、専門家の平均値にどれだけ近づけたかを見る

さらに、統計的な信頼性を確保するため、すべての結果に95パーセント信頼区間がつけられました。

### 汎用LLMの性能を相対的に確かめる比較対象

LLMの実力を測るために、比較対象として2つの基準が用意されました。

-   大学を卒業した非専門家3人が同じタスクを実施した場合、正答率は36.2パーセント、コーエンのカッパは14.9パーセントという結果だった
-   気候科学の知識を中心に学習させた小型モデル（RoBERTa）は、正答率53.7パーセント、コーエンのカッパ38.3パーセントを記録した

### 結果の信頼性を高めるための工夫

実験の精度と公平性を担保するために、いくつか工夫が施されました。

-   モデルが適当に回答していないかを確かめるため、意味不明な文や風刺ニュースの文章も混ぜて提示した
-   モデルの判断に一貫性があるかを確認するため、温度設定をゼロにして同じ文を複数回試した
-   プロンプトの表現によるブレを抑えるために、言い回しを変えた複数のバージョンで同様の実験を繰り返した

## 実験結果が示した予想外の姿

### 専門家の確信度をある程度は再現できる

最初に問われたのは、LLMが専門家の判断を正しく読み取れるかという点です。

#### 最新モデルの実力

気候科学の文章を対象にした実験では、Gemini 2.5 Proが最も高い成績を出しました。正答率は48.7パーセントです。

ランダム回答の25パーセントや、大学卒業者3人による36.2パーセントを大きく上回り、一般人よりも専門家に近い感覚を持っていることが示されました。ただし、気候分野に特化して訓練されたRoBERTaの53.7パーセントには届かず、まだ差があることも明らかになりました。

|   分類    |                        モデル（原表記）                         | 正答率（Accuracy） | Cohen’s κ | バイアス（Bias） |   パラメータ等    |
|---------|---------------------------------------------------------|---------------|-----------|------------|-------------|
| 検索併用モデル |            Google Gemini 2.5 Pro with Search            |     89.3%     |   85.7%   |   +0.030   |     不明      |
| 検索併用モデル |           Google Gemini 2.5 Flash with Search           |     88.3%     |   84.4%   |   +0.097   |     不明      |
|  推論モデル  |                  Google Gemini 2.5 Pro                  |     48.7%     |   31.6%   |   +0.066   |     不明      |
|  推論モデル  |      Google Gemini 2.5 Pro — 一括処理（Bulk processing）      |     45.3%     |   27.1%   |   +0.353   |     不明      |
|  推論モデル  | Google Gemini 2.5 Flash — 最適な思考予算（Best thinking budget） |     45.0%     |   26.7%   |   +0.265   |     不明      |
|  推論モデル  |         OpenAI o3 — プログラム合成（Program synthesis）          |     40.7%     |   20.9%   |   +0.167   |     不明      |
| 非推論モデル  |                  Google Gemini 1.5 Pro                  |     45.0%     |   26.7%   |   +0.230   |     不明      |
| 非推論モデル  |                      OpenAI GPT-4o                      |     44.0%     |   25.3%   |   +0.283   |     不明      |
| 非推論モデル  |                      OpenAI GPT-4                       |     42.4%     |   23.2%   |   +0.197   |     不明      |
| 非推論モデル  |                  OpenAI GPT-3.5 Turbo                   |     39.7%     |   19.6%   |   +0.226   |     不明      |
| オープンソース |                  Meta Llama 3 8B Chat                   |     41.1%     |   21.5%   |   −0.001   |     8B      |
| オープンソース |               Mixtral-8x22B Instruct v0.1               |     38.1%     |   17.1%   |   +0.418   |    8×22B    |
| オープンソース |                  Meta Llama 3 70B Chat                  |     36.2%     |   14.9%   |   +0.444   |     70B     |
| オープンソース |               Mixtral-8x7B Instruct v0.1                |     35.9%     |   14.5%   |   +0.303   |    8×7B     |
| オープンソース |                Mistral 7B Instruct v0.3                 |     35.0%     |   13.3%   |   +0.423   |     7B      |
| オープンソース |                Google Gemma Instruct 2B                 |     33.9%     |   11.9%   |   +0.010   |     2B      |
| オープンソース |                Google Gemma Instruct 7B                 |     33.4%     |   11.2%   |   +0.305   |     7B      |
| ベースライン  |                   RoBERTa-Large（微調整）                    |     53.7%     |   38.3%   |     —      |    355M     |
| ベースライン  |                       非専門家（人間、3名）                       |     36.2%     |   14.9%   |     —      | 表示は60T（原表記） |

#### 検索による飛躍的な改善

検索機能を組み合わせると結果は劇的に変わりました。Gemini 2.5 Proは、正答率48.7パーセントから一気に89.3パーセントへ向上しました。Proに加えFlashも88.3まで上がりました。

つまり、LLMの確信度判断での課題は推論力ではなく、適切な情報にアクセスできるかどうかにあることが浮き彫りになったのです。

### モデルに共通する「自信過剰」

確信度を数値化して分析すると、多くのモデルが実際より高い確信を示す傾向が見られました。

たとえば、Llama 3 70B Chatは+0.444という強い過信バイアスを示しました。一方で、Llama 3 8B Chatは-0.001とほぼ中立で、規模が大きければ良いという単純な話ではないこともわかりました。

### 推論時間を増やした結果

#### 計算量を増やすと精度が低下することも

Gemini 2.5 Flashで推論に使うトークン数を増やす実験では、64〜192トークンまでは正答率が改善しましたが、それを超えると逆に下がり、768トークンでは35.7パーセントまで落ち込みました。医学分野のIARCデータセットでは、推論を導入した時点で精度が下がっています。

下の図はGemini 2.5 Flashで考える予算（トークン数）を変えたときの正答率の推移を示します。

![[自信過剰になるLLM 長く考えさせることの副作用と検索機能が果たす役割 - AIDB/AIDB_94246_2-1024x377.png]]

精度の変化と推論予算 Gemini 2.5 Flashにおける正答率の推移

また、同条件でのモデルの平均確信度（0.0〜3.0）を示し、推論予算増加で過信が強まる様子が読み取れます。

![[自信過剰になるLLM 長く考えさせることの副作用と検索機能が果たす役割 - AIDB/AIDB_94246_3-1024x384.png]]

確信度の変化と推論予算 Gemini 2.5 Flashにおける平均確信度の推移

#### 過信も強まる

推論時間を長くすると、誤った答えに強い確信を示す傾向も悪化しました。気候データセットでは過信が+6パーセントから+21.3パーセントに拡大。医学データセットでも+15.6パーセントから+35.6パーセントに上昇しました。

### 予想外の能力 プログラム生成

OpenAIのo3に300件の文をまとめて処理させたところ、直接分類を拒否し、代わりにPythonスクリプトを生成しました。このスクリプトは「very likelyがあればhigh confidence」などの言語的な手がかりで分類するもので、40.7パーセントの正答率を記録。非専門家を上回る結果を出しました。タスクが難しいときに自動でプログラム合成に切り替える能力があるのかもしれません。

### 長文をまとめて処理する利点

Gemini 2.5 Proの大きなコンテキストウィンドウを使い、300件の文を一度に入力した実験では45.3パーセントの正答率を達成しました。文ごとに処理するよりも良い成績で、CSV形式での整理された出力も得られました。長文処理やバッチ処理の有効性が示唆されています。

## まとめ

本記事では、LLMが専門家の「確信度」をどこまで再現できるのかを検証した研究を紹介しました。

結果として、LLMは非専門家よりも高い精度で判断できるものの、専門的に訓練された小型モデルには及ばないことがわかりました。また、長く考えさせることで性能が上がるという期待とは逆に、推論時間が長くなるほど過信が強まり、かえって精度が下がる傾向も見られました。

一方で、検索機能を併用すれば大きく精度が向上し、LLMにとって重要なのは「推論力」より「情報へのアクセス」であることが示唆されています。LLMを活用する際は、モデルに長く考えさせるよりも、必要な情報をどう提示するかを工夫することが効果的だといえそうです。

**参照文献情報**

-   タイトル：Don’t Think Twice! Over-Reasoning Impairs Confidence Calibration
-   URL：[https://doi.org/10.48550/arXiv.2508.15050](https://doi.org/10.48550/arXiv.2508.15050)
-   著者：Romain Lacombe, Kerrie Wu, Eddie Dilworth
-   所属：Stanford University
-   ICML 2025 Workshop on Reliable and Responsible Foundation Modelsに採択
