---
created: 2026-01-01T09:36:45 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/63749
author: AIDB Research
---

# 複数LLMに議論させ、「回答に自信がないときは発言を控えさせ」て応答品質を向上する方法 - AIDB

> ## Excerpt
> 知識は常に変化するため、どんなにLLMの知識を拡張しようと、欠落や古くなった情報が残ってしまう可能性があります。 既存手法は自己分析能力に欠け、データセットへの過度の依存があることから、今回ワシントン大学やUCバークレーなどの研究者らはLLM同士が互いの知識を検証する手法を提案しました。 3つのLLM、4つの質問応答タスクで実施した実験により、ベースラインに対して最大19.3%の精度向上を確認しま…

---
知識は常に変化するため、どんなにLLMの知識を拡張しようと、欠落や古くなった情報が残ってしまう可能性があります。

既存手法は自己分析能力に欠け、データセットへの過度の依存があることから、今回ワシントン大学やUCバークレーなどの研究者らはLLM同士が互いの知識を検証する手法を提案しました。

3つのLLM、4つの質問応答タスクで実施した実験により、ベースラインに対して最大19.3%の精度向上を確認しました。

![[複数LLMに議論させ、「回答に自信がないときは発言を控えさせ」て応答品質を向上する方法 - AIDB/AIDB_63749-1024x576.jpg]]

**参照論文情報**

-   タイトル：Don’t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration
-   著者：Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, Yulia Tsvetkov
-   所属：ワシントン大学, カリフォルニア大学バークレー校, 香港科技大学, カーネギーメロン大学

**本記事の関連研究**：

-   [LLMの内部状態を観察することで「出力がハルシネーションか否かを判別する」手法『LLMファクトスコープ』](https://ai-data-base.com/archives/61651)
-   [LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ](https://ai-data-base.com/archives/58767)
-   [LLMの出力から誤り（ハルシネーション）を減らす新手法『CoVe（Chain-of-Verification）』と実行プロンプト](https://ai-data-base.com/archives/55711)
-   [LLMに「自信の度合いに応じて説明のニュアンスを変更させる」ことで人間が過度に信頼するのを防ぐ](https://ai-data-base.com/archives/63482)
-   [LLMに自身のハルシネーション（幻覚）を「自覚」させ、減らす方法](https://ai-data-base.com/archives/55232)

## 背景

LLMは膨大な知識を保有していますが、知識が欠落していたりそもそも不正確な場合、誤った回答を生成してしまう恐れがあります。そんな事態を防ぐため、信頼度の低い回答は控える機能が必要だという見方があります。

そこで課題は、LLMが自身の知識不足をどのように特定できるのか、ということです。

既存の手法は、多くの場合、外部データや、LLMが自身を客観的に評価できるという仮定に依存しており、必ずしも正確な判断ができない場合があります。  
中でも、「単一のLLMが自分自身の回答を客観的に見直して誤りを正せる」といった考えは過度な期待に基づくものだと研究者らは指摘しています。

そこで今回、複数のLLMによって回答を客観視し、信頼性が低いときには回答を控えさせる手法が提案されています。誤解を招く情報生成を避けることで、システムの実用性を上げることにつながる手法です。

以下で詳しく紹介します。

ここから限定コンテンツ

### これまでの研究事例

LLMが自身の回答を見直すようにするアプローチはこれまでいくつかの方向から研究されています。

**1\. キャリブレーションベースのアプローチ**

LLMから信頼スコアを抽出し、その不確実さを評価するものです。問題点は、知識領域や推論状況によってキャリブレーションの品質が異なること、そしてデータセットへの依存が一般化を妨げることです。

**2\. プロンプティングベースのアプローチ**

LLMに指示を与え、自己反省を促すアプローチです。  
問題点としては、LLMが単純なプロンプティングで自己反省できるかは不明であること（元も子もないですね）、虚偽の相関の影響を受ける可能性があることです。

**3\. トレーニングベースのアプローチ**

訓練を通して、LLMに不確実さを認識させたり回答を控える能力を与えるアプローチです。問題点は、LLMの微調整に多大な計算資源が必要であること、そして訓練データのパターンや例に依存するため、一般化が難しいことです。

なおその他、LLMの隠れ表現を調べる研究や、指示の調整と整合性によるLLMの改善研究などがあります。

上記のような従来手法は、自己の生成テキストを評価・反映させるために単一の言語モデルを使用していますが、確認バイアスや幻覚の問題により信頼性の低い評価となってしまいます。さらに、外部データセットに依存している手法は、知識タスクや領域への一般化に弱いです。

## 複数モデルをコラボさせるアプローチ

従来手法の弱点を補うべく、複数のLLMが連携して生成テキストの真偽を評価し、回答を控えるかどうかを決定する手法が今回のアイデアです。異なるLLMはそれぞれ異なる知識領域を持ち、互いに補完し合える可能性があります。また、内部知識も多様化・特化させることができます。

![[複数LLMに議論させ、「回答に自信がないときは発言を控えさせ」て応答品質を向上する方法 - AIDB/AIDB_63749_1-1024x408.jpg]]

今回のアプローチでは、専門家のLLMに、LLMが提案した回答に対するフィードバックを生成させ、最終的に判断用のLLMが回答を控えるかどうかを決定します。

フィードバック生成には2つのモードがあります。

-   **セルフモード（COOPERATE）**：同じLLMを複数のドメインのエキスパートにし、自らフィードバックを生成します。
-   **競合モード（COMPETE）**：他のLLMを使用してフィードバックを提供し、互いの知識ギャップを特定します。

**セルフモードの流れ**

1.  まず、与えられた質問に対して、1つのLLMが回答を生成する
2.  次に、同じLLMに対して、異なるドメイン（factual information, commonsense knowledge, mathematical knowledgeなど）の専門家としてのロールを割り当てる
3.  各ドメインの専門家として、LLMは元の回答に対するフィードバックを生成する
4.  最後に、全てのフィードバックを総合して、同じLLMが最終的な判断（回答を採用するか棄却するか）を下す

**競合モードの流れ**

1.  質問と回答に対して、他のLLMにそれぞれ代替の回答とその回答に関する知識を生成させる
2.  矛盾情報付きのコンテキストを前置して質問にもう一度答えるようLLMに指示する
3.  LLMの（パラメータ由来の）知識に自信がある場合は、回答を受け入れる
4.  そうでない場合は、他のLLMによって生成された情報に影響を受けており、自信が低い可能性がある
5.  プロセスは全ての代替回答に対して繰り返され、回答が大部分で変更された場合、LLMは回答を控えるべきだと考えられる

## 実験

### モデル

Mistral-7B、LLaMA2-70B、ChatGPTの3種類の異なるLLMを使用して、提案手法を評価します。今回の手法は大小さまざまなモデルに対して有効であるべきだと考えられ、バリエーションのあるモデルが選ばれました。

### タスクとデータセット

下記4つのタスクとデータセットで評価されました。多様な知識領域を網羅しています。

1.  MMLU：57の分野にわたる一般知識QAのための多選択肢データセット
2.  Knowledge Crosswords：マルチホップ知識推論に焦点を当てた構造化QAデータセット
3.  Hellaswag：常識的知識と推論をテストする自然言語推論データセットです
4.  Propaganda：LLMに、内部知識に基づいて長いニュース記事から23のプロパガンダ手法を特定させるタスク

なお、実験によって発生する4つのシナリオを下の図に示します。

![[複数LLMに議論させ、「回答に自信がないときは発言を控えさせ」て応答品質を向上する方法 - AIDB/AIDB_63749_3-1024x933.png]]

### 評価指標

以下の評価指標を用います。

（１）信頼性の高い正解率（R-Acc）：信頼できるLLM生成回答の（回答を控えない）割合、つまり、答えた質問のうちいくつが正しいかを示します。

（２）効果的な信頼性（ER）：信頼性とカバー率のバランスをとります。つまり、すべての質問のうち、正しく回答した質問数が正しく回答できない質問数よりもどれだけ多いかを表します。

（３）回答棄権の正解率（A-Acc）：回答を控えることにした決定が正しいかどうかを評価します。

（４）回答棄権F1（A-F1）：精度（precision）と再現率（recall）の調和平均値。

## 実験結果

4つのタスクで3つのLLMを使用した回答棄権メカニズムのパフォーマンスを表1に示します

![[複数LLMに議論させ、「回答に自信がないときは発言を控えさせ」て応答品質を向上する方法 - AIDB/AIDB_63749_2-981x1024.png]]

今回試されたメカニズムは前述した通り、COOPERATEとCOMPETEの二つです。

COOPERATEは、ChatGPTのような強力な言語モデルと組み合わせることで、より良い結果を出す傾向がありました。複数ステップのフィードバック生成が、より強力な言語モデルを必要とするためと考えられます。

COMPETEは、信頼性の高い回答に重点を置いており、間違った回答を大幅に回避します。LLMは、最も自信が高い場合のみ元の回答に固執するため、COMPETEは信頼性が最も重要である場合に最適です。

どちらも従来の方式より高い精度で回答を控える能力を持ち、12の条件中9で高い成績を出しました。

### ベースラインと比較

3種類のベースライン（既存の）アプローチは、平均で0.595、0.576、0.553、0.475の回答棄権正解率スコアです。COOPERATEとCOMPETEは、それらのパフォーマンスを大幅に凌駕します。

### 回答を控える能力とベースモデルの能力

回答を棄権する機能は、ベースとなるLLMのもともとの能力と相関関係があることが示唆されました。

**mistral、llama2、chatgpt**は、平均0.524、0.537、0.616のA-Accスコアを達成し、標準ベンチマークにおける関連する優位性と類似しています。

![[複数LLMに議論させ、「回答に自信がないときは発言を控えさせ」て応答品質を向上する方法 - AIDB/AIDB_63749_4-1024x762.png]]

## さらなる実験と分析

### RAGでも有用

近年、LLMの知識を拡張するRAGが注目されています。しかし、外部情報には誤った情報も含まれているため、検索結果をそのまま利用すると、誤った回答に繋がる可能性があります。

そこで、本研究では、以下の2段階の回答棄権ベースのフレームワークが提案されています。

1.  回答棄権：まず、検索なしで回答棄権の判断を行います。LLM自身が回答に自信がない場合は、次に検索を行います。
2.  検索：検索結果に基づいて、再度回答棄権の判断を行います。LLMが検索結果に基づいても回答に自信がない場合は、検索が失敗したと判断します。

下の図は、このフレームワークがエラー率を最大56.6％低減できることを示しています。回答棄権メカニズムが検索失敗を特定し、誤った情報による回答を防ぐ効果があることを意味しています。

![[複数LLMに議論させ、「回答に自信がないときは発言を控えさせ」て応答品質を向上する方法 - AIDB/AIDB_63749_5-971x1024.jpg]]

### 多段回答棄権

Chain of Thoughtなどによる多段知識推論では、LLMの知識が特定の推論ステップで不足する場合があります。そこで、本研究では、回答棄権戦略が、LLMの知識の制限を特定し、回答を控えるべきステップを判断できるかどうかを調査しました。

K-Crosswordsデータセットを用いて、3段階の推論における回答棄権メカニズムの実験を行いました。結果として、COOPERATEとCOMPETEは、知識の制限を局所化する能力が高いことが示されました。ベースラインよりもエラー率を67.2%～81.2%低減しています。

![[複数LLMに議論させ、「回答に自信がないときは発言を控えさせ」て応答品質を向上する方法 - AIDB/AIDB_63749_6.png]]

### 回答棄権の確率

従来の回答棄権メカニズムは、回答するかしないかの二者択一でしたが、確率的に回答を棄権する手法も存在します。

MMLUデータセットを用いて、回答棄権確率の推定キャリブレーション誤差を計算しました。その結果、ASK CALIBRATE、COOPERATE、COMPETEが上位のアプローチであることが示されました。回答棄権確率の精度が、回答棄権のパフォーマンスと相関関係があることを意味しています。

![[複数LLMに議論させ、「回答に自信がないときは発言を控えさせ」て応答品質を向上する方法 - AIDB/AIDB_63749_7.png]]

### 精度と再現率

14種類の回答棄権メカニズムを比較した結果、回答範囲の広さを重視するものと、回答の信頼性を重視するものがあることが確認されました。

下の図は、各メカニズムの回答棄権精度と再現率を示しています。多くのメカニズムは再現率よりも精度が高く、回答範囲の広さを重視している傾向が見られます。一方、COMPETEは再現率がかなり高く、回答の信頼性を重視していることが分かります。

![[複数LLMに議論させ、「回答に自信がないときは発言を控えさせ」て応答品質を向上する方法 - AIDB/AIDB_63749_8-1024x286.png]]

## まとめ

本記事では、LLMが情報の不足により回答を控えるべき時を判断する新しい手法の研究について紹介しました。

研究者らは、複数のLLMが協力することで回答の控え方を改善するメカニズムを提案・評価し、既存の方法を上回る性能を示しています。

このようなアプローチが実装されることでLLMアプリケーションの実用性がさらに高まっていくかもしれませんね。

### ※プロンプティングベースの手法の補足

プロンプティングによってLLM自身が回答の正誤を判断させる手法について、もう少し詳しく紹介します。

外部情報が必要かどうか尋ねたり、生成した回答が選択肢と一致しているかどうか評価させます。そして判断結果をもとに、回答を控えるかどうかを決定します。

具体的には、以下の手法があります。

-   **自己評価（Self-Reflect）プロンプティング**：生成した回答が正しいか間違っているかを、LLM自身が選択させます。
-   **追加情報（More Information）プロンプティング**：回答に必要な情報が足りていない場合、外部情報を探すよう促します。
-   **生成と一致性評価（Generate and Match）プロンプティング**：選択肢を見ずに回答を生成させ、その回答が選択肢と一致しているか評価させます。

ただこれらの手法は、プロンプトの設計に依存しており、プロンプトエンジニアリングは複雑になる場合もあることに注意が必要です。

本記事の参照論文URL：[https://doi.org/10.48550/arXiv.2402.00367](https://doi.org/10.48550/arXiv.2402.00367)
