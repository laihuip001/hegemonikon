---
created: 2026-01-01T11:17:43 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/94949
author: AIDB Research
---

# 複数ターンで変わるLLMの振る舞い、タスクごとにどう違うか 安定性と崩壊の境目を探る - AIDB

> ## Excerpt
> 本記事では、複数ターンで変わるLLMの振る舞いと、その違いをタスク別に検証した研究を紹介します。 創造、コーディング、数学の三つの領域で、反復プロンプトが出力にどう影響するかを調べています。一見シンプルに見えるやりとりでも、モデルの挙動には意外な変化が現れることがあります。 本記事の関連研究 背景 LLMとのやりとりが一問一答ではなく複数回に及ぶ方は多いのではないでしょうか。対話を重ねながら少しず…

---
本記事では、複数ターンで変わるLLMの振る舞いと、その違いをタスク別に検証した研究を紹介します。

創造、コーディング、数学の三つの領域で、反復プロンプトが出力にどう影響するかを調べています。一見シンプルに見えるやりとりでも、モデルの挙動には意外な変化が現れることがあります。

![[複数ターンで変わるLLMの振る舞い、タスクごとにどう違うか 安定性と崩壊の境目を探る - AIDB/AIDB_94949-1024x576.png]]

**本記事の関連研究**

-   [LLMはなぜマルチターンの会話でつまずくのか　Microsoftなどが徹底分析　ユーザーに実用的なアドバイスも](https://ai-data-base.com/archives/89709)
-   [自信過剰になるLLM 長く考えさせることの副作用と検索機能が果たす役割](https://ai-data-base.com/archives/94246)

## 背景

LLMとのやりとりが一問一答ではなく複数回に及ぶ方は多いのではないでしょうか。対話を重ねながら少しずつ出力を改善していく使い方は広く定着しつつあります。その背景に、モデルが人の反応に応じて、柔軟に振る舞うよう調整されている仕組みがあります。

しかし、実際には、複数ターンの対話で性能が下がることもあると報告されています。何げない反復プロンプトが、かえって誤信や事実誤認を誘発することもあり、自己修正の不安定さが課題とされています。やりとりが長くなるほど、過去の履歴を適切に参照できず、自分自身の出力に振り回される「モデル崩壊」も懸念されています。

こうした状況を踏まえ、本記事は次の3点に注目した研究を取り上げます。他ターン会話による改善は言葉の使い方や指示の細かさにどれほど影響を受けるのか。どのような条件で効果を発揮し、どのような場面でうまく機能しなくなるのか。そして問題が起きた場合、どのモデルでも同じように崩れてしまうのか。

以下で詳しく見ていきましょう。

ここから限定コンテンツ

## 研究手法

### 創造・実装・推論を横断比較するタスクとデータセット

LLMの反復的な改善がどのように働くかを確かめるために、研究では3つの異なる認知タスクが選ばれました。いずれも50問ずつ用意されています。

**アイデア創出タスク**

LiveIdeaBenchから科学的アイデア生成の課題を使用しています。たとえば「海流を利用した新しい研究手法を考えてください」といった、正解のない創造的な問いが含まれます。

**コーディングタスク**

DS-1000というベンチマークからPythonライブラリを使った問題を選定。単体テストで正誤を客観的に評価できる、構造のはっきりした課題です。

**数学的推論タスク**

OmniMATHから難易度の高い問題（7/10以上）を使用しています。複数ステップにわたる複雑な推論が求められる内容です。

![[複数ターンで変わるLLMの振る舞い、タスクごとにどう違うか 安定性と崩壊の境目を探る - AIDB/AIDB_94949_1.png]]

### 12ターンで自己改善の軌道を追うプロトコル

人間とLLMの対話を模した自動化プロトコルが用いられました。

各タスクに対して12ターンの対話を行う設計です。おおむね10ターンを標準とする既存研究に2ターンを加え、後半の変化にも目を向けます。

手順はシンプルです。最初のターンでタスクが提示され、モデルが初期回答を出します。2ターン目以降は、前の自分の出力だけを見せられ、「改善してください」といった短い指示を受けて再生成を繰り返します。

ポイントは、モデルが各ターンで元のタスク文に戻れないことです。あえて制限することで、指示なしに出力を洗練させていく力や一貫性が問われる設計になっています。

### フィードバックの違いが改善の流れを変える

改善の指示がどのように影響するかを確かめるために、2種類の条件が設定されました。

ひとつは曖昧なフィードバックを与える条件です。モデルが意味の近い表現にどう反応するかを観察するため、以下のような指示を用いています。

-   「この\[アイデア/コード/解答\]はもっと良くなります。改善してください」
-   「この\[アイデア/コード/解答\]はもっと良くなります。より良くしてください」
-   「この\[アイデア/コード/解答\]はもっと良くなります。洗練してください」

もうひとつは明確な方向性を示すフィードバックです。各領域でしばしば対立するような目標を与え、出力の変化を比べました。

アイデア創出の場合は、「より斬新にしてください」と「より実用的にしてください」。  
コーディングでは、「速度重視でリファクタリングしてください」と「読みやすさ重視でリファクタリングしてください」。  
数学では、「各ステップを詳細に説明してください」と「別の解法を提示してください」といった指示が与えられました。

### 性質の異なる4つのモデルを比較

比較に使われたのは、設計や学習方針の異なる4つのLLMです。  
選ばれたのは、GPT-3.5-Turbo、Claude-Sonnet-4.0、Llama-3.1-8B-Instruct、GPT-OSS-20Bの4つです。  
いずれも、標準APIまたはHugging Faceのローカル環境を通じて実行しました。

すべてのモデルで温度は0.7、最大トークン数は10,000に統一しています。創造性と一貫性のバランスを取るための設定です。

### 多面的に評価

複数ターンのやりとりによる出力の変化を捉えるため、3つの観点から評価が行われました。ひとつは客観的な成果を測る指標、もうひとつは行動の変化をとらえる指標、そしてもうひとつが意味的な品質に関する評価です。これらを組み合わせることで、モデルの改善プロセスを多面的にとらえることを目指しています。

### 客観的な成果を数値で測る

数学とコーディングの2領域では、各ターンにおける出力の正確さや論理性が自動で評価されました。

コーディングタスク（DS-1000）では、出力されたコードを仮想環境で実行し、テストケースに正しく通るかどうかを判定しています。

数学タスク（OmniMATH）では、12ターンの出力を評価用のLLM（Gemini）に渡し、ターンごとに2種類のスコアを得ました。ひとつは最終回答が正解と一致しているかどうかを示す二値のスコア、もうひとつは推論全体の妥当性を1〜10の段階で示すスコアです。

### 出力内容の変化を捉える

モデルがどのように改善を進めたかを把握するため、意味的な変化を測る2つの指標が使われました。

ひとつ目は、出力が初回のアイデアからどれだけ離れたかを示す「原点からの逸脱」です。ふたつ目は、直前のターンからどれだけ変化したかを測る「ターン間変動性」です。

これらは、各ターンの出力をベクトルに変換し、類似度を使って計算しています。変換には、精度と軽さのバランスを考慮してQwen3-Embedding-0.6Bが使われました。

### 創造性の持続力と出力の膨張を測定

モデルが新しい表現をどれだけ出し続けられるかを見るための指標が導入されました。過去のターンに登場していないフレーズがどの程度含まれているかを測るためです。2-gramと3-gramの両方を組み合わせて算出されています。

また、出力が無駄に長くなっていないかも追跡されました。アイデア創出や数学では単語数、コーディングではコード行数を用いて、初回からの相対的な増加量を指標としています。

### 意味的な品質をLLMで評価

出力の質を細かく見ていくために、Gemini 2.5 ProというLLMを判定者として用いました。いわゆる「LLM-as-a-Judge」と呼ばれる手法で、人間による評価に代わるものとして使われています。

各領域に応じてスコア項目が用意されており、たとえばアイデア創出では新規性と実現可能性、コーディングでは実用性と読みやすさ、数学では論理の正しさと説明の明確さが評価されています。

この方法は、専門家の評価と高い一致率を示すことが確認されており、今回選ばれたGemini 2.5 Proも、実験当時の性能比較で上位に位置するモデルでした。

## 実験結果

3つのタスクでLLMがどのように振る舞ったのかを見ていきます。モデルごとに傾向の違いがはっきりと現れました。

### アイデア創出では、探索スタイルにモデル差

創造的な課題では、モデルによって探索の広がりや持続力に大きな違いが見られました。

#### 広がりのある探索ができるモデルとできないモデル

新規性を重視する指示では、ClaudeとGPT-OSS-20Bが初期のアイデアから大きく離れた提案を出し続けました（最終逸脱スコア：Claude 0.734、GPT-OSS-20B 0.657）。GPT-3.5も似た傾向を示しましたが、条件によって制限されることもありました。

一方で、Llama-3.1-8Bは最初の発想に固執し、ほとんど変化しない出力を繰り返しました（逸脱スコアは0.384にとどまります）。

#### 続けて新しい表現を出せるかどうか

語彙レベルの新規性でも差が出ました。ClaudeとGPT-OSS-20Bは12ターン目でも高い新規性を保ち（Claude 0.843、GPT-OSS-20B 0.812）、粘り強く創造的な表現を続けました。  
Llama-3.1-8Bはすべての条件でスコアが0.084未満に落ち込み、同じ表現を繰り返す傾向が強まりました。

#### 長く書いても創造性があるとは限らない

回答の長さと創造性は一致しませんでした。Llama-3.1-8Bは最も長いテキストを生成した一方で、新規性スコアは最低。逆に、ClaudeやGPT-OSS-20Bは控えめな分量で高い新規性を維持しました。

#### 指示の違いが創造の質に影響する

新規性を求める指示は、初期ターンで独創的な提案を引き出す一方、後半では実現可能性が下がる傾向がありました。  
対照的に、実用性を求める指示では、Claudeが8ターン目で明確性と実現可能性のスコアで高得点を獲得し、堅実なアイデアに導かれていきました。

### コーディングでは、初期ターンで決まる

コーディングの課題では、モデルの多くが最初の数ターンで成功・失敗が決まるパターンを示しました。

#### 最初の数ターンが勝負

ClaudeやGPT-OSS-20Bは、1ターン目で最も高い正解率を記録しました。その後はターンを重ねるごとに正解率が落ち、4ターン目にはほぼゼロになることもありました。

GPT-3.5やLlama-3.1-8Bも同様の傾向を示し、初期にうまくいかなかった場合、その後の反復では改善が見られませんでした。

#### 明確な指示が品質を支える

精緻化を促す指示は、出力の論理性をやや向上させました。Claudeでは健全性スコアが5.00から5.19に上昇。  
可読性を重視する指示はコードの読みやすさを維持し、GPT-OSS-20Bではスコアが8.64から9.10に上がりました。

一方で、性能を優先する指示は逆効果になることがありました。Claudeでは実用性と可読性のスコアが大きく落ちています（9.34→2.44）。

#### 固定されたロジックと際限ない肥大化

ターンを重ねるごとに、モデルの出力は変化しにくくなり、ひとつの解法に固着する傾向が見られました。Llama-3.1-8Bはとくにこの傾向が強く、出力の類似度スコアは0.741まで上昇しました。

一方で、コードの長さは抑えられず、ClaudeとGPT-OSS-20Bは曖昧な指示のもとでそれぞれ40倍、34倍に膨張。正確性や新規性が崩れても、出力だけが増えていく退化的なループが観察されました。

### 数学では、遅れて成功するケースも

数学的推論では、モデルは最初から安定しすぎる傾向がありました。しかし、反復を重ねることで後半に大きな改善が現れる場面もありました。

![[複数ターンで変わるLLMの振る舞い、タスクごとにどう違うか 安定性と崩壊の境目を探る - AIDB/AIDB_94949_2.png]]

数学タスク分析の一部で、Llama-3.1-8Bが 12ターンの反復でどのように振る舞ったかを、4つの指標で示したサブグラフ

#### 推論は安定、ただし変化しにくい

語彙的新規性は早い段階でほとんど失われ、Llama-3.1-8Bは12ターン目に新規性スコア0.010まで低下しました。  
ターン間の出力の変化も乏しく、ほとんどのモデルで早期に変動スコアが0.05以下になりました。

#### 深い反復が後半の正解を導く

こうした安定状態のなかでも、反復を重ねることで新たな正解が見つかることがありました。OmniMATH全体の評価では、多くの正解がターン8〜12で現れています。

Claude-Sonnet-4.0は、12ターンで正解率が32.4%から45.2%に上昇。Llama-3.1-8Bも、6.9%から40.5%へと大幅に改善しました。数学のようなタスクでは、反復が「発見」に直結する場合があると示されています。

#### 「詳細化」指示が突破のカギ

成功を導いたのは、推論を深める「詳細化」プロンプトでした。Claudeは76%、Llamaは82%、GPT-OSS-20Bは74%と、高い正解率を記録しました。

対照的に、「別の方法を出す」といった探索的な指示は効果が低く、正解率が20%未満で停滞するケースが多く見られました。

なお、詳細化プロンプトは出力を大幅に長くする傾向がありました（GPT-OSS-20Bで37.8倍）。しかし数学においては、こうした冗長性が有効に働いたと考えられます。

## 反復は「収束、逸脱、崩壊」の流れをたどる

今回の実験では、LLMが指導なしで改善を繰り返すと、一定のパターンに沿って動くことが見えてきました。  
多くの場合、「収束 → 逸脱 → 崩壊」という3段階の流れが現れます。

最初は妥当な出力に向かって収束し、その後しばらくは方向性の異なるアイデアを模索します。やがて意味のない反復や、過度に複雑な出力が続くようになり、改善としての機能を失います。

崩壊のパターンはタスクによって異なります。  
創造タスクでは似たようなアイデアの繰り返し、コーディングでは複雑性の暴走、数学では誤った推論に対する過信が典型的です。  
タスクの性質がモデルの振る舞いに大きく影響していると考えられます。

### 単一モデルの反復ループには限界がある

単一モデルに曖昧な指示を繰り返すだけでは、安定した改善は望めません。  
とくに複雑な作業では、この構成では破綻しやすく、再現性も確保しづらくなります。

より信頼性の高い挙動を目指すなら、モデルに役割を持たせて、目的に応じて使い分ける設計が有効です。  
同じモデルを繰り返し回すより、場面に応じて切り替える方が安定します。

### 領域ごとに異なる進め方が合う

タスクの性質に応じて、反復の活かし方にも違いが出てきます。

#### 創造タスクは「拡げてから絞る」

まずは新しい発想を広く出せるモデルを使い、幅を出します。  
その後、出力が停滞したり方向性がブレ始めたタイミングで、整理や改善に強いモデルに切り替えると、実用的でまとまりのある内容になりやすくなります。

#### 数学では「後半に深掘りのチャンス」

初期に正解が出なくても、後のターンで改善されるケースが多く見られます。  
詳細化を促すプロンプトを後半に入れることで、論理を深めて正答にたどり着くルートが開けます。

#### コーディングは「早期に見極め、だめなら止める」

成功する場合は最初の数ターンで正解にたどり着くことが多いです。  
うまくいかない場合は、曖昧な反復を続けるより、早めに停止または初期化してやり直す方が効率的です。

### 実装面では技術的な課題もある

複数モデルや役割分担の導入には、いくつかの技術的なハードルがあります。

#### 挙動の監視とタイミングの判断が必要

モデルの出力傾向をリアルタイムで把握するには、行動指標の監視システムが欠かせません。  
さらに、どのタイミングでモデルやプロンプトを切り替えるかを判断するロジックも必要になります。

#### 文脈の受け渡しやコストの制約も無視できない

モデルを切り替える際には、やり取りの履歴や意図をうまく引き継ぐ仕組みが必要です。  
また、同時に複数のモデルを動かす構成が、応答時間やコスト面で現実的かどうかも慎重に検討する必要があります。

### 小さな役割分担でも効果は出る

反復という手法は、それ単体で万能ではありません。  
効果を出すには、タスクごとの特性やプロンプト設計とセットで考える必要があります。

たとえば創造タスクであれば、アイデアを広げる生成者と、それをまとめる改良者という2つの役割を設けるだけでも出力は安定します。  
大がかりなシステムにしなくても、こうした小さな工夫から始めることで、より良い反復ループがつくれるはずです。

## まとめ

本記事では、複数ターンでの反復的なやりとりに関する研究を紹介しました。

調査では、出力が「収束、逸脱、崩壊」という流れをたどる傾向があることが明らかになりました。タスクの種類や指示の内容によって、改善の効果にも大きな差が出ることが確認されています。単一モデルに曖昧なフィードバックだけを繰り返す設計では、信頼性が下がりやすい点も示唆されました。

目的に応じてモデルの役割を分け、反復の止めどきを見極めたりする工夫から試してみるとよさそうです。

**参照文献情報**

-   タイトル：Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting
-   URL：[https://doi.org/10.48550/arXiv.2509.06770](https://doi.org/10.48550/arXiv.2509.06770)
-   著者：Shashidhar Reddy Javaji, Bhavul Gauri, Zining Zhu
-   所属：Stevens Institute of Technology, Meta
