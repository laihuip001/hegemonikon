---
created: 2026-01-01T11:17:12 (UTC +09:00)
tags: []
source: https://ai-data-base.com/archives/96152
author: AIDB Research
---

# 長文脈タスクでもLLMの精度を下げないための対策 - AIDB

> ## Excerpt
> 最新のLLMは、数十万トークン、場合によっては百万トークン以上の長い文章を処理できると謳われています。しかし実際のところ、長い文章を入れると期待したほどの精度が出ないという問題が、多くの現場で観察されてきました。その理由は何なのか。そして、精度を改善するための工夫とは？綿密な実験に裏付けられた提案を見ていきましょう。 本記事の関連研究 背景 最近のLLMは、一度に処理できる文章の長さがどんどん長く…

---
最新のLLMは、数十万トークン、場合によっては百万トークン以上の長い文章を処理できると謳われています。しかし実際のところ、長い文章を入れると期待したほどの精度が出ないという問題が、多くの現場で観察されてきました。その理由は何なのか。そして、精度を改善するための工夫とは？綿密な実験に裏付けられた提案を見ていきましょう。

![[長文脈タスクでもLLMの精度を下げないための対策 - AIDB/AIDB_96152-1024x576.png]]

**本記事の関連研究**

-   [長文コンテキスト処理はRAGを進化させるのか？最新モデル20種類での実験結果](https://ai-data-base.com/archives/79561)
-   [RAGにおいて長文を検索することで一気に効率を上げるアプローチ『LongRAG』](https://ai-data-base.com/archives/71774)
-   [Googleが開発した「LLMに長文を高精度で読解させる方法論」と実行プロンプト](https://ai-data-base.com/archives/68821)

## 背景

最近のLLMは、一度に処理できる文章の長さがどんどん長くなってきています。例えば、Claudeは20万トークン以上、つまり本数冊分の文章を一度に読み込めますし、Geminiというモデルに至っては百万トークンを扱えると報告されています。ちなみにトークンというのは、だいたい単語の一部や単語そのものに相当する単位です。

LLMが長い文章を処理できるようになったことで、ユーザーは大きな期待を抱きました。複数の本にまたがる情報を統合したり、プログラムのコード全体を理解したり、長時間の会話履歴を把握したりできるのではないかと考えたわけです。しかし実際には、長い文章を読み込めることと、その文章を使って問題を解決できることは、必ずしもイコールではないということが分かってきました。

つまり、”仕様上”長い文章を扱えるからといって、本当に長い文章をいれても、精度が良くない場合があるのです。

この問題について、原因が考えられてきました。そして「LLMが長い文章の中から必要な情報を見つけ出せないから、問題が解けないのだ」と推測されてきました。

しかし、実際のところは少し事情が違いそうです。

以下ではLLMの長文脈処理で精度が落ちる原因の詳しい分析と、実際に有効さが確かめられた対策案について取り上げていきます。

ここから限定コンテンツ

**参照文献情報**

-   タイトル：Context Length Alone Hurts LLM Performance Despite Perfect Retrieval
-   URL：[https://doi.org/10.48550/arXiv.2510.05381](https://doi.org/10.48550/arXiv.2510.05381)
-   著者：Yufeng Du, Minyang Tian, Srikanth Ronanki, Subendhu Rongali, Sravan Bodapati, Aram Galstyan, Azton Wells, Roy Schwartz, Eliu A Huerta, Hao Peng
-   所属：University of Illinois at Urbana-Champaign, Amazon.com Inc., USC Information Sciences Institute, Argonne National Laboratory, The Hebrew University of Jerusalem, University of Chicago

## 長文脈処理の失敗原因を探る

LLMが長文脈処理で精度を落とす理由は何か？

もしかしたら、文脈内検索の失敗や無関係な情報による妨害以外にも、見落とされている要因があるのではないか。

そこで研究者らは、文章の長さ、トークンレベルでの妨害、タスクの複雑さといった要因を一つ一つ分離して調べるべきだと考えました。このような分析はこれまでほとんど行われてこなかったのですが、モデルの真の限界を理解し、今後の改善の方向性を示すためには不可欠です。

モデルが必要な情報すべてに完璧にアクセスできる状態にあるとき、それでもなお問題を解けないとしたら、その原因は何なのか。これまで見過ごされてきた要因を明らかにしようとしました。

### さまざまなタスクベンチマークを用意

研究者たちは、LLMの多様な能力を調査するために、4種類のタスクを用意しました。どれも、実際のビジネスシーンで頻繁に使われるタイプの作業です。

一つ目は変数和問題。

これは研究者たちが独自に作った非常にシンプルなタスクです。50個の整数変数に値が与えられていて、そのうちランダムに選ばれた3つの変数の合計を求めるというものです。例えば、「x1=100、x2=50、x3=75…」と50個の変数があって、「x3、x15、x42の合計は？」と聞かれるイメージです。これは本当に基本的な算数ですが、だからこそ、モデルの基礎的な処理能力を測るのに適しています。実務で言えば、大量のデータから特定の値を拾って簡単な計算をするような場面に相当します。

二つ目は数学問題。

使用されたのはGSM8Kというデータセットで、これは小学校高学年レベルの算数の文章題です。例えば「リンゴが何個あって、何個食べたら残りは何個か」といった問題です。ただし、このデータセットには解答の過程、つまりステップバイステップの考え方も含まれています。これは実務で、手順を追って論理的に考える能力に関連します。

三つ目は質問応答。

MMLUというデータセットが使われました。これは大学レベルの知識を問う多肢選択問題で、歴史、科学、法律など、57の分野をカバーしています。問題文があって、4つの選択肢から正解を選ぶ形式です。企業での知識ベースシステムや、専門的な質問に答えるチャットボットのようなユースケースに近いですね。

四つ目はコード生成タスク。

HumanEvalデータセットが使われています。これは関数の定義とドキュメントが与えられて、その仕様に従ってPythonのコードを書くというものです。例えば、「この関数は文字列のリストを受け取って、長さでソートして返す」という説明があって、実際にその関上手くいっているかです。開発現場でLLMにコードを書かせるシーンそのものですね。

### 長文処理能力を確かめるために「無関係な情報」を混入させた

長文処理能力を測定するため、研究者たちは各問題を意図的に長くする工夫をしました。具体的には、それぞれの問題を「証拠」と「質問」の二つの部分に分けて、その間に無関係な文章を大量に挿入したのです。

証拠というのは、問題を解くために必要なすべての情報を含む部分です。

無関係な文章としては、ポール・グラハムという技術系エッセイストのエッセイを挿入しました。このエッセイは、問題とは全く関係のない内容ですが、それなりに知的で読みごたえのある文章です。つまり、単なるランダムな文字列ではなく、意味のある文章を妨害要素として使っているわけです。

最終的な構造は「証拠→無関係なエッセイ→質問」という形になります。証拠は文章の最初に、質問は最後に配置されています。

### 文脈内検索がうまくできているか測定

次に、検索性能の測定方法について説明します。ここが、この研究の最も厳密な部分の一つです。研究者たちは、モデルに質問し、元の文章と完全一致するかどうかで評価しました。完全一致というのは、1文字、1スペース、1句読点に至るまで、完璧に同じでなければならないということです。

これは非常に厳しい基準です。実際の業務では、多少表現が変わっても、内容さえ正しく理解していれば問題ないことが多いですよね。例えば、「リンゴが5個ある」を「5つのリンゴがある」と言い換えても、意味は同じです。しかし、この研究ではあえて最も厳しい基準を採用しています。

### 実験の結果、検索成功率90%超でも問題解決能力は最大59ポイント低下

実験には、Llama-3.1-8B InstructとMistral-v0.3-7B Instructという2つのモデルが使われました。前者は12万8千トークン、後者は3万2千トークンまで処理できると謳われている、長文処理能力を持つモデルです。どちらも短い文章では優れた性能を示し、様々な用途でファインチューニング、つまり追加学習のベースとして使われている人気のモデルです。

結果を見てみましょう。グラフを見ると、検索性能（Retrieval）と問題解決性能（Accuracy）の間に、明確で大きな乖離があることが分かります。

![[長文脈タスクでもLLMの精度を下げないための対策 - AIDB/AIDB_96152_1.png]]

まず検索性能についてです。3万トークンまでは、比較的安定していました。特に1万5千トークンより短い入力では、両方のモデルとも、8.2%以下の問題でしか検索に失敗していません。つまり、91.8%以上の問題で、証拠と質問を1文字も間違えずに取り出せているのです。これは、モデルが必要な情報にきちんとアクセスできていることを意味します。

しかし、問題を解く精度は、全く異なる様相を呈していました。すべてのタスクと文章の長さで、検索性能の低下よりもはるかに大きな精度低下が観察されたのです。

これは何を意味しているのでしょうか。

モデルは必要な情報、つまり関数の仕様書を以前よりも正確に取り出せているのに、その仕様に従ったコードを書く能力は失われているのです。

特に注目すべきは、大幅な性能低下が7千トークンという比較的短い範囲で起きているということです。7千トークンは、日本語なら原稿用紙で20枚程度、英語の長文記事数本分といったところでしょうか。これは、どちらのモデルも公式に処理できると謳っている範囲内です。しかも、検索性能が本格的に劣化し始めるよりもずっと前から、問題解決能力は落ち始めているのです。

### 「邪魔な情報」の影響は少ない

検索が成功していても問題解決能力が大きく低下することが分かりました。しかし、ここで一つ疑問が残ります。もしかしたら、無関係な文章が、モデルの注意を逸らして推論を妨げているのではないか、という可能性です。無関係な情報がたくさんあると、人間でも気が散って集中しにくくなりますよね。

研究者たちは、さらに条件を厳しくした実験を行いました。無関係な情報による妨害という要因も排除して、それでも性能が落ちるのかを確かめようとしたのです。

最初に、無関係な文章を最小限の妨害しか持たない要素に置き換えました。ポール・グラハムのエッセイをすべて空白文字、つまりスペースに置き換えたのです。

まず、Llamaモデルと Mistralモデルでの結果を見てみましょう。確かに、エッセイを挿入した場合と比べると、全体的に改善が見られました。しかし、その改善は限定的でした。依然として、大幅な性能低下が観察されたのです。

次にクローズドソースモデル、つまり商用モデルでの結果を見てみましょう。テストされたのは、GPT-4o、Claude-3.5-Sonnet、Gemini-2.0という3つのモデルです。これらの大規模商用モデルは、オープンソースのモデルとは異なるパターンを示しました。全体的に、より堅牢で、長さによる性能低下が小さい傾向がありました。

例えば、変数和という単純なタスクでは、GPT-4oとGemini-2.0は、3万トークンの空白を挿入しても、完璧な100%の性能を維持しています。これは素晴らしい結果です。しかし、すべてのタスクでこうした堅牢性があるわけではありません。

MMLUという質問応答タスクを見ると、興味深い結果が出ています。Claude-3.5は、3万トークンで実に67.6ポイントもの大幅な低下を示しています。元々82.2%だった精度が、14.6%まで落ちているのです。これは、最先端のモデルでさえ、特定のタスクでは長さの影響を大きく受けることを示しています。

一方で、Geminiでは面白い現象が見られました。GSM8Kの数学問題で、3万トークンの入力において、性能が6.2ポイント向上しているのです。短い入力よりも、長い入力の方が良い結果を出しているわけです。HumanEvalのコード生成でも、同様の改善が一部見られました。これは、モデルによっては、ある程度の長さまでなら、追加のコンテキストがプラスに働く場合もあることを示唆しています。

ただし、全体として見ると、ほとんどのモデルとタスクの組み合わせで、一貫した性能低下が観察されています。たとえ優れた商用モデルであっても、長さそのものが性能に影響を与えるという基本的な傾向からは逃れられないようです。

### 質問と参照情報を隣接させてもあまり意味なし

以上の実験では、証拠は文章の最初に、質問は最後に配置されていました。その間に大量の関係ない文脈があるため、証拠と質問の距離が非常に遠くなっています。

以下のようなイメージです。

```
[証拠]（参照すべき情報）

[大量の関係ない文脈]
～～～～
～～～～

[質問]（～～ですか？）
```

研究者らは「もしかしたら、証拠と質問が離れているから性能が落ちるのではないか」と考え、証拠と質問を隣接させた実験を追加で行いました。

つまり文脈をこのように配置換えしたのです。

```
[大量の関係ない文脈]
～～～～
～～～～

[証拠]（参照すべき情報）

[質問]（～～ですか？）
```

しかし、結果は依然として、大幅な性能低下が観察されました。

Mistralモデルでは、3万トークンで最大17ポイントの低下が見られました。Llamaモデルでは、最大20ポイントの低下です。確かに、証拠が最初にある場合と比べると、若干の改善が見られるケースもありますが、依然として無視できない低下が残っています。

この結果が示すのは、証拠と質問の相対的な距離だけが問題なのではない、ということです。たとえ証拠と質問を隣接させても、入力全体が長ければ、性能は低下するのです。

### 長さそのものが原因

研究者たちは究極の実験を行いました。すべての妨害要素を完全にマスクする、つまり見えなくしてしまうのです。

マスクというのは、技術的には、モデルの注意機構、アテンション機構と呼ばれる部分で、特定のトークンを無視するように設定することです。少し専門的になりますが、LLMは入力を処理する際、各トークンが他のどのトークンに注目すべきかを計算します。マスクを使うと、「この部分には絶対に注目しないでください」と指示できるのです。

この実験では、証拠と質問の間にある大量の空白トークンをすべてマスクしました。モデルからすると、入力は「証拠→質問」だけに見えます。内容的には、短い入力の場合と全く同じです。唯一の違いは、証拠と質問の間に、見えないけれど存在する大量のマスクされたトークンがあることだけです。

もし性能低下の原因が、無関係なトークンに注意が向いてしまうことだとすれば、完全にマスクすれば性能は完全に回復するはずです。しかし、驚くべきことに、それでも性能低下が観察されたのです。

これが意味するのは、性能低下の原因は、注意が逸れることではない、ということです。入力の長さそのもの、つまりトークン数の総量が、モデルの処理能力に何らかの制約を課していることを強く示唆しています。内容がどうであれ、見えようが見えまいが、入力が長いというだけで、モデルの推論能力、計算能力が低下するのです。

この発見は、RAGシステムの設計や、長い会話履歴を持つチャットボットの実装、大量のドキュメントを分析するシステムの構築など、あらゆる長文処理アプリケーションに影響します。単に情報を正しく見つけるだけでなく、その情報をいかに簡潔に、短くモデルに渡すかが、重要になってくるのです。

入力が長いというだけで性能が落ちる。これは非常に根本的な問題です。しかし、この発見は同時に、一つの明確な仮説を導きます。それは、入力トークン数を制限すれば性能を改善できるのではないか、というものです。

考えてみれば、これは論理的な帰結です。長さが問題なら、短くすれば良い。しかし長い文書から必要な情報を見つけ出す必要があります。どうすれば、長い入力を扱いながら、実質的には短い入力として処理させることができるのでしょうか。

### 「検索してから推論」という二段階アプローチ

研究者たちが提案したのは、非常にシンプルで直感的な戦略です。

長文処理タスクを二つの明確なステップに分割することです。第一ステップで検索を行い、第二ステップで問題解決を行う。そして重要なのは、この二つのステップで、LLMに渡す入力を完全に入れ替えることです。

具体的なプロセスを説明しましょう。

まず第一ステップでは、長い入力文書全体をLLMに渡して、「この中から関連する情報を見つけて、そのまま書き出してください」と指示します。LLMは文書を読んで、必要な証拠を抽出し、それを出力します。

次に第二ステップでは、まったく新しいプロンプトを作ります。先ほどLLMが出力した証拠だけを取り出して、それと元の質問を組み合わせます。そして、この短い新しいプロンプトを使って、LLMに問題を解かせるのです。このとき、元の長い文書は入力に含まれません。LLMからすると、まるで新しい会話を始めたかのように見えます。

![[長文脈タスクでもLLMの精度を下げないための対策 - AIDB/AIDB_96152_2.png]]

### ベンチマークで最大31.2%の改善

では、この戦略は実際にどれくらい効果があるのでしょうか。

Mistralを使用した結果を見てみましょう。入力が長くなっても、性能の低下が大幅に抑えられました。3750トークンで71.4%、7500トークンで66.7%、15000トークンで69.1%、26250トークンでも66.7%を維持しています。

重要なのは、提案手法の性能が、入力の長さによってあまり変動していないことです。短い入力での76.2%と比べても、26250トークンでの66.7%は約10ポイントの差に収まっています。これは、長文処理タスクを実質的に短文処理タスクに変換できていることを示しています。

実務的に考えると、これは非常に有望な結果です。プロンプトの工夫だけで、大幅な性能改善が得られるのです。開発コストも運用コストも抑えられます。

ただし、完璧ではないことにも注意が必要です。0トークンの76.2%と比べると、依然として性能は若干低下しています。これは、検索ステップで情報の一部が失われたり、二段階に分けることによる何らかのオーバーヘッドがあったりする可能性を示唆しています。それでも、ベースラインと比べれば、はるかに良い結果です。

GPT-4oを使用した実験でも、もともと高精度だったタスクがさらに改善されるという結果が得られています。

## 精度改善の意義

ビジネスの現場では、こうした精度改善が重要になることがあります。

例えば、カスタマーサポートのチャットボットで、回答精度が上がれば、誤回答による顧客不満が減り、サポートコストも削減できます。法律文書の分析で精度が上がれば、弁護士の確認作業が大幅に軽減されます。医療文献の検索では、改善が見落とされていた重要な情報の発見につながるかもしれません。

ただし、注意点もあります。この方法は、LLMを二回呼び出すため、APIの使用コストが二倍になります。また、処理時間も長くなります。ですから、コストと性能のトレードオフを考慮する必要があります。高精度が求められる重要なタスクでは有効ですが、すべての場面で使うべきというわけではありません。

また、この方法が有効なのは、モデルがある程度の検索能力を持っている場合です。もし第一ステップの検索で失敗すれば、第二ステップでいくら短い入力にしても、正しい答えは出せません。

## まとめ

今回得られた知見と提案手法は、RAGシステム、チャットボット、文書分析ツール、コードアシスタントなど、様々な長文処理アプリケーションの性能改善に応用できる可能性があります。シンプルだからこそ、広く実務に取り入れやすい手法と言えそうです。
