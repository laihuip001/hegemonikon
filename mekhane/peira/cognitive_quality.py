#!/usr/bin/env python3
# PROOF: [L2/æ¤œè¨¼] <- A0â†’èªçŸ¥å“è³ªã®å®šé‡æ¸¬å®šãŒå¿…è¦â†’n=1ã§ã‚‚ãƒ‡ãƒ¼ã‚¿ã§èªã‚‹
"""
Cognitive Quality Dashboard â€” èªçŸ¥å“è³ªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

Handoff ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ 4 æŒ‡æ¨™ã‚’é›†è¨ˆã—ã€HGK ã®å®Ÿç”¨æ€§ã‚’å®šé‡çš„ã«è©•ä¾¡ã™ã‚‹ã€‚
å¤–éƒ¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ (B-) ã®ã€Œå®Ÿç”¨æ€§ã®æœªæ¸¬å®šã€æ‰¹åˆ¤ã«å¯¾ã™ã‚‹å¿œç­”ã¨ã—ã¦å®Ÿè£…ã€‚

æŒ‡æ¨™:
  1. ã‚»ãƒƒã‚·ãƒ§ãƒ³å“è³ªã‚¹ã‚³ã‚¢ (â˜… ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)
  2. ç”Ÿç”£æ€§ (ã‚³ãƒŸãƒƒãƒˆæ•°/ã‚»ãƒƒã‚·ãƒ§ãƒ³)
  3. BC éµå®ˆç‡ (é•åå¯†åº¦)
  4. å®šç†æ´»ç”¨ç‡ (ç›´æ¥ä½¿ç”¨å®šç†æ•°/24)

Usage:
    python3 cognitive_quality.py              # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
    python3 cognitive_quality.py --days 30    # éå»30æ—¥
    python3 cognitive_quality.py --json       # JSON å‡ºåŠ›
    python3 cognitive_quality.py --trend      # æœˆæ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰
"""

import argparse
import json
import re
import sys
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

# --- Configuration ---

HANDOFF_DIR = Path.home() / "oikos/mneme/.hegemonikon/sessions"

# â˜… ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
QUALITY_PATTERN = re.compile(r'[â˜…â˜†]{3,5}|(\d(?:\.\d)?)/5')

# â˜… ã‚’æ•°å€¤ã«å¤‰æ›
STAR_MAP = {
    "â˜…â˜…â˜…â˜…â˜…": 5.0, "â˜…â˜…â˜…â˜…â˜†": 4.0, "â˜…â˜…â˜…â˜†â˜†": 3.0,
    "â˜…â˜…â˜†â˜†â˜†": 2.0, "â˜…â˜†â˜†â˜†â˜†": 1.0,
    # 4æ–‡å­—ã®å ´åˆ
    "â˜…â˜…â˜…â˜…": 4.5, "â˜…â˜…â˜…â˜†": 3.5, "â˜…â˜…â˜†â˜†": 2.5, "â˜…â˜†â˜†â˜†": 1.5,
    # 3æ–‡å­—ã®å ´åˆ
    "â˜…â˜…â˜…": 3.0, "â˜…â˜…â˜†": 2.5, "â˜…â˜†â˜†": 1.5,
}

# ã‚³ãƒŸãƒƒãƒˆæ•°ãƒ‘ã‚¿ãƒ¼ãƒ³
COMMIT_PATTERN = re.compile(
    r'(?:ã‚³ãƒŸãƒƒãƒˆ|commit)[:\s]*(\d+)\s*(?:æœ¬|ä»¶|commits?)?',
    re.IGNORECASE
)

# BC é•åãƒ‘ã‚¿ãƒ¼ãƒ³
VIOLATION_PATTERN = re.compile(
    r'(?:BC-\d+\s*é•å|é•å.*BC-\d+|violation)',
    re.IGNORECASE
)


# PURPOSE: [L2-auto] Handoff ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
def parse_date(path: Path) -> Optional[datetime]:
    """Handoff ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º"""
    patterns = [
        r'handoff_(\d{4}-\d{2}-\d{2})',
        r'handoff_(\d{8})',
        r'handoff_.*?(\d{4}-\d{2}-\d{2})',
    ]
    for pat in patterns:
        m = re.search(pat, path.stem)
        if m:
            date_str = m.group(1)
            try:
                if '-' in date_str:
                    return datetime.strptime(date_str, "%Y-%m-%d")
                else:
                    return datetime.strptime(date_str, "%Y%m%d")
            except ValueError:
                continue
    return None


# PURPOSE: [L2-auto] â˜… æ–‡å­—åˆ—ã‚’æ•°å€¤ã«å¤‰æ›
def stars_to_score(text: str) -> Optional[float]:
    """â˜… æ–‡å­—åˆ—ã‚’æ•°å€¤ã‚¹ã‚³ã‚¢ã«å¤‰æ›"""
    # ç›´æ¥ãƒãƒƒãƒ
    if text in STAR_MAP:
        return STAR_MAP[text]
    # N/5 å½¢å¼
    m = re.match(r'(\d(?:\.\d)?)/5', text)
    if m:
        return float(m.group(1))
    # â˜… ã®æ•°ã‚’æ•°ãˆã‚‹
    star_count = text.count('â˜…')
    if star_count > 0:
        return min(float(star_count), 5.0)
    return None


# PURPOSE: [L2-auto] Handoff ã‹ã‚‰å“è³ªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
def extract_quality(content: str) -> Optional[float]:
    """Handoff ã‹ã‚‰å“è³ªãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’æŠ½å‡º"""
    # å“è³ªè¡Œã‚’æ¢ã™
    for line in content.split('\n'):
        if 'å“è³ª' in line or 'Quality' in line or 'â˜…' in line:
            # â˜…â˜…â˜…â˜…â˜† å½¢å¼
            star_match = re.search(r'[â˜…â˜†]{3,5}', line)
            if star_match:
                return stars_to_score(star_match.group())
            # N/5 å½¢å¼
            score_match = re.search(r'(\d(?:\.\d)?)\s*/\s*5', line)
            if score_match:
                return float(score_match.group(1))
    return None


# PURPOSE: [L2-auto] Handoff ã‹ã‚‰ã‚³ãƒŸãƒƒãƒˆæ•°ã‚’æŠ½å‡º
def extract_commits(content: str) -> int:
    """Handoff ã‹ã‚‰ã‚³ãƒŸãƒƒãƒˆæ•°ã‚’æŠ½å‡º"""
    total = 0
    for m in COMMIT_PATTERN.finditer(content):
        total = max(total, int(m.group(1)))
    return total


# PURPOSE: [L2-auto] Handoff ã‹ã‚‰ BC é•åã‚’æ¤œå‡º
def extract_violations(content: str) -> int:
    """Handoff ã‹ã‚‰ BC é•åã®è¨˜è¼‰æ•°ã‚’æŠ½å‡º"""
    return len(VIOLATION_PATTERN.findall(content))


# PURPOSE: [L2-auto] å…¨ Handoff ã‚’èµ°æŸ»ã—ã¦æŒ‡æ¨™ã‚’é›†è¨ˆ
def scan_all(days: Optional[int] = None) -> dict:
    """å…¨ Handoff ã‚’èµ°æŸ»ã—ã¦å“è³ªæŒ‡æ¨™ã‚’é›†è¨ˆ"""
    cutoff = None
    if days:
        cutoff = datetime.now() - timedelta(days=days)

    all_files = sorted(HANDOFF_DIR.glob("handoff_*.md"))
    results = {
        "total": 0,
        "quality_scores": [],
        "commits": [],
        "violations": [],
        "by_month": defaultdict(lambda: {
            "scores": [], "commits": [], "violations": [], "count": 0
        }),
        "theorems_direct": Counter(),
    }

    # WF ãƒ‘ã‚¿ãƒ¼ãƒ³ (theorem_activity.py ã¨åŒã˜)
    theorem_ids = [
        "noe", "bou", "zet", "ene", "met", "mek", "sta", "pra",
        "pro", "pis", "ore", "dox", "kho", "hod", "tro", "tek",
        "euk", "chr", "tel", "sop", "pat", "dia", "gno", "epi",
    ]
    wf_pattern = re.compile(
        r'(?:^|(?<=\s))/(' +
        '|'.join(sorted(theorem_ids, key=len, reverse=True)) +
        r')([+\-]?)(?=\s|$|[,.\)}\]|])',
        re.MULTILINE
    )

    for f in all_files:
        fdate = parse_date(f)
        if cutoff and fdate and fdate < cutoff:
            continue

        content = f.read_text(errors="replace")
        results["total"] += 1

        month_key = fdate.strftime("%Y-%m") if fdate else "unknown"
        month = results["by_month"][month_key]
        month["count"] += 1

        # Quality score
        score = extract_quality(content)
        if score is not None:
            results["quality_scores"].append(score)
            month["scores"].append(score)

        # Commits
        commits = extract_commits(content)
        if commits > 0:
            results["commits"].append(commits)
            month["commits"].append(commits)

        # Violations
        violations = extract_violations(content)
        results["violations"].append(violations)
        month["violations"].append(violations)

        # Direct theorem usage
        for match in wf_pattern.finditer(content):
            results["theorems_direct"][match.group(1)] += 1

    return results


# PURPOSE: [L2-auto] ãƒˆãƒ¬ãƒ³ãƒ‰çŸ¢å°ã‚’è¨ˆç®—
def trend_arrow(values: list, window: int = 3) -> str:
    """ç›´è¿‘ window ä»¶ã®å¹³å‡ vs å…¨ä½“å¹³å‡ã§â†‘â†“â†’ã‚’æ±ºå®š"""
    if len(values) < window + 1:
        return "â€”"
    recent = sum(values[-window:]) / window
    overall = sum(values) / len(values)
    diff = recent - overall
    if diff > 0.2:
        return "â†‘"
    elif diff < -0.2:
        return "â†“"
    return "â†’"


# PURPOSE: [L2-auto] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
def generate_dashboard(
    days: Optional[int] = None,
    as_json: bool = False,
    show_trend: bool = False,
) -> str:
    """èªçŸ¥å“è³ªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
    data = scan_all(days)
    period = f"éå»{days}æ—¥" if days else "å…¨æœŸé–“"

    # æŒ‡æ¨™è¨ˆç®—
    scores = data["quality_scores"]
    avg_quality = sum(scores) / len(scores) if scores else 0.0
    star_display = "â˜…" * int(avg_quality) + ("â˜†" if avg_quality % 1 >= 0.5 else "")
    if not star_display:
        star_display = "N/A"

    commits = data["commits"]
    avg_commits = sum(commits) / len(commits) if commits else 0.0

    violations = data["violations"]
    total_violations = sum(violations)
    sessions_with_violations = sum(1 for v in violations if v > 0)
    compliance_rate = (
        (1 - sessions_with_violations / data["total"]) * 100
        if data["total"] > 0 else 0
    )

    direct_used = sum(1 for c in data["theorems_direct"].values() if c > 0)
    coverage = direct_used / 24 * 100

    if as_json:
        return json.dumps({
            "period": period,
            "total_sessions": data["total"],
            "quality": {
                "average": round(avg_quality, 2),
                "count": len(scores),
                "trend": trend_arrow(scores),
            },
            "productivity": {
                "avg_commits": round(avg_commits, 1),
                "total_commits": sum(commits),
                "trend": trend_arrow(commits),
            },
            "compliance": {
                "rate": round(compliance_rate, 1),
                "total_violations": total_violations,
                "sessions_with_violations": sessions_with_violations,
            },
            "theorem_coverage": {
                "direct_used": direct_used,
                "total": 24,
                "rate": round(coverage, 1),
            },
        }, ensure_ascii=False, indent=2)

    # --- Dashboard ---
    lines = []
    lines.append("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    lines.append("â•‘  Cognitive Quality Dashboard             â•‘")
    lines.append(f"â•‘  {datetime.now().strftime('%Y-%m-%d %H:%M')}  {period:>16}  â•‘")
    lines.append("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
    lines.append(
        f"â•‘  ğŸ“Š Session Quality:  {star_display:<6} "
        f"({avg_quality:.1f}/5, n={len(scores)})"
    )
    lines.append(
        f"â•‘  âš¡ Productivity:     "
        f"{avg_commits:.1f} commits/session "
        f"(total: {sum(commits)})"
    )
    lines.append(
        f"â•‘  ğŸ›¡ï¸ BC Compliance:    "
        f"{compliance_rate:.0f}% "
        f"({total_violations} violations in "
        f"{sessions_with_violations}/{data['total']} sessions)"
    )
    lines.append(
        f"â•‘  ğŸ§­ Theorem Coverage: "
        f"{coverage:.0f}% "
        f"({direct_used}/24 directly used)"
    )

    # Trend
    q_trend = trend_arrow(scores)
    p_trend = trend_arrow(commits)
    lines.append(
        f"â•‘  ğŸ“ˆ Trend:            "
        f"Quality {q_trend}  "
        f"Productivity {p_trend}"
    )
    lines.append("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    # Monthly trend (optional)
    if show_trend:
        lines.append("")
        lines.append("## æœˆæ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰")
        lines.append("")
        lines.append(
            "| æœˆ | Sessions | Quality | Commits/S | Violations | Coverage |"
        )
        lines.append(
            "|:---|:---------|:--------|:----------|:-----------|:---------|"
        )
        for month_key in sorted(data["by_month"].keys()):
            m = data["by_month"][month_key]
            m_scores = m["scores"]
            m_commits = m["commits"]
            m_violations = m["violations"]
            avg_s = (
                f"{sum(m_scores) / len(m_scores):.1f}"
                if m_scores else "â€”"
            )
            avg_c = (
                f"{sum(m_commits) / len(m_commits):.1f}"
                if m_commits else "â€”"
            )
            total_v = sum(m_violations)
            lines.append(
                f"| {month_key} | {m['count']} | "
                f"{avg_s} | {avg_c} | "
                f"{total_v} | â€” |"
            )

    # Caveats
    lines.append("")
    lines.append("> **âš ï¸ åˆ¶é™**: å…¨æŒ‡æ¨™ã¯ãƒ—ãƒ­ã‚­ã‚·ã€‚")
    lines.append(
        "> â˜… ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯ Claude ã®è‡ªå·±è©•ä¾¡ã€‚"
        "å¤–éƒ¨æ¤œè¨¼ã•ã‚Œã¦ã„ãªã„ã€‚"
    )
    lines.append(
        "> n=1 å•é¡Œã¯è§£æ±ºã—ãªã„ãŒã€n=1 ã®å†…éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’"
        "å¯è¦–åŒ–ã™ã‚‹ã“ã¨ã«æ„å‘³ãŒã‚ã‚‹ã€‚"
    )

    return "\n".join(lines)


# PURPOSE: [L2-auto] é–¢æ•°: main
def main():
    parser = argparse.ArgumentParser(
        description="èªçŸ¥å“è³ªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ â€” Handoff ã‹ã‚‰å“è³ªæŒ‡æ¨™ã‚’é›†è¨ˆ"
    )
    parser.add_argument(
        "--days", type=int, default=None,
        help="éå»Næ—¥é–“ã«é™å®š (default: å…¨æœŸé–“)"
    )
    parser.add_argument(
        "--json", action="store_true",
        help="JSON å½¢å¼ã§å‡ºåŠ›"
    )
    parser.add_argument(
        "--trend", action="store_true",
        help="æœˆæ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¡¨ç¤º"
    )
    args = parser.parse_args()

    report = generate_dashboard(
        days=args.days, as_json=args.json, show_trend=args.trend
    )
    print(report)


if __name__ == "__main__":
    main()
