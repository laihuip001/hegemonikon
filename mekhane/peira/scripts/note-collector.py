#!/usr/bin/env python3
# PROOF: [L2/„Ç§„É≥„Éï„É©] <- mekhane/peira/ A0‚Üínote.comË®ò‰∫ãÂèéÈõÜ„ÅåÂøÖË¶Å‚Üínote-collector„ÅåÊãÖ„ÅÜ
"""
note.com Ë®ò‰∫ãÂèéÈõÜ„Çπ„ÇØ„É™„Éó„Éà v2
„Ç∑„É≥„Éó„É´Áâà - Âç≥ÊôÇÂÆüË°å
"""

import asyncio
import aiohttp
import json
import os
import time
from pathlib import Path
from datetime import datetime

# Ë®≠ÂÆö
USER_URLNAME = "tasty_dunlin998"
OUTPUT_DIR = Path("/tmp/note_collector_out")
API_BASE = "https://note.com/api/v2"

async def fetch_page(session, page, semaphore):
    url = f"{API_BASE}/creators/{USER_URLNAME}/contents"
    params = {"kind": "note", "page": page, "per_page": 20}
    headers = {"User-Agent": "Mozilla/5.0", "Accept": "application/json"}

    async with semaphore:
        try:
            async with session.get(url, params=params, headers=headers, timeout=30) as resp:
                resp.raise_for_status()
                result = await resp.json()
                # Á∞°ÊòìÁöÑ„Å™„É¨„Éº„Éà„É™„Éü„ÉÉ„ÉàÂØæÁ≠ñÔºà0.5Áßí„Åß„ÅØ„Å™„Åè„ÄÅÂêåÊôÇÂÆüË°åÊï∞„ÇíÁµû„Çä„Å§„Å§Â∞ë„ÅóÂæÖ„Å§Ôºâ
                await asyncio.sleep(0.1)
                return page, result
        except Exception as e:
            print(f"‚ùå Error fetching page {page}: {e}")
            return page, None

# PURPOSE: CLI „Ç®„É≥„Éà„É™„Éù„Ç§„É≥„Éà ‚Äî „Éá„Éº„Çø„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÁõ¥Êé•ÂÆüË°å
async def amain():
    print(f"üîç Collecting articles from note.com/{USER_URLNAME}")
    
    # Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™„Çí‰ΩúÊàê
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    all_articles = []
    
    print(f"üìÑ Fetching up to 20 pages concurrently (with rate limiting)...", flush=True)

    # ÂêåÊôÇÂÆüË°åÊï∞„ÇíÂà∂Èôê„Åó„Å¶API„Å∏„ÅÆË≤†Ëç∑„ÇíÊäë„Åà„Çã
    semaphore = asyncio.Semaphore(5)

    # ÂÆüÈöõ„Å´„ÅØÂ≠òÂú®„Åô„Çã„Éö„Éº„Ç∏Êï∞„Å†„ÅëÂèñÂæó„Åô„Çã„Åü„ÇÅ„Å´„ÄÅ
    # 5„Éö„Éº„Ç∏„Åö„Å§„ÉÅ„É£„É≥„ÇØ„ÅßÂèñÂæó„Åó„ÄÅisLastPage„ÅåÂá∫„Åü„ÇâÁµÇ‰∫Ü„Åô„Çã
    async with aiohttp.ClientSession() as session:
        for chunk_start in range(1, 21, 5):
            chunk_end = min(chunk_start + 5, 21)
            tasks = [fetch_page(session, page, semaphore) for page in range(chunk_start, chunk_end)]
            results = await asyncio.gather(*tasks)
            
            last_page_reached = False
            for page, result in results:
                if not result:
                    last_page_reached = True
                    break

                contents = result.get("data", {}).get("contents", [])
                if not contents:
                    last_page_reached = True
                    break

                all_articles.extend(contents)
                print(f"   Found {len(contents)} articles on page {page} (total: {len(all_articles)})", flush=True)

                if result.get("data", {}).get("isLastPage", True):
                    last_page_reached = True
                    break
            
            if last_page_reached:
                break
    
    print(f"\nüìä Total: {len(all_articles)} articles")
    
    # ÂêÑË®ò‰∫ã„Çí‰øùÂ≠ò
    for i, article in enumerate(all_articles, 1):
        key = article.get("key", "unknown")
        title = article.get("name", "untitled")
        body = article.get("body", "")
        publish_at = article.get("publishAt", "")
        
        # „Éï„Ç°„Ç§„É´ÂêçÁîüÊàê
        safe_title = "".join(c if c.isalnum() or c in "-_" else "_" for c in title[:40])
        filename = f"{key}_{safe_title}.md"
        
        # Markdown ÁîüÊàê
        md = f"""# {title}

> **Source**: https://note.com/{USER_URLNAME}/n/{key}
> **Published**: {publish_at}
> **Collected**: {datetime.now().isoformat()}

---

{body}
"""
        
        filepath = OUTPUT_DIR / filename
        filepath.write_text(md, encoding="utf-8")
        print(f"[{i}/{len(all_articles)}] ‚úÖ {filename[:50]}", flush=True)
    
    # „Éû„Éã„Éï„Çß„Çπ„Éà‰øùÂ≠ò
    manifest = {
        "user": USER_URLNAME,
        "collected_at": datetime.now().isoformat(),
        "total_articles": len(all_articles),
        "articles": [
            {"key": a.get("key"), "name": a.get("name"), "publishAt": a.get("publishAt")}
            for a in all_articles
        ]
    }
    
    manifest_path = OUTPUT_DIR / "manifest.json"
    manifest_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding="utf-8")
    
    print(f"\n‚úÖ Done! {len(all_articles)} articles saved to {OUTPUT_DIR}")

def main():
    asyncio.run(amain())

if __name__ == "__main__":
    main()
