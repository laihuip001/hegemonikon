#!/usr/bin/env python3
# PROOF: [L2/æ¤œè¨¼] <- A0â†’ä½“ç³»ã®å¥å…¨æ€§æ¤œè¨¼ãŒå¿…è¦â†’ä½¿ã‚ã‚Œãªã„å®šç†=dead parameter
"""
Theorem Activity Report â€” å®šç†æ´»æ€§åº¦ãƒ¬ãƒãƒ¼ãƒˆ

Handoff ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ WF ç™ºå‹•é »åº¦ã‚’é›†è¨ˆã—ã€ã€Œç”Ÿå­˜/ä¼‘çœ /æ­»äº¡å€™è£œã€ã‚’åˆ¤å®šã™ã‚‹ã€‚
Desktop Claude DX-008 ã®æŒ‡æ‘˜ã«åŸºã¥ãå®Ÿè£…ã€‚

Usage:
    python3 theorem_activity.py                  # å…¨æœŸé–“ãƒ¬ãƒãƒ¼ãƒˆ
    python3 theorem_activity.py --days 30        # éå»30æ—¥
    python3 theorem_activity.py --days 90        # éå»90æ—¥ (æ­»äº¡å€™è£œæ¤œå‡º)
    python3 theorem_activity.py --json           # JSON å‡ºåŠ›
"""

import argparse
import json
import re
import sys
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

# --- Configuration ---

HANDOFF_DIR = Path.home() / "oikos/mneme/.hegemonikon/sessions"

# 24 å®šç† WF (Î”å±¤) â€” ãƒ™ãƒ¼ã‚¹ID ã®ã¿
# æ­£æœ¬: kernel/doctrine.md çµ±ä¸€éšä¼´è¡¨ + .agent/skills/*/SKILL.md
THEOREM_WORKFLOWS = {
    # O-series (Ousia) â€” L1Ã—L1
    "noe": "O1 NoÄ“sis",
    "bou": "O2 BoulÄ“sis",
    "zet": "O3 ZÄ“tÄ“sis",
    "ene": "O4 Energeia",
    # S-series (Schema) â€” L1Ã—L1.5
    "met": "S1 Metron",
    "mek": "S2 MekhanÄ“",
    "sta": "S3 Stathmos",
    "pra": "S4 Praxis",
    # H-series (HormÄ“) â€” L1Ã—L1.75
    "pro": "H1 Propatheia",
    "pis": "H2 Pistis",
    "ore": "H3 Orexis",
    "dox": "H4 Doxa",
    # P-series (PerigraphÄ“) â€” L1.5Ã—L1.5
    "kho": "P1 KhÅra",
    "hod": "P2 Hodos",
    "tro": "P3 Trokhia",
    "tek": "P4 TekhnÄ“",
    # K-series (Kairos) â€” L1.5Ã—L1.75
    "euk": "K1 Eukairia",
    "chr": "K2 Chronos",
    "tel": "K3 Telos",
    "sop": "K4 Sophia",
    # A-series (Akribeia) â€” L1.75Ã—L1.75
    "pat": "A1 Pathos",
    "dia": "A2 Krisis",
    "gno": "A3 GnÅmÄ“",
    "epi": "A4 EpistÄ“mÄ“",
}

# Î©å±¤ Peras WF
PERAS_WORKFLOWS = {
    "o": "O Peras",
    "s": "S Peras",
    "h": "H Peras",
    "p": "P Peras",
    "k": "K Peras",
    "a": "A Peras",
    "x": "X-series",
    "ax": "AX Peras",
}

# Peras â†’ å†…éƒ¨å®šç†ã®ãƒãƒ–å±•é–‹ãƒãƒƒãƒ”ãƒ³ã‚°
# /o ã‚’å®Ÿè¡Œã™ã‚Œã° O1-O4 ãŒæš—é»™çš„ã«ç™ºå‹•ã™ã‚‹
# æ­£æœ¬: kernel/doctrine.md çµ±ä¸€éšä¼´è¡¨
HUB_EXPANSION = {
    "o": ["noe", "bou", "zet", "ene"],
    "s": ["met", "mek", "sta", "pra"],
    "h": ["pro", "pis", "ore", "dox"],
    "p": ["kho", "hod", "tro", "tek"],
    "k": ["euk", "chr", "tel", "sop"],
    "a": ["pat", "dia", "gno", "epi"],
    "ax": ["pat", "dia", "gno", "epi",  # A-series
           "noe", "bou", "zet", "ene",  # + O-series (å…¨ Series)
           "met", "mek", "sta", "pra",
           "pro", "pis", "ore", "dox",
           "kho", "hod", "tro", "tek",
           "euk", "chr", "tel", "sop"],
}

# ãƒã‚¯ãƒ­ â†’ å†…éƒ¨å®šç†ã®ãƒãƒƒãƒ”ãƒ³ã‚° (æ˜ç¤ºçš„ã«å®šç†ã‚’å‘¼ã¶ãƒã‚¯ãƒ­)
MACRO_EXPANSION = {
    "dig": ["gno", "sop"],       # @dig = /s+~(/p*/a)_/dia*/o+ (gno.analogy çµŒç”±)
    "vet": ["dia", "ene", "pis", "dox", "kho"],  # @vet
    "proof": ["noe", "dia", "ene"],  # @proof = V:{/noe~/dia}
    "plan": ["bou", "dia"],       # @plan = /bou+_/s+~/p*/k_V:{/dia}
    "build": ["bou", "ene", "dia", "dox"],  # @build
    "ready": ["kho", "chr", "euk", "tek"],  # @ready = /kho_/chr_/euk_/tek-
    "feel": ["pro", "ore", "pis"],  # @feel = /pro_/ore~(/pis_/gno.analogy)
    "clean": ["pat", "gno"],     # @clean = /kat_/sym~(/tel_/dia-) â†’ pat+gno çµ±åˆ
}

# Ï„å±¤ ã‚¿ã‚¹ã‚¯WF
TAU_WORKFLOWS = {
    "boot": "Boot",
    "bye": "Bye",
    "dev": "Dev",
    "now": "Now",
    "plan": "Plan",
    "eat": "Eat",
    "why": "Why",
    "vet": "Vet",
    "rev": "Rev",
    "exp": "Exp",
    "lib": "Lib",
    "lex": "Lex",
}

# æ´»æ€§åº¦ã®é–¾å€¤
THRESHOLDS = {
    "alive": 1,         # æœˆ1å›ä»¥ä¸Š = ç”Ÿå­˜
    "dormant_months": 3, # 3ãƒ¶æœˆ 0å› = æ­»äº¡å€™è£œ
}

# WF åã«ãƒãƒƒãƒã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æ–­ç‰‡ã‚’é™¤å¤–ï¼‰
# /xxx, /xxx+, /xxx- ã‚’å¯¾è±¡ã€‚å…ˆé ­ãŒ / ã§ç›´å‰ãŒç©ºç™½ã‹è¡Œé ­
WF_PATTERN = re.compile(
    r'(?:^|(?<=\s))/(' +
    '|'.join(sorted(
        list(THEOREM_WORKFLOWS.keys()) +
        list(PERAS_WORKFLOWS.keys()) +
        list(TAU_WORKFLOWS.keys()),
        key=len, reverse=True  # longer matches first
    )) +
    r')([+\-]?)(?=\s|$|[,.\)}\]|])',
    re.MULTILINE
)


# PURPOSE: Handoff ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
def parse_date_from_filename(path: Path) -> Optional[datetime]:
    """Handoff ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º"""
    patterns = [
        r'handoff_(\d{4}-\d{2}-\d{2})',       # handoff_2026-02-11_...
        r'handoff_(\d{8})',                     # handoff_20260210_...
        r'handoff_.*?(\d{4}-\d{2}-\d{2})',     # fallback
    ]
    name = path.stem
    for pat in patterns:
        m = re.search(pat, name)
        if m:
            date_str = m.group(1)
            try:
                if '-' in date_str:
                    return datetime.strptime(date_str, "%Y-%m-%d")
                else:
                    return datetime.strptime(date_str, "%Y%m%d")
            except ValueError:
                continue
    return None


# PURPOSE: Handoff ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èµ°æŸ»ã—ã€WF ç™ºå‹•ã‚’é›†è¨ˆ
def scan_handoffs(days: Optional[int] = None) -> dict:
    """Handoff ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èµ°æŸ»ã—ã€WF ç™ºå‹•ã‚’é›†è¨ˆ"""
    cutoff = None
    if days:
        cutoff = datetime.now() - timedelta(days=days)

    all_wfs = list(HANDOFF_DIR.glob("handoff_*.md"))
    total_files = 0
    skipped = 0
    wf_counts: Counter = Counter()       # ç›´æ¥ç™ºå‹•
    hub_counts: Counter = Counter()       # ãƒãƒ–çµŒç”±ã®æš—é»™ç™ºå‹•
    wf_by_month: dict[str, Counter] = defaultdict(Counter)

    for f in sorted(all_wfs):
        fdate = parse_date_from_filename(f)
        if cutoff and fdate and fdate < cutoff:
            skipped += 1
            continue

        total_files += 1
        content = f.read_text(errors="replace")

        month_key = fdate.strftime("%Y-%m") if fdate else "unknown"

        # WF åã‚’æŠ½å‡º
        for match in WF_PATTERN.finditer(content):
            wf_id = match.group(1)
            # base ID ã§é›†è¨ˆ
            wf_counts[wf_id] += 1
            wf_by_month[month_key][wf_id] += 1

            # ãƒãƒ–å±•é–‹: Peras ã®ç™ºå‹•ã‚’å†…éƒ¨å®šç†ã«ã‚‚åŠ ç®—
            if wf_id in HUB_EXPANSION:
                for sub_wf in HUB_EXPANSION[wf_id]:
                    hub_counts[sub_wf] += 1

    return {
        "total_files": total_files,
        "skipped": skipped,
        "wf_counts": wf_counts,
        "hub_counts": hub_counts,
        "wf_by_month": dict(wf_by_month),
    }


# PURPOSE: æ´»æ€§åº¦ã‚’3æ®µéšã§åˆ†é¡
def classify_activity(wf_id: str, count: int, months_span: int) -> str:
    """æ´»æ€§åº¦ã‚’3æ®µéšã§åˆ†é¡"""
    if months_span == 0:
        months_span = 1
    monthly_rate = count / months_span
    if monthly_rate >= THRESHOLDS["alive"]:
        return "ğŸŸ¢ alive"
    elif count == 0:
        return "ğŸ”´ death-candidate"
    else:
        return "ğŸŸ¡ dormant"


# PURPOSE: æ´»æ€§åº¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
def generate_report(days: Optional[int] = None, as_json: bool = False) -> str:
    """æ´»æ€§åº¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    data = scan_handoffs(days)
    period_label = f"éå»{days}æ—¥" if days else "å…¨æœŸé–“"

    # æœˆæ•°ã®æ¨å®š
    months = sorted(data["wf_by_month"].keys())
    if months:
        months_span = len(months)
    else:
        months_span = 1

    # --- Theorem Activity ---
    theorem_rows = []
    alive_count = 0
    dormant_count = 0
    dead_count = 0

    for wf_id, label in sorted(THEOREM_WORKFLOWS.items(), key=lambda x: x[1]):
        direct = data["wf_counts"].get(wf_id, 0)
        via_hub = data["hub_counts"].get(wf_id, 0)
        total_count = direct + via_hub
        status = classify_activity(wf_id, total_count, months_span)
        monthly = total_count / months_span if months_span else 0
        theorem_rows.append({
            "id": wf_id,
            "label": label,
            "direct": direct,
            "via_hub": via_hub,
            "count": total_count,
            "monthly": round(monthly, 1),
            "status": status,
        })
        if "alive" in status:
            alive_count += 1
        elif "death" in status:
            dead_count += 1
        else:
            dormant_count += 1

    if as_json:
        return json.dumps({
            "period": period_label,
            "total_handoffs": data["total_files"],
            "months_span": months_span,
            "theorems": theorem_rows,
            "summary": {
                "alive": alive_count,
                "dormant": dormant_count,
                "dead": dead_count,
                "total": len(THEOREM_WORKFLOWS),
            },
            "thresholds": THRESHOLDS,
        }, ensure_ascii=False, indent=2)

    # --- Text Format ---
    lines = []
    lines.append(f"# å®šç†æ´»æ€§åº¦ãƒ¬ãƒãƒ¼ãƒˆ â€” {period_label}")
    lines.append(f"")
    lines.append(f"> åˆ†æå¯¾è±¡: Handoff {data['total_files']}ä»¶ ({months_span}ãƒ¶æœˆé–“)")
    lines.append(f"> é–¾å€¤: æœˆ1å›ä»¥ä¸Š=ç”Ÿå­˜, 3ãƒ¶æœˆ0å›=æ­»äº¡å€™è£œ")
    lines.append(f"> ãƒãƒ–å±•é–‹: Peras (Î©å±¤) ã®ç™ºå‹•ã‚’å†…éƒ¨å®šç†ã«ã‚‚åŠ ç®—")
    lines.append(f"> ç”Ÿæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    lines.append(f"")
    lines.append(f"## ã‚µãƒãƒªãƒ¼")
    lines.append(f"")
    lines.append(f"| çŠ¶æ…‹ | æ•° | å‰²åˆ |")
    lines.append(f"|:-----|:---|:-----|")
    total = len(THEOREM_WORKFLOWS)
    lines.append(f"| ğŸŸ¢ alive | {alive_count} | {alive_count*100//total}% |")
    lines.append(f"| ğŸŸ¡ dormant | {dormant_count} | {dormant_count*100//total}% |")
    lines.append(f"| ğŸ”´ death-candidate | {dead_count} | {dead_count*100//total}% |")
    lines.append(f"")
    lines.append(f"## è©³ç´°")
    lines.append(f"")
    lines.append(f"| WF | å®šç† | ç›´æ¥ | HubçµŒç”± | åˆè¨ˆ | æœˆå¹³å‡ | çŠ¶æ…‹ |")
    lines.append(f"|:---|:-----|:-----|:--------|:-----|:-------|:-----|")
    for row in sorted(theorem_rows, key=lambda r: r["count"], reverse=True):
        lines.append(
            f"| /{row['id']} | {row['label']} | {row['direct']} | "
            f"{row['via_hub']} | {row['count']} | "
            f"{row['monthly']}/æœˆ | {row['status']} |"
        )

    # --- Dead candidates ---
    dead_wfs = [r for r in theorem_rows if "death" in r["status"]]
    if dead_wfs:
        lines.append(f"")
        lines.append(f"## âš ï¸ æ­»äº¡å€™è£œ (ç›´æ¥+HubçµŒç”±ã¨ã‚‚ã«0å›)")
        lines.append(f"")
        lines.append(f"> ä»¥ä¸‹ã®å®šç†ã¯ç›´æ¥ç™ºå‹•ã‚‚ãƒãƒ–çµŒç”±ç™ºå‹•ã‚‚ã‚¼ãƒ­ã€‚")
        lines.append(f"> **å¿˜å´ã®å¯èƒ½æ€§**: ãƒã‚¯ãƒ­ã¸ã®çµ±åˆã§å¾©æ´»ã‚’æ¤œè¨ã€‚")
        lines.append(f"")
        for r in dead_wfs:
            lines.append(f"- **{r['label']}** (`/{r['id']}`) â€” ãƒã‚¯ãƒ­çµ±åˆå€™è£œ")

    # --- Peras & Tau (summary only) ---
    lines.append(f"")
    lines.append(f"## è£œè¶³: Î©å±¤ãƒ»Ï„å±¤")
    lines.append(f"")
    for wf_id, label in sorted(PERAS_WORKFLOWS.items()):
        count = data["wf_counts"].get(wf_id, 0)
        if count > 0:
            lines.append(f"- /{wf_id} ({label}): {count}å›")
    for wf_id, label in sorted(TAU_WORKFLOWS.items()):
        count = data["wf_counts"].get(wf_id, 0)
        if count > 0:
            lines.append(f"- /{wf_id} ({label}): {count}å›")

    return "\n".join(lines)


# PURPOSE: CLI ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
def main():
    parser = argparse.ArgumentParser(
        description="å®šç†æ´»æ€§åº¦ãƒ¬ãƒãƒ¼ãƒˆ â€” Handoff ã‹ã‚‰WFç™ºå‹•é »åº¦ã‚’é›†è¨ˆ"
    )
    parser.add_argument("--days", type=int, default=None,
                        help="éå»Næ—¥é–“ã«é™å®š (default: å…¨æœŸé–“)")
    parser.add_argument("--json", action="store_true",
                        help="JSON å½¢å¼ã§å‡ºåŠ›")
    args = parser.parse_args()

    report = generate_report(days=args.days, as_json=args.json)
    print(report)


if __name__ == "__main__":
    main()
