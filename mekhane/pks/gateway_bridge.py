#!/usr/bin/env python3
# PROOF: [L2/„Ç≥„Ç¢] <- mekhane/pks/
# PURPOSE: Gateway Bridge ‚Äî Ideas/Doxa/Handoff „Çí PKS „ÅÆ KnowledgeNugget „Å´Â§âÊèõ
"""
Gateway Bridge ‚Äî HGK ÂÜÖÈÉ®„Éá„Éº„Çø„Çí PKS „Éó„ÉÉ„Ç∑„É•ÂØæË±°„Å´Áµ±Âêà

Phase 3 „ÅÆÊ†∏ÂøÉ: Ideas/Doxa/Handoff ‚Üí KnowledgeNugget Â§âÊèõ„Å´„Çà„Çä„ÄÅ
Ë´ñÊñá„Å†„Åë„Åß„Å™„Åè HGK Ëá™Ë∫´„ÅÆÁü•Ë≠ò„ÇÇ„ÄåËá™„ÇâË™û„Çã„Äç„Çà„ÅÜ„Å´„Å™„Çã„ÄÇ

‰ΩøÁî®‰æã:
    bridge = GatewayBridge()
    nuggets = bridge.scan()  # ÂÖ®„ÇΩ„Éº„Çπ„Çí„Çπ„Ç≠„É£„É≥ ‚Üí Nuggets ÁîüÊàê
"""

import re
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

from .pks_engine import KnowledgeNugget, SessionContext


# --- Topic Alias Expansion ---
# Áü≠Á∏ÆÂΩ¢ ‚Üí Èñ¢ÈÄ£Ë™û„Å∏„ÅÆÂ±ïÈñã„ÄÇ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà‰∏ÄËá¥„ÅÆÂÅΩÈô∞ÊÄß„Çí‰ΩéÊ∏õ„Åô„Çã„ÄÇ
_TOPIC_ALIASES: dict[str, list[str]] = {
    "fep": ["Ëá™Áî±„Ç®„Éç„É´„ÇÆ„Éº", "free energy", "‰∫àÊ∏¨Ë™§Â∑Æ", "prediction error", "active inference", "ËÉΩÂãïÊé®Ë´ñ"],
    "ccl": ["cognitive control", "Ë™çÁü•Âà∂Âæ°", "„ÉØ„Éº„ÇØ„Éï„É≠„Éº", "workflow", "„Éû„ÇØ„É≠"],
    "hegemonik√≥n": ["hgk", "„Éò„Ç≤„É¢„Éã„Ç≥„É≥", "ÂÖ¨ÁêÜ", "ÂÆöÁêÜ", "axiom"],
    "hgk": ["hegemonik√≥n", "„Éò„Ç≤„É¢„Éã„Ç≥„É≥", "ÂÖ¨ÁêÜ", "ÂÆöÁêÜ"],
    "ÂúèË´ñ": ["category theory", "Èñ¢Êâã", "Ëá™ÁÑ∂Â§âÊèõ", "Èöè‰º¥"],
    "category theory": ["ÂúèË´ñ", "functor", "natural transformation", "adjunction"],
    "pks": ["proactive", "„Éó„É≠„Ç¢„ÇØ„ÉÜ„Ç£„Éñ", "push", "knowledge surfacing"],
    "autoph≈çnos": ["autophonos", "‰∏Ä‰∫∫Áß∞", "advocacy", "self-advocate"],
    "poiema": ["ÁîüÊàê", "„ÉÜ„É≥„Éó„É¨„Éº„Éà", "Âá∫Âäõ", "epoche", "metron"],
    "synteleia": ["Áõ£Êüª", "audit", "ÁôΩË°ÄÁêÉ", "wbc"],
    "desktop": ["tauri", "ui", "dashboard", "vite"],
    "jules": ["gemini code assist", "„Ç≥„Éº„Éá„Ç£„É≥„Ç∞", "coding"],
}


# --- Data Source Configuration ---

@dataclass
class GatewaySource:
    """Gateway „Éá„Éº„Çø„ÇΩ„Éº„Çπ„ÅÆË®≠ÂÆö„ÄÇ"""
    name: str
    directory: Path
    glob_pattern: str
    parse_fn: str  # GatewayBridge „ÅÆ„É°„ÇΩ„ÉÉ„ÉâÂêç
    enabled: bool = True


# Default paths
_MNEME_ROOT = Path.home() / "oikos" / "mneme" / ".hegemonikon"
_KI_DIR = Path.home() / ".gemini" / "antigravity" / "knowledge"


class GatewayBridge:
    """HGK ÂÜÖÈÉ®„Éá„Éº„Çø„Å® PKS „ÇíÊ©ãÊ∏°„Åó„Åô„Çã„Éñ„É™„ÉÉ„Ç∏„ÄÇ

    Ideas, Doxa, Handoff, KI „ÅÆÂêÑ„Éá„Éº„Çø„ÇΩ„Éº„Çπ„Çí„Çπ„Ç≠„É£„É≥„Åó„ÄÅ
    ÊñáËÑà„Å®„ÅÆÈñ¢ÈÄ£ÊÄß„Åå„ÅÇ„Çã„ÇÇ„ÅÆ„Çí KnowledgeNugget „Å´Â§âÊèõ„Åô„Çã„ÄÇ
    """

    def __init__(
        self,
        ideas_dir: Optional[Path] = None,
        doxa_dir: Optional[Path] = None,
        sessions_dir: Optional[Path] = None,
        ki_dir: Optional[Path] = None,
        max_age_days: int = 30,
    ):
        self.max_age_days = max_age_days
        self._sources = [
            GatewaySource(
                name="ideas",
                directory=ideas_dir or (_MNEME_ROOT / "ideas"),
                glob_pattern="idea_*.md",
                parse_fn="_parse_idea",
            ),
            GatewaySource(
                name="doxa",
                directory=doxa_dir or (_MNEME_ROOT / "doxa"),
                glob_pattern="*.md",
                parse_fn="_parse_doxa",
            ),
            GatewaySource(
                name="handoff",
                directory=sessions_dir or (_MNEME_ROOT / "sessions"),
                glob_pattern="handoff_*.md",
                parse_fn="_parse_handoff",
            ),
            GatewaySource(
                name="ki",
                directory=ki_dir or _KI_DIR,
                glob_pattern="*.md",
                parse_fn="_parse_ki",
            ),
        ]

    # PURPOSE: ÂÖ®„ÇΩ„Éº„Çπ„Çí„Çπ„Ç≠„É£„É≥„Åó KnowledgeNugget „Å´Â§âÊèõ„Åô„Çã
    def scan(
        self,
        context: Optional[SessionContext] = None,
        sources: Optional[list[str]] = None,
        max_results: int = 20,
    ) -> list[KnowledgeNugget]:
        """ÂÖ® Gateway „ÇΩ„Éº„Çπ„Çí„Çπ„Ç≠„É£„É≥„Åó„ÄÅ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÈñ¢ÈÄ£„ÅÆ„Éä„Ç≤„ÉÉ„Éà„ÇíËøî„Åô„ÄÇ

        Args:
            context: ÁèæÂú®„ÅÆ„Çª„ÉÉ„Ç∑„Éß„É≥„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà (None = „Éï„Ç£„É´„Çø„Å™„Åó)
            sources: ÂØæË±°„ÇΩ„Éº„ÇπÂêç„É™„Çπ„Éà (None = ÂÖ®„ÇΩ„Éº„Çπ)
            max_results: ÊúÄÂ§ßÁµêÊûúÊï∞
        """
        all_nuggets: list[KnowledgeNugget] = []

        for source in self._sources:
            if not source.enabled:
                continue
            if sources and source.name not in sources:
                continue
            if not source.directory.exists():
                continue

            parse_fn = getattr(self, source.parse_fn, None)
            if not parse_fn:
                continue

            files = sorted(
                source.directory.glob(source.glob_pattern),
                key=lambda f: f.stat().st_mtime,
                reverse=True,
            )[:max_results * 2]  # pre-filter buffer

            for f in files:
                try:
                    nugget = parse_fn(f)
                    if nugget:
                        all_nuggets.append(nugget)
                except (OSError, ValueError):
                    continue

        # Context-based relevance filtering
        if context and context.topics:
            all_nuggets = self._filter_by_context(all_nuggets, context)

        # Sort by relevance, then recency
        all_nuggets.sort(key=lambda n: n.relevance_score, reverse=True)
        return all_nuggets[:max_results]

    # PURPOSE: Ideas „Éï„Ç°„Ç§„É´„Çí KnowledgeNugget „Å´Â§âÊèõ„Åô„Çã
    def _parse_idea(self, path: Path) -> Optional[KnowledgeNugget]:
        """idea_*.md „Çí KnowledgeNugget „Å´Â§âÊèõ„Åô„Çã„ÄÇ"""
        content = path.read_text(encoding="utf-8")
        lines = content.split("\n")

        title = ""
        for line in lines:
            if line.startswith("# "):
                title = line[2:].strip()
                break

        # Extract tags
        tag_match = re.search(r"\*\*„Çø„Ç∞\*\*:\s*(.+)", content)
        tags = tag_match.group(1).strip() if tag_match else ""

        # Extract date from filename
        fname_match = re.search(r"idea_(\d{4})(\d{2})(\d{2})", path.stem)
        date_str = ""
        if fname_match:
            g = fname_match.groups()
            date_str = f"{g[0]}-{g[1]}-{g[2]}"

        # Extract H2 sections as abstract
        sections = re.findall(r"^## (.+)$", content, re.MULTILINE)
        abstract = f"„Ç¢„Ç§„Éá„Ç¢: {title}\n„Çª„ÇØ„Ç∑„Éß„É≥: {', '.join(sections[:5])}"
        if tags:
            abstract += f"\n„Çø„Ç∞: {tags}"

        nugget = KnowledgeNugget(
            title=title or path.stem,
            abstract=abstract,
            source=f"gateway:ideas:{path.name}",
            relevance_score=0.6,  # base score, adjusted by context
            push_reason=f"üí° „Ç¢„Ç§„Éá„Ç¢„É°„É¢ ({date_str})",
        )
        # Preserve tags as metadata for direct matching
        if tags:
            nugget.metadata = {"tags": [t.strip() for t in tags.split(",")]}
        return nugget

    # PURPOSE: Doxa „Éï„Ç°„Ç§„É´„Çí KnowledgeNugget „Å´Â§âÊèõ„Åô„Çã
    def _parse_doxa(self, path: Path) -> Optional[KnowledgeNugget]:
        """doxa_*.md / dox_*.md „Çí KnowledgeNugget „Å´Â§âÊèõ„Åô„Çã„ÄÇ"""
        if path.name == "README.md":
            return None

        content = path.read_text(encoding="utf-8")
        lines = content.split("\n")

        title = ""
        for line in lines:
            if line.startswith("# "):
                title = line[2:].strip()
                break

        # Extract date
        date_match = re.search(r"\*\*Êó•‰ªò\*\*:\s*(\d{4}-\d{2}-\d{2})", content)
        date_str = date_match.group(1) if date_match else ""

        # Extract DX- identifiers
        dx_ids = re.findall(r"(DX-\d{3})", content)

        # Extract confidence
        conf_match = re.search(r"\[Á¢∫‰ø°:\s*(\d+)%\]", content)
        confidence = int(conf_match.group(1)) if conf_match else 70

        abstract = f"‰ø°Âøµ: {title}"
        if dx_ids:
            abstract += f"\nË≠òÂà•Â≠ê: {', '.join(dx_ids)}"

        nugget = KnowledgeNugget(
            title=title or path.stem,
            abstract=abstract,
            source=f"gateway:doxa:{path.name}",
            relevance_score=confidence / 100.0 * 0.8,
            push_reason=f"üìú ‰ø°ÂøµË®òÈå≤ ({date_str})",
        )
        # Preserve DX-IDs as tag metadata for direct matching
        if dx_ids:
            nugget.metadata = {"tags": dx_ids}
        return nugget

    # PURPOSE: Handoff „Éï„Ç°„Ç§„É´„Çí KnowledgeNugget „Å´Â§âÊèõ„Åô„Çã
    def _parse_handoff(self, path: Path) -> Optional[KnowledgeNugget]:
        """handoff_*.md „Çí KnowledgeNugget „Å´Â§âÊèõ„Åô„Çã„ÄÇÁõ¥Ëøë30Êó•„ÅÆ„Åø„ÄÇ"""
        # Age filter
        mtime = datetime.fromtimestamp(path.stat().st_mtime)
        age_days = (datetime.now() - mtime).days
        if age_days > self.max_age_days:
            return None

        content = path.read_text(encoding="utf-8")
        lines = content.split("\n")

        title = ""
        for line in lines:
            if line.startswith("# "):
                title = line[2:].strip()
                break

        # Extract primary_task from YAML frontmatter
        task_match = re.search(r"primary_task:\s*(.+)", content)
        primary_task = task_match.group(1).strip() if task_match else ""

        # Extract date from filename: handoff_YYYYMMDD_HHMM.md
        fname_match = re.search(r"handoff_(\d{4})(\d{2})(\d{2})_(\d{2})(\d{2})", path.stem)
        date_str = ""
        if fname_match:
            g = fname_match.groups()
            date_str = f"{g[0]}-{g[1]}-{g[2]} {g[3]}:{g[4]}"

        abstract = f"Âºï„ÅçÁ∂ô„Åé: {primary_task or title}"

        # Recency boost: newer = higher score
        recency_boost = max(0, 1.0 - age_days / self.max_age_days) * 0.3

        return KnowledgeNugget(
            title=primary_task or title or path.stem,
            abstract=abstract,
            source=f"gateway:handoff:{path.name}",
            relevance_score=0.5 + recency_boost,
            push_reason=f"üìã Âºï„ÅçÁ∂ô„Åé ({date_str}, {age_days}Êó•Ââç)",
        )

    # PURPOSE: KI „Éï„Ç°„Ç§„É´„Çí KnowledgeNugget „Å´Â§âÊèõ„Åô„Çã
    def _parse_ki(self, path: Path) -> Optional[KnowledgeNugget]:
        """Knowledge Item (.md) „Çí KnowledgeNugget „Å´Â§âÊèõ„Åô„Çã„ÄÇ"""
        if path.name == "README.md":
            return None

        content = path.read_text(encoding="utf-8")
        lines = content.split("\n")

        title = ""
        for line in lines:
            if line.startswith("# "):
                title = line[2:].strip()
                break

        # Extract KI type
        ki_type_match = re.search(r"\*\*KI Á®ÆÂà•\*\*:\s*(.+)", content)
        ki_type = ki_type_match.group(1).strip() if ki_type_match else "unknown"

        # Extract confidence
        conf_match = re.search(r"\[Á¢∫‰ø°:\s*(\d+)%\]", content)
        confidence = int(conf_match.group(1)) if conf_match else 75

        abstract = f"Áü•Ë≠ò: {title}\nÁ®ÆÂà•: {ki_type}"

        nugget = KnowledgeNugget(
            title=title or path.stem,
            abstract=abstract,
            source=f"gateway:ki:{path.name}",
            relevance_score=confidence / 100.0 * 0.85,
            push_reason=f"üß† Knowledge Item ({ki_type})",
        )
        # Preserve KI type as tag metadata for direct matching
        nugget.metadata = {"tags": [ki_type.strip()]}
        return nugget

    # PURPOSE: „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Å´Âü∫„Å•„ÅèÂ§öÊÆµÈöé„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
    def _filter_by_context(
        self,
        nuggets: list[KnowledgeNugget],
        context: SessionContext,
    ) -> list[KnowledgeNugget]:
        """Â§öÊÆµÈöé„Éû„ÉÉ„ÉÅ„É≥„Ç∞„Åß„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÈñ¢ÈÄ£Â∫¶„ÇíË™øÊï¥„Åô„Çã„ÄÇ

        3ÊÆµÈöé:
          1. Áõ¥Êé•‰∏ÄËá¥ ‚Äî „Éà„Éî„ÉÉ„ÇØÂêç„Åå„ÉÜ„Ç≠„Çπ„ÉàÂÜÖ„Å´Â≠òÂú®
          2. „Ç®„Ç§„É™„Ç¢„ÇπÂ±ïÈñã ‚Äî FEP‚ÜíËá™Áî±„Ç®„Éç„É´„ÇÆ„ÉºÁ≠â„ÅÆÈñ¢ÈÄ£Ë™û„Åß‰∏ÄËá¥
          3. „Çø„Ç∞Áõ¥Êé•„Éû„ÉÉ„ÉÅ ‚Äî Ideas/KI „ÅÆ„Çø„Ç∞„É°„Çø„Éá„Éº„Çø„Åß‰∏ÄËá¥
        """
        topic_set = {t.lower() for t in context.topics}

        # Expand topics with aliases
        expanded: set[str] = set(topic_set)
        for topic in topic_set:
            if topic in _TOPIC_ALIASES:
                expanded.update(a.lower() for a in _TOPIC_ALIASES[topic])

        result = []

        for nugget in nuggets:
            text = f"{nugget.title} {nugget.abstract}".lower()
            match_score = 0.0
            match_labels: list[str] = []

            # Stage 1: Direct topic match (highest weight)
            direct_matches = sum(1 for t in topic_set if t in text)
            if direct_matches > 0:
                match_score += direct_matches * 0.15
                match_labels.append(f"Áõ¥Êé•{direct_matches}")

            # Stage 2: Alias expansion match
            alias_terms = expanded - topic_set
            alias_matches = sum(1 for a in alias_terms if a in text)
            if alias_matches > 0:
                match_score += alias_matches * 0.08  # lower weight than direct
                match_labels.append(f"Èñ¢ÈÄ£{alias_matches}")

            # Stage 3: Tag metadata match
            tags = getattr(nugget, 'metadata', {}).get('tags', []) if hasattr(nugget, 'metadata') else []
            if tags:
                tag_set = {t.lower() for t in tags}
                tag_matches = len(tag_set & expanded)
                if tag_matches > 0:
                    match_score += tag_matches * 0.12
                    match_labels.append(f"„Çø„Ç∞{tag_matches}")

            if match_score > 0:
                boost = min(match_score, 0.5)
                nugget.relevance_score = min(nugget.relevance_score + boost, 1.0)
                nugget.push_reason += f" [‰∏ÄËá¥: {'+'.join(match_labels)}]"
                result.append(nugget)
            elif nugget.relevance_score >= 0.7:
                # High base relevance passes through without topic match
                result.append(nugget)

        return result

    # PURPOSE: Áµ±Ë®àÊÉÖÂ†±„ÇíËøî„Åô
    def stats(self) -> dict[str, Any]:
        """Gateway „ÇΩ„Éº„Çπ„ÅÆÁµ±Ë®à„ÇíËøî„Åô„ÄÇ"""
        stats: dict[str, Any] = {}
        for source in self._sources:
            if not source.directory.exists():
                stats[source.name] = {"exists": False, "count": 0}
                continue
            files = list(source.directory.glob(source.glob_pattern))
            stats[source.name] = {
                "exists": True,
                "count": len(files),
                "directory": str(source.directory),
            }
        return stats
