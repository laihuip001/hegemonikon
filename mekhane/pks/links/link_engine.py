# noqa: AI-ALL
# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- mekhane/pks/links/
"""
PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å­˜åœ¨ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„

A0 (FEP) â†’ çŸ¥è­˜é–“ã®é–¢ä¿‚æ€§æŠŠæ¡ãŒäºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹
â†’ Obsidian ã® wikilink/backlink æ©Ÿæ§‹ã‚’å†ç¾
â†’ link_engine.py ãŒæ‹…ã†

# PURPOSE: æ§‹é€ åŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«é–“ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç®¡ç†
# [[wikilink]] ã®è§£æã€backlinks ã®è‡ªå‹•æ¤œå‡ºã€orphan detection, graph export
"""

from __future__ import annotations

import json
import re
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional


# --- Pattern ---

# Obsidian äº’æ›ã® [[wikilink]] ãƒ‘ã‚¿ãƒ¼ãƒ³
# [[target]] or [[target|alias]]
WIKILINK_PATTERN = re.compile(r"\[\[([^\]|]+)(?:\|([^\]]+))?\]\]")


# --- Data Models ---


@dataclass
# PURPOSE: ãƒ•ã‚¡ã‚¤ãƒ«é–“ãƒªãƒ³ã‚¯
class Link:
    """ãƒ•ã‚¡ã‚¤ãƒ«é–“ãƒªãƒ³ã‚¯"""

    source: Path  # ãƒªãƒ³ã‚¯å…ƒ
    target: str  # ãƒªãƒ³ã‚¯å…ˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å or ãƒ‘ã‚¹ï¼‰
    alias: Optional[str] = None  # è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆ
    line_number: int = 0  # ãƒªãƒ³ã‚¯ãŒå­˜åœ¨ã™ã‚‹è¡Œç•ªå·
    context: str = ""  # ãƒªãƒ³ã‚¯å‰å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ


@dataclass
# PURPOSE: ãƒªãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ â€” å…¨ãƒ•ã‚¡ã‚¤ãƒ«é–“ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ç´¢å¼•
class LinkIndex:
    """ãƒªãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ â€” å…¨ãƒ•ã‚¡ã‚¤ãƒ«é–“ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ç´¢å¼•"""

    forward_links: dict[str, list[Link]] = field(default_factory=lambda: defaultdict(list))
    backlinks: dict[str, list[Link]] = field(default_factory=lambda: defaultdict(list))
    orphans: list[str] = field(default_factory=list)  # ã©ã“ã‹ã‚‰ã‚‚ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«

    @property
    # PURPOSE: total_files â€” çŸ¥è­˜ãƒ—ãƒƒã‚·ãƒ¥ã®å‡¦ç†
    def total_files(self) -> int:
        all_files = set(self.forward_links.keys())
        for links in self.backlinks.values():
            for link in links:
                all_files.add(str(link.source))
        return len(all_files)

    @property
    # PURPOSE: total_links â€” çŸ¥è­˜ãƒ—ãƒƒã‚·ãƒ¥ã®å‡¦ç†
    def total_links(self) -> int:
        return sum(len(links) for links in self.forward_links.values())


# --- Engine ---
# PURPOSE: æ§‹é€ åŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«é–“ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç®¡ç†


class LinkEngine:
    """æ§‹é€ åŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«é–“ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç®¡ç†

    Obsidian ã® wikilink/backlink æ©Ÿæ§‹ã® mekhane å†ç¾ã€‚

    æ©Ÿèƒ½:
        - [[wikilink]] ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è§£æ
        - Forward links: ãƒ•ã‚¡ã‚¤ãƒ« â†’ å‚ç…§å…ˆã®æŠ½å‡º
        - Backlinks: ãƒ•ã‚¡ã‚¤ãƒ« â† è¢«å‚ç…§å…ƒã®è‡ªå‹•æ¤œå‡º
        - Orphan Detection: ã©ã“ã‹ã‚‰ã‚‚ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ç™ºè¦‹
        - Graph Export: ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚°ãƒ©ãƒ•ã® JSON/Mermaid å‡ºåŠ›
    """

    # PURPOSE: LinkEngine ã®åˆæœŸåŒ– â€” ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…¨ä½“ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ãƒªãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
    def __init__(self, root_dir: Path, extensions: tuple[str, ...] = (".md",)):
        self.root_dir = root_dir.resolve()
        self.extensions = extensions
        self._index: Optional[LinkIndex] = None

    # PURPOSE: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…¨ä½“ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ãƒªãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
    def build_index(self) -> LinkIndex:
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…¨ä½“ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ãƒªãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
        index = LinkIndex()

        # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
        all_files: dict[str, Path] = {}  # stem -> path
        for ext in self.extensions:
            for path in self.root_dir.rglob(f"*{ext}"):
                rel = path.relative_to(self.root_dir)
                all_files[path.stem] = rel
                all_files[str(rel)] = rel

        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªãƒ³ã‚¯ã‚’è§£æ
        for ext in self.extensions:
            for path in self.root_dir.rglob(f"*{ext}"):
                rel_path = str(path.relative_to(self.root_dir))
                links = self._extract_links(path)

                if links:
                    index.forward_links[rel_path] = links

                    # ãƒãƒƒã‚¯ãƒªãƒ³ã‚¯ç™»éŒ²
                    for link in links:
                        index.backlinks[link.target].append(link)

        # Orphan æ¤œå‡º
        all_file_stems = set(all_files.keys())
        linked_targets = set()
        for links in index.forward_links.values():
            for link in links:
                linked_targets.add(link.target)

        # ãƒãƒƒã‚¯ãƒªãƒ³ã‚¯ãŒã‚¼ãƒ­ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ orphan ã¨ã™ã‚‹
        for stem, rel in all_files.items():
            rel_str = str(rel)
            if (
                rel_str not in linked_targets
                and stem not in linked_targets
                and rel_str in index.forward_links  # ãƒ•ã‚¡ã‚¤ãƒ«è‡ªä½“ã¯å­˜åœ¨
            ):
                # è‡ªèº«ãŒãƒªãƒ³ã‚¯ã‚’æŒã£ã¦ã„ã‚‹ãŒã€èª°ã‹ã‚‰ã‚‚ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ãªã„
                if stem not in linked_targets:
                    index.orphans.append(rel_str)

        # é‡è¤‡é™¤å»
        index.orphans = sorted(set(index.orphans))

        self._index = index
        return index

    # PURPOSE: ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ [[wikilink]] ã‚’æŠ½å‡º
    def _extract_links(self, file_path: Path) -> list[Link]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ [[wikilink]] ã‚’æŠ½å‡º"""
        links = []

        try:
            content = file_path.read_text(encoding="utf-8", errors="replace")
        except (OSError, IOError):
            return links

        for line_num, line in enumerate(content.split("\n"), 1):
            for match in WIKILINK_PATTERN.finditer(line):
                target = match.group(1).strip()
                alias = match.group(2)
                if alias:
                    alias = alias.strip()

                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼šãƒªãƒ³ã‚¯å‰å¾Œ 50 æ–‡å­—
                start = max(0, match.start() - 50)
                end = min(len(line), match.end() + 50)
                context = line[start:end].strip()

                links.append(
                    Link(
                        source=file_path.relative_to(self.root_dir),
                        target=target,
                        alias=alias,
                        line_number=line_num,
                        context=context,
                    )
                )

        return links

    # PURPOSE: ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®è¢«å‚ç…§å…ƒã‚’å–å¾—
    def get_backlinks(self, target: str) -> list[Link]:
        """ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®è¢«å‚ç…§å…ƒã‚’å–å¾—"""
        if self._index is None:
            self.build_index()
        assert self._index is not None
        return self._index.backlinks.get(target, [])

    # PURPOSE: ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å‚ç…§å…ˆã‚’å–å¾—
    def get_forward_links(self, source: str) -> list[Link]:
        """ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å‚ç…§å…ˆã‚’å–å¾—"""
        if self._index is None:
            self.build_index()
        assert self._index is not None
        return self._index.forward_links.get(source, [])

    # PURPOSE: ã©ã“ã‹ã‚‰ã‚‚ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    def get_orphans(self) -> list[str]:
        """ã©ã“ã‹ã‚‰ã‚‚ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        if self._index is None:
            self.build_index()
        assert self._index is not None
        return self._index.orphans

    # --- Export ---

    # PURPOSE: ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚°ãƒ©ãƒ•ã‚’ JSON å‡ºåŠ›
    def export_graph_json(self) -> str:
        """ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚°ãƒ©ãƒ•ã‚’ JSON å‡ºåŠ›"""
        if self._index is None:
            self.build_index()
        assert self._index is not None

        nodes = set()
        edges = []

        for source, links in self._index.forward_links.items():
            nodes.add(source)
            for link in links:
                nodes.add(link.target)
                edges.append(
                    {
                        "source": source,
                        "target": link.target,
                        "alias": link.alias,
                    }
                )

        graph = {
            "nodes": [{"id": n, "orphan": n in self._index.orphans} for n in sorted(nodes)],
            "edges": edges,
            "stats": {
                "total_nodes": len(nodes),
                "total_edges": len(edges),
                "orphans": len(self._index.orphans),
            },
        }

        return json.dumps(graph, ensure_ascii=False, indent=2)

    # PURPOSE: ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚°ãƒ©ãƒ•ã‚’ Mermaid å½¢å¼ã§å‡ºåŠ›
    def export_graph_mermaid(self, max_nodes: int = 50) -> str:
        """ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚°ãƒ©ãƒ•ã‚’ Mermaid å½¢å¼ã§å‡ºåŠ›"""
        if self._index is None:
            self.build_index()
        assert self._index is not None

        lines = ["graph LR"]
        seen_edges = set()
        node_count = 0

        for source, links in self._index.forward_links.items():
            if node_count >= max_nodes:
                break

            # ãƒãƒ¼ãƒ‰ ID ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
            src_id = self._sanitize_mermaid_id(source)
            node_count += 1

            for link in links:
                tgt_id = self._sanitize_mermaid_id(link.target)
                edge_key = f"{src_id}->{tgt_id}"

                if edge_key not in seen_edges:
                    label = link.alias or ""
                    if label:
                        lines.append(f'    {src_id}["{source}"] -->|"{label}"| {tgt_id}["{link.target}"]')
                    else:
                        lines.append(f'    {src_id}["{source}"] --> {tgt_id}["{link.target}"]')
                    seen_edges.add(edge_key)

        return "\n".join(lines)

    @staticmethod
    # PURPOSE: Mermaid ãƒãƒ¼ãƒ‰ ID ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
    def _sanitize_mermaid_id(name: str) -> str:
        """Mermaid ãƒãƒ¼ãƒ‰ ID ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º"""
        return re.sub(r"[^a-zA-Z0-9_]", "_", name)

    # PURPOSE: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚µãƒãƒªãƒ¼ã‚’ Markdown ã§å‡ºåŠ›
    def summary_markdown(self) -> str:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚µãƒãƒªãƒ¼ã‚’ Markdown ã§å‡ºåŠ›"""
        if self._index is None:
            self.build_index()
        assert self._index is not None

        idx = self._index
        lines = [
            "## ğŸ”— Link Engine Summary",
            "",
            f"| é …ç›® | å€¤ |",
            f"|:-----|:---|",
            f"| ãƒ•ã‚¡ã‚¤ãƒ«æ•° | {idx.total_files} |",
            f"| ãƒªãƒ³ã‚¯æ•° | {idx.total_links} |",
            f"| Orphan æ•° | {len(idx.orphans)} |",
        ]

        if idx.orphans:
            lines.append("")
            lines.append("### Orphan Files")
            for orphan in idx.orphans[:20]:
                lines.append(f"- `{orphan}`")

        # æœ€ã‚‚ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ« Top 5
        backlink_counts = {
            target: len(links) for target, links in idx.backlinks.items()
        }
        if backlink_counts:
            top = sorted(backlink_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            lines.append("")
            lines.append("### Most Linked (Top 5)")
            lines.append("| File | Backlinks |")
            lines.append("|:-----|:---------|")
            for name, count in top:
                lines.append(f"| `{name}` | {count} |")

        return "\n".join(lines)
