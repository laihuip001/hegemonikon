# noqa: AI-ALL
# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- mekhane/pks/
"""
PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å­˜åœ¨ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„

A0 (FEP) â†’ èƒ½å‹•çš„çŸ¥è­˜è¡¨é¢åŒ–ã«ã¯æ“ä½œã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒå¿…è¦
â†’ pks_cli.py ãŒæ‹…ã†

# PURPOSE: PKS v2 CLI â€” èƒ½å‹•çš„çŸ¥è­˜ãƒ—ãƒƒã‚·ãƒ¥ã®å¯¾è©±ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
"""

from __future__ import annotations

import argparse
import sys
from pathlib import Path

_PKS_DIR = Path(__file__).resolve().parent
_MEKHANE_DIR = _PKS_DIR.parent
_HEGEMONIKON_ROOT = _MEKHANE_DIR.parent

if str(_HEGEMONIKON_ROOT) not in sys.path:
    sys.path.insert(0, str(_HEGEMONIKON_ROOT))


# PURPOSE: SelfAdvocate ä¸€äººç§°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºåŠ›ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼
def _print_advocacy(nuggets, engine) -> None:
    """SelfAdvocate ã§è«–æ–‡ä¸€äººç§°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆãƒ»å‡ºåŠ›"""
    try:
        from mekhane.pks.self_advocate import SelfAdvocate
    except ImportError:
        print("\nâš ï¸ SelfAdvocate ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return

    advocate = SelfAdvocate()
    context = engine.tracker.context if hasattr(engine, 'tracker') else None

    advocacies = advocate.generate_batch(nuggets, context)
    if advocacies:
        report = advocate.format_report(advocacies)
        print("\n" + report)
    else:
        print("\nğŸ“­ Advocacy ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")


# PURPOSE: `pks stats` â€” çŸ¥è­˜åŸºç›¤ã®å…¨ä½“çµ±è¨ˆã‚’è¡¨ç¤º
def cmd_stats(args: argparse.Namespace) -> None:
    """çŸ¥è­˜åŸºç›¤ (MnÄ“mÄ“ + GnÅsis + PKS) ã®çµ±è¨ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"""
    import os
    for key in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:
        os.environ.pop(key, None)
    os.environ.setdefault('HF_HUB_OFFLINE', '1')
    os.environ.setdefault('TRANSFORMERS_OFFLINE', '1')

    print("## ğŸ“Š PKS Knowledge Stats\n")

    # --- GnÅsis (LanceDB) ---
    gnosis_count = 0
    try:
        from mekhane.anamnesis.index import GnosisIndex as AnamnesisGnosisIndex
        gi = AnamnesisGnosisIndex()
        stats = gi.stats()
        gnosis_count = stats.get('total', stats.get('total_papers', 0))
    except Exception:
        pass

    # --- MnÄ“mÄ“ indices ---
    indices_dir = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "indices"
    kairos_count = 0
    sophia_count = 0
    chronos_count = 0
    if indices_dir.exists():
        for name in ["kairos", "sophia", "chronos"]:
            pkl = indices_dir / f"{name}.pkl"
            if pkl.exists():
                try:
                    from mekhane.symploke.adapters.embedding_adapter import EmbeddingAdapter
                    adapter = EmbeddingAdapter()
                    adapter.load(str(pkl))
                    count = adapter.count()
                    if name == "kairos":
                        kairos_count = count
                    elif name == "sophia":
                        sophia_count = count
                    else:
                        chronos_count = count
                except Exception:
                    pass

    # --- Handoffs ---
    handoff_dir = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "sessions"
    handoff_count = len(list(handoff_dir.glob("handoff_20??-??-??_????.md"))) if handoff_dir.exists() else 0

    # --- KI (Knowledge Items) ---
    ki_dir = Path.home() / ".gemini" / "antigravity" / "knowledge"
    ki_count = len(list(ki_dir.glob("*.md"))) if ki_dir.exists() else 0

    # --- Cooldown ---
    cooldown = os.environ.get("PKS_COOLDOWN_HOURS", "24.0")

    # --- Output ---
    total = gnosis_count + kairos_count + sophia_count + chronos_count
    print("| ã‚½ãƒ¼ã‚¹ | ä»¶æ•° | å‚™è€ƒ |")
    print("|:-------|-----:|:-----|")
    print(f"| ğŸ”¬ GnÅsis (LanceDB) | **{gnosis_count:,}** | è«–æ–‡ãƒ»å¤–éƒ¨çŸ¥è­˜ |")
    print(f"| ğŸ“‹ Kairos (.pkl) | **{kairos_count:,}** | Handoff + ä¼šè©±ãƒ­ã‚° |")
    print(f"| ğŸ“– Sophia (.pkl) | **{sophia_count:,}** | Knowledge Items |")
    print(f"| ğŸ• Chronos (.pkl) | **{chronos_count:,}** | æ™‚ç³»åˆ—ãƒãƒ£ãƒƒãƒˆå±¥æ­´ |")
    print(f"| **åˆè¨ˆ** | **{total:,}** | |")
    print()
    print(f"ğŸ“ Handoff ãƒ•ã‚¡ã‚¤ãƒ«: **{handoff_count}** ä»¶")
    print(f"ğŸ“ KI ãƒ•ã‚¡ã‚¤ãƒ«: **{ki_count}** ä»¶")
    print(f"â±ï¸ ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³: **{cooldown}** æ™‚é–“ (`PKS_COOLDOWN_HOURS`)")
    print()


# PURPOSE: `pks health` â€” AutophÅnos å…¨ã‚¹ã‚¿ãƒƒã‚¯ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
def cmd_health(args: argparse.Namespace) -> None:
    """AutophÅnos å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ä¸€æ‹¬æ¤œè¨¼"""
    import os, time
    for key in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:
        os.environ.pop(key, None)
    os.environ.setdefault('HF_HUB_OFFLINE', '1')
    os.environ.setdefault('TRANSFORMERS_OFFLINE', '1')

    print("## ğŸ¥ AutophÅnos Health Check\n")
    checks = []

    def _check(name: str, fn):
        t0 = time.time()
        try:
            result = fn()
            elapsed = time.time() - t0
            checks.append((name, "âœ…", result, f"{elapsed:.1f}s"))
        except Exception as e:
            elapsed = time.time() - t0
            checks.append((name, "âŒ", str(e)[:60], f"{elapsed:.1f}s"))

    # 1. LanceDB (GnÅsis)
    # PURPOSE: [L2-auto] å†…éƒ¨å‡¦ç†: check_gnosis
    def check_gnosis():
        from mekhane.anamnesis.index import GnosisIndex as AI
        gi = AI()
        s = gi.stats()
        return f"{s.get('total', 0):,} docs"
    _check("GnÅsis (LanceDB)", check_gnosis)

    # 2. Kairos index
    # PURPOSE: [L2-auto] å†…éƒ¨å‡¦ç†: check_kairos
    def check_kairos():
        pkl = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "indices" / "kairos.pkl"
        if not pkl.exists():
            raise FileNotFoundError("kairos.pkl not found")
        from mekhane.symploke.adapters.embedding_adapter import EmbeddingAdapter
        a = EmbeddingAdapter(); a.load(str(pkl))
        return f"{a.count():,} docs"
    _check("Kairos (.pkl)", check_kairos)

    # 3. Sophia index
    def check_sophia():
        pkl = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "indices" / "sophia.pkl"
        if not pkl.exists():
            raise FileNotFoundError("sophia.pkl not found")
        from mekhane.symploke.adapters.embedding_adapter import EmbeddingAdapter
        a = EmbeddingAdapter(); a.load(str(pkl))
        return f"{a.count():,} docs"
    _check("Sophia (.pkl)", check_sophia)

    # 4. Embedder
    def check_embedder():
        from mekhane.symploke.adapters.embedding_adapter import EmbeddingAdapter
        a = EmbeddingAdapter()
        v = a.encode("test query")
        return f"dim={len(v)}"
    _check("Embedder (BGE-M3)", check_embedder)

    # 5. GnosisLanceBridge
    def check_bridge():
        from mekhane.symploke.indices.gnosis_lance_bridge import GnosisLanceBridge
        b = GnosisLanceBridge()
        r = b.search("active inference", k=1)
        return f"{len(r)} results, score={r[0].score:.3f}" if r else "0 results"
    _check("GnosisLanceBridge", check_bridge)

    # 6. PKSEngine
    def check_engine():
        from mekhane.pks.pks_engine import PKSEngine
        e = PKSEngine(enable_questions=False, enable_serendipity=False)
        e.set_context(topics=["FEP"])
        n = e.proactive_push(k=3)
        return f"{len(n)} nuggets"
    _check("PKSEngine", check_engine)

    # 7. TopicExtractor
    def check_topics():
        from mekhane.pks.pks_engine import PKSEngine
        e = PKSEngine(enable_questions=False)
        t = e.auto_context_from_handoff()
        return f"{len(t)} topics: {', '.join(t[:3])}"
    _check("TopicExtractor", check_topics)

    # 8. SelfAdvocate
    def check_advocate():
        from mekhane.pks.self_advocate import SelfAdvocate
        a = SelfAdvocate()
        return f"LLM={'ok' if a.llm_available else 'template mode'}"
    _check("SelfAdvocate", check_advocate)

    # 9. Chronos index
    def check_chronos():
        pkl = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "indices" / "chronos.pkl"
        if not pkl.exists():
            raise FileNotFoundError("chronos.pkl not found")
        from mekhane.symploke.adapters.embedding_adapter import EmbeddingAdapter
        a = EmbeddingAdapter(); a.load(str(pkl))
        return f"{a.count():,} docs"
    _check("Chronos (.pkl)", check_chronos)

    # Output
    print("| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | çŠ¶æ…‹ | è©³ç´° | æ™‚é–“ |")
    print("|:--------------|:----:|:-----|-----:|")
    ok = 0
    for name, status, detail, elapsed in checks:
        print(f"| {name} | {status} | {detail} | {elapsed} |")
        if status == "âœ…":
            ok += 1
    print()
    total = len(checks)
    print(f"**çµæœ: {ok}/{total} OK** {'ğŸ‰' if ok == total else 'âš ï¸'}")
    print()


# PURPOSE: `pks search` â€” å…¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¨ªæ–­æ¤œç´¢
def cmd_search(args: argparse.Namespace) -> None:
    """GnÅsis, Kairos, Sophia, Chronos ã‚’æ¨ªæ–­æ¤œç´¢"""
    import os, time
    for key in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:
        os.environ.pop(key, None)
    os.environ.setdefault('HF_HUB_OFFLINE', '1')
    os.environ.setdefault('TRANSFORMERS_OFFLINE', '1')

    query = args.query
    k = args.k
    sources = args.sources.split(",") if args.sources else ["gnosis", "kairos", "sophia", "chronos"]

    print(f"## ğŸ” PKS Search: \"{query}\"\n")
    t0 = time.time()
    all_results = []

    # Source icons
    icons = {"gnosis": "ğŸ”¬", "kairos": "ğŸ“‹", "sophia": "ğŸ“–", "chronos": "ğŸ•"}

    # 1. GnÅsis (LanceDB)
    if "gnosis" in sources:
        try:
            from mekhane.anamnesis.index import GnosisIndex as AI
            gi = AI()
            results = gi.search(query, k=k)
            for r in results:
                title = r.get("title", r.get("primary_key", "?"))
                dist = float(r.get("_distance", 1.0))
                # LanceDB distance â†’ similarity (lower distance = higher similarity)
                score = max(0, 1 - dist / 2)
                snippet = r.get("abstract", r.get("content", ""))[:120]
                all_results.append(("gnosis", score, title, snippet))
        except Exception as e:
            print(f"  âš ï¸ GnÅsis: {e}")

    # 2-4. pkl indices (Kairos, Sophia, Chronos)
    indices_dir = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "indices"
    pkl_names = [n for n in ["kairos", "sophia", "chronos"] if n in sources]

    if pkl_names:
        try:
            from mekhane.symploke.adapters.embedding_adapter import EmbeddingAdapter
            adapter = EmbeddingAdapter()
            query_vec = adapter.encode(query)

            for name in pkl_names:
                pkl = indices_dir / f"{name}.pkl"
                if not pkl.exists():
                    continue
                try:
                    idx = EmbeddingAdapter()
                    idx.load(str(pkl))
                    hits = idx.search(query_vec, k=k)
                    for hit in hits:
                        # SearchResult object: .id, .score, .metadata
                        meta = hit.metadata if hasattr(hit, 'metadata') else {}
                        doc_id = meta.get("doc_id", meta.get("title", str(hit.id)))
                        score = hit.score if hasattr(hit, 'score') else 0
                        title = meta.get("title", doc_id)
                        all_results.append((name, score, title, ""))
                except Exception as e:
                    print(f"  âš ï¸ {name}: {e}")
        except Exception as e:
            print(f"  âš ï¸ Embedder: {e}")

    elapsed = time.time() - t0

    # Sort by score descending
    all_results.sort(key=lambda x: x[1], reverse=True)

    if not all_results:
        print("ğŸ“­ çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # Display top results
    top = all_results[:k]
    print(f"| # | ã‚½ãƒ¼ã‚¹ | ã‚¹ã‚³ã‚¢ | ã‚¿ã‚¤ãƒˆãƒ« / ID | ã‚¹ãƒ‹ãƒšãƒƒãƒˆ |")
    print(f"|--:|:-------|-------:|:--------------|:-----------|")
    for i, (src, score, title, snippet) in enumerate(top, 1):
        icon = icons.get(src, "ğŸ“¦")
        title_short = title[:40] + "â€¦" if len(title) > 40 else title
        snippet_short = snippet.replace("\n", " ")[:60]
        print(f"| {i} | {icon} {src} | {score:.3f} | {title_short} | {snippet_short} |")

    # Summary
    src_counts = {}
    for src, _, _, _ in top:
        src_counts[src] = src_counts.get(src, 0) + 1
    breakdown = ", ".join(f"{icons.get(s, 'ğŸ“¦')}{c}" for s, c in sorted(src_counts.items()))
    print(f"\n**{len(top)} ä»¶** ({breakdown}) â€” {elapsed:.1f}s")
    print()


# PURPOSE: `pks rebuild` â€” Chronos ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å†æ§‹ç¯‰
def cmd_rebuild(args: argparse.Namespace) -> None:
    """Chronos ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ Handoff ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å†æ§‹ç¯‰ã™ã‚‹"""
    import os, re, time
    for key in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:
        os.environ.pop(key, None)
    os.environ.setdefault('HF_HUB_OFFLINE', '1')
    os.environ.setdefault('TRANSFORMERS_OFFLINE', '1')

    target = args.target
    if target != "chronos":
        print(f"âŒ æœªå¯¾å¿œã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {target} (ç¾åœ¨ã¯ 'chronos' ã®ã¿å¯¾å¿œ)")
        return

    print("## ğŸ”„ Chronos Index Rebuild\n")
    t0 = time.time()

    handoff_dir = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "sessions"
    handoffs = sorted(handoff_dir.glob("handoff_20??-??-??_????.md"))
    print(f"ğŸ“ Handoff ãƒ•ã‚¡ã‚¤ãƒ«: {len(handoffs)} ä»¶")

    if not handoffs:
        print("ğŸ“­ Handoff ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # Split into chunks by ## headers
    chunks = []
    for hf in handoffs:
        content = hf.read_text(encoding='utf-8', errors='ignore')
        session_id = hf.stem
        sections = re.split(r'\n(?=## )', content)
        for i, section in enumerate(sections):
            section = section.strip()
            if len(section) < 50:
                continue
            if len(section) > 2000:
                section = section[:2000]
            chunks.append((f"{session_id}_s{i}", section,
                          {"session_id": session_id, "chunk": i}))

    print(f"ğŸ“ ãƒãƒ£ãƒ³ã‚¯: {len(chunks)} ä»¶")

    # Encode and index
    import numpy as np
    from mekhane.symploke.adapters.embedding_adapter import EmbeddingAdapter
    adapter = EmbeddingAdapter()
    adapter.create_index(dimension=1024)

    batch_size = 32
    for start in range(0, len(chunks), batch_size):
        batch = chunks[start:start + batch_size]
        texts = [c[1] for c in batch]
        vecs = np.array([adapter.encode(t) for t in texts], dtype=np.float32)
        metas = [{"doc_id": c[0], **c[2]} for c in batch]
        adapter.add_vectors(vecs, metadata=metas)
        done = start + len(batch)
        if done % 128 == 0 or done == len(chunks):
            print(f"  é€²æ—: {done}/{len(chunks)}")

    out_path = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "indices" / "chronos.pkl"
    out_path.parent.mkdir(parents=True, exist_ok=True)
    adapter.save(str(out_path))

    elapsed = time.time() - t0
    print(f"\nâœ… chronos.pkl ä¿å­˜å®Œäº†: **{adapter.count():,}** docs ({elapsed:.1f}s)")
    print()


# PURPOSE: `pks push` â€” ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ãèƒ½å‹•çš„ãƒ—ãƒƒã‚·ãƒ¥
def cmd_push(args: argparse.Namespace) -> None:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ãèƒ½å‹•çš„ãƒ—ãƒƒã‚·ãƒ¥"""
    from mekhane.pks.pks_engine import PKSEngine

    engine = PKSEngine(
        threshold=args.threshold,
        max_push=args.max,
        enable_questions=not args.no_questions,
        enable_serendipity=True,
    )

    if args.topics:
        topics = [t.strip() for t in args.topics.split(",")]
        engine.set_context(topics=topics)
        print(f"[PKS] ãƒˆãƒ”ãƒƒã‚¯è¨­å®š: {topics}")
    elif args.auto:
        topics = engine.auto_context_from_handoff()
        if not topics:
            print("[PKS] Handoff ã‹ã‚‰ã®ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚--topics ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
            return
    elif hasattr(args, 'infer') and args.infer:
        user_input = args.infer
        topics = engine.auto_context_from_input(user_input)
        if not topics:
            print("[PKS] Attractor ã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¨è«–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return
    else:
        print("[PKS] --topics / --auto / --infer ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        return

    print("[PKS] GnÅsis æ¤œç´¢ä¸­...")
    nuggets = engine.proactive_push(k=args.k)

    if not nuggets:
        print("ğŸ“­ ãƒ—ãƒƒã‚·ãƒ¥å¯¾è±¡ã®çŸ¥è­˜ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # è³ªå•ç”Ÿæˆ
    if not args.no_questions:
        print("[PKS] è³ªå•ç”Ÿæˆä¸­...")
        nuggets = engine.suggest_questions(nuggets)

    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    report = engine.format_push_report(nuggets)
    print(report)

    # Advocacy: è«–æ–‡ä¸€äººç§°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if getattr(args, 'advocacy', False):
        _print_advocacy(nuggets, engine)


# PURPOSE: `pks suggest` â€” ãƒˆãƒ”ãƒƒã‚¯æŒ‡å®šã§ã€Œèãã¹ãè³ªå•ã€ã‚’ç”Ÿæˆ
def cmd_suggest(args: argparse.Namespace) -> None:
    """ãƒˆãƒ”ãƒƒã‚¯æŒ‡å®šã§ã€Œèãã¹ãè³ªå•ã€ã‚’ç”Ÿæˆ"""
    from mekhane.pks.pks_engine import PKSEngine

    engine = PKSEngine(enable_questions=True, enable_serendipity=False)

    topic = args.topic
    engine.set_context(topics=[topic])

    print(f"[PKS] '{topic}' ã«é–¢ã™ã‚‹çŸ¥è­˜ã‚’æ¤œç´¢ä¸­...")
    nuggets = engine.search_and_push(topic, k=args.k)

    if not nuggets:
        print(f"ğŸ“­ '{topic}' ã«é–¢é€£ã™ã‚‹çŸ¥è­˜ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # ä¸Šä½ N ä»¶ã«è³ªå•ã‚’ç”Ÿæˆ
    top_nuggets = nuggets[: args.max]
    top_nuggets = engine.suggest_questions(top_nuggets)

    for i, nugget in enumerate(top_nuggets, 1):
        print(f"\n### [{i}] {nugget.title}")
        print(f"_é–¢é€£åº¦: {nugget.relevance_score:.2f} | ã‚½ãƒ¼ã‚¹: {nugget.source}_")
        if nugget.suggested_questions:
            print("\n**ğŸ’¡ èãã¹ãè³ªå•:**")
            for q in nugget.suggested_questions:
                print(f"  - {q}")
    print()


# PURPOSE: `pks backlinks` â€” æ“¬ä¼¼ãƒãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º
def cmd_backlinks(args: argparse.Namespace) -> None:
    """æŒ‡å®šãƒˆãƒ”ãƒƒã‚¯ã®æ“¬ä¼¼ãƒãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º"""
    from mekhane.pks.matrix_view import PKSBacklinks
    from mekhane.pks.pks_engine import PKSEngine

    engine = PKSEngine(enable_questions=False, enable_serendipity=False)

    query = args.query
    print(f"[PKS] '{query}' ã®æ“¬ä¼¼ãƒãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢ä¸­...")

    nuggets = engine.search_and_push(query, k=args.k)

    if not nuggets:
        print(f"ğŸ“­ '{query}' ã«é–¢é€£ã™ã‚‹çŸ¥è­˜ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    backlinks = PKSBacklinks()
    report = backlinks.generate(query, nuggets)
    print(report)


# PURPOSE: `pks auto` â€” Handoff ã‹ã‚‰è‡ªå‹•ã§ãƒ—ãƒƒã‚·ãƒ¥
def cmd_auto(args: argparse.Namespace) -> None:
    """Handoff ã‹ã‚‰è‡ªå‹•çš„ã«ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºã—ã¦ãƒ—ãƒƒã‚·ãƒ¥"""
    from mekhane.pks.pks_engine import PKSEngine

    engine = PKSEngine(
        enable_questions=not args.no_questions,
        enable_serendipity=True,
    )

    topics = engine.auto_context_from_handoff()
    if not topics:
        print("ğŸ“­ Handoff ã‹ã‚‰ã®ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return

    print(f"[PKS] æŠ½å‡ºãƒˆãƒ”ãƒƒã‚¯: {topics}")
    print("[PKS] GnÅsis æ¤œç´¢ä¸­...")

    # verbose: æ¤œç´¢çµæœã®è·é›¢ãƒ»ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
    verbose = getattr(args, 'verbose', False)
    if verbose:
        context = engine.tracker.context
        query_text = context.to_embedding_text()
        print(f"[PKS verbose] Query: {query_text[:200]}")
        print(f"[PKS verbose] Threshold: {engine.detector.threshold}")

        index = engine._get_index()
        results = index.search(query_text, k=args.k)
        print(f"[PKS verbose] æ¤œç´¢çµæœ: {len(results)} ä»¶")
        for i, r in enumerate(results[:10]):
            dist = r.get('_distance', float('inf'))
            score = max(0.0, 1.0 - (dist / 2.0))
            passed = 'âœ…' if score >= engine.detector.threshold else 'âŒ'
            print(f"  {i+1}. [{r.get('source', '?'):8s}] {r.get('title', '?')[:50]:50s}"
                  f" dist={dist:.3f} score={score:.3f} {passed}")

    nuggets = engine.proactive_push(k=args.k)

    if verbose:
        print(f"[PKS verbose] Nuggets after scoring+filter: {len(nuggets)}")

    if not nuggets:
        print("ğŸ“­ ãƒ—ãƒƒã‚·ãƒ¥å¯¾è±¡ã®çŸ¥è­˜ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    if not args.no_questions:
        print("[PKS] è³ªå•ç”Ÿæˆä¸­...")
        nuggets = engine.suggest_questions(nuggets)

    report = engine.format_push_report(nuggets)
    print(report)

    # Advocacy: è«–æ–‡ä¸€äººç§°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if getattr(args, 'advocacy', False):
        _print_advocacy(nuggets, engine)


# PURPOSE: `pks infer` â€” Attractor ãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¨è«– + ãƒ—ãƒƒã‚·ãƒ¥
def cmd_infer(args: argparse.Namespace) -> None:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‹ã‚‰ Attractor ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¨è«–ã—ã¦ãƒ—ãƒƒã‚·ãƒ¥"""
    from mekhane.pks.pks_engine import PKSEngine

    engine = PKSEngine(
        enable_questions=not args.no_questions,
        enable_serendipity=True,
    )

    user_input = " ".join(args.input)
    topics = engine.auto_context_from_input(user_input)
    if not topics:
        print("ğŸ“­ Attractor ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¨è«–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return

    print(f"[PKS] æ¨è«–ãƒˆãƒ”ãƒƒã‚¯: {topics}")
    print("[PKS] GnÅsis æ¤œç´¢ä¸­...")

    nuggets = engine.proactive_push(k=args.k)

    if not nuggets:
        print("ğŸ“­ ãƒ—ãƒƒã‚·ãƒ¥å¯¾è±¡ã®çŸ¥è­˜ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    if not args.no_questions:
        print("[PKS] è³ªå•ç”Ÿæˆä¸­...")
        nuggets = engine.suggest_questions(nuggets)

    report = engine.format_push_report(nuggets)
    print(report)


# PURPOSE: `pks feedback` â€” ãƒ—ãƒƒã‚·ãƒ¥åå¿œã‚’è¨˜éŒ²
def cmd_feedback(args: argparse.Namespace) -> None:
    """ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçŸ¥è­˜ã¸ã®ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²"""
    from mekhane.pks.pks_engine import PKSEngine

    engine = PKSEngine(
        enable_questions=False,
        enable_serendipity=False,
        enable_feedback=True,
    )

    if args.stats:
        # çµ±è¨ˆè¡¨ç¤º
        if engine._feedback:
            stats = engine._feedback.get_stats()
            if not stats:
                print("ğŸ“­ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                return
            print("## ğŸ“Š PKS Feedback Stats\n")
            print("| Series | Count | Avg Score | Threshold Adj |")
            print("|:------:|------:|----------:|--------------:|")
            for series, s in sorted(stats.items()):
                adj = s['threshold_adjustment']
                sign = "+" if adj >= 0 else ""
                print(f"| {series} | {s['count']} | {s['avg_score']:.2f} | {sign}{adj:.3f} |")
        return

    # åå¿œè¨˜éŒ²
    engine.record_feedback(
        nugget_title=args.title,
        reaction=args.reaction,
        series=args.series or "",
    )
    print(f"âœ… Feedback recorded: '{args.title}' â†’ {args.reaction}")


# PURPOSE: `pks dialog` â€” ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçŸ¥è­˜ã¸ã®å¯¾è©±
def cmd_dialog(args: argparse.Namespace) -> None:
    """ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçŸ¥è­˜ã«å¯¾ã—ã¦å¯¾è©±çš„ã«æ¢ç´¢"""
    from mekhane.pks.pks_engine import PKSEngine
    from mekhane.pks.push_dialog import PushDialog

    engine = PKSEngine(
        enable_questions=False,
        enable_serendipity=False,
    )

    # title ã§ nugget ã‚’æ¤œç´¢
    title = args.title
    nuggets = engine.search_and_push(title, k=3)
    if not nuggets:
        print(f"ğŸ“­ '{title}' ã«è©²å½“ã™ã‚‹çŸ¥è­˜ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    nugget = nuggets[0]  # æœ€ã‚‚é–¢é€£åº¦ãŒé«˜ã„ã‚‚ã®
    dialog = PushDialog(on_feedback=engine.make_feedback_callback())

    action = args.action
    if action == "why":
        print(dialog.why(nugget))
    elif action == "ask":
        if not args.question:
            print("è³ªå•ã‚’æŒ‡å®šã—ã¦ãã ã•ã„: pks dialog ask <title> -q 'è³ªå•'")
            return
        print(dialog.deeper(nugget, args.question))
    elif action == "related":
        related = dialog.related(nugget, k=args.k)
        if not related:
            print(f"ğŸ“­ '{nugget.title}' ã®é–¢é€£çŸ¥è­˜ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        print(f"## ğŸ”— '{nugget.title}' ã®é–¢é€£çŸ¥è­˜\n")
        for i, r in enumerate(related, 1):
            print(f"{i}. **{r.title}** (é–¢é€£åº¦: {r.relevance_score:.2f}) [{r.source}]")
    else:
        print(f"ä¸æ˜ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {action}")


# PURPOSE: ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
def main() -> None:
    """PKS CLI ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(
        description="PKS v2 â€” Proactive Knowledge Surface CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=(
            "Examples:\n"
            "  pks push --topics 'FEP,CCL'     # æŒ‡å®šãƒˆãƒ”ãƒƒã‚¯ã§ãƒ—ãƒƒã‚·ãƒ¥\n"
            "  pks push --auto                  # Handoff ã‹ã‚‰è‡ªå‹•æ¤œå‡º\n"
            "  pks push --infer 'FEPã‚’èª¿æŸ»'     # Attractor æ¨è«–ã§ãƒ—ãƒƒã‚·ãƒ¥\n"
            "  pks infer 'FEPã®ç†è«–çš„åŸºç›¤'       # Attractor æ¨è«– + ãƒ—ãƒƒã‚·ãƒ¥\n"
            "  pks suggest 'Active Inference'   # è³ªå•ç”Ÿæˆ\n"
            "  pks backlinks 'FEP'              # æ“¬ä¼¼ãƒãƒƒã‚¯ãƒªãƒ³ã‚¯\n"
            "  pks auto                         # å…¨è‡ªå‹•ãƒ—ãƒƒã‚·ãƒ¥\n"
            "  pks feedback -t 'paper' -r used   # åå¿œè¨˜éŒ²\n"
            "  pks feedback --stats              # çµ±è¨ˆè¡¨ç¤º\n"
            "  pks stats                         # çŸ¥è­˜åŸºç›¤çµ±è¨ˆ\n"
            "  pks health                        # å…¨ã‚¹ã‚¿ãƒƒã‚¯ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯\n"
            "  pks search 'FEP precision'        # å…¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¨ªæ–­æ¤œç´¢\n"
            "  pks search 'active inference' -s gnosis,chronos  # ã‚½ãƒ¼ã‚¹é™å®š\n"
            "  pks rebuild chronos               # Chronos ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰\n"
        ),
    )
    subparsers = parser.add_subparsers(dest="command", help="ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰")

    # --- stats ---
    p_stats = subparsers.add_parser("stats", help="çŸ¥è­˜åŸºç›¤ã®çµ±è¨ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    p_stats.set_defaults(func=cmd_stats)

    # --- health ---
    p_health = subparsers.add_parser("health", help="AutophÅnos å…¨ã‚¹ã‚¿ãƒƒã‚¯ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯")
    p_health.set_defaults(func=cmd_health)

    # --- search ---
    p_search = subparsers.add_parser("search", help="å…¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¨ªæ–­æ¤œç´¢")
    p_search.add_argument("query", help="æ¤œç´¢ã‚¯ã‚¨ãƒª")
    p_search.add_argument("--k", type=int, default=10, help="å–å¾—ä»¶æ•° (default: 10)")
    p_search.add_argument("--sources", "-s", default=None,
                          help="æ¤œç´¢ã‚½ãƒ¼ã‚¹ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š: gnosis,kairos,sophia,chronos)")
    p_search.set_defaults(func=cmd_search)

    # --- rebuild ---
    p_rebuild = subparsers.add_parser("rebuild", help="ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å†æ§‹ç¯‰")
    p_rebuild.add_argument("target", choices=["chronos"], help="å†æ§‹ç¯‰å¯¾è±¡")
    p_rebuild.set_defaults(func=cmd_rebuild)

    # --- push ---
    p_push = subparsers.add_parser("push", help="èƒ½å‹•çš„ãƒ—ãƒƒã‚·ãƒ¥ã‚’å®Ÿè¡Œ")
    p_push.add_argument("--topics", "-t", help="ãƒˆãƒ”ãƒƒã‚¯ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)")
    p_push.add_argument("--auto", "-a", action="store_true", help="Handoff ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯è‡ªå‹•æŠ½å‡º")
    p_push.add_argument("--infer", "-i", help="Attractor ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¨è«– (ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›)")
    p_push.add_argument("--threshold", type=float, default=0.50, help="é–¢é€£åº¦é–¾å€¤ (default: 0.50)")
    p_push.add_argument("--max", "-m", type=int, default=5, help="æœ€å¤§ãƒ—ãƒƒã‚·ãƒ¥ä»¶æ•° (default: 5)")
    p_push.add_argument("--k", type=int, default=20, help="æ¤œç´¢å€™è£œæ•° (default: 20)")
    p_push.add_argument("--no-questions", action="store_true", help="è³ªå•ç”Ÿæˆã‚’ç„¡åŠ¹åŒ–")
    p_push.add_argument("--advocacy", action="store_true", help="è«–æ–‡ä¸€äººç§°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ (AutophÅnos)")
    p_push.set_defaults(func=cmd_push)

    # --- suggest ---
    p_suggest = subparsers.add_parser("suggest", help="ã€Œèãã¹ãè³ªå•ã€ã‚’ç”Ÿæˆ")
    p_suggest.add_argument("topic", help="ãƒˆãƒ”ãƒƒã‚¯")
    p_suggest.add_argument("--max", "-m", type=int, default=3, help="å¯¾è±¡ä»¶æ•° (default: 3)")
    p_suggest.add_argument("--k", type=int, default=10, help="æ¤œç´¢å€™è£œæ•° (default: 10)")
    p_suggest.set_defaults(func=cmd_suggest)

    # --- backlinks ---
    p_backlinks = subparsers.add_parser("backlinks", help="æ“¬ä¼¼ãƒãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º")
    p_backlinks.add_argument("query", help="æ¤œç´¢ã‚¯ã‚¨ãƒª")
    p_backlinks.add_argument("--k", type=int, default=15, help="æ¤œç´¢å€™è£œæ•° (default: 15)")
    p_backlinks.set_defaults(func=cmd_backlinks)

    # --- auto ---
    p_auto = subparsers.add_parser("auto", help="Handoff ã‹ã‚‰å…¨è‡ªå‹•ãƒ—ãƒƒã‚·ãƒ¥")
    p_auto.add_argument("--k", type=int, default=20, help="æ¤œç´¢å€™è£œæ•° (default: 20)")
    p_auto.add_argument("--no-questions", action="store_true", help="è³ªå•ç”Ÿæˆã‚’ç„¡åŠ¹åŒ–")
    p_auto.add_argument("--verbose", "-v", action="store_true", help="æ¤œç´¢çµæœã®ã‚¹ã‚³ã‚¢è©³ç´°ã‚’è¡¨ç¤º")
    p_auto.add_argument("--advocacy", action="store_true", help="è«–æ–‡ä¸€äººç§°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ (AutophÅnos)")
    p_auto.set_defaults(func=cmd_auto)

    # --- infer ---
    p_infer = subparsers.add_parser("infer", help="Attractor æ¨è«–ã§ãƒ—ãƒƒã‚·ãƒ¥")
    p_infer.add_argument("input", nargs="+", help="æ¨è«–å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ")
    p_infer.add_argument("--k", type=int, default=20, help="æ¤œç´¢å€™è£œæ•° (default: 20)")
    p_infer.add_argument("--no-questions", action="store_true", help="è³ªå•ç”Ÿæˆã‚’ç„¡åŠ¹åŒ–")
    p_infer.set_defaults(func=cmd_infer)

    # --- feedback ---
    p_feedback = subparsers.add_parser("feedback", help="ãƒ—ãƒƒã‚·ãƒ¥åå¿œã‚’è¨˜éŒ²")
    p_feedback.add_argument("--title", "-t", help="ãƒŠã‚²ãƒƒãƒˆã‚¿ã‚¤ãƒˆãƒ«")
    p_feedback.add_argument(
        "--reaction", "-r",
        choices=["used", "dismissed", "deepened", "ignored"],
        help="åå¿œã‚¿ã‚¤ãƒ—",
    )
    p_feedback.add_argument("--series", "-s", help="Attractor series (ä»»æ„)")
    p_feedback.add_argument("--stats", action="store_true", help="ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çµ±è¨ˆã‚’è¡¨ç¤º")
    p_feedback.set_defaults(func=cmd_feedback)

    # --- dialog ---
    p_dialog = subparsers.add_parser("dialog", help="ãƒ—ãƒƒã‚·ãƒ¥çŸ¥è­˜ã¸ã®å¯¾è©±")
    p_dialog.add_argument("action", choices=["why", "ask", "related"], help="ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
    p_dialog.add_argument("title", help="ãƒŠã‚²ãƒƒãƒˆã‚¿ã‚¤ãƒˆãƒ« (æ¤œç´¢ã‚¯ã‚¨ãƒª)")
    p_dialog.add_argument("--question", "-q", help="è³ªå• (ask ç”¨)")
    p_dialog.add_argument("--k", type=int, default=5, help="é–¢é€£çŸ¥è­˜ä»¶æ•° (default: 5)")
    p_dialog.set_defaults(func=cmd_dialog)

    args = parser.parse_args()

    if args.command is None:
        parser.print_help()
        return

    args.func(args)


if __name__ == "__main__":
    main()
