#!/usr/bin/env python3
# PURPOSE: PKS Ê§úÁ¥¢ÂìÅË≥™„Éô„É≥„ÉÅ„Éû„Éº„ÇØ ‚Äî Precision@K, MRR, Coverage, Latency „ÇíËá™ÂãïË®àÊ∏¨
"""
PKS Search Quality Benchmark

4 „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ (Gn≈çsis, Kairos, Sophia, Chronos) „ÅÆÊ®™Êñ≠Ê§úÁ¥¢ÂìÅË≥™„Çí
15 „ÉÜ„Çπ„Éà„ÇØ„Ç®„É™„ÅßÂÆöÈáèË©ï‰æ°„Åô„Çã„ÄÇ

Usage:
    cd ~/oikos/hegemonikon && PYTHONPATH=. .venv/bin/python mekhane/pks/tests/benchmark_pks.py
"""

import json
import os
import sys
import time
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Optional

# Add project root
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

# Suppress proxy/offline warnings
for key in ["HTTP_PROXY", "HTTPS_PROXY", "http_proxy", "https_proxy"]:
    os.environ.pop(key, None)
os.environ.setdefault("HF_HUB_OFFLINE", "1")
os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")

# ‚îÄ‚îÄ Constants ‚îÄ‚îÄ
MNEME_ROOT = Path.home() / "oikos" / "mneme" / ".hegemonikon"
INDICES_DIR = MNEME_ROOT / "indices"
REPORT_PATH = Path(__file__).parent / "benchmark_report.md"
RESULTS_JSON = Path(__file__).parent / "benchmark_results.json"


# ‚îÄ‚îÄ Test Query Set (15 queries, 5 categories √ó 3) ‚îÄ‚îÄ
BENCHMARK_QUERIES = [
    # Category 1: Theoretical Core (FEP / Active Inference)
    {
        "id": "T1",
        "category": "theoretical_core",
        "query": "FEP variational free energy minimization",
        "expected_sources": ["gnosis"],
        "relevance_keywords": ["free energy", "FEP", "variational", "Friston", "inference"],
        "description": "FEP „ÅÆÂü∫Á§éÁêÜË´ñ",
    },
    {
        "id": "T2",
        "category": "theoretical_core",
        "query": "active inference expected free energy planning",
        "expected_sources": ["gnosis"],
        "relevance_keywords": ["active inference", "expected free energy", "planning", "EFE"],
        "description": "ËÉΩÂãïÁöÑÊé®Ë´ñ„Å®Ë®àÁîª",
    },
    {
        "id": "T3",
        "category": "theoretical_core",
        "query": "Markov blanket self organization biological systems",
        "expected_sources": ["gnosis"],
        "relevance_keywords": ["Markov blanket", "self-organization", "biological", "partition"],
        "description": "„Éû„É´„Ç≥„ÉïÊØõÂ∏É„Å®Ëá™Â∑±ÁµÑÁπîÂåñ",
    },
    # Category 2: HGK Concepts
    {
        "id": "H1",
        "category": "hgk_concepts",
        "query": "CCL cognitive control language workflow",
        "expected_sources": ["kairos", "sophia", "chronos"],
        "relevance_keywords": ["CCL", "cognitive", "control", "workflow", "Hegemonik√≥n"],
        "description": "CCL „ÉØ„Éº„ÇØ„Éï„É≠„ÉºË®ÄË™û",
    },
    {
        "id": "H2",
        "category": "hgk_concepts",
        "query": "Noƒìsis Boulƒìsis Zƒìtƒìsis ousia series theorem",
        "expected_sources": ["kairos", "sophia", "gnosis"],
        "relevance_keywords": ["Noƒìsis", "Boulƒìsis", "Zƒìtƒìsis", "ousia", "theorem", "O-series"],
        "description": "O-series ÂÆöÁêÜÁæ§",
    },
    {
        "id": "H3",
        "category": "hgk_concepts",
        "query": "‰∫åÂ±§„Éï„Ç£„É´„Çø„Éº criterion cognitive novelty tensor product",
        "expected_sources": ["kairos", "sophia", "gnosis"],
        "relevance_keywords": ["‰∫åÂ±§", "criterion", "novelty", "tensor", "„Éï„Ç£„É´„Çø„Éº"],
        "description": "‰∫åÂ±§„Éï„Ç£„É´„Çø„ÉºÁêÜË´ñ",
    },
    # Category 3: Implementation
    {
        "id": "I1",
        "category": "implementation",
        "query": "PKS proactive knowledge surface push embedding",
        "expected_sources": ["kairos", "chronos"],
        "relevance_keywords": ["PKS", "proactive", "knowledge", "push", "embedding"],
        "description": "PKS ÂÆüË£Ö",
    },
    {
        "id": "I2",
        "category": "implementation",
        "query": "Dendron PROOF existence proof verification",
        "expected_sources": ["kairos", "chronos", "sophia"],
        "relevance_keywords": ["Dendron", "PROOF", "existence", "proof", "verification"],
        "description": "Dendron Â≠òÂú®Ë®ºÊòé",
    },
    {
        "id": "I3",
        "category": "implementation",
        "query": "Hermƒìneus parser CCL AST dispatch runtime",
        "expected_sources": ["kairos", "chronos"],
        "relevance_keywords": ["Hermƒìneus", "parser", "CCL", "AST", "dispatch", "runtime"],
        "description": "Hermƒìneus „Éë„Éº„Çµ„Éº",
    },
    # Category 4: Cross-Domain
    {
        "id": "X1",
        "category": "cross_domain",
        "query": "category theory adjunction cognitive framework",
        "expected_sources": ["gnosis", "sophia"],
        "relevance_keywords": ["category", "adjunction", "cognitive", "functor", "ÂúèË´ñ"],
        "description": "ÂúèË´ñ„Å®Ë™çÁü•„ÅÆÊé•Á∂ö",
    },
    {
        "id": "X2",
        "category": "cross_domain",
        "query": "precision weighting attention interoception prediction error",
        "expected_sources": ["gnosis"],
        "relevance_keywords": ["precision", "attention", "interoception", "prediction", "error"],
        "description": "Á≤æÂ∫¶Âä†Èáç„Å®Ê≥®ÊÑè",
    },
    {
        "id": "X3",
        "category": "cross_domain",
        "query": "Cortex API direct access bypass language server",
        "expected_sources": ["kairos", "chronos"],
        "relevance_keywords": ["Cortex", "API", "direct", "bypass", "language server"],
        "description": "Cortex API Áõ¥Êé•„Ç¢„ÇØ„Çª„Çπ",
    },
    # Category 5: Edge Cases
    {
        "id": "E1",
        "category": "edge_case",
        "query": "xylophone quantum teleportation recipe cooking",
        "expected_sources": [],
        "relevance_keywords": [],
        "description": "ÂÆåÂÖ®ÁÑ°Èñ¢‰øÇ„ÇØ„Ç®„É™ („Éé„Ç§„Ç∫Ê§úÂá∫)",
    },
    {
        "id": "E2",
        "category": "edge_case",
        "query": "Êó•Êú¨Ë™û„Å†„Åë„ÅÆ„ÇØ„Ç®„É™ Ëá™Áî±„Ç®„Éç„É´„ÇÆ„ÉºÂéüÁêÜ ‰∫àÊ∏¨Ë™§Â∑ÆÊúÄÂ∞èÂåñ",
        "expected_sources": ["gnosis", "kairos"],
        "relevance_keywords": ["Ëá™Áî±„Ç®„Éç„É´„ÇÆ„Éº", "‰∫àÊ∏¨Ë™§Â∑Æ", "FEP"],
        "description": "Êó•Êú¨Ë™û„ÇØ„Ç®„É™„ÅÆÁ≤æÂ∫¶",
    },
    {
        "id": "E3",
        "category": "edge_case",
        "query": "boot handoff session bye workflow",
        "expected_sources": ["kairos", "chronos", "sophia"],
        "relevance_keywords": ["boot", "handoff", "session", "bye", "workflow"],
        "description": "ÈÅãÁî®Á≥ª„Ç≠„Éº„ÉØ„Éº„Éâ",
    },
]


@dataclass
class SearchResult:
    """A single search result."""
    source: str
    score: float
    title: str
    snippet: str


@dataclass
class QueryResult:
    """Results for a single benchmark query."""
    query_id: str
    query: str
    category: str
    description: str
    results: list[SearchResult] = field(default_factory=list)
    latency_seconds: float = 0.0
    precision_at_5: float = 0.0
    mrr: float = 0.0
    source_coverage: float = 0.0
    keyword_hits: int = 0
    total_keywords: int = 0


def run_search(query: str, k: int = 10) -> tuple[list[SearchResult], float]:
    """Execute PKS search and return results with latency.

    Per-source min-max normalization to unify score scales.
    """
    t0 = time.time()
    results_by_source: dict[str, list[SearchResult]] = {}

    # 1. Gn≈çsis (LanceDB)
    try:
        from mekhane.anamnesis.index import GnosisIndex
        gi = GnosisIndex()
        results = gi.search(query, k=k)
        gnosis_results = []
        for r in results:
            title = r.get("title", r.get("primary_key", "?"))
            dist = float(r.get("_distance", 1.0))
            score = max(0.0, min(1.0, 1.0 - dist / 2.0))
            snippet = r.get("abstract", r.get("content", ""))[:200]
            gnosis_results.append(SearchResult("gnosis", score, title, snippet))
        if gnosis_results:
            results_by_source["gnosis"] = gnosis_results
    except Exception as e:
        print(f"  ‚ö†Ô∏è Gn≈çsis error: {e}", file=sys.stderr)

    # 2-4. pkl indices
    try:
        from mekhane.symploke.adapters.embedding_adapter import EmbeddingAdapter
        adapter = EmbeddingAdapter()
        query_vec = adapter.encode(query)

        for name in ["kairos", "sophia", "chronos"]:
            pkl = INDICES_DIR / f"{name}.pkl"
            if not pkl.exists():
                continue
            try:
                idx = EmbeddingAdapter()
                idx.load(str(pkl))
                hits = idx.search(query_vec, k=k)
                src_results = []
                for hit in hits:
                    meta = hit.metadata if hasattr(hit, "metadata") else {}
                    title = meta.get("title", meta.get("doc_id", str(hit.id)))
                    hit_score = hit.score if hasattr(hit, "score") else 0
                    src_results.append(SearchResult(name, hit_score, title, ""))
                if src_results:
                    results_by_source[name] = src_results
            except Exception as e:
                print(f"  ‚ö†Ô∏è {name} error: {e}", file=sys.stderr)
    except Exception as e:
        print(f"  ‚ö†Ô∏è Embedder error: {e}", file=sys.stderr)

    elapsed = time.time() - t0

    # Max-ratio normalization: divide each score by its source's max score
    all_results = []
    for src, src_results in results_by_source.items():
        src_max = max(r.score for r in src_results)
        if src_max <= 0:
            continue
        for r in src_results:
            normalized = r.score / src_max
            all_results.append(SearchResult(r.source, normalized, r.title, r.snippet))

    # Hybrid reranking: keyword boost on top of vector similarity
    import re
    query_tokens = [t.lower() for t in re.split(r'[\s„ÄÄ,„ÄÅ„ÄÇ„Éª/]+', query) if len(t) >= 2]
    KW_BOOST_MAX = 0.3

    for i, r in enumerate(all_results):
        if query_tokens:
            text = (r.title + " " + r.snippet).lower()
            hits = sum(1 for t in query_tokens if t in text)
            kw_ratio = hits / len(query_tokens)
            boost = kw_ratio * KW_BOOST_MAX
            all_results[i] = SearchResult(r.source, r.score + boost, r.title, r.snippet)

    all_results.sort(key=lambda x: x.score, reverse=True)
    return all_results, elapsed


def evaluate_relevance(
    result: SearchResult,
    keywords: list[str],
    expected_sources: list[str] | None = None,
) -> bool:
    """Check if a result is relevant based on keyword matching OR source match.

    Relevance is determined by:
    1. Keyword hit (at least 1/3 of keywords found in title+snippet), OR
    2. Source match (result comes from an expected source with high normalized score)
    """
    if not keywords and not expected_sources:
        return False

    # Keyword-based relevance
    kw_relevant = False
    if keywords:
        text = (result.title + " " + result.snippet).lower()
        hits = sum(1 for kw in keywords if kw.lower() in text)
        kw_relevant = hits >= max(1, len(keywords) // 3)

    # Source-based relevance (high-score results from expected sources)
    src_relevant = False
    if expected_sources and result.source in expected_sources:
        src_relevant = result.score >= 0.7  # only high-confidence matches

    return kw_relevant or src_relevant


def compute_metrics(
    results: list[SearchResult],
    expected_sources: list[str],
    keywords: list[str],
    k: int = 5,
) -> tuple[float, float, float, int, int]:
    """Compute Precision@K, MRR, and Source Coverage."""
    top_k = results[:k]

    # Precision@K: fraction of relevant results in top-k
    relevant_count = sum(
        1 for r in top_k
        if evaluate_relevance(r, keywords, expected_sources)
    )
    precision = relevant_count / k if k > 0 else 0.0

    # MRR: reciprocal rank of first relevant result
    mrr = 0.0
    for i, r in enumerate(results):
        if evaluate_relevance(r, keywords, expected_sources):
            mrr = 1.0 / (i + 1)
            break

    # Source Coverage: fraction of expected sources present in results
    if expected_sources:
        found_sources = set(r.source for r in top_k)
        coverage = len(found_sources & set(expected_sources)) / len(expected_sources)
    else:
        # Edge case: if no sources expected, coverage = 1.0 if results have low scores
        avg_score = sum(r.score for r in top_k) / len(top_k) if top_k else 0
        coverage = 1.0 if avg_score < 0.3 else 0.0

    # Keyword hit count
    all_text = " ".join((r.title + " " + r.snippet).lower() for r in top_k)
    keyword_hits = sum(1 for kw in keywords if kw.lower() in all_text) if keywords else 0

    return precision, mrr, coverage, keyword_hits, len(keywords)


def run_benchmark() -> list[QueryResult]:
    """Run the full benchmark suite."""
    print("=" * 60)
    print("PKS Search Quality Benchmark")
    print("=" * 60)
    print()

    query_results = []

    for i, q in enumerate(BENCHMARK_QUERIES, 1):
        print(f"[{i:2d}/{len(BENCHMARK_QUERIES)}] {q['id']}: {q['description']}...")

        results, latency = run_search(q["query"])
        precision, mrr, coverage, kw_hits, kw_total = compute_metrics(
            results, q["expected_sources"], q["relevance_keywords"]
        )

        qr = QueryResult(
            query_id=q["id"],
            query=q["query"],
            category=q["category"],
            description=q["description"],
            results=results[:10],
            latency_seconds=latency,
            precision_at_5=precision,
            mrr=mrr,
            source_coverage=coverage,
            keyword_hits=kw_hits,
            total_keywords=kw_total,
        )
        query_results.append(qr)

        # Progress indicator
        status = "‚úÖ" if precision >= 0.4 else "‚ö†Ô∏è" if precision > 0 else "‚ùå"
        print(f"       {status} P@5={precision:.2f} MRR={mrr:.2f} Cov={coverage:.2f} {latency:.1f}s")

    return query_results


def generate_report(results: list[QueryResult]) -> str:
    """Generate markdown benchmark report."""
    lines = []
    lines.append("# PKS Ê§úÁ¥¢ÂìÅË≥™„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„É¨„Éù„Éº„Éà")
    lines.append("")
    lines.append(f"ÂÆüË°åÊó•ÊôÇ: {time.strftime('%Y-%m-%d %H:%M')}")
    lines.append("")

    # Overall metrics
    avg_p5 = sum(r.precision_at_5 for r in results) / len(results)
    avg_mrr = sum(r.mrr for r in results) / len(results)
    avg_cov = sum(r.source_coverage for r in results) / len(results)
    avg_lat = sum(r.latency_seconds for r in results) / len(results)
    total_lat = sum(r.latency_seconds for r in results)

    lines.append("## „Çµ„Éû„É™„Éº")
    lines.append("")
    lines.append("| ÊåáÊ®ô | ÂÄ§ | ÁõÆÊ®ô | Âà§ÂÆö |")
    lines.append("|:-----|---:|:----:|:----:|")
    lines.append(f"| **Precision@5** (Âπ≥Âùá) | {avg_p5:.3f} | ‚â• 0.60 | {'‚úÖ' if avg_p5 >= 0.6 else '‚ö†Ô∏è' if avg_p5 >= 0.4 else '‚ùå'} |")
    lines.append(f"| **MRR** (Âπ≥Âùá) | {avg_mrr:.3f} | ‚â• 0.50 | {'‚úÖ' if avg_mrr >= 0.5 else '‚ö†Ô∏è' if avg_mrr >= 0.3 else '‚ùå'} |")
    lines.append(f"| **Source Coverage** (Âπ≥Âùá) | {avg_cov:.3f} | ‚â• 0.70 | {'‚úÖ' if avg_cov >= 0.7 else '‚ö†Ô∏è' if avg_cov >= 0.5 else '‚ùå'} |")
    lines.append(f"| **Latency** (Âπ≥Âùá) | {avg_lat:.1f}s | ‚â§ 15s | {'‚úÖ' if avg_lat <= 15 else '‚ùå'} |")
    lines.append(f"| **Latency** (ÂêàË®à) | {total_lat:.1f}s | ‚Äî | ‚Äî |")
    lines.append("")

    # Per-query results
    lines.append("## „ÇØ„Ç®„É™Âà•ÁµêÊûú")
    lines.append("")
    lines.append("| ID | „Ç´„ÉÜ„Ç¥„É™ | Ë™¨Êòé | P@5 | MRR | Cov | Latency | KW |")
    lines.append("|:---|:---------|:-----|----:|----:|----:|--------:|---:|")
    for r in results:
        cat_short = r.category[:12]
        p5_icon = "‚úÖ" if r.precision_at_5 >= 0.6 else "‚ö†Ô∏è" if r.precision_at_5 > 0 else "‚ùå"
        kw_str = f"{r.keyword_hits}/{r.total_keywords}" if r.total_keywords > 0 else "‚Äî"
        lines.append(
            f"| {r.query_id} | {cat_short} | {r.description} | "
            f"{r.precision_at_5:.2f} {p5_icon} | {r.mrr:.2f} | {r.source_coverage:.2f} | "
            f"{r.latency_seconds:.1f}s | {kw_str} |"
        )
    lines.append("")

    # Category analysis
    lines.append("## „Ç´„ÉÜ„Ç¥„É™Âà•ÂàÜÊûê")
    lines.append("")
    categories = {}
    for r in results:
        if r.category not in categories:
            categories[r.category] = []
        categories[r.category].append(r)

    cat_names = {
        "theoretical_core": "ÁêÜË´ñ„Ç≥„Ç¢",
        "hgk_concepts": "HGK Ê¶ÇÂøµ",
        "implementation": "ÂÆüË£Ö",
        "cross_domain": "„ÇØ„É≠„Çπ„Éâ„É°„Ç§„É≥",
        "edge_case": "„Ç®„ÉÉ„Ç∏„Ç±„Éº„Çπ",
    }

    for cat, cat_results in categories.items():
        cat_p5 = sum(r.precision_at_5 for r in cat_results) / len(cat_results)
        cat_mrr = sum(r.mrr for r in cat_results) / len(cat_results)
        cat_cov = sum(r.source_coverage for r in cat_results) / len(cat_results)
        icon = "‚úÖ" if cat_p5 >= 0.6 else "‚ö†Ô∏è" if cat_p5 >= 0.3 else "‚ùå"
        lines.append(f"### {icon} {cat_names.get(cat, cat)} (P@5={cat_p5:.2f}, MRR={cat_mrr:.2f}, Cov={cat_cov:.2f})")
        lines.append("")
        for r in cat_results:
            lines.append(f"- **{r.query_id}** {r.description}: P@5={r.precision_at_5:.2f}")
            if r.results:
                top3 = r.results[:3]
                for j, sr in enumerate(top3, 1):
                    title_short = sr.title[:50]
                    lines.append(f"  {j}. [{sr.source}] {sr.score:.3f} ‚Äî {title_short}")
        lines.append("")

    # Source distribution
    lines.append("## „ÇΩ„Éº„ÇπÂàÜÂ∏É")
    lines.append("")
    source_counts: dict[str, int] = {}
    source_scores: dict[str, list[float]] = {}
    for r in results:
        for sr in r.results[:5]:
            source_counts[sr.source] = source_counts.get(sr.source, 0) + 1
            if sr.source not in source_scores:
                source_scores[sr.source] = []
            source_scores[sr.source].append(sr.score)

    lines.append("| „ÇΩ„Éº„Çπ | Âá∫ÁèæÂõûÊï∞ (Top-5) | Âπ≥Âùá„Çπ„Ç≥„Ç¢ | ÊúÄÈ´ò | ÊúÄ‰Ωé |")
    lines.append("|:-------|-----------------:|-----------:|-----:|-----:|")
    for src in ["gnosis", "kairos", "sophia", "chronos"]:
        cnt = source_counts.get(src, 0)
        scores = source_scores.get(src, [])
        if scores:
            avg_s = sum(scores) / len(scores)
            max_s = max(scores)
            min_s = min(scores)
            lines.append(f"| {src} | {cnt} | {avg_s:.3f} | {max_s:.3f} | {min_s:.3f} |")
        else:
            lines.append(f"| {src} | 0 | ‚Äî | ‚Äî | ‚Äî |")
    lines.append("")

    # Weaknesses and recommendations
    lines.append("## Âº±ÁÇπÂàÜÊûê & ÊîπÂñÑÊèêÊ°à")
    lines.append("")

    weak_queries = [r for r in results if r.precision_at_5 < 0.4]
    if weak_queries:
        lines.append("### Âº±„ÅÑ„ÇØ„Ç®„É™")
        lines.append("")
        for r in weak_queries:
            lines.append(f"- **{r.query_id}** ({r.description}): P@5={r.precision_at_5:.2f}")
            if r.category == "edge_case" and r.query_id == "E1":
                lines.append("  ‚Üí ÊúüÂæÖÈÄö„ÇäÔºàÁÑ°Èñ¢‰øÇ„ÇØ„Ç®„É™Ôºâ")
            else:
                lines.append(f"  ‚Üí „Ç≠„Éº„ÉØ„Éº„Éâ„Éí„ÉÉ„Éà: {r.keyword_hits}/{r.total_keywords}")
        lines.append("")

    # Coverage issues
    low_cov = [r for r in results if r.source_coverage < 0.5 and r.category != "edge_case"]
    if low_cov:
        lines.append("### „ÇΩ„Éº„Çπ„Ç´„Éê„É¨„ÉÉ„Ç∏‰∏çË∂≥")
        lines.append("")
        for r in low_cov:
            found = set(sr.source for sr in r.results[:5])
            expected = set(r.category)  # simplified
            lines.append(f"- **{r.query_id}**: ÁµêÊûú„ÇΩ„Éº„Çπ={found}")
        lines.append("")

    lines.append("### ÊîπÂñÑÊèêÊ°à")
    lines.append("")
    lines.append("1. **„Çπ„Ç≥„Ç¢Ê≠£Ë¶èÂåñ**: Gn≈çsis (L2Ë∑ùÈõ¢) „Å® pkl (cosine) „ÅÆÁï∞„Å™„Çã„Çπ„Ç≥„Ç¢‰ΩìÁ≥ª„ÇíÁµ±‰∏Ä")
    lines.append("2. **Êó•Êú¨Ë™ûÂØæÂøú**: multilingual embedding „É¢„Éá„É´„ÅÆË©ï‰æ°")
    lines.append("3. **„É™„É©„É≥„Ç≠„É≥„Ç∞**: „Ç≠„Éº„ÉØ„Éº„Éâ + „Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„ÅÆ„Éè„Ç§„Éñ„É™„ÉÉ„Éâ„Çπ„Ç≥„Ç¢„É™„É≥„Ç∞")
    lines.append("4. **„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÊõ¥Êñ∞**: Sophia (116‰ª∂) „ÅÆÊã°ÂÖÖ")
    lines.append("")

    return "\n".join(lines)


def main():
    results = run_benchmark()

    # Generate report
    report = generate_report(results)
    REPORT_PATH.write_text(report, encoding="utf-8")
    print(f"\nüìù Report: {REPORT_PATH}")

    # Save raw results as JSON
    json_data = []
    for r in results:
        d = {
            "query_id": r.query_id,
            "query": r.query,
            "category": r.category,
            "description": r.description,
            "precision_at_5": r.precision_at_5,
            "mrr": r.mrr,
            "source_coverage": r.source_coverage,
            "latency_seconds": r.latency_seconds,
            "keyword_hits": r.keyword_hits,
            "total_keywords": r.total_keywords,
            "top_results": [
                {"source": sr.source, "score": sr.score, "title": sr.title}
                for sr in r.results[:5]
            ],
        }
        json_data.append(d)
    RESULTS_JSON.write_text(json.dumps(json_data, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"üìä Raw Data: {RESULTS_JSON}")

    # Summary
    avg_p5 = sum(r.precision_at_5 for r in results) / len(results)
    avg_mrr = sum(r.mrr for r in results) / len(results)
    print(f"\n{'='*40}")
    print(f"  Precision@5 = {avg_p5:.3f}  {'‚úÖ' if avg_p5 >= 0.6 else '‚ö†Ô∏è'}")
    print(f"  MRR         = {avg_mrr:.3f}  {'‚úÖ' if avg_mrr >= 0.5 else '‚ö†Ô∏è'}")
    print(f"{'='*40}")


if __name__ == "__main__":
    main()
