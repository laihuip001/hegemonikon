# PROOF: [L3/ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£] O4â†’é‹ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå¿…è¦â†’fep_demo ãŒæ‹…ã†
#!/usr/bin/env python3
"""
FEP Demo: HegemonikÃ³n Active Inference Agent

Demonstrates the pymdp integration with Stoic philosophy concepts.

Usage:
    python scripts/fep_demo.py
"""

import sys
sys.path.insert(0, ".")

from mekhane.fep import HegemonikÃ³nFEPAgent
from mekhane.fep.state_spaces import (
    PHANTASIA_STATES,
    ASSENT_STATES,
    HORME_STATES,
    OBSERVATION_MODALITIES,
    index_to_state,
)
import numpy as np


def print_separator(title: str = ""):
    """Print a visual separator."""
    if title:
        print(f"\n{'â•' * 60}")
        print(f"  {title}")
        print(f"{'â•' * 60}")
    else:
        print(f"{'â”€' * 60}")


def print_beliefs(beliefs: np.ndarray, title: str = "ä¿¡å¿µåˆ†å¸ƒ"):
    """Pretty print belief distribution."""
    print(f"\nğŸ“Š {title}:")
    for idx, prob in enumerate(beliefs):
        if prob > 0.01:  # Only show significant beliefs
            p, a, h = index_to_state(idx)
            bar = "â–ˆ" * int(prob * 20)
            print(f"   [{p:9s} / {a:8s} / {h:7s}]: {prob:.2%} {bar}")


def demo_single_observation():
    """Demonstrate single observation inference."""
    print_separator("O1 NoÄ“sis: å˜ä¸€è¦³æ¸¬ã‹ã‚‰ã®ä¿¡å¿µæ›´æ–°")
    
    agent = HegemonikÃ³nFEPAgent(use_defaults=True)
    
    # Initial beliefs
    print("\nğŸ”¹ åˆæœŸä¿¡å¿µ (Epistemic Humility ã‚’åæ˜ ):")
    print_beliefs(agent.beliefs)
    
    # Observation: clear context (index 1)
    print("\nğŸ”¹ è¦³æ¸¬: clear (æ˜ç¢ºãªæ–‡è„ˆ)")
    result = agent.infer_states(observation=1)
    
    print_beliefs(result["beliefs"], "æ›´æ–°å¾Œã®ä¿¡å¿µ")
    print(f"\n   MAP çŠ¶æ…‹: {result['map_state_names']}")
    print(f"   ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {result['entropy']:.3f}")


def demo_policy_selection():
    """Demonstrate policy selection (O2 BoulÄ“sis)."""
    print_separator("O2 BoulÄ“sis: ãƒãƒªã‚·ãƒ¼é¸æŠ")
    
    agent = HegemonikÃ³nFEPAgent(use_defaults=True)
    
    # First, update beliefs
    agent.infer_states(observation=1)  # Clear context observed
    
    # Then, infer policies
    q_pi, neg_efe = agent.infer_policies()
    
    print("\nğŸ“Š ãƒãƒªã‚·ãƒ¼ç¢ºç‡:")
    actions = ["observe (O1 NoÄ“sis)", "act (O4 Energeia)"]
    for i, (prob, efe) in enumerate(zip(q_pi, neg_efe)):
        bar = "â–ˆ" * int(prob * 20)
        print(f"   Action {i} ({actions[i]}): {prob:.2%} {bar}")
        print(f"      Expected Free Energy: {-efe:.3f}")


def demo_full_cycle():
    """Demonstrate full inference-action cycle."""
    print_separator("å®Œå…¨ã‚µã‚¤ã‚¯ãƒ«: O1 â†’ O2 â†’ O4")
    
    agent = HegemonikÃ³nFEPAgent(use_defaults=True)
    
    observations = [
        (1, "clear context"),
        (6, "high confidence"),
        (3, "medium urgency"),
    ]
    
    for obs_idx, obs_name in observations:
        print(f"\nğŸ”¹ è¦³æ¸¬ {obs_idx}: {obs_name}")
        result = agent.step(observation=obs_idx)
        
        print(f"   MAP çŠ¶æ…‹: {result['map_state_names']}")
        print(f"   ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {result['entropy']:.3f}")
        print(f"   é¸æŠè¡Œå‹•: {result['action_name']}")
        print_separator()


def demo_entropy_as_uncertainty():
    """Demonstrate entropy as a measure of uncertainty."""
    print_separator("ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–")
    
    agent = HegemonikÃ³nFEPAgent(use_defaults=True)
    
    print("\nè¦³æ¸¬ã«ã‚ˆã‚‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¤‰åŒ–:")
    print(f"   åˆæœŸã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {-np.sum(agent.beliefs * np.log(agent.beliefs + 1e-10)):.3f}")
    
    # Different observations
    observations = [
        (0, "ambiguous context"),
        (1, "clear context"),
        (2, "low urgency"),
        (4, "high urgency"),
        (5, "low confidence"),
        (7, "high confidence"),
    ]
    
    for obs_idx, obs_name in observations:
        agent = HegemonikÃ³nFEPAgent(use_defaults=True)  # Reset
        result = agent.infer_states(observation=obs_idx)
        print(f"   {obs_name:20s} â†’ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {result['entropy']:.3f}")


def main():
    """Run all demonstrations."""
    import argparse
    
    parser = argparse.ArgumentParser(description="HegemonikÃ³n FEP Demo")
    parser.add_argument("-i", "--interactive", action="store_true",
                        help="å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•")
    args = parser.parse_args()
    
    if args.interactive:
        return interactive_mode()
    
    # Non-interactive demos
    print_separator("HegemonikÃ³n FEP Demo")
    print("\npymdp Active Inference ã‚’ç”¨ã„ãŸã‚¹ãƒˆã‚¢æ´¾èªçŸ¥ãƒ¢ãƒ‡ãƒ«")
    print("O1 NoÄ“sis (èªè­˜) â†’ O2 BoulÄ“sis (æ„å¿—) â†’ O4 Energeia (è¡Œå‹•)")
    
    try:
        demo_single_observation()
        demo_policy_selection()
        demo_full_cycle()
        demo_entropy_as_uncertainty()
        
        print_separator("ãƒ‡ãƒ¢å®Œäº†")
        print("\nâœ… pymdp çµ±åˆã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
        print("   å¯¾è©±ãƒ¢ãƒ¼ãƒ‰: python scripts/fep_demo.py -i")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


def interactive_mode():
    """å¯¾è©±å‹ FEP ãƒ‡ãƒ¢ (REPL)"""
    from mekhane.fep.encoding import encode_to_flat_index, decode_observation
    
    print_separator("FEP Interactive Mode")
    print("\nğŸ§  HegemonikÃ³n Active Inference Agent")
    print("å…¥åŠ›: è‡ªç„¶è¨€èªãƒ†ã‚­ã‚¹ãƒˆ â†’ FEP æ¨è«– â†’ è¡Œå‹•æ¨å¥¨")
    print("ãƒ˜ãƒ«ãƒ—: /help\n")
    
    agent = HegemonikÃ³nFEPAgent(use_defaults=True)
    
    # åˆæœŸAè¡Œåˆ—ã‚’ä¿å­˜ (diff è¨ˆç®—ç”¨)
    initial_A = agent.agent.A[0].copy() if hasattr(agent.agent.A, '__getitem__') else agent.agent.A.copy()
    
    # å±¥æ­´
    history = []
    learning_count = 0
    
    def get_entropy():
        """ç¾åœ¨ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’è¨ˆç®—"""
        beliefs = agent.beliefs
        # Handle nested structure from pymdp
        if isinstance(beliefs, np.ndarray):
            if beliefs.dtype == object:
                qs = np.asarray(beliefs[0], dtype=np.float64).flatten()
            else:
                qs = np.asarray(beliefs, dtype=np.float64).flatten()
        elif isinstance(beliefs, list):
            qs = np.asarray(beliefs[0], dtype=np.float64).flatten()
        else:
            qs = np.asarray(beliefs, dtype=np.float64).flatten()
        return float(-np.sum(qs * np.log(qs + 1e-10)))
    
    def show_help():
        print("""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ FEP Interactive Mode - ã‚³ãƒãƒ³ãƒ‰ä¸€è¦§                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›]  â†’ FEP æ¨è«–ã‚’å®Ÿè¡Œ                         â”‚
â”‚ /help          â†’ ã“ã®ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º                        â”‚
â”‚ /entropy       â†’ ç¾åœ¨ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (ä¸ç¢ºå®Ÿæ€§) ã‚’è¡¨ç¤º    â”‚
â”‚ /diff          â†’ Aè¡Œåˆ—ã®ç´¯ç©å­¦ç¿’é‡ã‚’è¡¨ç¤º                 â”‚
â”‚ /history       â†’ ç›´è¿‘ã®å…¥åŠ›å±¥æ­´ã‚’è¡¨ç¤º                    â”‚
â”‚ /save          â†’ å­¦ç¿’æ¸ˆã¿Aè¡Œåˆ—ã‚’ä¿å­˜                     â”‚
â”‚ /load          â†’ ä¿å­˜æ¸ˆã¿Aè¡Œåˆ—ã‚’èª­è¾¼                     â”‚
â”‚ /reset         â†’ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–                    â”‚
â”‚ /quit, /q      â†’ çµ‚äº†                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
        """)
    
    try:
        while True:
            try:
                user_input = input("fep> ").strip()
            except EOFError:
                break
            
            # ç©ºå…¥åŠ›ã¯ç„¡è¦–
            if not user_input:
                continue
            
            # ã‚³ãƒãƒ³ãƒ‰å‡¦ç†
            if user_input.startswith("/"):
                cmd = user_input.lower()
                
                if cmd in ("/quit", "/q"):
                    print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™")
                    break
                    
                elif cmd == "/help":
                    show_help()
                    
                elif cmd == "/reset":
                    agent = HegemonikÃ³nFEPAgent(use_defaults=True)
                    initial_A = agent.agent.A[0].copy() if hasattr(agent.agent.A, '__getitem__') else agent.agent.A.copy()
                    history.clear()
                    learning_count = 0
                    print("âœ… ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ")
                    
                elif cmd == "/entropy":
                    print(f"ğŸ“Š ç¾åœ¨ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {get_entropy():.3f}")
                    
                elif cmd == "/diff":
                    current_A = agent.agent.A[0].copy() if hasattr(agent.agent.A, '__getitem__') else agent.agent.A.copy()
                    diff = np.abs(current_A - initial_A).sum()
                    print(f"ğŸ“ˆ Aè¡Œåˆ—å¤‰åŒ–é‡ (L1 norm): {diff:.4f}")
                    print(f"   å­¦ç¿’å›æ•°: {learning_count}")
                    
                elif cmd == "/history":
                    if not history:
                        print("ğŸ“œ å±¥æ­´ãªã—")
                    else:
                        print("ğŸ“œ ç›´è¿‘ã®å±¥æ­´:")
                        for i, h in enumerate(history[-10:], 1):
                            print(f"   {i}. \"{h['input'][:30]}...\" â†’ {h['action']} (H={h['entropy']:.2f})")
                            
                elif cmd == "/save":
                    path = agent.save_learned_A()
                    print(f"ğŸ’¾ Aè¡Œåˆ—ä¿å­˜: {path}")
                    
                elif cmd == "/load":
                    if agent.load_learned_A():
                        print("âœ… Aè¡Œåˆ—èª­è¾¼å®Œäº†")
                    else:
                        print("âš ï¸ å­¦ç¿’æ¸ˆã¿Aè¡Œåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        
                else:
                    print(f"â“ ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰: {cmd}")
                    print("   /help ã§ã‚³ãƒãƒ³ãƒ‰ä¸€è¦§ã‚’è¡¨ç¤º")
                    
                continue
            
            # ãƒ†ã‚­ã‚¹ãƒˆ â†’ FEP æ¨è«–
            try:
                obs = encode_to_flat_index(user_input)
                result = agent.infer_states(obs)
                
                # Dirichlet å­¦ç¿’
                agent.update_A_dirichlet(obs)
                learning_count += 1
                
                # ãƒãƒªã‚·ãƒ¼é¸æŠ
                q_pi, _ = agent.infer_policies()
                action = agent.sample_action()
                action_name = "observe (æ·±ãè€ƒãˆã‚‹)" if action == 0 else "act (å®Ÿè¡Œã™ã‚‹)"
                
                # å‡ºåŠ›
                entropy = result['entropy']
                print(f"  ğŸ“¥ obs={obs} â†’ çŠ¶æ…‹: {result['map_state_names']}")
                print(f"  ğŸ“Š ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {entropy:.2f}")
                print(f"  ğŸ¯ æ¨å¥¨è¡Œå‹•: {action_name} ({q_pi[action]:.1%})")
                
                # å±¥æ­´ã«è¿½åŠ 
                history.append({
                    "input": user_input,
                    "obs": obs,
                    "entropy": entropy,
                    "action": action_name.split()[0],
                })
                
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Ctrl+C ã§çµ‚äº†")
    
    print(f"\nğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ: {len(history)} æ¨è«–, {learning_count} å­¦ç¿’")
    return 0


if __name__ == "__main__":
    sys.exit(main())

