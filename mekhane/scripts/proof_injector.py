#!/usr/bin/env python3
# PROOF: [L3/ãƒ„ãƒ¼ãƒ«] <- mekhane/scripts/
"""
PROOF Header Batch Injector

Usage:
    python3 proof_injector.py [--dry-run]
"""

import argparse
import re
from pathlib import Path

# ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®PROOFãƒ¬ãƒ™ãƒ«è¨­å®š
FILE_LEVELS = {
    # mekhane/fep - å®šç†å®Ÿè£…
    "fep_agent.py": "L1/å®šç†",
    "horme_evaluator.py": "L1/å®šç†",
    "akribeia_evaluator.py": "L1/å®šç†",
    "telos_checker.py": "L1/å®šç†",
    "krisis_judge.py": "L1/å®šç†",
    "chronos_evaluator.py": "L1/å®šç†",
    "eukairia_detector.py": "L1/å®šç†",
    "zetesis_inquirer.py": "L1/å®šç†",
    "sophia_researcher.py": "L1/å®šç†",
    "energeia_executor.py": "L1/å®šç†",
    "doxa_persistence.py": "L1/å®šç†",
    "derivative_selector.py": "L1/å®šç†",
    "meaningful_traces.py": "L1/å®šç†",
    "perigraphe_engine.py": "L1/å®šç†",
    # mekhane/fep - ã‚¤ãƒ³ãƒ•ãƒ©
    "fep_bridge.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "encoding.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "persistence.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "state_spaces.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "llm_evaluator.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "config.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "schema_analyzer.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "tekhne_registry.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "se_principle_validator.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "__init__.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    # mekhane/anamnesis
    "module_indexer.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "mneme_cli.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "memory_search.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "antigravity_logs.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "logger.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "index_v2.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "vault.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "test_extract.py": "L3/ãƒ†ã‚¹ãƒˆ",
    "export_chats.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "index.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "export_simple.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "night_review.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "workflow_inventory.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "lancedb_indexer.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "cli.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "workflow_artifact_batch.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    # collectors
    "arxiv.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "semantic_scholar.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "base.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    "openalex.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    # models
    "paper.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
    # symploke
    "factory.py": "L2/ã‚¤ãƒ³ãƒ•ãƒ©",
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¬ãƒ™ãƒ«
DEFAULT_LEVEL = "L2/ã‚¤ãƒ³ãƒ•ãƒ©"


# PURPOSE: ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰PROOFãƒ¬ãƒ™ãƒ«ã‚’å–å¾—
def get_proof_level(filename: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰PROOFãƒ¬ãƒ™ãƒ«ã‚’å–å¾—"""
    return FILE_LEVELS.get(filename, DEFAULT_LEVEL)


# PURPOSE: ãƒ•ã‚¡ã‚¤ãƒ«ã«PROOFãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ 
def add_proof_header(filepath: Path, dry_run: bool = False) -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã«PROOFãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ """
    try:
        content = filepath.read_text(encoding="utf-8")
    except Exception as e:
        print(f"  âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return False

    # æ—¢å­˜ã®PROOFãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
    if re.search(r"#\s*PROOF:", content[:500]):
        print(f"  â­ï¸ æ—¢å­˜: {filepath}")
        return True

    level = get_proof_level(filepath.name)
    header = f"# PROOF: [{level}]\n"

    # shebangãŒã‚ã‚‹å ´åˆã¯ãã®å¾Œã«æŒ¿å…¥
    if content.startswith("#!"):
        lines = content.split("\n", 1)
        new_content = lines[0] + "\n" + header + (lines[1] if len(lines) > 1 else "")
    else:
        new_content = header + content

    if dry_run:
        print(f"  ğŸ” [DRY-RUN] {filepath} â†’ [{level}]")
    else:
        filepath.write_text(new_content, encoding="utf-8")
        print(f"  âœ… è¿½åŠ : {filepath} â†’ [{level}]")

    return True


# PURPOSE: é–¢æ•°: main
def main():
    parser = argparse.ArgumentParser(description="PROOF Header Batch Injector")
    parser.add_argument("--dry-run", action="store_true", help="å®Ÿéš›ã«ã¯å¤‰æ›´ã—ãªã„")
    args = parser.parse_args()

    root = Path(__file__).parent.parent  # mekhane ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
    targets = [
        # fep
        root / "fep/schema_analyzer.py",
        root / "fep/chronos_evaluator.py",
        root / "fep/fep_agent.py",
        root / "fep/eukairia_detector.py",
        root / "fep/telos_checker.py",
        root / "fep/krisis_judge.py",
        root / "fep/horme_evaluator.py",
        root / "fep/fep_bridge.py",
        root / "fep/__init__.py",
        root / "fep/encoding.py",
        root / "fep/perigraphe_engine.py",
        root / "fep/doxa_persistence.py",
        root / "fep/persistence.py",
        root / "fep/energeia_executor.py",
        root / "fep/state_spaces.py",
        root / "fep/zetesis_inquirer.py",
        root / "fep/sophia_researcher.py",
        root / "fep/derivative_selector.py",
        root / "fep/llm_evaluator.py",
        root / "fep/config.py",
        root / "fep/meaningful_traces.py",
        root / "fep/akribeia_evaluator.py",
        root / "fep/tekhne_registry.py",
        root / "fep/se_principle_validator.py",
        # anamnesis
        root / "anamnesis/module_indexer.py",
        root / "anamnesis/mneme_cli.py",
        root / "anamnesis/memory_search.py",
        root / "anamnesis/antigravity_logs.py",
        root / "anamnesis/logger.py",
        root / "anamnesis/index_v2.py",
        root / "anamnesis/vault.py",
        root / "anamnesis/test_extract.py",
        root / "anamnesis/export_chats.py",
        root / "anamnesis/index.py",
        root / "anamnesis/export_simple.py",
        root / "anamnesis/night_review.py",
        root / "anamnesis/workflow_inventory.py",
        root / "anamnesis/lancedb_indexer.py",
        root / "anamnesis/cli.py",
        root / "anamnesis/workflow_artifact_batch.py",
        root / "anamnesis/models/paper.py",
        root / "anamnesis/collectors/arxiv.py",
        root / "anamnesis/collectors/semantic_scholar.py",
        root / "anamnesis/collectors/base.py",
        root / "anamnesis/collectors/openalex.py",
        # symploke
        root / "symploke/factory.py",
        root / "symploke/config.py",
    ]

    print(f"ğŸ“‹ PROOF Header Injector")
    print(f"   å¯¾è±¡: {len(targets)} ãƒ•ã‚¡ã‚¤ãƒ«")
    print(f"   ãƒ¢ãƒ¼ãƒ‰: {'DRY-RUN' if args.dry_run else 'å®Ÿè¡Œ'}")
    print()

    success = 0
    for target in targets:
        if target.exists():
            if add_proof_header(target, args.dry_run):
                success += 1
        else:
            print(f"  âš ï¸ è¦‹ã¤ã‹ã‚‰ãªã„: {target}")

    print()
    print(f"âœ… å®Œäº†: {success}/{len(targets)} ãƒ•ã‚¡ã‚¤ãƒ«")


if __name__ == "__main__":
    main()
