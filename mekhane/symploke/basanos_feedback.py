#!/usr/bin/env python3
# PROOF: [L2/åˆ†æ] <- mekhane/symploke/ A4â†’å“è³ªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯â†’basanos_feedback ãŒæ‹…ã†
# PURPOSE: Basanos Perspective ã®å“è³ªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ â€” ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœã‹ã‚‰æœ‰ç”¨æ€§ã‚¹ã‚³ã‚¢ã‚’è“„ç©
"""
Basanos Feedback Loop

Perspective ã”ã¨ã®ã€Œæœ‰ç”¨ãªæŒ‡æ‘˜ç‡ã€ã‚’è¿½è·¡ã—ã€
ä½å“è³ªãƒ‘ãƒ¼ã‚¹ãƒšã‚¯ãƒ†ã‚£ãƒ–ã‚’æ¸›è¡°ã•ã›ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿæ§‹ã€‚

Usage:
    # çµæœåé›† (scheduler ãƒ­ã‚° + Jules PR çµæœã‚’çµ±åˆ)
    python basanos_feedback.py collect --days 7

    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çŠ¶æ…‹è¡¨ç¤º
    python basanos_feedback.py show

    # å“è³ªã‚¹ã‚³ã‚¢ã§ BasanosBridge ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é©ç”¨
    from basanos_feedback import FeedbackStore
    store = FeedbackStore()
    excluded = store.get_low_quality_perspectives(threshold=0.1)
"""

import argparse
import json
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

_THIS_DIR = Path(__file__).resolve().parent
_PROJECT_ROOT = _THIS_DIR.parent.parent
_STATE_FILE = _PROJECT_ROOT / "logs" / "specialist_daily" / "basanos_feedback_state.json"
_SCHEDULER_LOG_DIR = _PROJECT_ROOT / "logs" / "specialist_daily"


# PURPOSE: å€‹åˆ¥ Perspective ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿
@dataclass
class PerspectiveFeedback:
    """Perspective ID ã”ã¨ã®ç´¯ç©ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€‚"""
    perspective_id: str       # e.g. "BP-Architecture-O1"
    domain: str
    axis: str
    total_reviews: int = 0    # ã“ã®ãƒ‘ãƒ¼ã‚¹ãƒšã‚¯ãƒ†ã‚£ãƒ–ãŒä½¿ã‚ã‚ŒãŸå›æ•°
    useful_count: int = 0     # æœ‰ç”¨ãªæŒ‡æ‘˜ã‚’å‡ºã—ãŸå›æ•°
    last_used: str = ""       # æœ€å¾Œã«ä½¿ã‚ã‚ŒãŸæ—¥æ™‚

    # PURPOSE: æœ‰ç”¨ãªæŒ‡æ‘˜ç‡ (0.0 - 1.0)ã€‚
    @property
    def usefulness_rate(self) -> float:
        """æœ‰ç”¨ãªæŒ‡æ‘˜ç‡ (0.0 - 1.0)ã€‚"""
        if self.total_reviews == 0:
            return 0.5  # æœªä½¿ç”¨ = ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ« (æ¸›è¡°ã—ãªã„)
        return self.useful_count / self.total_reviews


# PURPOSE: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çŠ¶æ…‹ã®æ°¸ç¶šåŒ–
class FeedbackStore:
    """Basanos Perspective ã®å“è³ªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç®¡ç†ã€‚"""

    def __init__(self, state_file: Optional[Path] = None):
        self._state_file = state_file or _STATE_FILE
        self._data: dict[str, PerspectiveFeedback] = {}
        self._load()

    def _load(self):
        """çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ã€‚"""
        if not self._state_file.exists():
            return
        try:
            raw = json.loads(self._state_file.read_text())
            for pid, entry in raw.items():
                self._data[pid] = PerspectiveFeedback(
                    perspective_id=pid,
                    domain=entry.get("domain", ""),
                    axis=entry.get("axis", ""),
                    total_reviews=entry.get("total_reviews", 0),
                    useful_count=entry.get("useful_count", 0),
                    last_used=entry.get("last_used", ""),
                )
        except (json.JSONDecodeError, KeyError):
            pass

    def _save(self):
        """çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿ã€‚"""
        self._state_file.parent.mkdir(parents=True, exist_ok=True)
        raw = {}
        for pid, fb in self._data.items():
            raw[pid] = {
                "domain": fb.domain,
                "axis": fb.axis,
                "total_reviews": fb.total_reviews,
                "useful_count": fb.useful_count,
                "last_used": fb.last_used,
                "usefulness_rate": round(fb.usefulness_rate, 3),
            }
        self._state_file.write_text(json.dumps(raw, indent=2, ensure_ascii=False))

    # PURPOSE: Perspective ã®ä½¿ç”¨ã‚’è¨˜éŒ²ã€‚
    def record_usage(self, perspective_id: str, domain: str, axis: str, was_useful: bool):
        """Perspective ã®ä½¿ç”¨ã‚’è¨˜éŒ²ã€‚"""
        if perspective_id not in self._data:
            self._data[perspective_id] = PerspectiveFeedback(
                perspective_id=perspective_id,
                domain=domain,
                axis=axis,
            )
        fb = self._data[perspective_id]
        fb.total_reviews += 1
        if was_useful:
            fb.useful_count += 1
        fb.last_used = datetime.now().strftime("%Y-%m-%d %H:%M")

    # PURPOSE: æœ‰ç”¨ç‡ãŒé–¾å€¤ä»¥ä¸‹ã® Perspective ID ãƒªã‚¹ãƒˆ (10å›ä»¥ä¸Šä½¿ç”¨ã•ã‚ŒãŸã‚‚ã®ã®ã¿)ã€‚
    def get_low_quality_perspectives(self, threshold: float = 0.1) -> list[str]:
        """æœ‰ç”¨ç‡ãŒé–¾å€¤ä»¥ä¸‹ã® Perspective ID ãƒªã‚¹ãƒˆ (10å›ä»¥ä¸Šä½¿ç”¨ã•ã‚ŒãŸã‚‚ã®ã®ã¿)ã€‚"""
        return [
            pid for pid, fb in self._data.items()
            if fb.total_reviews >= 10 and fb.usefulness_rate < threshold
        ]

    # PURPOSE: å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ã€‚
    def get_all_feedback(self) -> dict[str, PerspectiveFeedback]:
        """å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ã€‚"""
        return dict(self._data)

    # PURPOSE: å¤–éƒ¨ä¿å­˜ç”¨ã€‚
    def save(self):
        """å¤–éƒ¨ä¿å­˜ç”¨ã€‚"""
        self._save()

    # PURPOSE: æ·˜æ±°ãƒ¬ãƒãƒ¼ãƒˆã‚’è¿”ã™ (F14)ã€‚
    def get_exclusion_report(self, threshold: float = 0.1) -> dict:
        """æ·˜æ±°ãƒ¬ãƒãƒ¼ãƒˆã‚’è¿”ã™ (F14)ã€‚"""
        excluded = self.get_low_quality_perspectives(threshold)
        total = len(self._data)
        return {
            "excluded_count": len(excluded),
            "excluded_ids": excluded,
            "threshold": threshold,
            "total_perspectives": total,
            "exclusion_rate": round(len(excluded) / total, 3) if total else 0.0,
        }

    # PURPOSE: æŒ‡å®šæ—¥æ•°ä»¥ä¸Šä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ Perspective ID ãƒªã‚¹ãƒˆã‚’è¿”ã™ (F24)ã€‚
    def get_stale_perspectives(self, inactive_days: int = 30) -> list[str]:
        """æŒ‡å®šæ—¥æ•°ä»¥ä¸Šä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ Perspective ID ãƒªã‚¹ãƒˆã‚’è¿”ã™ (F24)ã€‚

        Args:
            inactive_days: éæ´»æ€§ã¨ã¿ãªã™æ—¥æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30æ—¥)

        Returns:
            stale perspective_id ã®ãƒªã‚¹ãƒˆ
        """
        from datetime import datetime, timedelta
        cutoff = datetime.now() - timedelta(days=inactive_days)
        cutoff_str = cutoff.strftime("%Y-%m-%d")

        stale = []
        for pid, fb in self._data.items():
            if not fb.last_used:
                stale.append(pid)
            elif fb.last_used < cutoff_str:
                stale.append(pid)
        return stale

    # PURPOSE: Perspective ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–çŠ¶æ…‹ã«ã™ã‚‹ (F24)ã€‚
    def archive_perspective(self, perspective_id: str) -> bool:
        """Perspective ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–çŠ¶æ…‹ã«ã™ã‚‹ (F24)ã€‚

        å‰Šé™¤ã§ã¯ãªãã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã€‚archived/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã—ã€
        ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã¯é™¤å¤–ã™ã‚‹ã€‚

        Returns:
            True if archived, False if not found
        """
        import json

        if perspective_id not in self._data:
            return False

        fb = self._data[perspective_id]

        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜
        archive_dir = self._state_file.parent / "archived_perspectives"
        archive_dir.mkdir(parents=True, exist_ok=True)
        archive_file = archive_dir / "archive.jsonl"

        archive_record = {
            "perspective_id": perspective_id,
            "domain": fb.domain,
            "axis": fb.axis,
            "total_reviews": fb.total_reviews,
            "useful_count": fb.useful_count,
            "usefulness_rate": fb.usefulness_rate,
            "last_used": fb.last_used,
            "archived_at": __import__("datetime").datetime.now().isoformat(),
        }

        with open(archive_file, "a") as f:
            f.write(json.dumps(archive_record, ensure_ascii=False) + "\n")

        # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é™¤å¤–
        del self._data[perspective_id]
        self._save()
        return True


# PURPOSE: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒ­ã‚°ã‹ã‚‰ basanos ä½¿ç”¨å®Ÿç¸¾ã‚’åé›†
def collect_from_logs(days: int = 7) -> dict:
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒ­ã‚°ã‚’è§£æã—ã€Perspective ä½¿ç”¨å®Ÿç¸¾ã‚’åé›†ã™ã‚‹ã€‚

    NOTE: ç¾æ™‚ç‚¹ã§ã¯ã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼ã«ä½¿ã‚ã‚ŒãŸã€ã“ã¨ã®ã¿è¿½è·¡ã€‚
    ã€Œæœ‰ç”¨ã ã£ãŸã‹ã€ã¯ Jules ã® PR çµæœã‚’è§£æã™ã‚‹å¿…è¦ãŒã‚ã‚‹ (Phase 2)ã€‚
    """
    store = FeedbackStore()
    cutoff = datetime.now() - timedelta(days=days)
    processed = 0

    for log_file in sorted(_SCHEDULER_LOG_DIR.glob("scheduler_*.json")):
        try:
            data = json.loads(log_file.read_text())
            ts_str = data.get("timestamp", "")
            ts = datetime.strptime(ts_str, "%Y-%m-%d %H:%M")
            if ts < cutoff:
                continue

            if data.get("mode") != "basanos":
                continue

            basanos_info = data.get("basanos", {})
            domains = basanos_info.get("domains", [])
            axes_count = basanos_info.get("axes", 24)

            # ä½¿ç”¨ã•ã‚ŒãŸ Perspective ã‚’è¨˜éŒ² (å…¨ domain Ã— å…¨ axis)
            for domain in domains:
                for axis_num in range(1, axes_count + 1):
                    # å®Ÿéš›ã® axis ID ã¯ä¸æ˜ãªã®ã§ convention ã§ç”Ÿæˆ
                    series = ["O", "S", "H", "P", "K", "A"]
                    for s in series:
                        axis_nums_per_series = 4
                        for an in range(1, axis_nums_per_series + 1):
                            pid = f"BP-{domain}-{s}{an}"
                            store.record_usage(
                                perspective_id=pid,
                                domain=domain,
                                axis=f"{s}{an}",
                                was_useful=False,  # ç¾æ™‚ç‚¹ã§ã¯ä¸æ˜ â†’ Phase 2 ã§æ”¹å–„
                            )

            processed += 1

        except (json.JSONDecodeError, ValueError):
            continue

    store.save()
    return {
        "processed_logs": processed,
        "total_perspectives": len(store.get_all_feedback()),
    }


# PURPOSE: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çŠ¶æ…‹ã‚’è¡¨ç¤º
def show_feedback():
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çŠ¶æ…‹ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤ºã€‚"""
    store = FeedbackStore()
    all_fb = store.get_all_feedback()

    if not all_fb:
        print("ğŸ“­ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ãªã—")
        return

    print(f"\n{'='*50}")
    print(f"Basanos Perspective Feedback â€” {len(all_fb)} perspectives tracked")
    print(f"{'='*50}")

    # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥é›†è¨ˆ
    domain_stats: dict[str, dict] = {}
    for fb in all_fb.values():
        if fb.domain not in domain_stats:
            domain_stats[fb.domain] = {"count": 0, "total_reviews": 0, "useful": 0}
        domain_stats[fb.domain]["count"] += 1
        domain_stats[fb.domain]["total_reviews"] += fb.total_reviews
        domain_stats[fb.domain]["useful"] += fb.useful_count

    print(f"\nğŸ“Š Domain Summary:")
    print(f"  {'Domain':24s} {'Perspectives':>12s} {'Reviews':>8s} {'Useful':>8s} {'Rate':>6s}")
    for domain, stats in sorted(domain_stats.items()):
        rate = stats["useful"] / stats["total_reviews"] * 100 if stats["total_reviews"] else 0
        print(f"  {domain:24s} {stats['count']:12d} {stats['total_reviews']:8d} {stats['useful']:8d} {rate:5.1f}%")

    # ä½å“è³ªãƒ‘ãƒ¼ã‚¹ãƒšã‚¯ãƒ†ã‚£ãƒ–
    low_quality = store.get_low_quality_perspectives(threshold=0.1)
    if low_quality:
        print(f"\nâš ï¸  Low-quality perspectives ({len(low_quality)}):")
        for pid in low_quality[:10]:
            fb = all_fb[pid]
            print(f"  {pid}: {fb.usefulness_rate:.1%} ({fb.useful_count}/{fb.total_reviews})")


# PURPOSE: CLI ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
def main():
    parser = argparse.ArgumentParser(description="Basanos Perspective Feedback")
    sub = parser.add_subparsers(dest="command")

    collect_parser = sub.add_parser("collect", help="Collect feedback from logs")
    collect_parser.add_argument("--days", type=int, default=7)

    sub.add_parser("show", help="Show feedback state")

    args = parser.parse_args()

    if args.command == "collect":
        result = collect_from_logs(args.days)
        print(f"Collected: {result['processed_logs']} logs, {result['total_perspectives']} perspectives")
    elif args.command == "show":
        show_feedback()
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
