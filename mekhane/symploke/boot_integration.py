#!/usr/bin/env python3
# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- mekhane/symploke/ A0â†’ç¶™ç¶šã™ã‚‹ç§ãŒå¿…è¦â†’boot_integration ãŒæ‹…ã†
"""
Boot Integration - 8è»¸ã‚’çµ±åˆã—ãŸ /boot ç”¨ API

Usage:
    python boot_integration.py                    # æ¨™æº–èµ·å‹•
    python boot_integration.py --mode fast        # é«˜é€Ÿèµ·å‹•
    python boot_integration.py --mode detailed    # è©³ç´°èµ·å‹•
    python boot_integration.py --postcheck /tmp/boot_report.md --mode detailed  # ãƒã‚¹ãƒˆãƒã‚§ãƒƒã‚¯
"""

import re
import sys
import json
import argparse
from datetime import datetime
from pathlib import Path
from typing import Optional

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


def extract_dispatch_info(context: str, gpu_ok: bool = True) -> dict:
    """Extract Dispatcher dispatch plan from context.

    Graceful degradation: returns empty-primary dict on any failure.
    Separated from _run_attractor() for testability (dia+ issue #1).
    """
    dispatch_info = {"primary": "", "alternatives": [], "dispatch_formatted": ""}
    try:
        from mekhane.fep.attractor_dispatcher import AttractorDispatcher
        dispatcher = AttractorDispatcher(force_cpu=not gpu_ok)
        plan = dispatcher.dispatch(context)
        if plan:
            dispatch_info = {
                "primary": plan.primary.workflow,
                "alternatives": [d.workflow for d in plan.alternatives[:3]],
                "dispatch_formatted": dispatcher.format_compact(plan),
            }
    except Exception:
        pass  # Dispatcher failure should not block boot
    return dispatch_info


def _load_projects(project_root: Path) -> dict:
    """Load project registry from .agent/projects/registry.yaml.

    Returns:
        dict: {
            "projects": [...],   # å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
            "active": int,
            "dormant": int,
            "total": int,
            "formatted": str     # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿å‡ºåŠ›
        }
    """
    result = {"projects": [], "active": 0, "dormant": 0, "total": 0, "formatted": ""}
    registry_path = project_root / ".agent" / "projects" / "registry.yaml"
    if not registry_path.exists():
        return result

    try:
        import yaml
        data = yaml.safe_load(registry_path.read_text(encoding="utf-8"))
        projects = data.get("projects", [])
        if not projects:
            return result

        active = [p for p in projects if p.get("status") == "active"]
        dormant = [p for p in projects if p.get("status") == "dormant"]
        archived = [p for p in projects if p.get("status") == "archived"]

        lines = ["ğŸ“¦ **Projects** (registry.yaml)"]
        # Group by category based on path patterns
        categories = {
            "ã‚³ã‚¢ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ": [],
            "Mekhane ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«": [],
            "ç†è«–ãƒ»è¨€èªåŸºç›¤": [],
            "ç ”ç©¶ãƒ»æ¦‚å¿µ": [],
            "è£œåŠ©": [],
        }
        for p in projects:
            path = p.get("path", "")
            status = p.get("status", "")
            if status == "archived":
                categories["è£œåŠ©"].append(p)
            elif path.startswith("mekhane/"):
                categories["Mekhane ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"].append(p)
            elif path.startswith(".") or p.get("id") in ("kalon", "aristos", "autophonos"):
                categories["ç ”ç©¶ãƒ»æ¦‚å¿µ"].append(p)
            elif p.get("id") in ("ccl", "kernel", "pythosis"):
                categories["ç†è«–ãƒ»è¨€èªåŸºç›¤"].append(p)
            elif p.get("id") in ("hegemonikon-guide",):
                categories["è£œåŠ©"].append(p)
            else:
                categories["ã‚³ã‚¢ãƒ©ãƒ³ã‚¿ã‚¤ãƒ "].append(p)

        status_icons = {"active": "ğŸŸ¢", "dormant": "ğŸ’¤", "archived": "ğŸ—„ï¸", "planned": "ğŸ“‹"}
        for cat_name, cat_projects in categories.items():
            if not cat_projects:
                continue
            lines.append(f"  [{cat_name}]")
            for p in cat_projects:
                icon = status_icons.get(p.get("status", ""), "â“")
                name = p.get("name", p.get("id", "?"))
                phase = p.get("phase", "")
                summary = p.get("summary", "")
                if len(summary) > 50:
                    summary = summary[:50] + "..."
                line = f"    {icon} {name} [{phase}] â€” {summary}"
                # entry_point: CLI ãŒã‚ã‚Œã°è¡¨ç¤º
                ep = p.get("entry_point")
                if ep and isinstance(ep, dict):
                    cli = ep.get("cli", "")
                    if cli:
                        line += f"\n       ğŸ“ `{cli}`"
                lines.append(line)
                # usage_trigger: åˆ©ç”¨æ¡ä»¶ã‚’è¡¨ç¤º
                trigger = p.get("usage_trigger", "")
                if trigger and p.get("status") == "active":
                    lines.append(f"       âš¡ {trigger}")

        lines.append(f"  çµ±è¨ˆ: {len(projects)}ä»¶ / Active {len(active)} / Dormant {len(dormant)} / Archived {len(archived)}")

        result = {
            "projects": projects,
            "active": len(active),
            "dormant": len(dormant),
            "total": len(projects),
            "formatted": "\n".join(lines),
        }
    except Exception:
        pass  # Registry loading failure should not block boot

    return result


def get_boot_context(mode: str = "standard", context: Optional[str] = None) -> dict:
    """
    /boot çµ±åˆ API: 9è»¸ï¼ˆHandoff, Sophia, Persona, PKS, Safety, EPT, Digestor, Attractor, Projectsï¼‰ã‚’çµ±åˆã—ã¦è¿”ã™

    GPU ãƒ—ãƒªãƒ•ãƒ©ã‚¤ãƒˆãƒã‚§ãƒƒã‚¯ä»˜ã: GPU å æœ‰æ™‚ã¯ embedding ç³»ã‚’ CPU ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å®Ÿè¡Œ

    Args:
        mode: "fast" (/boot-), "standard" (/boot), "detailed" (/boot+)
        context: ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆHandoff ã®ä¸»é¡Œãªã©ï¼‰

    Returns:
        dict: {
            "handoffs": {...},    # è»¸ A
            "ki": {...},          # è»¸ B
            "persona": {...},     # è»¸ C
            "pks": {...},         # è»¸ D
            "safety": {...},      # è»¸ E
            "ept": {...},         # è»¸ H
            "attractor": {...},   # è»¸ F
            "projects": {...},    # è»¸ I
            "formatted": str      # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿å‡ºåŠ›
        }
    """
    # GPU ãƒ—ãƒªãƒ•ãƒ©ã‚¤ãƒˆãƒã‚§ãƒƒã‚¯ (G)
    gpu_ok = True
    gpu_reason = ""
    try:
        from mekhane.symploke.gpu_guard import gpu_preflight, force_cpu_env
        gpu_status = gpu_preflight()
        gpu_ok = gpu_status.gpu_available
        gpu_reason = gpu_status.reason
        if not gpu_ok:
            print(f" âš ï¸ GPU busy ({gpu_reason}), embedding ç³»ã¯ CPU ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯", file=sys.stderr)
            force_cpu_env()  # CUDA_VISIBLE_DEVICES="" ã‚’è¨­å®š
        else:
            print(f" ğŸŸ¢ GPU available ({gpu_status.utilization}%, {gpu_status.memory_used_mb}MiB)", file=sys.stderr)
    except Exception:
        pass  # GPU ãƒã‚§ãƒƒã‚¯å¤±æ•—æ™‚ã¯ç„¡è¦–ã—ã¦ç¶šè¡Œ

    # è»¸ A: Handoff æ´»ç”¨
    print(" [1/9] ğŸ“‹ Searching Handoffs...", file=sys.stderr, end="", flush=True)
    from mekhane.symploke.handoff_search import get_boot_handoffs, format_boot_output

    handoffs_result = get_boot_handoffs(mode=mode, context=context)
    print(" Done.", file=sys.stderr)

    # è»¸ B: Sophia ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ (ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ã)
    print(" [2/9] ğŸ“š Ingesting Knowledge (Sophia)...", file=sys.stderr, end="", flush=True)
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ Handoff ã‹ã‚‰å–å¾—
    ki_context = context
    if not ki_context and handoffs_result["latest"]:
        ki_context = handoffs_result["latest"].metadata.get("primary_task", "")
        if not ki_context:
            ki_context = handoffs_result["latest"].content[:200]

    ki_result = {"ki_items": [], "count": 0}
    try:
        from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeout

        def _run_sophia():
            from mekhane.symploke.sophia_ingest import get_boot_ki, format_ki_output
            return get_boot_ki(context=ki_context, mode=mode)

        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(_run_sophia)
            ki_result = future.result(timeout=15.0)
        print(" Done.", file=sys.stderr)
    except (FutureTimeout, TimeoutError):
        print(" Timeout (skipped).", file=sys.stderr)
    except Exception as e:
        print(f" Failed ({str(e)}).", file=sys.stderr)
    print(" [3/9] ğŸ‘¤ Loading Persona...", file=sys.stderr, end="", flush=True)
    from mekhane.symploke.persona import get_boot_persona

    persona_result = get_boot_persona(mode=mode)
    print(" Done.", file=sys.stderr)

    # è»¸ D: PKS (èƒ½å‹•çš„çŸ¥è­˜ãƒ—ãƒƒã‚·ãƒ¥)
    # é‡ã„å‡¦ç†ãªã®ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
    pks_result = {"nuggets": [], "count": 0, "formatted": ""}
    
    if mode != "fast":  # fastãƒ¢ãƒ¼ãƒ‰ã§ã¯PKSã‚’ã‚¹ã‚­ãƒƒãƒ—
        print(" [4/9] ğŸ§  Activating PKS Engine...", file=sys.stderr, end="", flush=True)
        try:
            from concurrent.futures import ThreadPoolExecutor
            
            def _run_pks():
                from mekhane.pks.pks_engine import PKSEngine
                pks_engine = PKSEngine(threshold=0.5, max_push=3)
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¨­å®š
                pks_topics = []
                if context:
                    pks_topics = [t.strip() for t in context.split(",")]
                elif ki_context:
                    # KI ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
                    words = ki_context.split()[:5]
                    pks_topics = [w for w in words if len(w) > 2]
                
                if pks_topics:
                    pks_engine.set_context(topics=pks_topics)
                    return pks_engine.proactive_push(k=10)
                return []

            # 10ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (detailedã§ã‚‚å¾…ãŸã›ã™ããªã„)
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(_run_pks)
                nuggets = future.result(timeout=10.0)
                
            if nuggets:
                from mekhane.pks.pks_engine import PKSEngine  # å‹ãƒ’ãƒ³ãƒˆç”¨
                # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã›ãšã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã ã‘å€Ÿç”¨ã—ãŸã„ãŒã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰ãªã®ã§
                # ç°¡æ˜“ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€å†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹ï¼ˆè»½é‡ï¼‰
                pks_engine_dummy = PKSEngine()
                pks_result = {
                    "nuggets": nuggets,
                    "count": len(nuggets),
                    "formatted": pks_engine_dummy.format_push_report(nuggets),
                }
            print(" Done.", file=sys.stderr)
            
        except TimeoutError:
            print(" Timeout (skipped).", file=sys.stderr)
        except Exception as e:
            print(f" Failed ({str(e)}).", file=sys.stderr)
    else:
         print(" [4/9] ğŸ§  PKS Engine skipped (fast mode).", file=sys.stderr)

    # è»¸ E: Safety Contract Audit (v3.1)
    safety_result = {"skills": 0, "workflows": 0, "errors": 0, "warnings": 0, "formatted": ""}
    print(" [5/9] ğŸ›¡ï¸ Running Safety Contract Audit...", file=sys.stderr, end="", flush=True)
    try:
        from mekhane.dendron.skill_checker import run_audit, AuditResult
        agent_dir = Path(__file__).parent.parent.parent / ".agent"
        if agent_dir.exists():
            audit = run_audit(agent_dir)
            dist = audit.risk_distribution()
            lcm = audit.lcm_distribution()
            safety_lines = []
            safety_lines.append("ğŸ›¡ï¸ **Safety Contract**")
            safety_lines.append(f"  Skills: {audit.skills_checked} | WF: {audit.workflows_checked}")
            risk_parts = [f"{k}:{v}" for k, v in dist.items() if v > 0]
            if risk_parts:
                safety_lines.append(f"  Risk: {' '.join(risk_parts)}")
            lcm_parts = [f"{k}:{v}" for k, v in lcm.items() if v > 0]
            if lcm_parts:
                safety_lines.append(f"  LCM:  {' '.join(lcm_parts)}")
            if audit.errors > 0:
                safety_lines.append(f"  âš ï¸ {audit.errors} error(s), {audit.warnings} warning(s)")
            safety_result = {
                "skills": audit.skills_checked,
                "workflows": audit.workflows_checked,
                "errors": audit.errors,
                "warnings": audit.warnings,
                "formatted": "\n".join(safety_lines),
            }
        print(" Done.", file=sys.stderr)
    except Exception as e:
        print(f" Failed ({str(e)}).", file=sys.stderr)

    # è»¸ H: EPT (Existence Purpose Tensor)
    ept_result = {"score": 0, "total": 0, "pct": 0, "formatted": ""}
    print(" [6/9] ğŸ“ Running EPT Matrix...", file=sys.stderr, end="", flush=True)
    try:
        from concurrent.futures import ThreadPoolExecutor
        def _run_ept():
            from mekhane.dendron.checker import DendronChecker
            c = DendronChecker(
                check_structure=True,
                check_function_nf=True,
                check_verification=True,
            )
            r = c.check(Path(__file__).parent.parent)  # mekhane/
            total = r.total_structure_checks + r.total_function_nf_checks + r.total_verification_checks
            ok = r.structure_ok + r.function_nf_ok + r.verification_ok
            pct = (ok / total * 100) if total > 0 else 0
            return {
                "score": ok, "total": total, "pct": pct,
                "nf2": f"{r.structure_ok}/{r.total_structure_checks}",
                "nf3": f"{r.function_nf_ok}/{r.total_function_nf_checks}",
                "bcnf": f"{r.verification_ok}/{r.total_verification_checks}",
                "formatted": f"ğŸ“ **EPT**: {ok}/{total} ({pct:.0f}%) [NF2:{r.structure_ok}/{r.total_structure_checks} NF3:{r.function_nf_ok}/{r.total_function_nf_checks} BCNF:{r.verification_ok}/{r.total_verification_checks}]",
            }
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(_run_ept)
            ept_result = future.result(timeout=10.0)
        print(" Done.", file=sys.stderr)
    except TimeoutError:
        print(" Timeout (skipped).", file=sys.stderr)
    except Exception as e:
        print(f" Failed ({str(e)}).", file=sys.stderr)

    # è»¸ G: Digestor å€™è£œ (è«–æ–‡ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰)
    digestor_result = {"candidates": [], "count": 0, "formatted": ""}
    print(" [7/9] ğŸ“„ Loading Digest Candidates...", file=sys.stderr, end="", flush=True)
    try:
        import glob
        digest_dir = Path.home() / ".hegemonikon" / "digestor"
        reports = sorted(glob.glob(str(digest_dir / "digest_report_*.json")), reverse=True)
        if reports:
            with open(reports[0], "r", encoding="utf-8") as f:
                report = json.load(f)
            candidates = report.get("candidates", [])[:3]
            if candidates:
                digest_lines = ["ğŸ“„ **Digest Candidates** (ä»Šæ—¥ã®è«–æ–‡æ¨è–¦)"]
                for i, c in enumerate(candidates, 1):
                    title = c.get("title", "Unknown")[:60]
                    score = c.get("score", 0)
                    topics = ", ".join(c.get("matched_topics", [])[:2])
                    digest_lines.append(f"  {i}. [{score:.2f}] {title}... ({topics})")
                digestor_result = {
                    "candidates": candidates,
                    "count": len(candidates),
                    "formatted": "\n".join(digest_lines),
                }
        print(" Done.", file=sys.stderr)
    except Exception as e:
        print(f" Failed ({str(e)}).", file=sys.stderr)

    # è»¸ F: Attractor Dispatch Engine
    attractor_result = {"series": [], "workflows": [], "llm_format": "", "formatted": ""}
    # Handoff-derived context (ki_context) ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«ä½¿ç”¨
    # â†’ æ˜ç¤ºçš„ context ãŒãªãã¦ã‚‚ã€Handoff ä¸»é¡Œã‹ã‚‰è‡ªå‹•æ¨è–¦ãŒç™ºå‹•ã™ã‚‹
    attractor_context = context or ki_context
    if attractor_context:
        print(" [8/9] ğŸ¯ Attractor Dispatch...", file=sys.stderr, end="", flush=True)
        try:
            from concurrent.futures import ThreadPoolExecutor

            def _run_attractor():
                from mekhane.fep.attractor_advisor import AttractorAdvisor
                advisor = AttractorAdvisor(force_cpu=not gpu_ok)

                # Problem C: éå»ã® basin bias ã‚’é©ç”¨
                try:
                    from mekhane.fep.basin_logger import BasinLogger
                    basin_logger = BasinLogger()
                    log_files = sorted(basin_logger.log_dir.glob("attractor_log_*.jsonl"))
                    if log_files:
                        for lf in log_files[-3:]:  # ç›´è¿‘3æ—¥åˆ†
                            basin_logger.load_biases(lf)
                        advisor._attractor.apply_bias(basin_logger._biases)
                except Exception:
                    pass  # Bias loading failure should not block boot

                rec = advisor.recommend(attractor_context)
                llm_fmt = advisor.format_for_llm(attractor_context)

                # Dispatcher integration (Problem A)
                dispatch_info = extract_dispatch_info(attractor_context, gpu_ok=gpu_ok)

                # Theorem-level attractor (24 å®šç†: 6 Series ã®ç²’åº¦æ‹¡å¼µ)
                theorem_detail = {}
                try:
                    from mekhane.fep.theorem_attractor import TheoremAttractor
                    ta = TheoremAttractor(force_cpu=not gpu_ok)
                    top_theorems = ta.suggest(attractor_context, top_k=5)
                    flow = ta.simulate_flow(attractor_context, steps=10)
                    theorem_detail = {
                        "top_theorems": [
                            {"theorem": r.theorem, "name": r.name,
                             "series": r.series, "sim": round(r.similarity, 3),
                             "command": r.command}
                            for r in top_theorems
                        ],
                        "flow_converged": flow.converged_at,
                        "flow_final": [t for t, _ in flow.final_theorems[:3]],
                    }
                except Exception:
                    pass  # Theorem attractor failure should not block boot

                # FEP v2 Agent: çµ±åˆèªçŸ¥åˆ¤æ–­ (48-state)
                # Attractor ã®æ¨è–¦ã‚’ topic observation ã¨ã—ã¦æ³¨å…¥ã—ã€
                # Agent ãŒè‡ªå¾‹çš„ã« Series + act/observe ã‚’åˆ¤æ–­ã™ã‚‹
                fep_v2_result = {}
                learning_diff_fmt = ""
                try:
                    from mekhane.fep.fep_agent_v2 import HegemonikÃ³nFEPAgentV2
                    from mekhane.fep.state_spaces_v2 import SERIES_STATES
                    from mekhane.fep.persistence import (
                        save_snapshot, list_snapshots, diff_A,
                        format_learning_diff,
                    )

                    agent_v2 = HegemonikÃ³nFEPAgentV2()
                    agent_v2.load_learned_A()  # å‰å›ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å­¦ç¿’ã‚’å¾©å…ƒ

                    # Snapshot: capture A BEFORE learning
                    import copy
                    A_before = copy.deepcopy(agent_v2.agent.A)

                    # Attractor series â†’ topic observation index
                    # rec.series can be list (oscillation) or str
                    _s2obs = {s: 8 + i for i, s in enumerate(SERIES_STATES)}
                    att_series = rec.series
                    if isinstance(att_series, list):
                        att_series = att_series[0]  # Primary series
                    topic_obs = _s2obs.get(att_series, 8)

                    # 2-cycle inference (observe â†’ act pattern ã‚’è¨±å®¹)
                    r1 = agent_v2.step(topic_obs)
                    r2 = agent_v2.step(topic_obs)
                    final = r2  # 2nd cycle = å­¦ç¿’å¾Œã®åˆ¤æ–­

                    # Dirichlet å­¦ç¿’ + æ°¸ç¶šåŒ–
                    agent_v2.update_A_dirichlet(topic_obs)
                    agent_v2.save_learned_A()

                    # Snapshot: save AFTER learning + compute diff
                    save_snapshot(agent_v2, label="boot")
                    A_after = agent_v2.agent.A
                    learning_diff = diff_A(A_before, A_after)
                    learning_diff_fmt = format_learning_diff(learning_diff)

                    conf_pct = int(100.0 * max(final["beliefs"]))
                    explanation = agent_v2.explain(final)
                    fep_v2_result = {
                        "action": final["action_name"],
                        "selected_series": final.get("selected_series"),
                        "entropy": round(final["entropy"], 3),
                        "confidence_pct": conf_pct,
                        "attractor_series": att_series,
                        "agreement": final.get("selected_series") == att_series,
                        "map_state": final["map_state_names"],
                        "explanation": explanation,
                        "learning_diff": learning_diff,
                    }
                except Exception:
                    pass  # FEP v2 failure should not block boot

                formatted_parts = []
                if llm_fmt:
                    formatted_parts.append(f"ğŸ¯ **Attractor**: {llm_fmt}")
                if theorem_detail.get("top_theorems"):
                    tops = ", ".join(
                        f"{t['theorem']}({t['sim']:.2f})"
                        for t in theorem_detail["top_theorems"][:3]
                    )
                    formatted_parts.append(f"   ğŸ”¬ Theorems: {tops}")
                if dispatch_info["primary"]:
                    formatted_parts.append(f"   ğŸ“ Dispatch: {dispatch_info['dispatch_formatted']}")
                if fep_v2_result:
                    act = fep_v2_result["action"]
                    sel = fep_v2_result.get("selected_series") or "-"
                    ent = fep_v2_result["entropy"]
                    conf = fep_v2_result["confidence_pct"]
                    att_s = fep_v2_result.get("attractor_series", "?")
                    agree = "âœ“ä¸€è‡´" if fep_v2_result.get("agreement") else "âœ—ä¸ä¸€è‡´"
                    formatted_parts.append(
                        f"   ğŸ§  FEP v2: {act} [Series={sel}] "
                        f"(entropy={ent}, conf={conf}%) â†” ATT={att_s} [{agree}]"
                    )
                    # Explanation (indented under FEP v2 line)
                    expl = fep_v2_result.get("explanation", "")
                    if expl:
                        for line in expl.split("\n"):
                            formatted_parts.append(f"      {line}")
                if learning_diff_fmt:
                    for line in learning_diff_fmt.split("\n"):
                        formatted_parts.append(f"   {line}")

                return {
                    "series": rec.series,
                    "workflows": rec.workflows,
                    "llm_format": llm_fmt,
                    "confidence": rec.confidence,
                    "oscillation": rec.oscillation.value,
                    "advice": rec.advice,
                    "dispatch_primary": dispatch_info["primary"],
                    "dispatch_alternatives": dispatch_info["alternatives"],
                    "theorem_detail": theorem_detail,
                    "fep_v2": fep_v2_result,
                    "formatted": "\n".join(formatted_parts) if formatted_parts else "",
                }

            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(_run_attractor)
                attractor_result = future.result(timeout=30.0)
            print(" Done.", file=sys.stderr)
        except TimeoutError:
            print(" Timeout (skipped).", file=sys.stderr)
        except Exception as e:
            print(f" Failed ({str(e)}).", file=sys.stderr)
    else:
        print(" [8/9] ğŸ¯ Attractor skipped (no context & no Handoff).", file=sys.stderr)

    # è»¸ I: Projects (registry.yaml)
    projects_result = {"projects": [], "active": 0, "dormant": 0, "total": 0, "formatted": ""}
    print(" [9/9] ğŸ“¦ Loading Projects Registry...", file=sys.stderr, end="", flush=True)
    try:
        project_root = Path(__file__).parent.parent.parent
        projects_result = _load_projects(project_root)
        print(" Done.", file=sys.stderr)
    except Exception as e:
        print(f" Failed ({str(e)}).", file=sys.stderr)

    # çµ±åˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    lines = []

    # Persona (æœ€åˆã«)
    if persona_result.get("formatted"):
        lines.append(persona_result["formatted"])
        lines.append("")

    # Handoff
    if handoffs_result["latest"]:
        lines.append(format_boot_output(handoffs_result, verbose=(mode == "detailed")))
        lines.append("")

    # KI
    if ki_result["ki_items"]:
        from mekhane.symploke.sophia_ingest import format_ki_output
        lines.append(format_ki_output(ki_result))

    # PKS
    if pks_result["formatted"]:
        lines.append("")
        lines.append(pks_result["formatted"])

    # Safety Contract
    if safety_result["formatted"]:
        lines.append("")
        lines.append(safety_result["formatted"])

    # EPT
    if ept_result["formatted"]:
        lines.append("")
        lines.append(ept_result["formatted"])

    # Digestor
    if digestor_result["formatted"]:
        lines.append("")
        lines.append(digestor_result["formatted"])

    # Attractor
    if attractor_result["formatted"]:
        lines.append("")
        lines.append(attractor_result["formatted"])

    # Projects
    if projects_result["formatted"]:
        lines.append("")
        lines.append(projects_result["formatted"])

    # n8n WF-06: Session Start é€šçŸ¥
    try:
        import urllib.request
        n8n_payload = json.dumps({
            "mode": mode,
            "context": context or "",
            "agent": "Claude",
            "handoff_count": handoffs_result["count"],
            "ki_count": ki_result["count"],
        }).encode("utf-8")
        req = urllib.request.Request(
            "http://localhost:5678/webhook/session-start",
            data=n8n_payload,
            headers={"Content-Type": "application/json"},
            method="POST",
        )
        urllib.request.urlopen(req, timeout=5)
        print(" ğŸ“¡ n8n: session started", file=sys.stderr)
    except Exception:
        pass  # n8n æœªèµ·å‹•ã§ã‚‚ãƒ–ãƒ¼ãƒˆã¯ç¶™ç¶š

    return {
        "handoffs": handoffs_result,
        "ki": ki_result,
        "persona": persona_result,
        "pks": pks_result,
        "safety": safety_result,
        "ept": ept_result,
        "digestor": digestor_result,
        "attractor": attractor_result,
        "projects": projects_result,
        "formatted": "\n".join(lines),
    }


def print_boot_summary(mode: str = "standard", context: Optional[str] = None):
    """Print formatted boot summary."""
    result = get_boot_context(mode=mode, context=context)
    print(result["formatted"])

    # Summary line
    print()
    print("â”€" * 50)
    h_count = result["handoffs"]["count"]
    ki_count = result["ki"]["count"]
    sessions = result["persona"].get("sessions", 0)
    pks_count = result.get("pks", {}).get("count", 0)
    safety_errors = result.get("safety", {}).get("errors", 0)
    attractor_series = result.get("attractor", {}).get("series", [])
    attractor_str = "+".join(attractor_series) if attractor_series else "â€”"
    ept_pct = result.get("ept", {}).get("pct", 0)
    ept_str = f"{ept_pct:.0f}%" if ept_pct > 0 else "â€”"
    proj_total = result.get("projects", {}).get("total", 0)
    proj_active = result.get("projects", {}).get("active", 0)
    proj_str = f"{proj_active}/{proj_total}" if proj_total > 0 else "â€”"
    print(f"ğŸ“Š Handoff: {h_count}ä»¶ | KI: {ki_count}ä»¶ | Sessions: {sessions} | PKS: {pks_count}ä»¶ | Safety: {'âœ…' if safety_errors == 0 else f'âš ï¸{safety_errors}'} | EPT: {ept_str} | PJ: {proj_str} | Attractor: {attractor_str}")

    # detailed ãƒ¢ãƒ¼ãƒ‰: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
    if mode == "detailed":
        template_path = generate_boot_template(result)
        print(f"\nğŸ“ Boot Report Template: {template_path}", file=sys.stderr)
        print(f"TEMPLATE:{template_path}")


# ============================================================
# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ (A+C) â€” ç’°å¢ƒå¼·åˆ¶: ç©´åŸ‹ã‚å¼ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
# ============================================================

# ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æœ€ä½è¦ä»¶å®šç¾©
MODE_REQUIREMENTS = {
    "detailed": {
        "handoff_count": 10,
        "ki_count": 5,
        "min_chars": 3000,
        "required_sections": [
            "Handoff å€‹åˆ¥è¦ç´„",
            "KI æ·±èª­ã¿",
            "Self-Profile æ‘©æ“¦",
            "æ„å‘³ã‚ã‚‹ç¬é–“",
            "Phase è©³ç´°",
            "é–‹ç™ºä¸­ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
            "ã‚¿ã‚¹ã‚¯ææ¡ˆ",
        ],
    },
    "standard": {
        "handoff_count": 3,
        "ki_count": 3,
        "min_chars": 1000,
        "required_sections": [
            "Handoff ã‚µãƒãƒªãƒ¼",
            "ã‚¿ã‚¹ã‚¯ææ¡ˆ",
        ],
    },
    "fast": {
        "handoff_count": 0,
        "ki_count": 0,
        "min_chars": 0,
        "required_sections": [],
    },
}


def generate_boot_template(result: dict) -> Path:
    """
    ç’°å¢ƒå¼·åˆ¶: ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ç©´åŸ‹ã‚ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚

    <!-- REQUIRED --> ãƒãƒ¼ã‚«ãƒ¼ä»˜ãã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…é ˆã€‚
    <!-- FILL --> ãƒãƒ¼ã‚«ãƒ¼ã¯ LLM ãŒè¨˜å…¥ã™ã¹ãç®‡æ‰€ã€‚
    postcheck ã§æœªè¨˜å…¥ã® FILL ãŒæ¤œå‡ºã•ã‚Œã‚‹ã¨ FAIL ã«ãªã‚‹ã€‚
    """
    now = datetime.now()
    template_path = Path(f"/tmp/boot_report_{now.strftime('%Y%m%d_%H%M')}.md")

    lines = []
    lines.append(f"# Boot Report â€” {now.strftime('%Y-%m-%d %H:%M')}")
    lines.append("")
    lines.append("## å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ")
    lines.append("")

    reqs = MODE_REQUIREMENTS.get("detailed", {})
    for section in reqs.get("required_sections", []):
        lines.append(f"- [ ] {section}")
    lines.append("")

    # --- Handoff å€‹åˆ¥è¦ç´„ ---
    lines.append("## Handoff å€‹åˆ¥è¦ç´„")
    lines.append("<!-- REQUIRED: å„ Handoff ã® S/A/R ã‚’1è¡Œä»¥ä¸Š -->")
    lines.append("")

    handoffs = result.get("handoffs", {})
    related = handoffs.get("related", [])
    latest = handoffs.get("latest")

    all_handoffs = []
    if latest:
        all_handoffs.append(latest)
    if related:
        all_handoffs.extend(related)

    for i, h in enumerate(all_handoffs[:10], 1):
        title = "Unknown"
        if hasattr(h, "metadata"):
            title = h.metadata.get("primary_task", h.metadata.get("title", "Unknown"))
        elif isinstance(h, dict):
            title = h.get("primary_task", h.get("title", "Unknown"))
        lines.append(f"### Handoff {i}: {title}")
        lines.append("")
        lines.append("> è¦ç´„: <!-- FILL -->")
        lines.append("")

    # --- KI æ·±èª­ã¿ ---
    lines.append("## KI æ·±èª­ã¿")
    lines.append("<!-- REQUIRED: ã‚µãƒãƒªãƒ¼å¼•ç”¨ + è‡ªåˆ†ã®è§£é‡ˆã‚’è¨˜è¿° -->")
    lines.append("")

    ki_items = result.get("ki", {}).get("ki_items", [])
    for i, ki in enumerate(ki_items[:5], 1):
        name = "Unknown"
        summary = "N/A"
        if hasattr(ki, "metadata"):
            name = ki.metadata.get("ki_name", "Unknown")
            summary = ki.metadata.get("summary", "N/A")
        elif isinstance(ki, dict):
            name = ki.get("ki_name", "Unknown")
            summary = ki.get("summary", "N/A")
        lines.append(f"### KI {i}: {name}")
        lines.append("")
        lines.append(f"> ã‚µãƒãƒªãƒ¼: {summary[:100]}")
        lines.append("> è§£é‡ˆ: <!-- FILL -->")
        lines.append("")

    # ä¸è¶³åˆ†ã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    for i in range(len(ki_items) + 1, 6):
        lines.append(f"### KI {i}: (session context ã‹ã‚‰é¸æŠ)")
        lines.append("")
        lines.append("> ã‚µãƒãƒªãƒ¼: <!-- FILL -->")
        lines.append("> è§£é‡ˆ: <!-- FILL -->")
        lines.append("")

    # --- Self-Profile æ‘©æ“¦ ---
    lines.append("## Self-Profile æ‘©æ“¦")
    lines.append("<!-- REQUIRED: ãƒŸã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®æ‘©æ“¦ã‚’æ˜ç¤º -->")
    lines.append("")
    lines.append("ä»Šå›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§æ³¨æ„ã™ã¹ããƒŸã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³: <!-- FILL -->")
    lines.append("")

    # --- æ„å‘³ã‚ã‚‹ç¬é–“ ---
    lines.append("## æ„å‘³ã‚ã‚‹ç¬é–“")
    lines.append("<!-- REQUIRED: å„ç¬é–“ã«å¯¾ã™ã‚‹è‡ªåˆ†ã®è§£é‡ˆã‚’è¨˜è¿° -->")
    lines.append("")
    lines.append("è§£é‡ˆ: <!-- FILL -->")
    lines.append("")

    # --- Phase è©³ç´° ---
    lines.append("## Phase è©³ç´°")
    lines.append("<!-- REQUIRED: å„ Phase ã®å±•é–‹ã•ã‚ŒãŸè©³ç´°ã‚’å‡ºåŠ› -->")
    lines.append("")
    for phase in range(7):
        lines.append(f"### Phase {phase}")
        lines.append("")
        lines.append("<!-- FILL -->")
        lines.append("")

    # --- é–‹ç™ºä¸­ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ ---
    lines.append("## é–‹ç™ºä¸­ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ")
    lines.append("<!-- REQUIRED: registry.yaml ã‹ã‚‰èª­ã¿è¾¼ã‚“ã  PJ ä¸€è¦§ -->")
    lines.append("")

    projects = result.get("projects", {}).get("projects", [])
    if projects:
        active_projects = [p for p in projects if p.get("status") == "active"]
        for p in active_projects:
            name = p.get("name", p.get("id", "?"))
            phase = p.get("phase", "")
            summary_text = p.get("summary", "")
            lines.append(f"- **{name}** [{phase}]: {summary_text}")
        lines.append("")
        lines.append(f"çµ±è¨ˆ: Active {len(active_projects)} / Total {len(projects)}")
    else:
        lines.append("<!-- FILL: registry.yaml ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ -->")
    lines.append("")

    # --- ã‚¿ã‚¹ã‚¯ææ¡ˆ ---
    lines.append("## ã‚¿ã‚¹ã‚¯ææ¡ˆ")
    lines.append("<!-- REQUIRED: Handoff ã‹ã‚‰æŠ½å‡ºã—ãŸã‚¿ã‚¹ã‚¯ææ¡ˆ -->")
    lines.append("")
    lines.append("1. <!-- FILL -->")
    lines.append("")

    template_path.write_text("\n".join(lines), encoding="utf-8")
    return template_path


# ============================================================
# ãƒã‚¹ãƒˆãƒã‚§ãƒƒã‚¯ (B) â€” ç’°å¢ƒå¼·åˆ¶: è¨˜å…¥æ¸ˆã¿ãƒ¬ãƒãƒ¼ãƒˆã®æ¤œè¨¼
# ============================================================

def postcheck_boot_report(report_path: str, mode: str = "detailed") -> dict:
    """
    è¨˜å…¥æ¸ˆã¿ boot report ã‚’æ¤œè¨¼ã™ã‚‹ã€‚

    Returns:
        dict: {
            "passed": bool,
            "checks": [{"name": str, "passed": bool, "detail": str}],
            "formatted": str
        }
    """
    path = Path(report_path)
    if not path.exists():
        return {
            "passed": False,
            "checks": [{"name": "file_exists", "passed": False, "detail": f"File not found: {report_path}"}],
            "formatted": f"âŒ Boot Report Validation: FAIL\n  âŒ File not found: {report_path}",
        }

    content = path.read_text(encoding="utf-8")
    reqs = MODE_REQUIREMENTS.get(mode, MODE_REQUIREMENTS["standard"])
    checks = []

    # Check 1: <!-- FILL --> ã®æ®‹å­˜æ•°
    fill_count = content.count("<!-- FILL -->")
    checks.append({
        "name": "unfilled_sections",
        "passed": fill_count == 0,
        "detail": f"{'No' if fill_count == 0 else fill_count} unfilled sections"
            + ("" if fill_count == 0 else f" remaining (<!-- FILL --> found {fill_count}x)"),
    })

    # Check 2: REQUIRED ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°
    required_count = content.count("<!-- REQUIRED")
    expected = len(reqs.get("required_sections", []))
    checks.append({
        "name": "required_sections",
        "passed": required_count >= expected,
        "detail": f"Required sections: {required_count}/{expected}",
    })

    # Check 3: ç·æ–‡å­—æ•°
    min_chars = reqs.get("min_chars", 0)
    char_count = len(content)
    checks.append({
        "name": "content_length",
        "passed": char_count >= min_chars,
        "detail": f"Content length: {char_count} chars"
            + (f" (â‰¥ {min_chars})" if char_count >= min_chars else f" (< {min_chars}, need {min_chars - char_count} more)"),
    })

    # Check 4: Handoff å¼•ç”¨æ•° (### Handoff N: ã®æ•°)
    handoff_refs = len(re.findall(r"^### Handoff \d+:", content, re.MULTILINE))
    expected_h = reqs.get("handoff_count", 0)
    checks.append({
        "name": "handoff_references",
        "passed": handoff_refs >= expected_h,
        "detail": f"Handoff references: {handoff_refs}"
            + (f" (â‰¥ {expected_h})" if handoff_refs >= expected_h else f" (< {expected_h})"),
    })

    # Check 5: ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆå®Œäº†ç‡
    unchecked = content.count("- [ ]")
    checked = content.count("- [x]")
    total_checks = unchecked + checked
    all_checked = unchecked == 0 and total_checks > 0
    checks.append({
        "name": "checklist_completion",
        "passed": all_checked,
        "detail": f"Checklist: {checked}/{total_checks} completed"
            + ("" if all_checked else f" ({unchecked} remaining)"),
    })

    # Check 6: éšä¼´ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (Adjunction LâŠ£R)
    # Drift = 1 - Îµ (å¤±ã‚ã‚ŒãŸæ–‡è„ˆã®é‡)
    # Îµ precision: Handoff ã¸ã®è¨€åŠ + Self-Profile å‚ç…§ + æ„å‘³ã‚ã‚‹ç¬é–“ã®è¨˜è¿°
    # BS-3b fix: FILL æ®‹å­˜ç‡ã§ Îµ ã‚’å‰²ã‚Šå¼•ã
    #   ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¦‹å‡ºã—ã« "Handoff" ç­‰ãŒå«ã¾ã‚Œã‚‹ãŸã‚ã€
    #   è¨˜å…¥å‰ã§ã‚‚ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãŒæˆç«‹ã—ã¦ã—ã¾ã†å•é¡Œã‚’è§£æ¶ˆ
    adjunction_indicators = {
        "handoff_context": bool(re.search(r"(?:å¼•ãç¶™ã|handoff|Handoff|å‰å›)", content, re.IGNORECASE)),
        "self_profile_ref": bool(re.search(r"(?:self.profile|ãƒŸã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³|èƒ½åŠ›å¢ƒç•Œ|Self-Profile)", content, re.IGNORECASE)),
        "meaningful_moment": bool(re.search(r"(?:æ„å‘³ã‚ã‚‹ç¬é–“|å°è±¡çš„|æ„Ÿå‹•|ç™ºè¦‹)", content, re.IGNORECASE)),
        "task_continuity": bool(re.search(r"(?:å‰å›ã®ç¶šã|ç¶™ç¶š|å†é–‹|æ®‹ã‚¿ã‚¹ã‚¯)", content, re.IGNORECASE)),
    }
    epsilon_count = sum(adjunction_indicators.values())
    epsilon_raw = epsilon_count / len(adjunction_indicators)

    # BS-3b: FILL æ®‹å­˜ãƒšãƒŠãƒ«ãƒ†ã‚£ (dia+ TH-005)
    # æœªè¨˜å…¥ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå¤šã„ â†’ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¦‹å‡ºã—ã®ãƒãƒƒãƒã¯ä¿¡é ¼ã§ããªã„
    fill_remaining = content.count("<!-- FILL -->")
    if fill_remaining > 0:
        # fill_ratio = è¨˜å…¥å®Œäº†ç‡ (0.0 = å…¨æœªè¨˜å…¥, 1.0 = å…¨è¨˜å…¥)
        # æ¨å®š: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ ~25 FILL ãƒãƒ¼ã‚«ãƒ¼ã‚’å«ã‚€ (detailed mode)
        estimated_total_fills = max(fill_remaining, 25)
        fill_ratio = 1.0 - (fill_remaining / estimated_total_fills)
        epsilon_precision = epsilon_raw * fill_ratio
    else:
        epsilon_precision = epsilon_raw

    drift = 1.0 - epsilon_precision
    checks.append({
        "name": "adjunction_metrics",
        "passed": True,  # Informational only, never blocks
        "detail": f"Adjunction LâŠ£R: Îµ={epsilon_precision:.0%}, Drift={drift:.0%}"
            + (f" (fill_penalty: {fill_remaining} FILL remaining)" if fill_remaining > 0 else "")
            + f" ({', '.join(k for k, v in adjunction_indicators.items() if v)})"
            if epsilon_count > 0
            else f"Adjunction LâŠ£R: Îµ=0%, Drift=100% (no context restoration detected)",
    })

    # çµæœé›†è¨ˆ
    passed_count = sum(1 for c in checks if c["passed"])
    total = len(checks)
    all_passed = all(c["passed"] for c in checks)

    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    status = "PASS" if all_passed else "FAIL"
    icon = "âœ…" if all_passed else "âŒ"
    lines = [f"{icon} Boot Report Validation: {status} ({passed_count}/{total} checks)"]
    for c in checks:
        ci = "âœ…" if c["passed"] else "âŒ"
        lines.append(f"  {ci} {c['detail']}")

    return {
        "passed": all_passed,
        "checks": checks,
        "formatted": "\n".join(lines),
    }


def main():
    parser = argparse.ArgumentParser(description="Boot integration API")
    parser.add_argument(
        "--mode",
        choices=["fast", "standard", "detailed"],
        default="standard",
        help="Boot mode",
    )
    parser.add_argument("--context", type=str, help="Context for search")
    parser.add_argument(
        "--postcheck",
        type=str,
        metavar="REPORT_PATH",
        help="Post-check a completed boot report file",
    )
    args = parser.parse_args()

    import warnings

    warnings.filterwarnings("ignore")

    # ãƒã‚¹ãƒˆãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰
    if args.postcheck:
        result = postcheck_boot_report(args.postcheck, mode=args.mode)
        print(result["formatted"])
        sys.exit(0 if result["passed"] else 1)

    # é€šå¸¸ãƒ–ãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰
    print(f"â³ Boot Mode: {args.mode}", file=sys.stderr)

    try:
        print_boot_summary(mode=args.mode, context=args.context)
    except KeyboardInterrupt:
        print("\nâš ï¸ Boot sequence interrupted.", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Boot sequence failed: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()

