#!/usr/bin/env python3
# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- mekhane/symploke/
"""
Chronos Ingest - Conversation History ã‚’ LanceDB ã«è‡ªå‹•æŠ•å…¥

Usage:
    python chronos_ingest.py
"""

import re
import sys
from pathlib import Path
from typing import List, Optional

import lancedb
from pydantic import BaseModel

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š (overridable)
DEFAULT_SESSION_DIR = Path.home() / "oikos/.gemini/antigravity/conversations"
DEFAULT_DB_PATH = Path.home() / "oikos/mneme/.hegemonikon/lancedb"
TABLE_NAME = "sessions"

class SessionDocument(BaseModel):
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¹ã‚­ãƒ¼ãƒ"""
    filename: str
    title: str
    exported_at: str
    message_count: int
    content: str  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¨æ–‡
    content_preview: str  # æ¤œç´¢çµæœè¡¨ç¤ºç”¨

def get_session_files(directory: Path = DEFAULT_SESSION_DIR) -> List[Path]:
    """Get all session files from directory."""
    if not directory.exists():
        return []
    return list(directory.glob("*.md"))

def parse_session_file(filepath: Path) -> Optional[SessionDocument]:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ md ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å¤‰æ›"""
    try:
        content = filepath.read_text(encoding="utf-8")
        lines = content.split("\n")

        # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºï¼ˆ# ã§å§‹ã¾ã‚‹è¡Œï¼‰
        title = "Untitled"
        for line in lines[:5]:
            if line.startswith("# "):
                title = line[2:].strip()
                break

        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ—¥æ™‚æŠ½å‡º
        exported_at = ""
        for line in lines[:10]:
            if "**Exported**" in line:
                match = re.search(r"\d{4}-\d{2}-\d{2}T[\d:.]+", line)
                if match:
                    exported_at = match.group()
                break

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°æŠ½å‡º
        message_count = 0
        for line in lines[:10]:
            if "**Messages**" in line:
                match = re.search(r"(\d+)", line)
                if match:
                    message_count = int(match.group(1))
                break

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æœ¬æ–‡æŠ½å‡ºï¼ˆ--- ä»¥é™ï¼‰
        body_start = 0
        for i, line in enumerate(lines):
            if line.strip() == "---":
                body_start = i + 1
                break

        body_lines = []
        for line in lines[body_start:]:
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if line.startswith("## ğŸ¤–") or line.startswith("## ğŸ‘¤"):
                continue
            if line.strip() == "---":
                continue
            if line.strip():
                body_lines.append(line.strip())

        full_content = "\n".join(body_lines)

        # CSS ãƒã‚¤ã‚ºã‚’é™¤å»
        full_content = re.sub(r"/\*.*?\*/", "", full_content, flags=re.DOTALL)
        full_content = re.sub(r"@media\s*\([^)]*\)\s*\{[^}]*\}", "", full_content)
        full_content = re.sub(r"\.markdown[-\w]*\s*\{[^}]*\}", "", full_content)
        full_content = re.sub(r"Thought for \d+s\s*", "", full_content)
        full_content = re.sub(r"Thought for <\d+s\s*", "", full_content)
        full_content = re.sub(r"\n{3,}", "\n\n", full_content).strip()

        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€åˆã® 500 æ–‡å­—ï¼‰
        preview = full_content[:500].replace("\n", " ")

        return SessionDocument(
            filename=filepath.name,
            title=title,
            exported_at=exported_at,
            message_count=message_count,
            content=full_content[:10000],
            content_preview=preview,
        )

    except Exception as e:
        print(f"[!] Error parsing {filepath.name}: {e}")
        return None

def ingest_to_chronos(docs: List[SessionDocument], db_path: Path = DEFAULT_DB_PATH) -> int:
    """Ingest documents to LanceDB (returns count)."""
    if not docs:
        # print("[!] No documents to index")
        return 0

    print(f"[*] Ingesting {len(docs)} documents to {db_path}")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    db_path.mkdir(parents=True, exist_ok=True)
    db = lancedb.connect(str(db_path))

    # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å‰Šé™¤ã—ã¦å†ä½œæˆ
    # Note: Incremental update logic could be added here
    if TABLE_NAME in db.list_tables():
        db.drop_table(TABLE_NAME)
        # print(f"[*] Dropped existing table: {TABLE_NAME}")

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¾æ›¸ã«å¤‰æ›
    data = [doc.model_dump() for doc in docs]

    # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
    table = db.create_table(TABLE_NAME, data)
    print(f"[âœ“] Created table: {TABLE_NAME} ({len(docs)} rows)")

    # Full-Text Search ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
    try:
        table.create_fts_index("content", replace=True)
        print(f"[âœ“] Created FTS index on 'content'")
    except Exception as e:
        print(f"[!] FTS index creation failed: {e}")

    return len(docs)

def search_chronos(query: str, db_path: Path = DEFAULT_DB_PATH, limit: int = 5):
    """Search Chronos index."""
    if not db_path.exists():
        return []

    db = lancedb.connect(str(db_path))
    if TABLE_NAME not in db.list_tables():
        return []

    table = db.open_table(TABLE_NAME)

    try:
        results = table.search(query, query_type="fts").limit(limit).to_list()
        return results
    except Exception as e:
        print(f"[!] Search error: {e}")
        return []

if __name__ == "__main__":
    # Simple CLI for testing
    import argparse
    parser = argparse.ArgumentParser(description="Chronos Ingest CLI")
    parser.add_argument("--search", type=str, help="Search query")
    args = parser.parse_args()

    if args.search:
        results = search_chronos(args.search)
        for r in results:
            print(f"- {r['title']} ({r['filename']})")
    else:
        files = get_session_files()
        docs = []
        for f in files:
            d = parse_session_file(f)
            if d:
                docs.append(d)
        ingest_to_chronos(docs)
