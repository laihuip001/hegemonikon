#!/usr/bin/env python3
# PROOF: [L3/ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£] <- mekhane/symploke/ O4â†’çµæœåé›†ãŒå¿…è¦â†’collect_results ãŒæ‹…ã†
"""
Jules Specialist ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœåé›†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ v1.0

run_specialists.py ã®å®Ÿè¡Œçµæœã‚’åé›†ãƒ»é›†è¨ˆãƒ»åˆ†æã—ã€
/boot ã§æ¶ˆåŒ–å¯èƒ½ãªå½¢å¼ã§ä¿å­˜ã™ã‚‹ã€‚

Output:
  - logs/specialist_daily/digest_YYYYMMDD.md  (äººé–“å‘ã‘ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ)
  - logs/specialist_daily/digest_YYYYMMDD.json (æ©Ÿæ¢°å‘ã‘ãƒ‡ãƒ¼ã‚¿)
"""

import json
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path
from collections import Counter

# PURPOSE: çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
def find_result_files(
    base_dir: str = "logs/specialist_daily",
    date_filter: str = "",
    days_back: int = 1,
) -> list[Path]:
    """çµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢ã™ã‚‹

    Args:
        base_dir: çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        date_filter: ç‰¹å®šæ—¥ä»˜ (YYYYMMDD)
        days_back: éå»Næ—¥åˆ†ã‚’å–å¾—

    ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ (ä¸¡å¯¾å¿œ):
        - YYYYMMDD_HHMM_*.json  (cron å®Ÿè¡Œ)
        - run_YYYYMMDD_*.json   (æ‰‹å‹•å®Ÿè¡Œ)
    """
    result_dir = Path(base_dir)
    if not result_dir.exists():
        return []

    # å¯¾è±¡æ—¥ä»˜ã‚’æ±ºå®š
    if date_filter:
        dates = [date_filter]
    else:
        dates = []
        for d in range(days_back):
            dt = datetime.now() - timedelta(days=d)
            dates.append(dt.strftime("%Y%m%d"))

    # å…¨ JSON ã‚’èµ°æŸ»ã—ã€æ—¥ä»˜ã§ãƒ•ã‚£ãƒ«ã‚¿
    files = []
    for f in sorted(result_dir.glob("*.json"), reverse=True):
        if f.name.startswith("digest_"):
            continue
        for date_prefix in dates:
            if date_prefix in f.name:
                files.append(f)
                break
    return files


# PURPOSE: çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã‚“ã§é›†è¨ˆã™ã‚‹
def aggregate_results(files: list[Path]) -> dict:
    """è¤‡æ•°ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’é›†è¨ˆ"""
    summary = {
        "total_runs": 0,
        "total_files_reviewed": 0,
        "total_specialists": 0,
        "total_started": 0,
        "total_failed": 0,
        "by_file": {},
        "by_category": Counter(),
        "errors": [],
        "sessions": [],
    }

    for fp in files:
        try:
            data = json.loads(fp.read_text())
        except (json.JSONDecodeError, OSError) as e:
            summary["errors"].append({"file": str(fp), "error": str(e)})
            continue

        summary["total_runs"] += 1

        # v4.0 å½¢å¼
        if "files" in data:
            for file_entry in data["files"]:
                target = file_entry.get("target_file", "unknown")
                specialists_count = file_entry.get("specialists_count", 0)
                started = file_entry.get("started", 0)
                failed = file_entry.get("failed", 0)

                summary["total_files_reviewed"] += 1
                summary["total_specialists"] += specialists_count
                summary["total_started"] += started
                summary["total_failed"] += failed

                if target not in summary["by_file"]:
                    summary["by_file"][target] = {"started": 0, "failed": 0, "total": 0}
                summary["by_file"][target]["started"] += started
                summary["by_file"][target]["failed"] += failed
                summary["by_file"][target]["total"] += specialists_count

                # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¿½è·¡
                for result in file_entry.get("results", []):
                    if "session_id" in result:
                        summary["sessions"].append({
                            "session_id": result["session_id"],
                            "specialist": result.get("specialist_id", ""),
                            "category": result.get("category", ""),
                            "target": target,
                        })
                    if "category" in result:
                        summary["by_category"][result["category"]] += 1

        # v3.0 å½¢å¼ (å¾Œæ–¹äº’æ›)
        elif "results" in data:
            target = data.get("target_file", "unknown")
            results = data.get("results", [])
            summary["total_files_reviewed"] += 1
            summary["total_specialists"] += len(results)
            started = sum(1 for r in results if "session_id" in r)
            failed = sum(1 for r in results if "error" in r)
            summary["total_started"] += started
            summary["total_failed"] += failed

    return summary


# PURPOSE: Markdown ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’ç”Ÿæˆ
def generate_digest(summary: dict, date_str: str = "") -> str:
    """äººé–“å‘ã‘ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ (Markdown) ã‚’ç”Ÿæˆ"""
    if not date_str:
        date_str = datetime.now().strftime("%Y-%m-%d")

    total = summary["total_specialists"]
    started = summary["total_started"]
    failed = summary["total_failed"]
    rate = (started / total * 100) if total else 0

    lines = [
        f"# ğŸ“Š Specialist Daily Digest â€” {date_str}",
        "",
        "## Summary",
        "",
        f"| Metric | Value |",
        f"|:-------|------:|",
        f"| Runs | {summary['total_runs']} |",
        f"| Files reviewed | {summary['total_files_reviewed']} |",
        f"| Total specialists | {total} |",
        f"| Started | {started} ({rate:.0f}%) |",
        f"| Failed | {failed} |",
        "",
    ]

    # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥
    if summary["by_file"]:
        lines.append("## By File")
        lines.append("")
        lines.append("| File | Started | Failed | Total |")
        lines.append("|:-----|--------:|-------:|------:|")
        for fname, stats in sorted(summary["by_file"].items()):
            lines.append(
                f"| `{fname}` | {stats['started']} | {stats['failed']} | {stats['total']} |"
            )
        lines.append("")

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥
    if summary["by_category"]:
        lines.append("## By Category")
        lines.append("")
        lines.append("| Category | Count |")
        lines.append("|:---------|------:|")
        for cat, count in summary["by_category"].most_common(20):
            lines.append(f"| {cat} | {count} |")
        lines.append("")

    # ã‚¨ãƒ©ãƒ¼
    if summary["errors"]:
        lines.append("## âš ï¸ Errors")
        lines.append("")
        for err in summary["errors"]:
            lines.append(f"- `{err['file']}`: {err['error']}")
        lines.append("")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°
    lines.append(f"---")
    lines.append(f"*Active sessions: {len(summary['sessions'])}*")

    return "\n".join(lines)


# PURPOSE: ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
def main():
    import argparse

    parser = argparse.ArgumentParser(description="Jules Specialist Results Collector v1.0")
    parser.add_argument("--date", "-d", default="", help="Date filter (YYYYMMDD)")
    parser.add_argument("--days", type=int, default=1, help="Days back to collect")
    parser.add_argument("--dir", default="logs/specialist_daily", help="Results directory")
    parser.add_argument("--output", "-o", default="", help="Output directory for digest")
    parser.add_argument("--dry-run", action="store_true", help="Print digest without saving")
    parser.add_argument("--json-only", action="store_true", help="Output JSON only")

    args = parser.parse_args()

    # çµæœãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢
    files = find_result_files(
        base_dir=args.dir,
        date_filter=args.date,
        days_back=args.days,
    )

    if not files:
        print(f"No result files found in {args.dir}")
        return

    print(f"Found {len(files)} result file(s)")

    # é›†è¨ˆ
    summary = aggregate_results(files)

    # æ—¥ä»˜æ–‡å­—åˆ—
    date_str = args.date if args.date else datetime.now().strftime("%Y%m%d")

    if args.json_only or args.dry_run:
        if args.json_only:
            # JSON å‡ºåŠ›
            print(json.dumps(summary, indent=2, ensure_ascii=False, default=str))
        else:
            # Markdown å‡ºåŠ›
            digest = generate_digest(summary, date_str)
            print(digest)
        return

    # ä¿å­˜
    output_dir = Path(args.output) if args.output else Path(args.dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Markdown
    digest = generate_digest(summary, date_str)
    md_path = output_dir / f"digest_{date_str}.md"
    md_path.write_text(digest)
    print(f"Digest: {md_path}")

    # JSON
    json_path = output_dir / f"digest_{date_str}.json"
    json_path.write_text(json.dumps(summary, indent=2, ensure_ascii=False, default=str))
    print(f"Data: {json_path}")


if __name__ == "__main__":
    main()
