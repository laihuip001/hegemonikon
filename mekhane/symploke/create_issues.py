#!/usr/bin/env python3
# PROOF: [L3/ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£] <- mekhane/symploke/ O4â†’ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœã‹ã‚‰Issueç”ŸæˆãŒå¿…è¦â†’create_issues ãŒæ‹…ã†
"""
Jules Specialist ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœ â†’ GitHub Issue è‡ªå‹•ä½œæˆ v1.0

å®Œäº†ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã®çµæœã‚’å–å¾—ã—ã€é‡è¦ãªç™ºè¦‹ã‚’ GitHub Issue ã¨ã—ã¦èµ·ç¥¨ã™ã‚‹ã€‚

Usage:
  python create_issues.py --results /tmp/jules_test_*.json
  python create_issues.py --dir logs/specialist_daily --days 1
  python create_issues.py --results result.json --dry-run  # Issue ä½œæˆã›ãšå†…å®¹ã‚’è¡¨ç¤º

Requires:
  - gh CLI (GitHub CLI) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»èªè¨¼æ¸ˆã¿
  - JULES_API_KEY_01 ç’°å¢ƒå¤‰æ•°ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœå–å¾—ç”¨ï¼‰
"""

import asyncio
import json
import os
import subprocess
import sys
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path

try:
    import aiohttp
except ImportError:
    print("ERROR: aiohttp not installed. Run: pip install aiohttp")
    sys.exit(1)


# PURPOSE: Issue ãƒ¢ãƒ‡ãƒ«
@dataclass
class Issue:
    """GitHub Issue ã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã€‚"""
    title: str
    body: str
    labels: list[str] = field(default_factory=list)
    assignees: list[str] = field(default_factory=list)


# === å®šæ•° ===
JULES_API_BASE = "https://jules.googleapis.com/v1alpha/sessions"
MAX_ISSUES_PER_RUN = 3  # 1å›ã®å®Ÿè¡Œã§ä½œã‚‹ Issue ã®ä¸Šé™
REPO = os.getenv("JULES_REPO_SOURCE", "laihuip001/hegemonikon")


# PURPOSE: API ã‚­ãƒ¼ã‚’1ã¤å–å¾—
def get_api_key() -> str:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¢ºèªç”¨ã® API ã‚­ãƒ¼ã‚’å–å¾—"""
    for i in range(1, 20):
        key = os.getenv(f"JULES_API_KEY_{i:02d}")
        if key:
            return key
    raise RuntimeError("No API key found (JULES_API_KEY_01~19)")


# PURPOSE: ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’å–å¾—
async def get_session(session_id: str, api_key: str) -> dict:
    """Jules ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®çŠ¶æ…‹ã¨çµæœã‚’å–å¾—"""
    headers = {"X-Goog-Api-Key": api_key}
    async with aiohttp.ClientSession() as session:
        async with session.get(
            f"{JULES_API_BASE}/{session_id}",
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=15),
        ) as resp:
            if resp.status == 200:
                return await resp.json()
            return {"error": resp.status, "session_id": session_id}


# PURPOSE: è¤‡æ•°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒãƒƒãƒå–å¾—
async def fetch_sessions(session_ids: list[str], api_key: str) -> list[dict]:
    """è¤‡æ•°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¸¦åˆ—ã§å–å¾—ï¼ˆrate limit è€ƒæ…®ï¼‰"""
    results = []
    for sid in session_ids:
        result = await get_session(sid, api_key)
        results.append(result)
        await asyncio.sleep(0.5)  # rate limit å¯¾ç­–
    return results


# PURPOSE: çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æŠ½å‡º
def extract_sessions(result_file: Path) -> list[dict]:
    """çµæœ JSON ã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æŠ½å‡º"""
    data = json.loads(result_file.read_text())
    sessions = []

    if "files" in data:
        for file_entry in data["files"]:
            target = file_entry.get("target_file", "unknown")
            for result in file_entry.get("results", []):
                if "session_id" in result:
                    sessions.append({
                        "session_id": result["session_id"],
                        "specialist_id": result.get("id", ""),
                        "specialist_name": result.get("name", ""),
                        "category": result.get("category", ""),
                        "archetype": result.get("archetype", ""),
                        "target_file": target,
                        "url": result.get("url", ""),
                    })
    elif "results" in data:
        target = data.get("target_file", "unknown")
        for result in data.get("results", []):
            if "session_id" in result:
                sessions.append({
                    "session_id": result["session_id"],
                    "specialist_id": result.get("id", ""),
                    "specialist_name": result.get("name", ""),
                    "category": result.get("category", ""),
                    "target_file": target,
                    "url": result.get("url", ""),
                })

    return sessions


# PURPOSE: ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‹ã‚‰ Issue æœ¬æ–‡ã‚’ç”Ÿæˆ
def format_issue(session_info: dict, session_data: dict) -> Issue | None:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’ Issue ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›

    Returns:
        Issue ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ or None (Issue ä¸è¦)
    """
    state = session_data.get("state", "UNKNOWN")

    # å®Œäº†ã—ã¦ã„ãªã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
    if state not in ("COMPLETED", "COMPLETED_WITH_CHANGES"):
        return None

    # PR/å¤‰æ›´æƒ…å ±ãŒã‚ã‚‹ã‹
    changes = session_data.get("codeChanges", [])
    pr_url = session_data.get("pullRequestUrl", "")
    summary = session_data.get("summary", "")

    if not summary and not changes and not pr_url:
        return None

    specialist = session_info.get("specialist_name", "Unknown")
    specialist_id = session_info.get("specialist_id", "")
    target = session_info.get("target_file", "unknown")
    category = session_info.get("category", "specialist-review")
    session_url = session_info.get("url", "")

    title = f"[Jules/{specialist_id}] {specialist}: {target}"

    body_lines = [
        f"## ğŸ” Specialist Review Result",
        f"",
        f"| Item | Value |",
        f"|:-----|:------|",
        f"| **Specialist** | {specialist} (`{specialist_id}`) |",
        f"| **Category** | `{category}` |",
        f"| **Target** | `{target}` |",
        f"| **Session** | [{session_info.get('session_id', '')}]({session_url}) |",
    ]

    if pr_url:
        body_lines.append(f"| **PR** | {pr_url} |")

    body_lines.extend(["", "## Summary", "", summary or "_No summary available_"])

    if changes:
        body_lines.extend(["", "## Changes", ""])
        for change in changes[:10]:  # æœ€å¤§10ä»¶
            path = change.get("path", "unknown")
            action = change.get("action", "modified")
            body_lines.append(f"- `{path}` ({action})")

    body_lines.extend([
        "",
        "---",
        f"*Auto-generated by Jules Specialist Reviews at {datetime.now().strftime('%Y-%m-%d %H:%M')}*",
    ])

    labels = ["jules-review", category]
    if pr_url:
        labels.append("has-pr")

    return Issue(title=title, body="\n".join(body_lines), labels=labels)


# PURPOSE: gh CLI ã§ Issue ã‚’ä½œæˆ
def create_github_issue(issue: Issue, repo: str, dry_run: bool = False) -> str | None:
    """gh CLI ã§ GitHub Issue ã‚’ä½œæˆ"""
    if dry_run:
        print(f"\n{'='*60}")
        print(f"[DRY RUN] Issue: {issue.title}")
        print(f"Labels: {', '.join(issue.labels)}")
        print(f"{'='*60}")
        print(issue.body)
        print(f"{'='*60}\n")
        return None

    cmd = [
        "gh", "issue", "create",
        "--repo", repo,
        "--title", issue.title,
        "--body", issue.body,
    ]
    for label in issue.labels:
        cmd.extend(["--label", label])

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        if result.returncode == 0:
            url = result.stdout.strip()
            print(f"  âœ“ Issue created: {url}")
            return url
        else:
            print(f"  âœ— Failed: {result.stderr.strip()}")
            return None
    except (subprocess.TimeoutExpired, FileNotFoundError) as e:
        print(f"  âœ— Error: {e}")
        return None


# PURPOSE: ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
async def main():
    import argparse

    parser = argparse.ArgumentParser(description="Jules â†’ GitHub Issue Creator v1.0")
    parser.add_argument("--results", "-r", nargs="+", help="Result JSON file(s)")
    parser.add_argument("--dir", "-d", default="", help="Results directory")
    parser.add_argument("--days", type=int, default=1, help="Days back to scan")
    parser.add_argument("--repo", default=REPO, help=f"GitHub repo (default: {REPO})")
    parser.add_argument("--max-issues", type=int, default=MAX_ISSUES_PER_RUN, help="Max issues per run")
    parser.add_argument("--dry-run", action="store_true", help="Print issues without creating")
    parser.add_argument("--status-only", action="store_true", help="Only check session statuses")

    args = parser.parse_args()

    # çµæœãƒ•ã‚¡ã‚¤ãƒ«åé›†
    result_files = []
    if args.results:
        result_files = [Path(f) for f in args.results if Path(f).exists()]
    elif args.dir:
        from collect_results import find_result_files
        result_files = find_result_files(base_dir=args.dir, days_back=args.days)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: logs/specialist_daily
        from collect_results import find_result_files
        result_files = find_result_files(days_back=args.days)

    if not result_files:
        print("No result files found.")
        return

    print(f"ğŸ“ Result files: {len(result_files)}")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±æŠ½å‡º
    all_sessions = []
    for rf in result_files:
        sessions = extract_sessions(rf)
        all_sessions.extend(sessions)

    if not all_sessions:
        print("No sessions found in result files.")
        return

    print(f"ğŸ“‹ Sessions found: {len(all_sessions)}")

    # API ã‚­ãƒ¼å–å¾—
    api_key = get_api_key()

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ç¢ºèª
    session_ids = [s["session_id"] for s in all_sessions]
    print(f"ğŸ” Checking {len(session_ids)} session(s)...")

    session_results = await fetch_sessions(session_ids, api_key)

    # çŠ¶æ…‹é›†è¨ˆ
    states = {}
    for sr in session_results:
        state = sr.get("state", sr.get("error", "UNKNOWN"))
        states[state] = states.get(state, 0) + 1

    print(f"\nğŸ“Š Session states:")
    for state, count in sorted(states.items(), key=lambda x: -x[1]):
        print(f"  {state}: {count}")

    if args.status_only:
        return

    # Issue ä½œæˆ
    issues_created = 0
    for session_info, session_data in zip(all_sessions, session_results):
        if issues_created >= args.max_issues:
            remaining = len(all_sessions) - issues_created
            print(f"\nâš ï¸ Max issues reached ({args.max_issues}). {remaining} sessions remaining.")
            break

        issue = format_issue(session_info, session_data)
        if not issue:
            continue

        url = create_github_issue(
            issue=issue,
            repo=args.repo,
            dry_run=args.dry_run,
        )
        if url or args.dry_run:
            issues_created += 1

    print(f"\n{'='*40}")
    print(f"Issues {'previewed' if args.dry_run else 'created'}: {issues_created}")
    print(f"{'='*40}")


if __name__ == "__main__":
    asyncio.run(main())
