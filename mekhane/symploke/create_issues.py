#!/usr/bin/env python3
# PROOF: [L3/„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£] <- mekhane/symploke/ O4‚Üí„É¨„Éì„É•„ÉºÁµêÊûú„Åã„ÇâIssueÁîüÊàê„ÅåÂøÖË¶Å‚Üícreate_issues „ÅåÊãÖ„ÅÜ
"""
Jules Specialist „É¨„Éì„É•„ÉºÁµêÊûú ‚Üí GitHub Issue Ëá™Âãï‰ΩúÊàê v1.0

ÂÆå‰∫Ü„Åó„Åü„Çª„ÉÉ„Ç∑„Éß„É≥„ÅÆÁµêÊûú„ÇíÂèñÂæó„Åó„ÄÅÈáçË¶Å„Å™Áô∫Ë¶ã„Çí GitHub Issue „Å®„Åó„Å¶Ëµ∑Á•®„Åô„Çã„ÄÇ

Usage:
  python create_issues.py --results /tmp/jules_test_*.json
  python create_issues.py --dir logs/specialist_daily --days 1
  python create_issues.py --results result.json --dry-run  # Issue ‰ΩúÊàê„Åõ„ÅöÂÜÖÂÆπ„ÇíË°®Á§∫

Requires:
  - gh CLI (GitHub CLI) „Åå„Ç§„É≥„Çπ„Éà„Éº„É´„ÉªË™çË®ºÊ∏à„Åø
  - JULES_API_KEY_01 Áí∞Â¢ÉÂ§âÊï∞Ôºà„Çª„ÉÉ„Ç∑„Éß„É≥ÁµêÊûúÂèñÂæóÁî®Ôºâ
"""

import asyncio
import json
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path

try:
    import aiohttp
except ImportError:
    print("ERROR: aiohttp not installed. Run: pip install aiohttp")
    sys.exit(1)


# === ÂÆöÊï∞ ===
JULES_API_BASE = "https://jules.googleapis.com/v1alpha/sessions"
MAX_ISSUES_PER_RUN = 3  # 1Âõû„ÅÆÂÆüË°å„Åß‰Ωú„Çã Issue „ÅÆ‰∏äÈôê
REPO = os.getenv("JULES_REPO_SOURCE", "laihuip001/hegemonikon")


# PURPOSE: API „Ç≠„Éº„Çí1„Å§ÂèñÂæó
def get_api_key() -> str:
    """„Çª„ÉÉ„Ç∑„Éß„É≥Á¢∫Ë™çÁî®„ÅÆ API „Ç≠„Éº„ÇíÂèñÂæó"""
    for i in range(1, 20):
        key = os.getenv(f"JULES_API_KEY_{i:02d}")
        if key:
            return key
    raise RuntimeError("No API key found (JULES_API_KEY_01~19)")


# PURPOSE: „Çª„ÉÉ„Ç∑„Éß„É≥Áä∂ÊÖã„ÇíÂèñÂæó
async def get_session(session_id: str, api_key: str) -> dict:
    """Jules „Çª„ÉÉ„Ç∑„Éß„É≥„ÅÆÁä∂ÊÖã„Å®ÁµêÊûú„ÇíÂèñÂæó"""
    headers = {"X-Goog-Api-Key": api_key}
    async with aiohttp.ClientSession() as session:
        async with session.get(
            f"{JULES_API_BASE}/{session_id}",
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=15),
        ) as resp:
            if resp.status == 200:
                return await resp.json()
            return {"error": resp.status, "session_id": session_id}


# PURPOSE: Ë§áÊï∞„Çª„ÉÉ„Ç∑„Éß„É≥„Çí„Éê„ÉÉ„ÉÅÂèñÂæó
async def fetch_sessions(session_ids: list[str], api_key: str) -> list[dict]:
    """Ë§áÊï∞„Çª„ÉÉ„Ç∑„Éß„É≥„Çí‰∏¶Âàó„ÅßÂèñÂæóÔºàrate limit ËÄÉÊÖÆÔºâ"""
    results = []
    for sid in session_ids:
        result = await get_session(sid, api_key)
        results.append(result)
        await asyncio.sleep(0.5)  # rate limit ÂØæÁ≠ñ
    return results


# PURPOSE: ÁµêÊûú„Éï„Ç°„Ç§„É´„Åã„Çâ„Çª„ÉÉ„Ç∑„Éß„É≥ÊÉÖÂ†±„ÇíÊäΩÂá∫
def extract_sessions(result_file: Path) -> list[dict]:
    """ÁµêÊûú JSON „Åã„Çâ„Çª„ÉÉ„Ç∑„Éß„É≥ÊÉÖÂ†±„ÇíÊäΩÂá∫"""
    data = json.loads(result_file.read_text())
    sessions = []

    if "files" in data:
        for file_entry in data["files"]:
            target = file_entry.get("target_file", "unknown")
            for result in file_entry.get("results", []):
                if "session_id" in result:
                    sessions.append({
                        "session_id": result["session_id"],
                        "specialist_id": result.get("id", ""),
                        "specialist_name": result.get("name", ""),
                        "category": result.get("category", ""),
                        "archetype": result.get("archetype", ""),
                        "target_file": target,
                        "url": result.get("url", ""),
                    })
    elif "results" in data:
        target = data.get("target_file", "unknown")
        for result in data.get("results", []):
            if "session_id" in result:
                sessions.append({
                    "session_id": result["session_id"],
                    "specialist_id": result.get("id", ""),
                    "specialist_name": result.get("name", ""),
                    "category": result.get("category", ""),
                    "target_file": target,
                    "url": result.get("url", ""),
                })

    return sessions


# PURPOSE: „Çª„ÉÉ„Ç∑„Éß„É≥ÁµêÊûú„Åã„Çâ Issue Êú¨Êñá„ÇíÁîüÊàê
def format_issue(session_info: dict, session_data: dict) -> dict | None:
    """„Çª„ÉÉ„Ç∑„Éß„É≥ÁµêÊûú„Çí GitHub Issue „ÅÆ title/body „Å´Â§âÊèõ

    Returns:
        {"title": ..., "body": ..., "labels": [...]} or None (Issue ‰∏çË¶Å)
    """
    state = session_data.get("state", "UNKNOWN")

    # ÂÆå‰∫Ü„Åó„Å¶„ÅÑ„Å™„ÅÑ„Çª„ÉÉ„Ç∑„Éß„É≥„ÅØ„Çπ„Ç≠„ÉÉ„Éó
    if state not in ("COMPLETED", "COMPLETED_WITH_CHANGES"):
        return None

    # PR/Â§âÊõ¥ÊÉÖÂ†±„Åå„ÅÇ„Çã„Åã
    changes = session_data.get("codeChanges", [])
    pr_url = session_data.get("pullRequestUrl", "")
    summary = session_data.get("summary", "")

    if not summary and not changes and not pr_url:
        return None

    specialist = session_info.get("specialist_name", "Unknown")
    specialist_id = session_info.get("specialist_id", "")
    target = session_info.get("target_file", "unknown")
    category = session_info.get("category", "specialist-review")
    session_url = session_info.get("url", "")

    title = f"[Jules/{specialist_id}] {specialist}: {target}"

    body_lines = [
        f"## üîç Specialist Review Result",
        f"",
        f"| Item | Value |",
        f"|:-----|:------|",
        f"| **Specialist** | {specialist} (`{specialist_id}`) |",
        f"| **Category** | `{category}` |",
        f"| **Target** | `{target}` |",
        f"| **Session** | [{session_info.get('session_id', '')}]({session_url}) |",
    ]

    if pr_url:
        body_lines.append(f"| **PR** | {pr_url} |")

    body_lines.extend(["", "## Summary", "", summary or "_No summary available_"])

    if changes:
        body_lines.extend(["", "## Changes", ""])
        for change in changes[:10]:  # ÊúÄÂ§ß10‰ª∂
            path = change.get("path", "unknown")
            action = change.get("action", "modified")
            body_lines.append(f"- `{path}` ({action})")

    body_lines.extend([
        "",
        "---",
        f"*Auto-generated by Jules Specialist Reviews at {datetime.now().strftime('%Y-%m-%d %H:%M')}*",
    ])

    labels = ["jules-review", category]
    if pr_url:
        labels.append("has-pr")

    return {
        "title": title,
        "body": "\n".join(body_lines),
        "labels": labels,
    }


# PURPOSE: gh CLI „Åß Issue „Çí‰ΩúÊàê
def create_github_issue(title: str, body: str, labels: list[str], repo: str, dry_run: bool = False) -> str | None:
    """gh CLI „Åß GitHub Issue „Çí‰ΩúÊàê"""
    if dry_run:
        print(f"\n{'='*60}")
        print(f"[DRY RUN] Issue: {title}")
        print(f"Labels: {', '.join(labels)}")
        print(f"{'='*60}")
        print(body)
        print(f"{'='*60}\n")
        return None

    cmd = [
        "gh", "issue", "create",
        "--repo", repo,
        "--title", title,
        "--body", body,
    ]
    for label in labels:
        cmd.extend(["--label", label])

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        if result.returncode == 0:
            url = result.stdout.strip()
            print(f"  ‚úì Issue created: {url}")
            return url
        else:
            print(f"  ‚úó Failed: {result.stderr.strip()}")
            return None
    except (subprocess.TimeoutExpired, FileNotFoundError) as e:
        print(f"  ‚úó Error: {e}")
        return None


# PURPOSE: „É°„Ç§„É≥„Ç®„É≥„Éà„É™„Éù„Ç§„É≥„Éà
async def main():
    import argparse

    parser = argparse.ArgumentParser(description="Jules ‚Üí GitHub Issue Creator v1.0")
    parser.add_argument("--results", "-r", nargs="+", help="Result JSON file(s)")
    parser.add_argument("--dir", "-d", default="", help="Results directory")
    parser.add_argument("--days", type=int, default=1, help="Days back to scan")
    parser.add_argument("--repo", default=REPO, help=f"GitHub repo (default: {REPO})")
    parser.add_argument("--max-issues", type=int, default=MAX_ISSUES_PER_RUN, help="Max issues per run")
    parser.add_argument("--dry-run", action="store_true", help="Print issues without creating")
    parser.add_argument("--status-only", action="store_true", help="Only check session statuses")

    args = parser.parse_args()

    # ÁµêÊûú„Éï„Ç°„Ç§„É´ÂèéÈõÜ
    result_files = []
    if args.results:
        result_files = [Path(f) for f in args.results if Path(f).exists()]
    elif args.dir:
        from collect_results import find_result_files
        result_files = find_result_files(base_dir=args.dir, days_back=args.days)
    else:
        # „Éá„Éï„Ç©„É´„Éà: logs/specialist_daily
        from collect_results import find_result_files
        result_files = find_result_files(days_back=args.days)

    if not result_files:
        print("No result files found.")
        return

    print(f"üìÅ Result files: {len(result_files)}")

    # „Çª„ÉÉ„Ç∑„Éß„É≥ÊÉÖÂ†±ÊäΩÂá∫
    all_sessions = []
    for rf in result_files:
        sessions = extract_sessions(rf)
        all_sessions.extend(sessions)

    if not all_sessions:
        print("No sessions found in result files.")
        return

    print(f"üìã Sessions found: {len(all_sessions)}")

    # API „Ç≠„ÉºÂèñÂæó
    api_key = get_api_key()

    # „Çª„ÉÉ„Ç∑„Éß„É≥Áä∂ÊÖãÁ¢∫Ë™ç
    session_ids = [s["session_id"] for s in all_sessions]
    print(f"üîç Checking {len(session_ids)} session(s)...")

    session_results = await fetch_sessions(session_ids, api_key)

    # Áä∂ÊÖãÈõÜË®à
    states = {}
    for sr in session_results:
        state = sr.get("state", sr.get("error", "UNKNOWN"))
        states[state] = states.get(state, 0) + 1

    print(f"\nüìä Session states:")
    for state, count in sorted(states.items(), key=lambda x: -x[1]):
        print(f"  {state}: {count}")

    if args.status_only:
        return

    # Issue ‰ΩúÊàê
    issues_created = 0
    for session_info, session_data in zip(all_sessions, session_results):
        if issues_created >= args.max_issues:
            remaining = len(all_sessions) - issues_created
            print(f"\n‚ö†Ô∏è Max issues reached ({args.max_issues}). {remaining} sessions remaining.")
            break

        issue = format_issue(session_info, session_data)
        if not issue:
            continue

        url = create_github_issue(
            title=issue["title"],
            body=issue["body"],
            labels=issue["labels"],
            repo=args.repo,
            dry_run=args.dry_run,
        )
        if url or args.dry_run:
            issues_created += 1

    print(f"\n{'='*40}")
    print(f"Issues {'previewed' if args.dry_run else 'created'}: {issues_created}")
    print(f"{'='*40}")


if __name__ == "__main__":
    asyncio.run(main())
