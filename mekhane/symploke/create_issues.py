#!/usr/bin/env python3
# PROOF: [L3/ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£] <- mekhane/symploke/ O4â†’ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœã‹ã‚‰Issueç”ŸæˆãŒå¿…è¦â†’create_issues ãŒæ‹…ã†
"""
Jules Specialist ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœ â†’ GitHub Issue è‡ªå‹•ä½œæˆ v1.0

å®Œäº†ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã®çµæœã‚’å–å¾—ã—ã€é‡è¦ãªç™ºè¦‹ã‚’ GitHub Issue ã¨ã—ã¦èµ·ç¥¨ã™ã‚‹ã€‚

Usage:
  python create_issues.py --results /tmp/jules_test_*.json
  python create_issues.py --dir logs/specialist_daily --days 1
  python create_issues.py --results result.json --dry-run  # Issue ä½œæˆã›ãšå†…å®¹ã‚’è¡¨ç¤º

Requires:
  - gh CLI (GitHub CLI) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»èªè¨¼æ¸ˆã¿
  - JULES_API_KEY_01 ç’°å¢ƒå¤‰æ•°ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœå–å¾—ç”¨ï¼‰
"""

import asyncio
import json
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path

try:
    import aiohttp
except ImportError:
    print("ERROR: aiohttp not installed. Run: pip install aiohttp")
    sys.exit(1)


# === å®šæ•° ===
JULES_API_BASE = "https://jules.googleapis.com/v1alpha/sessions"
MIN_QUALITY_SCORE = 4  # ã“ã®é–¾å€¤ä»¥ä¸Šã®å“è³ªã‚¹ã‚³ã‚¢ã§ Issue ã‚’ä½œæˆ
REPO = os.getenv("JULES_REPO_SOURCE", "laihuip001/hegemonikon")


# PURPOSE: API ã‚­ãƒ¼ã‚’1ã¤å–å¾—
def get_api_key() -> str:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¢ºèªç”¨ã® API ã‚­ãƒ¼ã‚’å–å¾—"""
    for i in range(1, 20):
        key = os.getenv(f"JULES_API_KEY_{i:02d}")
        if key:
            return key
    raise RuntimeError("No API key found (JULES_API_KEY_01~19)")


# PURPOSE: ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’å–å¾—
async def get_session(session_id: str, api_key: str) -> dict:
    """Jules ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®çŠ¶æ…‹ã¨çµæœã‚’å–å¾—"""
    headers = {"X-Goog-Api-Key": api_key}
    async with aiohttp.ClientSession() as session:
        async with session.get(
            f"{JULES_API_BASE}/{session_id}",
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=15),
        ) as resp:
            if resp.status == 200:
                return await resp.json()
            return {"error": resp.status, "session_id": session_id}


# PURPOSE: è¤‡æ•°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒãƒƒãƒå–å¾—
async def fetch_sessions(session_ids: list[str], api_key: str) -> list[dict]:
    """è¤‡æ•°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¸¦åˆ—ã§å–å¾—ï¼ˆrate limit è€ƒæ…®ï¼‰"""
    results = []
    for sid in session_ids:
        result = await get_session(sid, api_key)
        results.append(result)
        await asyncio.sleep(0.5)  # rate limit å¯¾ç­–
    return results


# PURPOSE: çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æŠ½å‡º
def extract_sessions(result_file: Path) -> list[dict]:
    """çµæœ JSON ã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æŠ½å‡º"""
    data = json.loads(result_file.read_text())
    sessions = []

    if "files" in data:
        for file_entry in data["files"]:
            target = file_entry.get("target_file", "unknown")
            for result in file_entry.get("results", []):
                if "session_id" in result:
                    sessions.append({
                        "session_id": result["session_id"],
                        "specialist_id": result.get("id", ""),
                        "specialist_name": result.get("name", ""),
                        "category": result.get("category", ""),
                        "archetype": result.get("archetype", ""),
                        "target_file": target,
                        "url": result.get("url", ""),
                    })
    elif "results" in data:
        target = data.get("target_file", "unknown")
        for result in data.get("results", []):
            if "session_id" in result:
                sessions.append({
                    "session_id": result["session_id"],
                    "specialist_id": result.get("id", ""),
                    "specialist_name": result.get("name", ""),
                    "category": result.get("category", ""),
                    "target_file": target,
                    "url": result.get("url", ""),
                })

    return sessions


# PURPOSE: diff ã®å“è³ªã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
def score_quality(changed_files: list[str], patch_text: str) -> tuple[int, list[str]]:
    """å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã€ç†ç”±ã‚’è¿”ã™

    ã‚¹ã‚³ã‚¢åŸºæº–:
        +3: ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ (.py, .ts, .yml) ã‚’ä¿®æ­£
        +2: ãƒã‚°ä¿®æ­£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (fix, bug, missing, error)
        +2: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢é€£ (security, vulnerability, injection)
        +1: 2ãƒ•ã‚¡ã‚¤ãƒ«ä»¥ä¸Šã®å¤‰æ›´ (æ³¢åŠã‚ã‚Š)
        +1: ãƒ†ã‚¹ãƒˆé–¢é€£ã®å¤‰æ›´
        -2: reviews/*.md ã®ã¿ã®å¤‰æ›´ (ãƒ¬ãƒ“ãƒ¥ãƒ¼å‡ºåŠ›ã®ã¿)
        -1: ã‚³ãƒ¡ãƒ³ãƒˆ/ç©ºç™½ã®ã¿ã®å¤‰æ›´

    Returns:
        (score, reasons)
    """
    score = 0
    reasons = []

    # ãƒœã‚¤ãƒ©ãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆé™¤å¤–: Jules ãŒå®šå‹çš„ã«å¤‰æ›´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«
    boilerplate_patterns = (
        'reviews/',          # ãƒ¬ãƒ“ãƒ¥ãƒ¼å‡ºåŠ›
        '_review.md',        # ãƒ¬ãƒ“ãƒ¥ãƒ¼å‡ºåŠ›
        '.github/workflows', # CI è¨­å®š (Jules ãŒä¾å­˜é–¢ä¿‚ã‚’è¿½åŠ ã™ã‚‹å®šå‹ä¿®æ­£)
    )
    meaningful_files = [f for f in changed_files
                        if not any(bp in f for bp in boilerplate_patterns)]
    boilerplate_files = [f for f in changed_files if f not in meaningful_files]

    # å®Ÿã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰å¤‰æ›´?
    source_exts = ('.py', '.ts', '.js', '.yml', '.yaml', '.toml', '.cfg')
    has_meaningful_source = any(f.endswith(source_exts) for f in meaningful_files)

    if not meaningful_files:
        # ãƒœã‚¤ãƒ©ãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã¿ = å®Ÿè³ªçš„ãªç™ºè¦‹ãªã—
        score -= 2
        reasons.append(f'boilerplate only ({len(boilerplate_files)} files)')
    elif has_meaningful_source:
        score += 4
        reasons.append(f'meaningful source change: {", ".join(meaningful_files[:3])}')

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º (å®Ÿã‚³ãƒ¼ãƒ‰éƒ¨åˆ†ã®ã¿)
    # ãƒœã‚¤ãƒ©ãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆéƒ¨åˆ†ã® diff ã‚’é™¤å¤–
    patch_lower = patch_text.lower()
    bug_keywords = ['fix', 'bug', 'missing', 'error', 'broken', 'incorrect', 'wrong']
    security_keywords = ['security', 'vulnerability', 'injection', 'xss', 'csrf', 'auth']

    for kw in bug_keywords:
        if kw in patch_lower:
            score += 2
            reasons.append(f'bug keyword: {kw}')
            break

    for kw in security_keywords:
        if kw in patch_lower:
            score += 2
            reasons.append(f'security keyword: {kw}')
            break

    # æ³¢åŠç¯„å›² (ãƒœã‚¤ãƒ©ãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆé™¤å¤–)
    if len(meaningful_files) >= 2:
        score += 1
        reasons.append(f'{len(meaningful_files)} meaningful files affected')

    # ãƒ†ã‚¹ãƒˆé–¢é€£
    if any('test' in f.lower() for f in meaningful_files):
        score += 1
        reasons.append('test-related')

    # ã‚³ãƒ¡ãƒ³ãƒˆ/ç©ºç™½ã®ã¿?
    meaningful_lines = [l for l in patch_text.split('\n')
                        if l.startswith('+') or l.startswith('-')]
    content_lines = [l for l in meaningful_lines
                     if l.strip() not in ('+', '-', '+ ', '- ')
                     and not l.strip().startswith(('+#', '-#', '+ #', '- #'))]
    if meaningful_lines and not content_lines:
        score -= 1
        reasons.append('comments/whitespace only')

    return score, reasons


# PURPOSE: ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‹ã‚‰ Issue æœ¬æ–‡ã‚’ç”Ÿæˆ
def format_issue(session_info: dict, session_data: dict) -> dict | None:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’ GitHub Issue ã® title/body ã«å¤‰æ›

    Jules API ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ :
        state: "COMPLETED" | "FAILED" | ...
        outputs[].changeSet.gitPatch.unidiffPatch: diff ãƒ†ã‚­ã‚¹ãƒˆ
        url: ã‚»ãƒƒã‚·ãƒ§ãƒ³ URL

    Returns:
        {"title": ..., "body": ..., "labels": [...]} or None (Issue ä¸è¦)
    """
    state = session_data.get("state", "UNKNOWN")

    # å®Œäº†ã—ã¦ã„ãªã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
    if state != "COMPLETED":
        return None

    # diff ã‚’æŠ½å‡º
    outputs = session_data.get("outputs", [])
    patches = []
    changed_files = []
    for output in outputs:
        change_set = output.get("changeSet", {})
        git_patch = change_set.get("gitPatch", {})
        patch = git_patch.get("unidiffPatch", "")
        if patch:
            patches.append(patch)
            # diff ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º
            for line in patch.split("\n"):
                if line.startswith("diff --git"):
                    parts = line.split(" b/")
                    if len(parts) >= 2:
                        changed_files.append(parts[-1])

    # diff ãŒãªã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ— (= å¤‰æ›´ãªã— = æŒ‡æ‘˜ãªã—)
    if not patches:
        return None

    specialist = session_info.get("specialist_name", "Unknown")
    specialist_id = session_info.get("specialist_id", "")
    target = session_info.get("target_file", "unknown")
    category = session_info.get("category", "specialist-review")
    session_url = session_info.get("url", "")

    title = f"[Jules/{specialist_id}] {specialist}: {target}"

    body_lines = [
        "## ğŸ” Specialist Review Result",
        "",
        "| Item | Value |",
        "|:-----|:------|",
        f"| **Specialist** | {specialist} (`{specialist_id}`) |",
        f"| **Category** | `{category}` |",
        f"| **Target** | `{target}` |",
        f"| **Session** | [{session_info.get('session_id', '')}]({session_url}) |",
        f"| **Changed files** | {len(changed_files)} |",
        "",
    ]

    # å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
    if changed_files:
        body_lines.extend(["## Changed Files", ""])
        for cf in changed_files[:15]:
            body_lines.append(f"- `{cf}`")
        if len(changed_files) > 15:
            body_lines.append(f"- ... and {len(changed_files) - 15} more")
        body_lines.append("")

    # å“è³ªã‚¹ã‚³ã‚¢
    full_patch = "\n".join(patches)
    quality_score, quality_reasons = score_quality(changed_files, full_patch)

    # å“è³ªã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
    body_lines.extend([
        "## Quality",
        "",
        f"Score: **{quality_score}** (threshold: {MIN_QUALITY_SCORE})",
        f"Reasons: {', '.join(quality_reasons)}",
        "",
    ])

    # diff (æœ€å¤§2000æ–‡å­—ã¾ã§)
    if len(full_patch) > 2000:
        full_patch = full_patch[:2000] + "\n... (truncated)"
    body_lines.extend([
        "## Diff",
        "",
        "```diff",
        full_patch,
        "```",
        "",
        "---",
        f"*Auto-generated by Jules Specialist Reviews at {datetime.now().strftime('%Y-%m-%d %H:%M')}*",
    ])

    labels = ["jules-review", category]
    if changed_files:
        labels.append("has-changes")

    return {
        "title": title,
        "body": "\n".join(body_lines),
        "labels": labels,
        "quality_score": quality_score,
        "quality_reasons": quality_reasons,
    }


# PURPOSE: gh CLI ã§ Issue ã‚’ä½œæˆ
def create_github_issue(title: str, body: str, labels: list[str], repo: str, dry_run: bool = False) -> str | None:
    """gh CLI ã§ GitHub Issue ã‚’ä½œæˆ"""
    if dry_run:
        print(f"\n{'='*60}")
        print(f"[DRY RUN] Issue: {title}")
        print(f"Labels: {', '.join(labels)}")
        print(f"{'='*60}")
        print(body)
        print(f"{'='*60}\n")
        return None

    cmd = [
        "gh", "issue", "create",
        "--repo", repo,
        "--title", title,
        "--body", body,
    ]
    for label in labels:
        cmd.extend(["--label", label])

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        if result.returncode == 0:
            url = result.stdout.strip()
            print(f"  âœ“ Issue created: {url}")
            return url
        else:
            print(f"  âœ— Failed: {result.stderr.strip()}")
            return None
    except (subprocess.TimeoutExpired, FileNotFoundError) as e:
        print(f"  âœ— Error: {e}")
        return None


# PURPOSE: ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
async def main():
    import argparse

    parser = argparse.ArgumentParser(description="Jules â†’ GitHub Issue Creator v1.0")
    parser.add_argument("--results", "-r", nargs="+", help="Result JSON file(s)")
    parser.add_argument("--dir", "-d", default="", help="Results directory")
    parser.add_argument("--days", type=int, default=1, help="Days back to scan")
    parser.add_argument("--repo", default=REPO, help=f"GitHub repo (default: {REPO})")
    parser.add_argument("--min-score", type=int, default=MIN_QUALITY_SCORE, help="Minimum quality score for Issue creation")
    parser.add_argument("--dry-run", action="store_true", help="Print issues without creating")
    parser.add_argument("--status-only", action="store_true", help="Only check session statuses")
    parser.add_argument("--show-all", action="store_true", help="Show all issues including low-quality ones")

    args = parser.parse_args()

    # çµæœãƒ•ã‚¡ã‚¤ãƒ«åé›†
    result_files = []
    if args.results:
        result_files = [Path(f) for f in args.results if Path(f).exists()]
    elif args.dir:
        from collect_results import find_result_files
        result_files = find_result_files(base_dir=args.dir, days_back=args.days)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: logs/specialist_daily
        from collect_results import find_result_files
        result_files = find_result_files(days_back=args.days)

    if not result_files:
        print("No result files found.")
        return

    print(f"ğŸ“ Result files: {len(result_files)}")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±æŠ½å‡º
    all_sessions = []
    for rf in result_files:
        sessions = extract_sessions(rf)
        all_sessions.extend(sessions)

    if not all_sessions:
        print("No sessions found in result files.")
        return

    print(f"ğŸ“‹ Sessions found: {len(all_sessions)}")

    # API ã‚­ãƒ¼å–å¾—
    api_key = get_api_key()

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ç¢ºèª
    session_ids = [s["session_id"] for s in all_sessions]
    print(f"ğŸ” Checking {len(session_ids)} session(s)...")

    session_results = await fetch_sessions(session_ids, api_key)

    # çŠ¶æ…‹é›†è¨ˆ
    states = {}
    for sr in session_results:
        state = sr.get("state", sr.get("error", "UNKNOWN"))
        states[state] = states.get(state, 0) + 1

    print(f"\nğŸ“Š Session states:")
    for state, count in sorted(states.items(), key=lambda x: -x[1]):
        print(f"  {state}: {count}")

    if args.status_only:
        return

    # Issue ä½œæˆ (å“è³ªãƒ•ã‚£ãƒ«ã‚¿)
    issues_created = 0
    issues_skipped = 0
    for session_info, session_data in zip(all_sessions, session_results):
        issue = format_issue(session_info, session_data)
        if not issue:
            continue

        score = issue["quality_score"]
        reasons = issue["quality_reasons"]

        if score < args.min_score:
            issues_skipped += 1
            if args.show_all:
                print(f"  â­ Skip (score={score}): {issue['title']}")
                print(f"    Reasons: {', '.join(reasons)}")
            continue

        url = create_github_issue(
            title=issue["title"],
            body=issue["body"],
            labels=issue["labels"],
            repo=args.repo,
            dry_run=args.dry_run,
        )
        if url or args.dry_run:
            issues_created += 1

    print(f"\n{'='*40}")
    print(f"Issues {'previewed' if args.dry_run else 'created'}: {issues_created}")
    print(f"Issues skipped (score < {args.min_score}): {issues_skipped}")
    print(f"{'='*40}")


if __name__ == "__main__":
    asyncio.run(main())
