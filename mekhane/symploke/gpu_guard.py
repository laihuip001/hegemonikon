#!/usr/bin/env python3
# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- mekhane/symploke/ A0â†’GPUç«¶åˆé˜²æ­¢ãŒå¿…è¦â†’gpu_guard ãŒæ‹…ã†
"""
GPU Guard â€” GPU ãƒªã‚½ãƒ¼ã‚¹ç«¶åˆã‚’é˜²æ­¢ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

PURPOSE:
    å˜ä¸€ GPU ç’°å¢ƒ (RTX 2070 SUPER 8GB) ã§ã€é•·æ™‚é–“å®Ÿè¡Œãƒ—ãƒ­ã‚»ã‚¹ (gnosis_chat, LLM æ¨è«–ç­‰) ãŒ
    GPU ã‚’å æœ‰ã—ã¦ã„ã‚‹å ´åˆã«ã€æ–°è¦ãƒ—ãƒ­ã‚»ã‚¹ã® CUDA åˆæœŸåŒ–ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ãƒãƒ³ã‚°ã™ã‚‹å•é¡Œã‚’é˜²æ­¢ã™ã‚‹ã€‚

USAGE:
    from mekhane.symploke.gpu_guard import gpu_preflight, force_cpu_env

    # GPU çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
    status = gpu_preflight()
    if not status.gpu_available:
        print(f"GPU busy: {status.reason}")

    # å¼·åˆ¶ CPU ç’°å¢ƒã‚’è¨­å®š
    force_cpu_env()  # os.environ["CUDA_VISIBLE_DEVICES"] = ""
"""

import os
import subprocess
from dataclasses import dataclass
from typing import Optional


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

GPU_UTIL_THRESHOLD = 80      # GPU utilization % above which we consider it busy
GPU_MEM_THRESHOLD_MB = 6000  # Memory usage (MiB) above which we consider it busy
VRAM_TOTAL_MB = 8192         # RTX 2070 SUPER VRAM


# ---------------------------------------------------------------------------
# Data Classes
# ---------------------------------------------------------------------------

@dataclass
class GPUStatus:
    """GPU ãƒ—ãƒªãƒ•ãƒ©ã‚¤ãƒˆãƒã‚§ãƒƒã‚¯çµæœ"""
    gpu_available: bool       # True if GPU is available for new processes
    utilization: int          # GPU utilization %
    memory_used_mb: int       # Memory used (MiB)
    memory_total_mb: int      # Memory total (MiB)
    blocking_process: Optional[str]  # Process name blocking GPU (if any)
    reason: str               # Human-readable status


# ---------------------------------------------------------------------------
# Core Functions
# ---------------------------------------------------------------------------

def gpu_preflight() -> GPUStatus:
    """
    GPU ãƒ—ãƒªãƒ•ãƒ©ã‚¤ãƒˆãƒã‚§ãƒƒã‚¯ â€” GPU ãŒæ–°ã—ã„ãƒ—ãƒ­ã‚»ã‚¹ã§ä½¿ãˆã‚‹ã‹åˆ¤å®š

    Returns:
        GPUStatus with availability and details
    """
    try:
        result = subprocess.run(
            [
                "nvidia-smi",
                "--query-gpu=utilization.gpu,memory.used,memory.total",
                "--format=csv,noheader,nounits",
            ],
            capture_output=True,
            text=True,
            timeout=5,
        )

        if result.returncode != 0:
            return GPUStatus(
                gpu_available=False,
                utilization=0,
                memory_used_mb=0,
                memory_total_mb=0,
                blocking_process=None,
                reason="nvidia-smi failed",
            )

        line = result.stdout.strip()
        parts = [p.strip() for p in line.split(",")]
        utilization = int(parts[0])
        memory_used = int(parts[1])
        memory_total = int(parts[2])

        # Check if GPU is busy
        is_busy = utilization > GPU_UTIL_THRESHOLD or memory_used > GPU_MEM_THRESHOLD_MB

        blocking = None
        reason = "GPU available"

        if is_busy:
            # Find blocking process
            blocking = _find_blocking_process()
            reason = (
                f"GPU busy: util={utilization}%, "
                f"mem={memory_used}/{memory_total}MiB"
            )
            if blocking:
                reason += f" (by {blocking})"

        return GPUStatus(
            gpu_available=not is_busy,
            utilization=utilization,
            memory_used_mb=memory_used,
            memory_total_mb=memory_total,
            blocking_process=blocking,
            reason=reason,
        )

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return GPUStatus(
            gpu_available=False,
            utilization=0,
            memory_used_mb=0,
            memory_total_mb=0,
            blocking_process=None,
            reason="nvidia-smi not available",
        )


def _find_blocking_process() -> Optional[str]:
    """GPU ã‚’å æœ‰ã—ã¦ã„ã‚‹ Python ãƒ—ãƒ­ã‚»ã‚¹ã®åå‰ã‚’å–å¾—"""
    try:
        result = subprocess.run(
            [
                "nvidia-smi",
                "--query-compute-apps=pid,process_name,used_memory",
                "--format=csv,noheader,nounits",
            ],
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode == 0 and result.stdout.strip():
            lines = result.stdout.strip().split("\n")
            # Find the largest GPU consumer
            max_mem = 0
            max_name = None
            for line in lines:
                parts = [p.strip() for p in line.split(",")]
                if len(parts) >= 3:
                    try:
                        mem = int(parts[2])
                        if mem > max_mem:
                            max_mem = mem
                            max_name = parts[1]
                    except ValueError:
                        continue
            return max_name
    except Exception:
        pass
    return None


def force_cpu_env() -> None:
    """
    CUDA ã‚’ç„¡åŠ¹åŒ–ã—ã¦ CPU ã®ã¿ã§å®Ÿè¡Œã•ã›ã‚‹ã€‚

    os.environ ã«è¨­å®šã™ã‚‹ãŸã‚ã€ã“ã®ãƒ—ãƒ­ã‚»ã‚¹å†…ã®å…¨ torch/CUDA æ“ä½œã«å½±éŸ¿ã™ã‚‹ã€‚
    import torch ã®å‰ã«å‘¼ã¶å¿…è¦ãŒã‚ã‚‹ã€‚
    """
    os.environ["CUDA_VISIBLE_DEVICES"] = ""


def ensure_safe_gpu() -> bool:
    """
    GPU ãŒå®‰å…¨ã«ä½¿ãˆã‚‹ã‹ç¢ºèªã—ã€ä½¿ãˆãªã„å ´åˆã¯ CPU ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã€‚

    Returns:
        True if GPU is available, False if forced to CPU
    """
    status = gpu_preflight()
    if not status.gpu_available:
        force_cpu_env()
        return False
    return True


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

def main():
    """GPU ãƒ—ãƒªãƒ•ãƒ©ã‚¤ãƒˆãƒã‚§ãƒƒã‚¯ CLI"""
    status = gpu_preflight()
    icon = "ğŸŸ¢" if status.gpu_available else "ğŸ”´"
    print(f"{icon} GPU Status: {status.reason}")
    print(f"   Utilization: {status.utilization}%")
    print(f"   Memory: {status.memory_used_mb}/{status.memory_total_mb} MiB")
    if status.blocking_process:
        print(f"   Blocking: {status.blocking_process}")


if __name__ == "__main__":
    main()
