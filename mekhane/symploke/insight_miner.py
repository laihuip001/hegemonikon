#!/usr/bin/env python3
# PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] <- mekhane/symploke/ A3â†’çŸ¥æµã‚’è“„ç©ã™ã‚‹å¿…è¦â†’insight_miner ãŒæ‹…ã†
"""
Insight Miner - ä¼šè©±ãƒ­ã‚°ã‹ã‚‰åŸ‹ã‚‚ã‚ŒãŸæ´å¯Ÿã‚’ç™ºæ˜

Usage:
    python insight_miner.py                    # å…¨ãƒ­ã‚°ã‹ã‚‰æ´å¯Ÿã‚’æŠ½å‡º
    python insight_miner.py --limit 10         # æœ€æ–°10ä»¶ã®ã¿
    python insight_miner.py --pattern gnome    # æ ¼è¨€ãƒ‘ã‚¿ãƒ¼ãƒ³
"""

import sys
import re
import argparse
from pathlib import Path
from dataclasses import dataclass
from typing import List, Optional

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from mekhane.symploke.kairos_ingest import get_conversation_files


@dataclass
class Insight:
    """æŠ½å‡ºã•ã‚ŒãŸæ´å¯Ÿ"""

    text: str
    category: str  # gnome, principle, discovery, decision
    source_file: str
    context: str  # å‘¨è¾ºã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    confidence: float  # æŠ½å‡ºã®ç¢ºä¿¡åº¦


# æ´å¯Ÿã‚’ç¤ºå”†ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
INSIGHT_PATTERNS = {
    "gnome": [
        # æ ¼è¨€ãƒ»æ•™è¨“ãƒ‘ã‚¿ãƒ¼ãƒ³
        r"(?:é‡è¦ãª?æ•™è¨“|ä»Šæ—¥ã®ç™ºè¦‹|ä»Šæ—¥ã®å­¦ã³|ã‚­ãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆ)[:ï¼š]?\s*[ã€Œã€]?(.{10,100})[ã€ã€]?",
        r"(?:æœ¬è³ªã¯|æ ¸å¿ƒã¯|è¦ã¯)[:ï¼š]?\s*[ã€Œã€]?(.{10,80})[ã€ã€]?",
        r"> \*\*(.{10,100})\*\*",  # å¼·èª¿ã•ã‚ŒãŸå¼•ç”¨
    ],
    "principle": [
        # åŸå‰‡ãƒ‘ã‚¿ãƒ¼ãƒ³
        r"(?:åŸå‰‡|ãƒ«ãƒ¼ãƒ«|æ–¹é‡)[:ï¼š]?\s*(.{10,100})",
        r"(?:ã€œã™ã¹ã|ã€œã¹ãã |ã€œãŒé‰„å‰‡)(.{5,50})",
        r"(?:å¿…ãš|çµ¶å¯¾ã«|å¸¸ã«)(.{5,80})(?:ã™ã‚‹ã“ã¨|ã™ã‚‹)",
    ],
    "discovery": [
        # ç™ºè¦‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        r"(?:ç™ºè¦‹ã—ãŸ|æ°—ã¥ã„ãŸ|ã‚ã‹ã£ãŸ)[:ï¼š]?\s*(.{10,100})",
        r"(?:åˆ¤æ˜ã—ãŸ|æ˜ã‚‰ã‹ã«ãªã£ãŸ)[:ï¼š]?\s*(.{10,100})",
        r"(?:é¢ç™½ã„ã“ã¨ã«|èˆˆå‘³æ·±ã„ã“ã¨ã«)(.{10,100})",
    ],
    "decision": [
        # æ±ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³
        r"(?:æ±ºå®šã—ãŸ|æ¡ç”¨ã—ãŸ|æ±ºã‚ãŸ)[:ï¼š]?\s*(.{10,100})",
        r"(?:æ–¹é‡ã‚’|è¨­è¨ˆã‚’|å®Ÿè£…ã‚’)(.{10,80})(?:ã«æ±ºå®š|ã¨ã—ãŸ)",
    ],
}


def extract_context(content: str, match_start: int, context_size: int = 200) -> str:
    """ãƒãƒƒãƒå‘¨è¾ºã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    start = max(0, match_start - context_size)
    end = min(len(content), match_start + context_size)
    return content[start:end].strip()


def score_insight_quality(text: str) -> float:
    """æ´å¯Ÿã®å“è³ªã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° (0.0 - 1.0)"""
    score = 0.5  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢

    # ãƒã‚¸ãƒ†ã‚£ãƒ–è¦å› 
    if any(c in text for c in "ã€‚ï¼ï¼Ÿ"):  # æ–‡ãŒå®Œçµã—ã¦ã„ã‚‹
        score += 0.15
    if len(text) >= 30:  # ååˆ†ãªé•·ã•
        score += 0.1
    if any(kw in text for kw in ["ã§ã‚ã‚‹", "ã ã€‚", "ã“ã¨ã€‚", "ã¹ã"]):
        score += 0.1  # çµè«–èª¿ã®è¡¨ç¾

    # ãƒã‚¬ãƒ†ã‚£ãƒ–è¦å› 
    if text.startswith(("ãªã„", "ãªã‹ã£ãŸ", "...", "ã ã€‚\n")):
        score -= 0.3  # ä¸å®Œå…¨ãªæ–‡é ­
    if "\n## ğŸ¤–" in text or "\n##" in text:
        score -= 0.4  # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ§‹é€ ãŒæ··å…¥
    if len(text) < 20:
        score -= 0.2  # çŸ­ã™ã
    if text.count("\n") > 3:
        score -= 0.2  # æ”¹è¡ŒãŒå¤šã™ãï¼ˆè¤‡æ•°æ–‡ï¼‰
    if any(
        noise in text for noise in ["Claude", "Thought for", "Progress", "Files Edited"]
    ):
        score -= 0.5  # UIãƒã‚¤ã‚º

    return max(0.0, min(1.0, score))


def clean_insight_text(text: str) -> str:
    """æ´å¯Ÿãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    # æœ«å°¾ã®ãƒã‚¤ã‚ºã‚’é™¤å»
    text = re.sub(r"\n## ğŸ¤–.*$", "", text, flags=re.DOTALL)
    text = re.sub(r"\n\n+", "\n", text)
    text = text.strip()

    # æœ€åˆã®æ–‡ã ã‘ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°æ–‡ã®å ´åˆï¼‰
    if "ã€‚" in text:
        first_sentence_end = text.find("ã€‚") + 1
        if first_sentence_end < len(text) and first_sentence_end > 10:
            text = text[:first_sentence_end]

    return text.strip()


def mine_insights(
    file_path: Path, categories: Optional[List[str]] = None, min_quality: float = 0.4
) -> List[Insight]:
    """ä¼šè©±ãƒ­ã‚°ã‹ã‚‰æ´å¯Ÿã‚’æŠ½å‡ºï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
    content = file_path.read_text(encoding="utf-8")
    insights = []

    categories = categories or list(INSIGHT_PATTERNS.keys())

    for category in categories:
        if category not in INSIGHT_PATTERNS:
            continue

        for pattern in INSIGHT_PATTERNS[category]:
            for match in re.finditer(pattern, content, re.MULTILINE | re.DOTALL):
                text = match.group(1) if match.groups() else match.group(0)
                text = clean_insight_text(text)

                # çŸ­ã™ãã‚‹ã‚‚ã®ã¯é™¤å¤–
                if len(text) < 15:
                    continue

                # å“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
                quality = score_insight_quality(text)
                if quality < min_quality:
                    continue

                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if any(i.text == text for i in insights):
                    continue

                insights.append(
                    Insight(
                        text=text,
                        category=category,
                        source_file=file_path.name,
                        context=extract_context(content, match.start()),
                        confidence=quality,
                    )
                )

    return insights


def mine_all_logs(
    limit: Optional[int] = None, categories: Optional[List[str]] = None
) -> List[Insight]:
    """å…¨ãƒ­ã‚°ã‹ã‚‰æ´å¯Ÿã‚’æŠ½å‡º"""
    files = get_conversation_files()
    if limit:
        files = files[:limit]

    all_insights = []
    for f in files:
        insights = mine_insights(f, categories)
        all_insights.extend(insights)

    return all_insights


def generate_ki_candidates(insights: List[Insight]) -> str:
    """KIå€™è£œãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    lines = ["# åŸ‹ã‚‚ã‚ŒãŸæ´å¯Ÿãƒ¬ãƒãƒ¼ãƒˆ\n"]
    lines.append(f"æŠ½å‡ºä»¶æ•°: {len(insights)} ä»¶\n")

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ•´ç†
    by_category = {}
    for i in insights:
        by_category.setdefault(i.category, []).append(i)

    for cat, items in by_category.items():
        lines.append(f"\n## {cat.upper()} ({len(items)} ä»¶)\n")
        for item in items[:10]:  # å„ã‚«ãƒ†ã‚´ãƒªä¸Šä½10ä»¶
            lines.append(f"- **{item.text}**")
            lines.append(f"  - Source: `{item.source_file}`\n")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="Mine insights from conversation logs")
    parser.add_argument("--limit", type=int, help="Limit number of files to process")
    parser.add_argument(
        "--pattern",
        type=str,
        choices=list(INSIGHT_PATTERNS.keys()),
        help="Filter by insight category",
    )
    parser.add_argument("--output", type=str, help="Output file path")
    args = parser.parse_args()

    categories = [args.pattern] if args.pattern else None

    print(f"ğŸ” Mining insights from conversation logs...")
    insights = mine_all_logs(limit=args.limit, categories=categories)

    print(f"ğŸ“Š Found {len(insights)} insights")

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼
    by_cat = {}
    for i in insights:
        by_cat.setdefault(i.category, []).append(i)

    for cat, items in by_cat.items():
        print(f"  {cat}: {len(items)} ä»¶")

    # ä¸Šä½ã®æ´å¯Ÿã‚’è¡¨ç¤º
    print("\nğŸ† Top Insights:")
    for insight in insights[:5]:
        print(f"  [{insight.category}] {insight.text}")
        print(f"           â† {insight.source_file}")

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    if args.output:
        report = generate_ki_candidates(insights)
        Path(args.output).write_text(report, encoding="utf-8")
        print(f"\nğŸ’¾ Report saved: {args.output}")


if __name__ == "__main__":
    main()
