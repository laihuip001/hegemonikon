#!/usr/bin/env python3
# PROOF: [L2/Mekhane] <- mekhane/symploke/ A0->Auto->AddedByCI
# PROOF: [L3/ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£] <- mekhane/symploke/ O4â†’æ—¥æ¬¡ãƒãƒƒãƒæ¶ˆåŒ–â†’scheduler ãŒæ‹…ã†
# PURPOSE: Jules 720 tasks/day ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ â€” 6å¢åˆ†æ•£ + è‡ªå‹•ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
"""
Jules Daily Scheduler v2.1

æœ‰åŠ¹ã‚­ãƒ¼ãƒ—ãƒ¼ãƒ«æ–¹å¼ã€‚èµ·å‹•æ™‚ã«å…¨ API ã‚­ãƒ¼ã‚’æ¤œè¨¼ã—ã€
æœ‰åŠ¹ãªã‚­ãƒ¼ã ã‘ã‚’ä½¿ã£ã¦ãƒãƒƒãƒã‚’å®Ÿè¡Œã™ã‚‹ã€‚
cron ã‹ã‚‰ 3 ã‚¹ãƒ­ãƒƒãƒˆ (06:00/12:00/18:00) ã§å‘¼ã°ã‚Œã‚‹ã€‚

Architecture:
    cron â†’ jules_daily_scheduler.py --slot morning
           â”œâ”€ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ (å…¨ .py â†’ æ—¥æ¬¡ N ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ)
           â”œâ”€ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†é… (2å¢/slot Ã— 3 slots = 6å¢/day)
           â””â”€ run_specialists.py ã®ãƒãƒƒãƒå®Ÿè¡Œ

Usage:
    # Dry-run (ä½•ã‚‚å®Ÿè¡Œã—ãªã„ã€é…åˆ†ã ã‘è¡¨ç¤º)
    PYTHONPATH=. python mekhane/symploke/jules_daily_scheduler.py --slot morning --dry-run

    # Basanos mode (æ§‹é€ åŒ–ãƒ¬ãƒ“ãƒ¥ãƒ¼ + pre-audit)
    PYTHONPATH=. python mekhane/symploke/jules_daily_scheduler.py --slot morning --mode basanos --pre-audit

    # Small test (2 files Ã— 3 specialists = 6 tasks)
    PYTHONPATH=. python mekhane/symploke/jules_daily_scheduler.py --slot morning --max-files 2 --sample 3

    # Full slot (16 files Ã— 15 specialists = 240 tasks)
    PYTHONPATH=. python mekhane/symploke/jules_daily_scheduler.py --slot morning

Cron:
    # æ¨å¥¨: scripts/jules_basanos_cron.sh ã‚’ä½¿ç”¨ (æ›œæ—¥åˆ¥è‡ªå‹•åˆ‡æ›¿)
    0 6  * * * ~/oikos/hegemonikon/scripts/jules_basanos_cron.sh morning
    0 12 * * * ~/oikos/hegemonikon/scripts/jules_basanos_cron.sh midday
    0 18 * * * ~/oikos/hegemonikon/scripts/jules_basanos_cron.sh evening
"""

import argparse
import asyncio
import json
import os
import random
import subprocess
import sys
import time
from datetime import datetime
from typing import Optional
from pathlib import Path

# Project root
_PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(_PROJECT_ROOT))
sys.path.insert(0, str(Path(__file__).parent))

from specialist_v2 import (
    ALL_SPECIALISTS,
    generate_prompt,
    get_all_categories,
    get_specialists_by_category,
)
from specialist_bridge import get_unified_specialists
from basanos_bridge import BasanosBridge

# Optional: AIAuditor for pre-filtering
try:
    from mekhane.basanos.ai_auditor import AIAuditor, Severity as AuditSeverity
    HAS_AUDITOR = True
except ImportError:
    try:
        import sys as _sys
        _sys.path.insert(0, str(_PROJECT_ROOT / "mekhane" / "basanos"))
        from ai_auditor import AIAuditor, Severity as AuditSeverity
        HAS_AUDITOR = True
    except ImportError:
        HAS_AUDITOR = False

# === Settings ===
ACCOUNTS_FILE = _PROJECT_ROOT / "synergeia" / "jules_accounts.yaml"
USAGE_FILE = _PROJECT_ROOT / "synergeia" / "jules_usage.json"
ROTATION_STATE_FILE = _PROJECT_ROOT / "synergeia" / "jules_rotation_state.json"
LOG_DIR = _PROJECT_ROOT / "logs" / "specialist_daily"

# Default settings
DEFAULT_FILES_PER_SLOT = 16
DEFAULT_SPECIALISTS_PER_FILE = 15
DEFAULT_BASANOS_DOMAINS = 5  # basanos mode: domains per slot
MAX_ERROR_RATE = 0.20  # 20% ã‚¨ãƒ©ãƒ¼ã§ slot è‡ªå‹•åœæ­¢


# PURPOSE: å…¨ .py ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã€å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
def scan_all_py_files() -> list[dict]:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®å…¨ .py ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆåº¦ä»˜ãã§ãƒªã‚¹ãƒˆåŒ–ã€‚"""
    result = subprocess.run(
        ["find", str(_PROJECT_ROOT), "-name", "*.py",
         "-not", "-path", "*/__pycache__/*",
         "-not", "-path", "*/.venv/*",
         "-not", "-path", "*/_archive*/*",
         "-not", "-path", "*/node_modules/*"],
        capture_output=True, text=True, timeout=10,
    )
    all_files = [f.strip() for f in result.stdout.strip().split("\n") if f.strip()]

    # ç›¸å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
    files = []
    for f in all_files:
        try:
            rel = os.path.relpath(f, _PROJECT_ROOT)
        except ValueError:
            continue
        if rel.startswith("."):
            continue

        # å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        score = 1.0

        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½å„ªå…ˆ
        basename = os.path.basename(rel)
        if basename.startswith("test_") or basename == "conftest.py":
            score = 0.3
        elif basename == "__init__.py":
            score = 0.1

        # å¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ« = ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¾¡å€¤ãŒé«˜ã„
        try:
            size = os.path.getsize(f)
            if size > 5000:
                score *= min(3.0, size / 5000)
        except OSError:
            pass

        # kernel/ ã¯é«˜å„ªå…ˆ
        if "kernel/" in rel:
            score *= 2.0
        # mekhane/ ã¯é«˜å„ªå…ˆ
        elif "mekhane/" in rel:
            score *= 1.5

        files.append({"path": rel, "score": score, "size": os.path.getsize(f) if os.path.exists(f) else 0})

    return files


# PURPOSE: git diff ã§æœ€è¿‘å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
def get_recent_changes(days: int = 7) -> set[str]:
    """ç›´è¿‘ N æ—¥é–“ã®å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã€‚"""
    try:
        result = subprocess.run(
            ["git", "log", f"--since={days} days ago", "--name-only", "--pretty=format:", "--", "*.py"],
            capture_output=True, text=True, timeout=10,
            cwd=str(_PROJECT_ROOT),
        )
        return {f.strip() for f in result.stdout.strip().split("\n") if f.strip()}
    except Exception:
        return set()


# PURPOSE: ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’èª­è¾¼
def load_rotation_state() -> dict:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’èª­è¾¼ã€‚"""
    if ROTATION_STATE_FILE.exists():
        try:
            return json.loads(ROTATION_STATE_FILE.read_text())
        except (json.JSONDecodeError, OSError):
            pass
    return {"last_reviewed": {}, "cycle": 0}


# PURPOSE: ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ä¿å­˜
def save_rotation_state(state: dict) -> None:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ä¿å­˜ã€‚"""
    ROTATION_STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
    ROTATION_STATE_FILE.write_text(json.dumps(state, indent=2, ensure_ascii=False))


# PURPOSE: æ—¥æ¬¡ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ â€” å„ªå…ˆåº¦ + ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ + git diff
def select_daily_files(count: int, rotation_state: dict) -> list[str]:
    """æ—¥æ¬¡ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã€‚

    å„ªå…ˆåº¦:
      1. git diff ã§æœ€è¿‘å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« (2x boost)
      2. å¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ« (score by size)
      3. å‰å›ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰æœ€ã‚‚æ™‚é–“ãŒçµŒéã—ãŸãƒ•ã‚¡ã‚¤ãƒ«
    """
    all_files = scan_all_py_files()
    recent_changes = get_recent_changes(days=7)
    last_reviewed = rotation_state.get("last_reviewed", {})
    today = datetime.now().strftime("%Y-%m-%d")

    # ã‚¹ã‚³ã‚¢èª¿æ•´
    for f in all_files:
        path = f["path"]

        # æœ€è¿‘å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« â†’ ãƒ–ãƒ¼ã‚¹ãƒˆ
        if path in recent_changes:
            f["score"] *= 2.0

        # ä»Šæ—¥æ—¢ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼æ¸ˆã¿ â†’ é™¤å¤–
        if last_reviewed.get(path) == today:
            f["score"] = -1

        # é•·æœŸæœªãƒ¬ãƒ“ãƒ¥ãƒ¼ â†’ ãƒ–ãƒ¼ã‚¹ãƒˆ
        last_date = last_reviewed.get(path, "")
        if not last_date:
            f["score"] *= 1.5  # ä¸€åº¦ã‚‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã•ã‚Œã¦ã„ãªã„
        elif last_date < (datetime.now().strftime("%Y-%m-%d")):
            # å¤ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ (æœ€å¤§ 2x)
            pass  # åŸºæœ¬ã‚¹ã‚³ã‚¢ã®ã¾ã¾

    # ãƒ•ã‚£ãƒ«ã‚¿ & ã‚½ãƒ¼ãƒˆ
    candidates = [f for f in all_files if f["score"] > 0]
    candidates.sort(key=lambda f: f["score"], reverse=True)

    selected = [f["path"] for f in candidates[:count]]

    # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹æ›´æ–°
    for path in selected:
        last_reviewed[path] = today
    rotation_state["last_reviewed"] = last_reviewed
    rotation_state["cycle"] = rotation_state.get("cycle", 0) + 1

    return selected


# PURPOSE: å…¨ API ã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰åé›†
def collect_all_keys() -> list[str]:
    """å…¨ JULES_API_KEY_xx ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰åé›†ã€‚ã‚­ãƒ¼å€¤ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚"""
    keys = []
    for i in range(1, 30):  # æœ€å¤§ 30 ã‚­ãƒ¼
        key = os.getenv(f"JULES_API_KEY_{i:02d}")
        if key:
            keys.append(key)
    return keys


# PURPOSE: ä½¿ç”¨é‡ã‚’èª­è¾¼
def load_usage() -> dict:
    """æ—¥æ¬¡ä½¿ç”¨é‡ã‚’èª­è¾¼ã€‚æ—¥ä»˜ãŒå¤‰ã‚ã£ãŸã‚‰ãƒªã‚»ãƒƒãƒˆã€‚"""
    today = datetime.now().strftime("%Y-%m-%d")
    if USAGE_FILE.exists():
        try:
            data = json.loads(USAGE_FILE.read_text())
            if data.get("date") == today:
                return data
        except (json.JSONDecodeError, OSError):
            pass
    return {
        "date": today,
        "slots": {},
        "total_tasks": 0,
        "total_started": 0,
        "total_failed": 0,
        "files_reviewed": 0,
    }


# PURPOSE: ä½¿ç”¨é‡ã‚’ä¿å­˜
def save_usage(usage: dict) -> None:
    USAGE_FILE.write_text(json.dumps(usage, indent=2, ensure_ascii=False))


# PURPOSE: ãƒãƒƒãƒå®Ÿè¡Œ (run_specialists.py ã® run_batch ã‚’å‘¼å‡º)
async def run_slot_batch(
    files: list[str],
    specialists_per_file: int,
    api_keys: list[str],
    max_concurrent: int = 6,
    dry_run: bool = False,
    basanos_bridge: Optional["BasanosBridge"] = None,
    basanos_domains: Optional[list[str]] = None,
    hybrid_ratio: float = 0.0,
    audit_issue_codes: Optional[list[str]] = None,
    use_dynamic: bool = False,
    exclude_low_quality: bool = True,
) -> dict:
    """1 ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†ã®ãƒãƒƒãƒã‚’å®Ÿè¡Œã€‚

    basanos_bridge ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€Basanos ãƒ‘ãƒ¼ã‚¹ãƒšã‚¯ãƒ†ã‚£ãƒ–ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
    """
    import run_specialists as rs_short
    from run_specialists import create_session, run_batch, suggest_categories

    # API ã‚­ãƒ¼ã‚’ä¸€æ™‚å·®æ›¿ãˆ
    # NOTE: sys.path ã« mekhane/symploke ã‚’è¿½åŠ ã—ã¦ã„ã‚‹ãŸã‚ã€
    #   `run_specialists` ã¨ `mekhane.symploke.run_specialists` ã¯
    #   åˆ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ Python ã«ç™»éŒ²ã•ã‚Œã‚‹ã€‚
    #   run_batch ã¯ã‚·ãƒ§ãƒ¼ãƒˆãƒ‘ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã® API_KEYS ã‚’å‚ç…§ã™ã‚‹ãŸã‚ã€
    #   ã‚·ãƒ§ãƒ¼ãƒˆãƒ‘ã‚¹å´ã‚’å·®ã—æ›¿ãˆã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
    original_keys = rs_short.API_KEYS
    rs_short.API_KEYS = api_keys

    total_started = 0
    total_failed = 0
    all_results = []

    try:
        for file_idx, target_file in enumerate(files, 1):
            # å°‚é–€å®¶ãƒ—ãƒ¼ãƒ«é¸æŠ
            if basanos_bridge is not None and hybrid_ratio > 0 and hybrid_ratio < 1.0:
                # Hybrid mode: basanos + specialist ã‚’æ¯”ç‡ã§æ··åˆ
                basanos_specs = basanos_bridge.get_perspectives_as_specialists(
                    domains=basanos_domains,
                )
                basanos_count = max(1, int(specialists_per_file * hybrid_ratio))
                specialist_count = specialists_per_file - basanos_count
                # basanos specs ã‹ã‚‰ basanos_count å€‹ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                sampled_basanos = random.sample(
                    basanos_specs, min(basanos_count, len(basanos_specs)),
                )
                # specialist pool ã‹ã‚‰æ®‹ã‚Šã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                pool = list(ALL_SPECIALISTS)
                sampled_specialist = random.sample(
                    pool, min(specialist_count, len(pool)),
                )
                specs = sampled_basanos + sampled_specialist
                random.shuffle(specs)  # æ··åˆé †åºã‚’ãƒ©ãƒ³ãƒ€ãƒ åŒ–
            elif basanos_bridge is not None:
                # Basanos mode: æ§‹é€ åŒ–ãƒ‘ãƒ¼ã‚¹ãƒšã‚¯ãƒ†ã‚£ãƒ–ã‚’ä½¿ç”¨
                if use_dynamic:
                    # F10: ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹æ€§ã«åŸºã¥ãå‹•çš„ perspective
                    specs = basanos_bridge.get_dynamic_perspectives(
                        file_path=target_file,
                        audit_issues=audit_issue_codes,
                        max_perspectives=specialists_per_file,
                    )
                    if not specs:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é™çš„ perspective
                        specs = basanos_bridge.get_perspectives_as_specialists(
                            domains=basanos_domains,
                        )
                else:
                    specs = basanos_bridge.get_perspectives_as_specialists(
                        domains=basanos_domains,
                    )
            else:
                # Specialist mode: audit issue ãŒã‚ã‚Œã° adaptiveã€ãªã‘ã‚Œã°ãƒ©ãƒ³ãƒ€ãƒ 
                if audit_issue_codes:
                    from audit_specialist_matcher import AuditSpecialistMatcher
                    from specialist_v2 import get_specialists_by_category
                    matcher = AuditSpecialistMatcher()
                    categories = matcher.select_for_issues(
                        audit_issue_codes, total_budget=specialists_per_file,
                    )
                    specs = []
                    for cat in categories:
                        cat_pool = get_specialists_by_category(cat)
                        if cat_pool:
                            specs.append(random.choice(cat_pool))
                    # budget ã‚’æº€ãŸã•ãªã‘ã‚Œã°ãƒ©ãƒ³ãƒ€ãƒ ã§è£œå……
                    if len(specs) < specialists_per_file:
                        pool = [s for s in ALL_SPECIALISTS if s not in specs]
                        remaining = specialists_per_file - len(specs)
                        specs.extend(random.sample(pool, min(remaining, len(pool))))
                else:
                    pool = list(ALL_SPECIALISTS)
                    specs = random.sample(pool, min(specialists_per_file, len(pool)))

            # F14: ä½å“è³ª Perspective ã‚’å®Ÿè¡Œæ™‚é™¤å¤–
            if exclude_low_quality and specs:
                try:
                    from basanos_feedback import FeedbackStore as _FBStore
                    _excluded_ids = set(_FBStore().get_low_quality_perspectives(threshold=0.1))
                    if _excluded_ids:
                        before = len(specs)
                        specs = [s for s in specs if getattr(s, 'id', '') not in _excluded_ids]
                        culled = before - len(specs)
                        if culled > 0:
                            print(f"    ğŸ—‘ï¸  F14: {culled} low-quality perspectives excluded")
                except Exception:
                    pass  # FeedbackStore ä¸åœ¨æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—

            if dry_run:
                print(f"  [{file_idx}/{len(files)}] {target_file} Ã— {len(specs)} specialists (DRY-RUN)")
                all_results.append({
                    "file": target_file,
                    "specialists": len(specs),
                    "dry_run": True,
                })
                continue

            print(f"  [{file_idx}/{len(files)}] {target_file} Ã— {len(specs)} specialists")
            results = await run_batch(specs, target_file, max_concurrent)

            started = sum(1 for r in results if "session_id" in r)
            failed = sum(1 for r in results if "error" in r)
            total_started += started
            total_failed += failed

            # F9: session_id + perspective_id ã‚’ãƒ­ã‚°ä¿å­˜ (jules_result_parser é€£æº)
            sessions_info = []
            for i, r in enumerate(results):
                info = {}
                if "session_id" in r:
                    info["session_id"] = r["session_id"]
                if "error" in r:
                    info["error"] = str(r["error"])[:100]
                if i < len(specs):
                    info["specialist"] = specs[i].name
                    info["perspective_id"] = getattr(specs[i], "id", "")
                sessions_info.append(info)

            all_results.append({
                "file": target_file,
                "specialists": len(specs),
                "started": started,
                "failed": failed,
                "sessions": sessions_info,
            })

            # å®‰å…¨å¼: ã‚¨ãƒ©ãƒ¼ç‡ãƒã‚§ãƒƒã‚¯
            total_attempted = total_started + total_failed
            if total_attempted > 10:
                error_rate = total_failed / total_attempted
                if error_rate > MAX_ERROR_RATE:
                    print(f"  âš ï¸  Error rate {error_rate:.1%} > {MAX_ERROR_RATE:.0%}, stopping slot")
                    break

            print(f"    â†’ {started}/{len(specs)} started, {failed} failed")

    finally:
        # API ã‚­ãƒ¼å¾©å…ƒ
        rs_short.API_KEYS = original_keys

    return {
        "files": all_results,
        "total_started": total_started,
        "total_failed": total_failed,
        "total_tasks": total_started + total_failed,
    }


# PURPOSE: ãƒ¡ã‚¤ãƒ³
async def main():
    parser = argparse.ArgumentParser(description="Jules Daily Scheduler v2.0")
    parser.add_argument(
        "--slot", choices=["morning", "midday", "evening"], required=True,
        help="Time slot to execute",
    )
    parser.add_argument(
        "--mode", choices=["specialist", "basanos", "hybrid"], default="specialist",
        help="Review mode: specialist (random), basanos (structured), hybrid (mixed)",
    )
    parser.add_argument(
        "--max-files", type=int, default=None,
        help=f"Max total files for this slot (default: {DEFAULT_FILES_PER_SLOT})",
    )
    parser.add_argument(
        "--sample", "-s", type=int, default=None,
        help=f"Specialists per file (default: {DEFAULT_SPECIALISTS_PER_FILE})",
    )
    parser.add_argument(
        "--domains", type=int, default=DEFAULT_BASANOS_DOMAINS,
        help=f"Basanos mode: domains per slot (default: {DEFAULT_BASANOS_DOMAINS})",
    )
    parser.add_argument(
        "--max-concurrent", "-m", type=int, default=6,
        help="Max concurrent sessions (default: 6)",
    )
    parser.add_argument(
        "--basanos-ratio", type=float, default=0.6,
        help="Hybrid mode: ratio of basanos specs (default: 0.6 = 60%% basanos)",
    )
    parser.add_argument(
        "--pre-audit", action="store_true",
        help="Run AIAuditor pre-filter to prioritize files with critical issues",
    )
    parser.add_argument(
        "--dynamic", action="store_true",
        help="F10: Use dynamic perspectives based on file characteristics (basanos/hybrid mode)",
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        help="Print plan without executing",
    )

    args = parser.parse_args()

    total_files = args.max_files or DEFAULT_FILES_PER_SLOT
    specs_per_file = args.sample or DEFAULT_SPECIALISTS_PER_FILE
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")

    print(f"\n{'='*60}")
    print(f"Jules Daily Scheduler v2.0 â€” {args.slot} slot [{args.mode}]")
    print(f"{'='*60}")
    print(f"Time:     {timestamp}")

    # å…¨ã‚­ãƒ¼åé›† (æ¤œè¨¼ãªã— â€” EAFP: ä½¿ã£ã¦ã¿ã¦å£Šã‚ŒãŸã‚‰ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆ)
    all_keys = collect_all_keys()
    if not all_keys:
        print("ERROR: No API keys found. Check JULES_API_KEY_xx env vars.")
        return

    # --- Basanos mode: ãƒ‰ãƒ¡ã‚¤ãƒ³é¸æŠ & specialists per file ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ ---
    basanos_info = {}  # ãƒ­ã‚°ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    bridge: Optional[BasanosBridge] = None
    sampled_domains: Optional[list[str]] = None

    if args.mode in ("basanos", "hybrid"):
        bridge = BasanosBridge()
        sampled_domains = bridge.sample_domains(args.domains)

    if args.mode == "basanos":
        # Basanos ã§ã¯ specs_per_file = é¸æŠãƒ‰ãƒ¡ã‚¤ãƒ³æ•° Ã— 24è»¸ (å…¨ãƒ‘ãƒ¼ã‚¹ãƒšã‚¯ãƒ†ã‚£ãƒ–)
        specs_per_file = len(sampled_domains) * len(bridge.all_axes)
        basanos_info = {
            "domains": sampled_domains,
            "axes": len(bridge.all_axes),
            "perspectives_per_file": specs_per_file,
        }
        print(f"Mode:     basanos (structured orthogonal perspectives)")
        print(f"Domains:  {sampled_domains} ({len(sampled_domains)} selected)")
        print(f"Axes:     {len(bridge.all_axes)} (all theorems)")
        print(f"Specs:    {len(sampled_domains)} domains Ã— {len(bridge.all_axes)} axes = {specs_per_file}/file")
        if args.sample:
            print(f"  âš ï¸  --sample is ignored in basanos mode (using all {len(bridge.all_axes)} axes)")
    elif args.mode == "hybrid":
        # Hybrid: basanos specs + specialist specs ã‚’æ¯”ç‡ã§æ··åˆ
        ratio = args.basanos_ratio
        basanos_count = max(1, int(specs_per_file * ratio))
        specialist_count = specs_per_file - basanos_count
        basanos_info = {
            "domains": sampled_domains,
            "axes": len(bridge.all_axes),
            "basanos_count": basanos_count,
            "specialist_count": specialist_count,
            "ratio": ratio,
        }
        print(f"Mode:     hybrid ({ratio:.0%} basanos + {1-ratio:.0%} specialist)")
        print(f"Domains:  {sampled_domains} ({len(sampled_domains)} selected)")
        print(f"Specs:    {basanos_count} basanos + {specialist_count} specialist = {specs_per_file}/file")
    else:
        print(f"Mode:     specialist (random sampling from ~1000 pool)")

    total_tasks = total_files * specs_per_file

    print(f"Keys:     {len(all_keys)} loaded (EAFP: validated at runtime)")
    print(f"Files:    {total_files}")
    print(f"Specs:    {specs_per_file}/file")
    print(f"Tasks:    {total_tasks} (= {total_files} Ã— {specs_per_file})")
    print()

    # ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
    rotation_state = load_rotation_state()
    all_selected_files = select_daily_files(total_files, rotation_state)

    if not all_selected_files:
        print("ERROR: No target files found.")
        return

    print(f"Selected files: {len(all_selected_files)}")
    for i, f in enumerate(all_selected_files[:5], 1):
        print(f"  [{i}] {f}")
    if len(all_selected_files) > 5:
        print(f"  ... and {len(all_selected_files) - 5} more")

    # --- Pre-audit: AIAuditor ã§ãƒ•ã‚¡ã‚¤ãƒ«å„ªå…ˆåº¦ã‚’å†è¨ˆç®— ---
    audit_info = {}  # ãƒ­ã‚°ç”¨
    if args.pre_audit:
        if not HAS_AUDITOR:
            print("  âš ï¸  --pre-audit requested but AIAuditor not available, skipping")
        else:
            print("\nğŸ” Pre-audit: scanning files with AIAuditor...")
            auditor = AIAuditor(strict=False)
            file_scores: dict[str, int] = {}

            for fpath in all_selected_files:
                try:
                    # select_daily_files ã¯ç›¸å¯¾ãƒ‘ã‚¹ã‚’è¿”ã™ â†’ _PROJECT_ROOT ã§çµ¶å¯¾åŒ–
                    abs_path = _PROJECT_ROOT / fpath
                    result = auditor.audit_file(abs_path)
                    # Score: Critical=10, High=5, Medium=1, Low=0
                    score = sum(
                        10 if i.severity == AuditSeverity.CRITICAL
                        else 5 if i.severity == AuditSeverity.HIGH
                        else 1 if i.severity == AuditSeverity.MEDIUM
                        else 0
                        for i in result.issues
                    )
                    file_scores[fpath] = score
                    if score > 0:
                        crit = sum(1 for i in result.issues if i.severity == AuditSeverity.CRITICAL)
                        high = sum(1 for i in result.issues if i.severity == AuditSeverity.HIGH)
                        print(f"  {fpath}: score={score} (C:{crit} H:{high})")
                except Exception as e:
                    file_scores[fpath] = 0
                    print(f"  {fpath}: audit failed ({e})")

            # ã‚¹ã‚³ã‚¢é™é †ã§å†ã‚½ãƒ¼ãƒˆ (å•é¡Œã®å¤šã„ãƒ•ã‚¡ã‚¤ãƒ«ãŒå…ˆ)
            all_selected_files.sort(key=lambda f: file_scores.get(f, 0), reverse=True)

            total_issues = sum(file_scores.values())
            files_with_issues = sum(1 for s in file_scores.values() if s > 0)
            # F7: issue ã‚³ãƒ¼ãƒ‰ã‚’åé›† (audit_specialist_matcher é€£æºç”¨)
            all_issue_codes: list[str] = []
            for fpath in all_selected_files:
                try:
                    abs_path = _PROJECT_ROOT / fpath
                    re_result = auditor.audit_file(abs_path)
                    all_issue_codes.extend(i.code for i in re_result.issues)
                except Exception:
                    pass

            audit_info = {
                "total_score": total_issues,
                "files_with_issues": files_with_issues,
                "file_scores": {f: s for f, s in file_scores.items() if s > 0},
                "issue_codes": list(set(all_issue_codes)),
            }
            print(f"  â†’ {files_with_issues}/{len(all_selected_files)} files with issues, reordered by priority")
            if all_issue_codes:
                print(f"  â†’ {len(set(all_issue_codes))} unique issue codes collected for adaptive matching")

    print()

    # ä½¿ç”¨é‡èª­è¾¼
    usage = load_usage()

    # ãƒãƒƒãƒå®Ÿè¡Œ (EAFP: å…¨ã‚­ãƒ¼ã‚’æ¸¡ã—ã€run_batch å†…ã§å£Šã‚ŒãŸã‚­ãƒ¼ã‚’è‡ªå‹•é™¤å¤–)
    slot_result = {
        "total_keys": len(all_keys),
        "total_tasks": 0,
        "total_started": 0,
        "total_failed": 0,
        "files_reviewed": 0,
    }

    print(f"--- Batch ({len(all_keys)} keys, {len(all_selected_files)} files) ---")

    # F7: pre-audit ã® issue codes ã‚’ specialist é¸æŠã«æ¸¡ã™
    collected_issue_codes = audit_info.get("issue_codes", []) if audit_info else None

    result = await run_slot_batch(
        files=all_selected_files,
        specialists_per_file=specs_per_file,
        api_keys=all_keys,
        max_concurrent=args.max_concurrent,
        dry_run=args.dry_run,
        basanos_bridge=bridge if args.mode in ("basanos", "hybrid") else None,
        basanos_domains=sampled_domains if args.mode in ("basanos", "hybrid") else None,
        hybrid_ratio=args.basanos_ratio if args.mode == "hybrid" else 0.0,
        audit_issue_codes=collected_issue_codes,
        use_dynamic=args.dynamic,
    )

    slot_result["total_tasks"] = result["total_tasks"]
    slot_result["total_started"] = result["total_started"]
    slot_result["total_failed"] = result["total_failed"]
    slot_result["files_reviewed"] = len(all_selected_files)
    print()

    # ä½¿ç”¨é‡æ›´æ–°
    usage["slots"][args.slot] = slot_result
    usage["total_tasks"] += slot_result["total_tasks"]
    usage["total_started"] += slot_result["total_started"]
    usage["total_failed"] += slot_result["total_failed"]
    usage["files_reviewed"] += slot_result["files_reviewed"]

    if not args.dry_run:
        save_usage(usage)
        save_rotation_state(rotation_state)

    # ã‚µãƒãƒªãƒ¼
    total = slot_result["total_tasks"]
    started = slot_result["total_started"]
    rate = (started / total * 100) if total else 0

    print(f"{'='*60}")
    print(f"Slot Summary: {args.slot}")
    print(f"  Tasks:   {started}/{total} ({rate:.1f}%)")
    print(f"  Files:   {slot_result['files_reviewed']}")
    print(f"  Daily:   {usage['total_started']}/{usage['total_tasks']} total")
    print(f"{'='*60}")

    # ãƒ­ã‚°ä¿å­˜
    if not args.dry_run:
        log_file = LOG_DIR / f"scheduler_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
        log_file.parent.mkdir(parents=True, exist_ok=True)
        log_data = {
            "slot": args.slot,
            "mode": args.mode,
            "timestamp": timestamp,
            # F11 Dashboard API ç”¨ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã‚­ãƒ¼
            "total_tasks": slot_result.get("total_tasks", 0),
            "total_started": slot_result.get("total_started", 0),
            "total_failed": slot_result.get("total_failed", 0),
            "files_reviewed": len(all_selected_files),
            "dynamic": getattr(args, "dynamic", False),
            # å¾Œæ–¹äº’æ›: è©³ç´°ãƒ‡ãƒ¼ã‚¿
            "result": slot_result,
            "daily_usage": usage,
        }
        if basanos_info:
            log_data["basanos"] = basanos_info
        if audit_info:
            log_data["pre_audit"] = audit_info
        log_file.write_text(json.dumps(log_data, indent=2, ensure_ascii=False))
        print(f"  Log: {log_file}")

        # F21: è‡ªå‹•ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›† (collect_and_update)
        try:
            from jules_result_parser import collect_and_update
            feedback_result = collect_and_update(days=1)
            if feedback_result:
                updated = feedback_result.get("updated", 0)
                if updated > 0:
                    print(f"  ğŸ“Š Feedback: {updated} perspectives updated")
        except Exception as fb_exc:
            print(f"  âš ï¸  Feedback collection failed: {fb_exc}")


if __name__ == "__main__":
    asyncio.run(main())
