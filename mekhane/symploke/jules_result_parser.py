#!/usr/bin/env python3
# PROOF: [L2/Mekhane] <- mekhane/symploke/ A0->Auto->AddedByCI
# PROOF: [L2/åˆ†æ] <- mekhane/symploke/ F9â†’é–‰ãƒ«ãƒ¼ãƒ—â†’jules_result_parser ãŒæ‹…ã†
# PURPOSE: Jules ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‹ã‚‰ Perspective æœ‰ç”¨æ€§ã‚’åˆ¤å®š
"""
Jules Result Parser

Jules ã® PR/ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’è§£æã—ã€å„ Perspective ã®æœ‰ç”¨æ€§ã‚’åˆ¤å®šã™ã‚‹ã€‚
basanos_feedback.FeedbackStore ã¨é€£æºã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã‚’é–‰ã˜ã‚‹ã€‚

Integration:
    - basanos_feedback.py: FeedbackStore.record_usage(was_useful=True/False) ã«æ¥ç¶š
    - jules_daily_scheduler.py: scheduler ãƒ­ã‚°ã® session_id æ‹¡å¼µã§ç´ä»˜ã‘
    - collect_and_update() ã‚’ cron ã‹ã‚‰å®šæœŸå®Ÿè¡Œ

Usage:
    # CLI: éå»7æ—¥åˆ†ã® Jules çµæœã‹ã‚‰æœ‰ç”¨æ€§ã‚’åˆ¤å®š
    python jules_result_parser.py analyze --days 7

    # ãƒ—ãƒ­ã‚°ãƒ©ãƒ : å˜ä¸€ã‚»ãƒƒã‚·ãƒ§ãƒ³è§£æ
    from jules_result_parser import JulesResultParser
    parser = JulesResultParser()
    result = parser.analyze_session("session-id-123")
"""

import argparse
import json
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

_THIS_DIR = Path(__file__).resolve().parent
_PROJECT_ROOT = _THIS_DIR.parent.parent
_SCHEDULER_LOG_DIR = _PROJECT_ROOT / "logs" / "specialist_daily"


# PURPOSE: ã‚»ãƒƒã‚·ãƒ§ãƒ³è§£æçµæœ
@dataclass
class SessionAnalysis:
    """å˜ä¸€ Jules ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è§£æçµæœã€‚"""
    session_id: str
    file_path: str
    specialist_name: str = ""
    perspective_id: str = ""  # BP-{domain}-{axis}
    status: str = "unknown"   # pending, completed, failed
    has_pr: bool = False
    pr_merged: bool = False
    pr_comments: int = 0
    was_useful: bool = False
    confidence: float = 0.0   # åˆ¤å®šã®ç¢ºä¿¡åº¦

    def to_dict(self) -> dict:
        return {
            "session_id": self.session_id,
            "file_path": self.file_path,
            "specialist_name": self.specialist_name,
            "perspective_id": self.perspective_id,
            "status": self.status,
            "has_pr": self.has_pr,
            "pr_merged": self.pr_merged,
            "was_useful": self.was_useful,
            "confidence": self.confidence,
        }


# PURPOSE: æœ‰ç”¨æ€§åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
class UsefulnessJudge:
    """PR/ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‹ã‚‰æœ‰ç”¨æ€§ã‚’åˆ¤å®šã™ã‚‹ãƒ«ãƒ¼ãƒ«é›†ã€‚

    åˆ¤å®šåŸºæº– (å„ªå…ˆé †):
        1. PR merged â†’ useful (confidence: 0.9)
        2. PR with comments > 0 â†’ useful (confidence: 0.7)
        3. PR closed without merge â†’ not useful (confidence: 0.6)
        4. Session completed (no PR) â†’ neutral (confidence: 0.3)
        5. Session failed â†’ not useful (confidence: 0.8)
    """

    @staticmethod
    def judge(analysis: SessionAnalysis) -> tuple[bool, float]:
        """æœ‰ç”¨æ€§ã¨ç¢ºä¿¡åº¦ã‚’è¿”ã™ã€‚"""
        if analysis.status == "failed":
            return False, 0.8

        if analysis.has_pr:
            if analysis.pr_merged:
                return True, 0.9
            if analysis.pr_comments > 0:
                return True, 0.7
            # PR ã‚ã‚‹ãŒ merge ã‚‚ comment ã‚‚ãªã— â†’ pending or closed
            return False, 0.6

        if analysis.status == "completed":
            # PR ãªã—ã ãŒå®Œäº† â†’ åˆ¤æ–­å›°é›£
            return False, 0.3

        return False, 0.2


# PURPOSE: ãƒ¡ã‚¤ãƒ³ãƒ‘ãƒ¼ã‚µãƒ¼
class JulesResultParser:
    """Jules ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’è§£æã™ã‚‹ãƒ‘ãƒ¼ã‚µãƒ¼ã€‚"""

    def __init__(self, log_dir: Optional[Path] = None):
        self._log_dir = log_dir or _SCHEDULER_LOG_DIR
        self._judge = UsefulnessJudge()

    def analyze_session(self, session_id: str) -> SessionAnalysis:
        """å˜ä¸€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®çµæœã‚’è§£æã™ã‚‹ã€‚

        NOTE: API ãƒ™ãƒ¼ã‚¹ã®å®Ÿè£… (MCP jules_get_status) ã¯å¤–éƒ¨å‘¼ã³å‡ºã—ãŒå¿…è¦ã€‚
        ã“ã“ã§ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ­ã‚°ãƒ™ãƒ¼ã‚¹ã®è§£æã‚’è¡Œã„ã€API è§£æã¯å‘¼ã³å‡ºã—å´ã«å§”è­²ã€‚
        """
        analysis = SessionAnalysis(session_id=session_id, file_path="")

        # æ‹¡å¼µãƒ­ã‚° (session_id å…¥ã‚Š) ã‚’æ¢ã™
        for log_file in sorted(self._log_dir.glob("scheduler_*.json"), reverse=True):
            try:
                data = json.loads(log_file.read_text())
                files = data.get("result", {}).get("files", [])
                if not isinstance(files, list):
                    files = data.get("files", [])

                for f in files:
                    sessions = f.get("sessions", [])
                    for s in sessions:
                        if s.get("session_id") == session_id:
                            analysis.file_path = f.get("file", "")
                            analysis.specialist_name = s.get("specialist", "")
                            analysis.perspective_id = s.get("perspective_id", "")
                            if "error" in s:
                                analysis.status = "failed"
                            else:
                                analysis.status = "completed"
                            break
            except (json.JSONDecodeError, KeyError):
                continue

        # æœ‰ç”¨æ€§åˆ¤å®š
        useful, conf = self._judge.judge(analysis)
        analysis.was_useful = useful
        analysis.confidence = conf

        return analysis

    def analyze_log_file(self, log_path: Path) -> list[SessionAnalysis]:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è§£æã™ã‚‹ã€‚"""
        results = []

        try:
            data = json.loads(log_path.read_text())
        except (json.JSONDecodeError, FileNotFoundError):
            return results

        files = data.get("files", [])
        if not isinstance(files, list):
            return results

        for f in files:
            sessions = f.get("sessions", [])
            file_path = f.get("file", "")
            for s in sessions:
                sid = s.get("session_id", "")
                if not sid:
                    continue

                analysis = SessionAnalysis(
                    session_id=sid,
                    file_path=file_path,
                    specialist_name=s.get("specialist", ""),
                    perspective_id=s.get("perspective_id", ""),
                    status="failed" if "error" in s else "completed",
                )

                useful, conf = self._judge.judge(analysis)
                analysis.was_useful = useful
                analysis.confidence = conf
                results.append(analysis)

        return results


# PURPOSE: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—çµ±åˆ
def collect_and_update(days: int = 7) -> dict:
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒ­ã‚°ã®æ‹¡å¼µç‰ˆã‹ã‚‰ Jules çµæœã‚’åé›†ã—ã€FeedbackStore ã«åæ˜ ã€‚

    collect_from_logs (basanos_feedback.py) ã® Phase 2 å®Ÿè£…ã€‚
    """
    # Lazy import for basanos_feedback
    import sys
    if str(_THIS_DIR) not in sys.path:
        sys.path.insert(0, str(_THIS_DIR))
    from basanos_feedback import FeedbackStore

    store = FeedbackStore()
    parser = JulesResultParser()

    cutoff = datetime.now() - timedelta(days=days)
    processed = 0
    sessions_analyzed = 0
    useful_count = 0

    for log_file in sorted(_SCHEDULER_LOG_DIR.glob("scheduler_*.json")):
        try:
            data = json.loads(log_file.read_text())
            ts_str = data.get("timestamp", "")
            if not ts_str:
                continue
            ts = datetime.strptime(ts_str, "%Y-%m-%d %H:%M")
            if ts < cutoff:
                continue
        except (json.JSONDecodeError, ValueError):
            continue

        analyses = parser.analyze_log_file(log_file)
        for a in analyses:
            if a.perspective_id:
                # domain ã¨ axis ã‚’ perspective_id ã‹ã‚‰æŠ½å‡º (BP-{domain}-{axis})
                parts = a.perspective_id.split("-", 2)
                domain = parts[1] if len(parts) > 1 else ""
                axis = parts[2] if len(parts) > 2 else ""

                store.record_usage(
                    perspective_id=a.perspective_id,
                    domain=domain,
                    axis=axis,
                    was_useful=a.was_useful,
                )
                sessions_analyzed += 1
                if a.was_useful:
                    useful_count += 1

        processed += 1

    store.save()
    return {
        "processed_logs": processed,
        "sessions_analyzed": sessions_analyzed,
        "useful": useful_count,
        "not_useful": sessions_analyzed - useful_count,
    }


# PURPOSE: CLI
def main():
    parser = argparse.ArgumentParser(description="Jules Result Parser â€” F9 Feedback Loop")
    sub = parser.add_subparsers(dest="command")

    analyze_parser = sub.add_parser("analyze", help="Analyze Jules results and update feedback")
    analyze_parser.add_argument("--days", type=int, default=7, help="Days to look back")

    sub.add_parser("show", help="Show recent analysis results")

    args = parser.parse_args()

    if args.command == "analyze":
        result = collect_and_update(args.days)
        print(f"ğŸ“Š Jules Result Analysis:")
        print(f"  Logs processed: {result['processed_logs']}")
        print(f"  Sessions analyzed: {result['sessions_analyzed']}")
        print(f"  Useful: {result['useful']}")
        print(f"  Not useful: {result['not_useful']}")
    elif args.command == "show":
        p = JulesResultParser()
        for log_file in sorted(_SCHEDULER_LOG_DIR.glob("scheduler_*.json"))[-3:]:
            print(f"\nğŸ“„ {log_file.name}:")
            analyses = p.analyze_log_file(log_file)
            for a in analyses:
                emoji = "âœ…" if a.was_useful else "âŒ"
                print(f"  {emoji} {a.session_id[:16]}... â†’ {a.file_path} ({a.status}, conf={a.confidence:.1f})")
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
