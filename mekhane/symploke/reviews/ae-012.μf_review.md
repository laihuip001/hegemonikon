# 視覚リズムの指揮者 レビュー

## 対象ファイル
`mekhane/anamnesis/index.py`

## 判定
発言（要改善）

## 発見事項
- **視覚的重心の偏り (Low)**: `Embedder.__init__` (L75-134) において、GPU検出ロジック (`if not force_cpu` -> `try` -> `if torch.cuda.is_available` -> `try`) が深くネストしており、コード全体の重心が右側に偏っています。これは視覚的なリズムを損ない、可読性を低下させています。

## 修正提案
`Embedder.__init__` 内の GPU/CPU 初期化ロジックをヘルパーメソッドに抽出することで、インデントを浅く保つことを推奨します。

```python
    def __init__(self, force_cpu: bool = False, model_name: str = "BAAI/bge-m3"):
        if self._initialized:
            return
        self._initialized = True

        import numpy as np
        self.np = np
        self._use_gpu = False
        self._st_model = None
        self._ort_session = None
        self._tokenizer = None
        self.model_name = model_name
        self._dimension: int = self._MODEL_DIMENSIONS.get(model_name, 0)
        self._is_onnx_fallback = False

        if not force_cpu and self._try_init_gpu():
            return

        if self._try_init_cpu_st():
            return

        self._init_onnx_fallback()

    def _try_init_gpu(self) -> bool:
        """Attempt to initialize GPU model (returns True if successful)."""
        try:
            import torch
            if not torch.cuda.is_available():
                return False

            from sentence_transformers import SentenceTransformer
            try:
                self._st_model = SentenceTransformer(
                    self.model_name, device='cuda',
                    model_kwargs={'torch_dtype': torch.float16},
                )
                self._use_gpu = True
                self._dimension = self._st_model.get_sentence_embedding_dimension()
                vram_mb = torch.cuda.memory_allocated() / 1e6
                print(f"[Embedder] GPU mode (CUDA fp16, {vram_mb:.0f}MB VRAM, dim={self._dimension})")
                return True
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print(f"[Embedder] CUDA OOM, falling back to CPU")
                    torch.cuda.empty_cache()
                else:
                    raise
        except ImportError:
            pass
        return False

    def _try_init_cpu_st(self) -> bool:
        """Attempt to initialize CPU sentence-transformers model."""
        try:
            from sentence_transformers import SentenceTransformer
            self._st_model = SentenceTransformer(self.model_name, device='cpu')
            self._use_gpu = False
            self._dimension = self._st_model.get_sentence_embedding_dimension()
            print(f"[Embedder] CPU mode (sentence-transformers: {self.model_name}, dim={self._dimension})")
            return True
        except ImportError:
            return False

    def _init_onnx_fallback(self):
        """Initialize ONNX fallback."""
        self._init_onnx()
        self._is_onnx_fallback = True
        self._dimension = 384
        print(f"[Embedder] CPU mode (ONNX bge-small, dim=384) ⚠️ 次元が bge-m3 (1024) と異なります")
```

## 重大度
Low
