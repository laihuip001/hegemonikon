# ã‚·ãƒ³ãƒ—ãƒªã‚·ãƒ†ã‚£ã®é–€ç•ª ãƒ¬ãƒ“ãƒ¥ãƒ¼

## å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
`mekhane/mcp/hgk_gateway.py`

## åˆ¤å®š
ç™ºè¨€ï¼ˆè¦æ”¹å–„ï¼‰

## ç™ºè¦‹äº‹é …

1.  **(Unused import / Redundant) é‡è¤‡ã‚¤ãƒ³ãƒãƒ¼ãƒˆ** (Low)
    - `_wbc_log_security_event` å†…ã® `import json` ã¯ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã§æ—¢ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãŸã‚ä¸è¦ã§ã™ã€‚
    - `_wbc_log_security_event` å†…ã® `from datetime import datetime` ã¯ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã§æ—¢ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãŸã‚ä¸è¦ã§ã™ã€‚
    - `_trace_tool_call` å†…ã® `from datetime import timezone` ã¯ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ã¾ã¨ã‚ã¦ `from datetime import datetime, timezone` ã¨ã™ã¹ãã§ã™ã€‚

2.  **(Unnecessary Nesting) `hgk_search` ã®æ·±ã„ãƒã‚¹ãƒˆ** (Medium)
    - `hgk_search` é–¢æ•°å†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆKI, Doxa, Handoffï¼‰ãŒæ·±ããƒã‚¹ãƒˆã•ã‚Œã¦ãŠã‚Šï¼ˆæœ€å¤§7éšå±¤ï¼‰ã€å¯èª­æ€§ã‚’æãªã£ã¦ã„ã¾ã™ã€‚ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã¸ã®æŠ½å‡ºã¾ãŸã¯ã‚¬ãƒ¼ãƒ‰ç¯€ï¼ˆearly returnï¼‰ã§å¹³å¦åŒ–ã™ã¹ãã§ã™ã€‚

3.  **(Unnecessary Complexity) `hgk_digest_check` ã®æ‰‹å‹•ãƒ‘ãƒ¼ã‚¹** (Low)
    - `hgk_digest_check` ã§ Frontmatter ã‚’æ‰‹å‹•ã§ãƒ‘ãƒ¼ã‚¹ã—ã¦ã„ã¾ã™ã€‚`hgk_digest_topics` ç­‰ã§ `import yaml` ã—ã¦ã„ã‚‹ãŸã‚ã€YAML ãƒ‘ãƒ¼ã‚µã‚’åˆ©ç”¨ã™ã‚‹ã‹ã€ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç°¡ç´ åŒ–ã™ã¹ãã§ã™ã€‚ã¾ãŸã€ã“ã®ãƒ‘ãƒ¼ã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã‚‚ãƒã‚¹ãƒˆãŒæ·±ã„ã§ã™ã€‚

4.  **(Unnecessary Complexity) PKS ã‚¢ã‚¯ã‚»ã‚¹æ‰‹æ®µã®ä¸çµ±ä¸€** (Medium)
    - `hgk_proactive_push` ã¯ `PKSEngine` ã‚’ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦åˆ©ç”¨ã™ã‚‹ä¸€æ–¹ã€`hgk_pks_search` ç­‰ã¯ `subprocess` çµŒç”±ã§ CLI ã‚’å‘¼ã³å‡ºã—ã¦ã„ã¾ã™ã€‚åŒä¸€ãƒ—ãƒ­ã‚»ã‚¹å†…ã§ `PKSEngine` ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹ãªã‚‰ã€CLI å‘¼ã³å‡ºã—ã¯ä¸è¦ãªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¨è¤‡é›‘ã•ï¼ˆãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†ã€ç’°å¢ƒå¤‰æ•°è¨­å®šã®é‡è¤‡ï¼‰ã‚’ç”Ÿã‚“ã§ã„ã¾ã™ã€‚çµ±ä¸€ã™ã¹ãã§ã™ã€‚

## ä¿®æ­£æ¡ˆ

### 1. ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ•´ç†

```python
<<<<<<< SEARCH
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
=======
import json
import os
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
>>>>>>> REPLACE
```

### 2. `hgk_search` ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚° (ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°æŠ½å‡º)

```python
def _search_ki(query: str, results: list[str]) -> None:
    """KI (Knowledge Items) ã‚’æ¤œç´¢ã—ã¦çµæœã«è¿½åŠ ã™ã‚‹"""
    ki_base = Path.home() / ".gemini" / "antigravity" / "knowledge"
    if not ki_base.exists():
        return

    query_lower = query.lower()
    for ki_dir in ki_base.iterdir():
        if not ki_dir.is_dir():
            continue

        metadata_path = ki_dir / "metadata.json"
        if not metadata_path.exists():
            continue

        try:
            meta = json.loads(metadata_path.read_text(encoding="utf-8"))
            summary = meta.get("summary", "")
            title = meta.get("title", ki_dir.name)
            if query_lower in title.lower() or query_lower in summary.lower():
                results.append(f"ğŸ“š **KI: {title}**\n   {summary[:150]}...")
        except Exception:
            pass

def _search_doxa(query: str, results: list[str]) -> None:
    """Doxa (ä¿¡å¿µ) ã‚’æ¤œç´¢ã—ã¦çµæœã«è¿½åŠ ã™ã‚‹"""
    if not DOXA_DIR.exists():
        return

    query_lower = query.lower()
    for doxa_file in sorted(DOXA_DIR.glob("*.json")):
        try:
            doxa = json.loads(doxa_file.read_text(encoding="utf-8"))
            content = json.dumps(doxa, ensure_ascii=False)
            if query_lower in content.lower():
                results.append(f"ğŸ’¡ **Doxa: {doxa_file.stem}**\n   {content[:150]}...")
        except Exception:
            pass

def _search_handoff(query: str, results: list[str]) -> None:
    """Handoff (ç›´è¿‘3ä»¶) ã‚’æ¤œç´¢ã—ã¦çµæœã«è¿½åŠ ã™ã‚‹"""
    if not SESSIONS_DIR.exists():
        return

    query_lower = query.lower()
    handoffs = sorted(SESSIONS_DIR.glob("handoff_*.md"), reverse=True)[:3]
    for hf in handoffs:
        try:
            content = hf.read_text(encoding="utf-8")
            if query_lower in content.lower():
                lines = content.split("\n")
                matches = [l.strip() for l in lines if query_lower in l.lower()][:3]
                match_text = " / ".join(matches) if matches else "(ãƒãƒƒãƒç®‡æ‰€çœç•¥)"
                results.append(f"ğŸ“‹ **Handoff: {hf.stem}**\n   {match_text[:150]}")
        except Exception:
            pass

# ... inside hgk_search ...
    if mode in ("hybrid", "keyword"):
        _search_ki(query, results)
        _search_doxa(query, results)
        _search_handoff(query, results)
```

### 3. `hgk_digest_check` ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°

```python
def _parse_digestor_frontmatter(content: str) -> dict:
    """Digestor ãƒ•ã‚¡ã‚¤ãƒ«ã®ç°¡æ˜“ Frontmatter ãƒ‘ãƒ¼ã‚¹"""
    meta = {"title": "(ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜)", "score": "", "topics": ""}
    lines = content.split("\n")
    if not lines or lines[0].strip() != "---":
        return meta

    for line in lines[1:]:
        line = line.strip()
        if line == "---":
            break
        if line.startswith("title:"):
            meta["title"] = line.split(":", 1)[1].strip().strip("\"'")
        elif line.startswith("score:"):
            meta["score"] = line.split(":", 1)[1].strip()
        elif line.startswith("topics:"):
            meta["topics"] = line.split(":", 1)[1].strip()
    return meta

# ... inside hgk_digest_check loop ...
    try:
        content = f.read_text(encoding="utf-8")
        meta = _parse_digestor_frontmatter(content)

        lines.append(f"### {i}. {meta['title']}")
        if meta['score']:
            lines.append(f"- **Score**: {meta['score']}")
        if meta['topics']:
            lines.append(f"- **Topics**: {meta['topics']}")
        lines.append(f"- **File**: `{f.name}`\n")
    except Exception as e:
        # ...
```

## é‡å¤§åº¦
Medium
