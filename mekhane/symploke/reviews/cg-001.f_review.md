# ネスト深度警報者 レビュー

## 対象ファイル
`mekhane/anamnesis/index.py`

## 判定
発言（要改善）

## 発見事項
- `Embedder.__init__` メソッド内での `try-except` ネスト (Medium)
  - `if not force_cpu:` (1)
  - `try:` (2)
  - `if torch.cuda.is_available():` (3)
  - `try:` (4) -> `except` -> `if "out of memory" ...:` (5)
  - `try-except` 内のネストが3段以上 (Medium)
- `GnosisIndex.add_papers` メソッド内でのリスト内包表記ネスト (Medium)
  - `filtered_data = [{k: v for ...} for record in data]`
  - 内包表記が2段以上 (Medium)

## 重大度
Medium

## 修正案

### 1. `Embedder.__init__` の改善案
GPU初期化ロジックを独立したメソッド `_init_gpu` に抽出することで、ネストを浅く保ちます。

```python
    def __init__(self, force_cpu: bool = False, model_name: str = "BAAI/bge-m3"):
        if self._initialized:
            return
        self._initialized = True

        import numpy as np
        self.np = np
        self._use_gpu = False
        self._st_model = None
        self._ort_session = None
        self._tokenizer = None
        self.model_name = model_name
        self._dimension: int = self._MODEL_DIMENSIONS.get(model_name, 0)
        self._is_onnx_fallback = False

        # GPU initialization attempt
        if not force_cpu and self._try_init_gpu(model_name):
            return

        # CPU with sentence-transformers
        try:
            from sentence_transformers import SentenceTransformer
            self._st_model = SentenceTransformer(model_name, device='cpu')
            self._use_gpu = False
            self._dimension = self._st_model.get_sentence_embedding_dimension()
            print(f"[Embedder] CPU mode (sentence-transformers: {model_name}, dim={self._dimension})")
            return
        except ImportError:
            pass

        # ONNX fallback (original implementation — BGE-small, 384d)
        self._init_onnx()
        self._is_onnx_fallback = True
        self._dimension = 384  # BGE-small ONNX model
        print(f"[Embedder] CPU mode (ONNX bge-small, dim=384) ⚠️ 次元が bge-m3 (1024) と異なります")

    def _try_init_gpu(self, model_name: str) -> bool:
        try:
            import torch
            if not torch.cuda.is_available():
                return False

            from sentence_transformers import SentenceTransformer
            try:
                self._st_model = SentenceTransformer(
                    model_name, device='cuda',
                    model_kwargs={'torch_dtype': torch.float16},
                )
                self._use_gpu = True
                self._dimension = self._st_model.get_sentence_embedding_dimension()
                vram_mb = torch.cuda.memory_allocated() / 1e6
                print(f"[Embedder] GPU mode (CUDA fp16, {vram_mb:.0f}MB VRAM, dim={self._dimension})")
                return True
            except RuntimeError as e:
                # CUDA OOM — fall through to CPU
                if "out of memory" in str(e).lower():
                    print(f"[Embedder] CUDA OOM, falling back to CPU")
                    torch.cuda.empty_cache()
                else:
                    raise
        except ImportError:
            pass  # torch or sentence-transformers not installed
        return False
```

### 2. `GnosisIndex.add_papers` の改善案
ネストされた内包表記を展開して可読性を向上させます。

```python
        # LanceDBに追加 (スキーマフィルタリング付き)
        if self._table_exists():
            table = self.db.open_table(self.TABLE_NAME)
            # P4: テーブルスキーマに合わせてフィールドをフィルタ
            schema_fields = {f.name for f in table.schema}

            filtered_data = []
            for record in data:
                filtered_record = {k: v for k, v in record.items() if k in schema_fields}
                filtered_data.append(filtered_record)

            table.add(filtered_data)
        else:
            self.db.create_table(self.TABLE_NAME, data=data)
```
