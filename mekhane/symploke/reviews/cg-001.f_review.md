# ネスト深度警報者 レビュー

## 対象ファイル
`mekhane/mcp/sophia_mcp_server.py`

## 判定
発言（要改善）

## 発見事項
- **High Severity (4+ levels)**:
    - `call_tool` -> `if name == "search"` -> `try` -> `if source in ...` -> `for r in sophia_results` (Level 4)
    - `call_tool` -> `if name == "search"` -> `try` -> `if source in ...` -> `for r in kairos_results` (Level 4)
    - `call_tool` -> `if name == "search"` -> `try` -> `if source in ...` -> `for r in kairos_results` -> `if recent_days` (Level 5)
    - `call_tool` -> `if name == "search"` -> `try` -> `if source in ...` -> `for r in kairos_results` -> `if recent_days` -> `if ts` (Level 6)
    - `call_tool` -> `if name == "search"` -> `try` -> `if source in ...` -> `for r in kairos_results` -> `if recent_days` -> `if ts` -> `try` (Level 7)
    - `call_tool` -> `if name == "search"` -> `try` -> `if source in ...` -> `for r in kairos_results` -> `if recent_days` -> `if ts` -> `try` -> `if (now - doc_date).days` (Level 8)
    - `call_tool` -> `if name == "search"` -> `try` -> `for i, r` -> `if r["source"]` (Level 4)
    - `call_tool` -> `if name == "backlinks"` -> `try` -> `if backlinks` -> `for link` (Level 4)
    - `call_tool` -> `if name == "backlinks"` -> `try` -> `if outlinks` -> `for link` (Level 4)
    - `call_tool` -> `if name == "graph_stats"` -> `try` -> `if stats["most_linked"]` -> `for name, count` (Level 4)
    - `call_tool` -> `if name == "graph_stats"` -> `try` -> `if stats["most_linked"]` -> `for name, count` -> `if count > 0` (Level 5)
- **Medium Severity (try-except 3+ levels)**:
    - `call_tool` functions for `search`, `stats`, `backlinks`, `graph_stats` all contain logic nested 3+ levels deep inside `try` blocks (Level 2).

## 重大度
High

## 修正案 (Correction Proposal)

以下のように `call_tool` 内の各ツール処理をヘルパー関数に分割し、ネストを解消することを提案します。

```python
async def _handle_search(arguments: dict) -> list[TextContent]:
    query = arguments.get("query", "")
    source = arguments.get("source", "both")
    limit = arguments.get("limit", 5)
    recent_days = arguments.get("recent_days")

    if not query:
        return [TextContent(type="text", text="Error: query is required")]

    try:
        with StdoutSuppressor():
            from mekhane.symploke.adapters.embedding_adapter import EmbeddingAdapter

        log(f"Searching for: {query}")
        results = []

        if source in ("sophia", "both") and SOPHIA_INDEX.exists():
            results.extend(_search_sophia(query, limit))

        if source in ("kairos", "both") and KAIROS_INDEX.exists():
            results.extend(_search_kairos(query, limit, recent_days))

        results.sort(key=lambda x: x["score"], reverse=True)
        results = results[: limit * 2]

        if not results:
            return [TextContent(type="text", text=f"No results found for: {query}")]

        return [TextContent(type="text", text=_format_search_results(results, query))]

    except Exception as e:
        log(f"Search error: {e}")
        return [TextContent(type="text", text=f"Error searching: {str(e)}")]


def _search_sophia(query: str, limit: int) -> list[dict]:
    from mekhane.symploke.adapters.embedding_adapter import EmbeddingAdapter
    adapter = EmbeddingAdapter()
    adapter.load(str(SOPHIA_INDEX))
    query_vec = adapter.encode([query])[0]
    sophia_results = adapter.search(query_vec, k=limit)

    return [
        {
            "source": "sophia",
            "score": r.score,
            "ki_name": r.metadata.get("ki_name", "N/A"),
            "artifact": r.metadata.get("artifact", ""),
            "summary": r.metadata.get("summary", "")[:150],
            "file_path": r.metadata.get("file_path", ""),
        }
        for r in sophia_results
    ]


def _search_kairos(query: str, limit: int, recent_days: int | None) -> list[dict]:
    from mekhane.symploke.adapters.embedding_adapter import EmbeddingAdapter
    from datetime import datetime

    adapter = EmbeddingAdapter()
    adapter.load(str(KAIROS_INDEX))
    query_vec = adapter.encode([query])[0]
    kairos_results = adapter.search(query_vec, k=limit)

    results = []
    now = datetime.now()

    for r in kairos_results:
        if recent_days and not _is_recent(r.metadata.get("timestamp", ""), now, recent_days):
            continue

        results.append({
            "source": "kairos",
            "score": r.score,
            "task": r.metadata.get("primary_task", "N/A"),
            "timestamp": r.metadata.get("timestamp", ""),
            "file_path": r.metadata.get("file_path", ""),
        })
    return results


def _is_recent(timestamp_str: str, now: datetime, days: int) -> bool:
    if not timestamp_str:
        return False
    try:
        doc_date = datetime.fromisoformat(timestamp_str.split("T")[0])
        return (now - doc_date).days <= days
    except Exception:
        return False


def _format_search_results(results: list[dict], query: str) -> str:
    output_lines = [f'# Sophia Search: "{query}"\n']
    output_lines.append(f"Found {len(results)} results:\n")

    for i, r in enumerate(results, 1):
        if r["source"] == "sophia":
            output_lines.append(f"## [{i}] [Sophia] {r['ki_name']}")
            output_lines.append(f"- **Artifact**: {r['artifact']}")
            output_lines.append(f"- **Summary**: {r['summary']}...")
            output_lines.append(f"- **Score**: {r['score']:.3f}")
        else:
            output_lines.append(f"## [{i}] [Kairos] {r['task']}")
            output_lines.append(f"- **Timestamp**: {r['timestamp']}")
            output_lines.append(f"- **Score**: {r['score']:.3f}")
        output_lines.append("")

    log(f"Search completed: {len(results)} results")
    return "\n".join(output_lines)


async def _handle_stats() -> list[TextContent]:
    try:
        with StdoutSuppressor():
            from mekhane.symploke.adapters.embedding_adapter import EmbeddingAdapter

        log("Getting stats...")
        sophia_count = 0
        kairos_count = 0

        if SOPHIA_INDEX.exists():
            adapter = EmbeddingAdapter()
            adapter.load(str(SOPHIA_INDEX))
            sophia_count = adapter.count()

        if KAIROS_INDEX.exists():
            adapter = EmbeddingAdapter()
            adapter.load(str(KAIROS_INDEX))
            kairos_count = adapter.count()

        output_lines = [
            "# Sophia/Kairos Statistics\n",
            f"- **Sophia (Knowledge Items)**: {sophia_count} documents",
            f"- **Kairos (Handoffs)**: {kairos_count} documents",
            f"- **Total**: {sophia_count + kairos_count} documents"
        ]

        log("Stats completed")
        return [TextContent(type="text", text="\n".join(output_lines))]

    except Exception as e:
        log(f"Stats error: {e}")
        return [TextContent(type="text", text=f"Error getting stats: {str(e)}")]


async def _handle_backlinks(arguments: dict) -> list[TextContent]:
    ki_name = arguments.get("ki_name", "")
    if not ki_name:
        return [TextContent(type="text", text="Error: ki_name is required")]

    try:
        with StdoutSuppressor():
            from mekhane.symploke.sophia_backlinker import SophiaBacklinker

        log(f"Getting backlinks for: {ki_name}")
        backlinker = SophiaBacklinker()
        backlinker.build_graph()

        backlinks = backlinker.get_backlinks(ki_name)
        outlinks = backlinker.get_outlinks(ki_name)

        output_lines = [f"# Backlinks: {ki_name}\n"]

        if backlinks:
            output_lines.append(f"## ← Backlinks ({len(backlinks)})")
            output_lines.extend([f"- {link}" for link in sorted(backlinks)])
        else:
            output_lines.append("No backlinks found.")

        output_lines.append("")

        if outlinks:
            output_lines.append(f"## → Outlinks ({len(outlinks)})")
            output_lines.extend([f"- {link}" for link in sorted(outlinks)])

        return [TextContent(type="text", text="\n".join(output_lines))]

    except Exception as e:
        log(f"Backlinks error: {e}")
        return [TextContent(type="text", text=f"Error: {str(e)}")]


async def _handle_graph_stats() -> list[TextContent]:
    try:
        with StdoutSuppressor():
            from mekhane.symploke.sophia_backlinker import SophiaBacklinker

        log("Getting graph stats...")
        backlinker = SophiaBacklinker()
        backlinker.build_graph()
        stats = backlinker.get_stats()

        output_lines = ["# Knowledge Graph Statistics\n"]
        output_lines.append(f"- **Nodes**: {stats['nodes']}")
        output_lines.append(f"- **Edges**: {stats['edges']}")
        output_lines.append(f"- **Isolated**: {stats['isolated']}")

        if stats["most_linked"]:
            output_lines.append("\n## Most Linked")
            for name, count in stats["most_linked"]:
                if count > 0:
                    output_lines.append(f"- **{name}**: {count} backlinks")

        return [TextContent(type="text", text="\n".join(output_lines))]

    except Exception as e:
        log(f"Graph stats error: {e}")
        return [TextContent(type="text", text=f"Error: {str(e)}")]


@server.call_tool(validate_input=True)
async def call_tool(name: str, arguments: dict):
    """Handle tool calls."""
    log(f"call_tool: {name} with {arguments}")

    handlers = {
        "search": _handle_search,
        "stats": _handle_stats,
        "backlinks": _handle_backlinks,
        "graph_stats": _handle_graph_stats,
    }

    handler = handlers.get(name)
    if handler:
        if name in ("search", "backlinks"):
            return await handler(arguments)
        else:
            return await handler()

    return [TextContent(type="text", text=f"Unknown tool: {name}")]
```
