#!/usr/bin/env python3
# PROOF: [L2/ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£] <- mekhane/symploke/ F5 benchmark ã§ Gemini 3 Flash ã®å“è³ªã‚’ç¢ºèªæ¸ˆã¿
# PURPOSE: Cortex Review Runner â€” Gemini 3 Flash/Pro ç›´æŽ¥å‘¼ã³å‡ºã—ã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼
"""
Cortex Review Runner â€” Gemini 3 Flash/Pro ç›´æŽ¥å‘¼ã³å‡ºã—ã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼

Jules API ã‚’çµŒç”±ã›ãšã€Cortex (Ochema MCP) ã‚’ç›´æŽ¥ä½¿ã†ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
run_specialists.py ã®è»½é‡ç‰ˆã€‚æ—¥æ¬¡ CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‘ã‘ã€‚

Usage:
    python run_cortex_review.py <target_file>
    python run_cortex_review.py <target_file> --model gemini-3-pro-preview
    python run_cortex_review.py <target_file> --output review_result.md
"""

from __future__ import annotations

import argparse
import json
import subprocess
import sys
from pathlib import Path

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«: F5 benchmark ã§ Flash ãŒã‚³ã‚¹ãƒˆæœ€é©ã¨ç¢ºèª
DEFAULT_MODEL = "gemini-3-flash-preview"
PRO_MODEL = "gemini-3-pro-preview"

# HGK ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (specialist_v2 ã® context/ ã‹ã‚‰æŠ½å‡º)
CONTEXT_DIR = Path(__file__).parent / "context"

# ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
SYSTEM_PROMPT = """You are reviewing code for HegemonikÃ³n, a cognitive hypervisor framework.
It uses 24 cognitive theorems (O1-O4, S1-S4, H1-H4, P1-P4, K1-K4, A1-A4) derived from FEP.
Each theorem represents a cognitive function with Greek names (NoÄ“sis, MekhanÄ“, Energeia, etc.).
Focus on: correctness, naming quality, design adherence, and potential improvements.
Output: top 5 findings as structured items with severity (Critical/High/Medium/Low)."""


# PURPOSE: context/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å…¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã¦è¿”ã™ã€‚
def load_context() -> str:
    """context/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å…¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã¦è¿”ã™ã€‚"""
    context_parts: list[str] = []
    if CONTEXT_DIR.exists():
        for md_file in sorted(CONTEXT_DIR.glob("*.md")):
            content = md_file.read_text(encoding="utf-8")
            context_parts.append(f"## {md_file.stem}\n\n{content}")
    return "\n\n---\n\n".join(context_parts)


# PURPOSE: ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
def build_review_prompt(code: str, filepath: str, context: str) -> str:
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚"""
    return f"""Review the following code from `{filepath}`.

## HGK Context
{context}

## Code to Review
```python
{code}
```

Provide your top 5 findings with severity (Critical/High/Medium/Low) and actionable recommendations."""


# PURPOSE: Ochema Cortex API ã‚’å‘¼ã³å‡ºã™ (MCP çµŒç”±)ã€‚
def call_cortex(prompt: str, model: str = DEFAULT_MODEL, max_tokens: int = 2048) -> str:
    """Ochema Cortex API ã‚’å‘¼ã³å‡ºã™ (MCP çµŒç”±)ã€‚

    NOTE: ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ MCP ç’°å¢ƒå¤–ã§å‹•ã‹ã™å ´åˆã€
    ochema ã® HTTP API ã«ç›´æŽ¥ POST ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
    MCP ç’°å¢ƒå†…ã§ã¯ mcp_ochema_ask_cortex ã§ä»£æ›¿ã€‚
    """
    # MCP ç’°å¢ƒå¤–ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ochema HTTP API
    import urllib.request

    payload = json.dumps({
        "model": model,
        "message": prompt,
        "system_instruction": SYSTEM_PROMPT,
        "max_tokens": max_tokens,
    })

    try:
        req = urllib.request.Request(
            "http://localhost:8765/api/cortex/ask",
            data=payload.encode("utf-8"),
            headers={"Content-Type": "application/json"},
        )
        with urllib.request.urlopen(req, timeout=120) as resp:
            result = json.loads(resp.read().decode("utf-8"))
            return result.get("response", result.get("text", str(result)))
    except Exception as e:
        return f"[Error] Cortex API call failed: {e}"


# PURPOSE: main ã®å‡¦ç†
def main() -> None:
    parser = argparse.ArgumentParser(description="Cortex Code Review (Gemini 3)")
    parser.add_argument("target", help="ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--model", default=DEFAULT_MODEL,
                        choices=[DEFAULT_MODEL, PRO_MODEL],
                        help=f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ« (default: {DEFAULT_MODEL})")
    parser.add_argument("--output", "-o", help="çµæžœå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--max-lines", type=int, default=200,
                        help="ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã®æœ€å¤§è¡Œæ•° (default: 200)")
    args = parser.parse_args()

    target_path = Path(args.target)
    if not target_path.exists():
        print(f"Error: {target_path} not found", file=sys.stderr)
        sys.exit(1)

    code = target_path.read_text(encoding="utf-8")
    lines = code.splitlines()
    if len(lines) > args.max_lines:
        code = "\n".join(lines[:args.max_lines])
        print(f"âš  Truncated to {args.max_lines} lines (original: {len(lines)})")

    context = load_context()
    prompt = build_review_prompt(code, str(target_path), context)

    print(f"ðŸ“‹ Reviewing {target_path} with {args.model}...")
    print(f"   Context: {len(context)} chars from {CONTEXT_DIR}")
    print(f"   Code: {len(lines)} lines")

    result = call_cortex(prompt, model=args.model)

    if args.output:
        output_path = Path(args.output)
        output_path.write_text(
            f"# Cortex Review: {target_path}\n\n"
            f"**Model**: {args.model}\n\n"
            f"---\n\n{result}\n",
            encoding="utf-8",
        )
        print(f"âœ… Result saved to {output_path}")
    else:
        print(f"\n{'='*60}")
        print(result)
        print(f"{'='*60}")


if __name__ == "__main__":
    main()
