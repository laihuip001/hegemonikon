#!/usr/bin/env python3
"""
PROOF: [L2/ã‚¤ãƒ³ãƒ•ãƒ©] ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å­˜åœ¨ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„

P3 â†’ /eat v3.0 ã® G é–¢æ‰‹ï¼ˆç¬¬ä¸€åŸç†åˆ†è§£ï¼‰ã‚’æ“ä½œçš„ã«ã‚µãƒãƒ¼ãƒˆã™ã‚‹
   â†’ HGK å†…éƒ¨å¯¾è±¡ã‹ã‚‰æ§‹é€ ã‚’å‰¥ã„ã§æœ€å°ãƒãƒ£ãƒ³ã‚¯ã«åˆ†è§£ã™ã‚‹ CLI ãƒ„ãƒ¼ãƒ«

G(Y) = Y ã®æ§‹é€ ã‚’åˆ—æŒ™ â†’ å…¨æ§‹é€ ã‚’å¿˜å´ â†’ åŸå­çš„ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ

Lineage: /bou+ P3 (2026-02-10) â€” G ã®ãƒ„ãƒ¼ãƒ«åŒ–
"""

import argparse
import json
import re
import sys
import yaml
from pathlib import Path
from typing import Any

# --- Constants ---

_HEGEMONIKON_ROOT = Path(__file__).resolve().parent.parent.parent
_WORKFLOWS_DIR = _HEGEMONIKON_ROOT / ".agent" / "workflows"
_SKILLS_DIR = _HEGEMONIKON_ROOT / ".agent" / "skills"
_KERNEL_DIR = _HEGEMONIKON_ROOT / "kernel"

# Structure markers in HGK workflow/skill files
_STRUCTURE_KEYS = [
    "hegemonikon",      # Series membership (e.g., "Ousia", "Schema")
    "modules",          # Theorem references (e.g., [O1, O2])
    "skill_ref",        # Skill references
    "derivatives",      # Derivative modes
    "trigonon",         # Trigonon structure (series, type, coordinates, bridge)
    "cognitive_algebra", # +/-/* modifiers
    "ccl_signature",    # CCL expression
    "category_theory",  # Category-theoretic metadata
    "absorbed",         # Absorbed techniques
    "sel_enforcement",  # SEL rules
    "related",          # X-series relations
]


def extract_frontmatter(filepath: Path) -> dict[str, Any] | None:
    """Extract YAML frontmatter from a markdown file."""
    try:
        content = filepath.read_text(encoding="utf-8")
        if not content.startswith("---"):
            return None
        parts = content.split("---", 2)
        if len(parts) < 3:
            return None
        return yaml.safe_load(parts[1])
    except Exception:
        return None


def extract_body_concepts(filepath: Path) -> list[str]:
    """
    Extract atomic concepts from markdown body (headings, bold terms).
    This is a heuristic extraction, not exhaustive.
    """
    try:
        content = filepath.read_text(encoding="utf-8")
        # Skip frontmatter
        parts = content.split("---", 2)
        body = parts[2] if len(parts) >= 3 else content

        concepts = []

        # Extract ## headings as concepts
        for match in re.finditer(r"^##\s+(.+)$", body, re.MULTILINE):
            heading = match.group(1).strip()
            # Remove markdown formatting
            heading = re.sub(r"[*_`#]", "", heading).strip()
            if heading and len(heading) < 80:
                concepts.append(heading)

        return concepts
    except Exception:
        return []


def decompose(target: str) -> dict[str, Any]:
    """
    G(Y): Apply the forgetful functor to an HGK internal object.

    Strips all HGK structure and returns atomic concept chunks.

    Args:
        target: Name of the WF/Skill/Kernel doc (e.g., "eat", "syn", "ousia")

    Returns:
        dict with:
          - target: input name
          - filepath: resolved file path
          - structures: dict of HGK structures found (what G forgets)
          - chunks: list of atomic concepts (what G preserves)
          - chunk_count: number of chunks
    """

    # Resolve target to file path
    filepath = _resolve_target(target)
    if filepath is None:
        return {
            "target": target,
            "error": f"Target '{target}' not found in workflows, skills, or kernel",
            "searched": [
                str(_WORKFLOWS_DIR),
                str(_SKILLS_DIR),
                str(_KERNEL_DIR),
            ],
        }

    # Phase 1: List all structures (what G will forget)
    fm = extract_frontmatter(filepath)
    structures = {}
    if fm:
        for key in _STRUCTURE_KEYS:
            if key in fm:
                structures[key] = fm[key]

    # Phase 2: Strip structures â†’ extract atomic concepts
    chunks = []

    # From description (if exists in frontmatter)
    if fm and "description" in fm:
        chunks.append(fm["description"])

    # From body headings/concepts
    body_concepts = extract_body_concepts(filepath)
    chunks.extend(body_concepts)

    # Phase 3: Return G(Y) = chunks (structures forgotten)
    return {
        "target": target,
        "filepath": str(filepath),
        "structures": structures,
        "structure_count": len(structures),
        "chunks": chunks,
        "chunk_count": len(chunks),
    }


def _resolve_target(target: str) -> Path | None:
    """Resolve target name to file path, searching WF â†’ Skill â†’ Kernel."""
    # Strip leading slash
    target = target.lstrip("/")

    # Search workflows
    wf_path = _WORKFLOWS_DIR / f"{target}.md"
    if wf_path.exists():
        return wf_path

    # Search skills (look for SKILL.md in subdirectories)
    for skill_dir in _SKILLS_DIR.iterdir():
        if skill_dir.is_dir():
            skill_file = skill_dir / "SKILL.md"
            if skill_file.exists() and target.lower() in skill_dir.name.lower():
                return skill_file

    # Search kernel
    kernel_path = _KERNEL_DIR / f"{target}.md"
    if kernel_path.exists():
        return kernel_path

    return None


# --- CLI Commands ---


def cmd_decompose(args: argparse.Namespace) -> int:
    """G(Y): Decompose an HGK internal object into atomic chunks."""
    result = decompose(args.target)

    if "error" in result:
        print(f"âŒ {result['error']}", file=sys.stderr)
        return 1

    if args.json:
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return 0

    # Human-readable output
    print(f"â”â”â” G({args.target}): ç¬¬ä¸€åŸç†åˆ†è§£ â”â”â”\n")
    print(f"ğŸ“‚ {result['filepath']}\n")

    print("â”â”â” å¿˜å´ã•ã‚Œã‚‹æ§‹é€  â”â”â”\n")
    if result["structures"]:
        for key, value in result["structures"].items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for k, v in value.items():
                    print(f"    {k}: {v}")
            elif isinstance(value, list):
                print(f"  {key}: {', '.join(str(v) for v in value)}")
            else:
                print(f"  {key}: {value}")
    else:
        print("  (æ§‹é€ ãªã—)")

    print(f"\nâ”â”â” æ®‹ã‚‹åŸå­ãƒãƒ£ãƒ³ã‚¯ ({result['chunk_count']}å€‹) â”â”â”\n")
    for i, chunk in enumerate(result["chunks"], 1):
        print(f"  {i}. {chunk}")

    print(f"\nğŸ“Š æ§‹é€ æ•°: {result['structure_count']} â†’ ãƒãƒ£ãƒ³ã‚¯æ•°: {result['chunk_count']}")
    return 0


def cmd_compare(args: argparse.Namespace) -> int:
    """Compare G(A) and G(B) â€” find shared atomic chunks."""
    result_a = decompose(args.target_a)
    result_b = decompose(args.target_b)

    if "error" in result_a:
        print(f"âŒ A: {result_a['error']}", file=sys.stderr)
        return 1
    if "error" in result_b:
        print(f"âŒ B: {result_b['error']}", file=sys.stderr)
        return 1

    chunks_a = set(result_a["chunks"])
    chunks_b = set(result_b["chunks"])
    shared = chunks_a & chunks_b
    only_a = chunks_a - chunks_b
    only_b = chunks_b - chunks_a

    if args.json:
        print(json.dumps({
            "a": args.target_a,
            "b": args.target_b,
            "shared": sorted(shared),
            "only_a": sorted(only_a),
            "only_b": sorted(only_b),
        }, ensure_ascii=False, indent=2))
        return 0

    print(f"â”â”â” G({args.target_a}) vs G({args.target_b}) â”â”â”\n")

    if shared:
        print(f"ğŸ”— å…±é€šãƒãƒ£ãƒ³ã‚¯ ({len(shared)}å€‹):")
        for c in sorted(shared):
            print(f"  â€¢ {c}")

    if only_a:
        print(f"\nğŸ“Œ {args.target_a} ã®ã¿ ({len(only_a)}å€‹):")
        for c in sorted(only_a):
            print(f"  â€¢ {c}")

    if only_b:
        print(f"\nğŸ“Œ {args.target_b} ã®ã¿ ({len(only_b)}å€‹):")
        for c in sorted(only_b):
            print(f"  â€¢ {c}")

    return 0


def main() -> int:
    parser = argparse.ArgumentParser(
        prog="decompose",
        description="G é–¢æ‰‹ (å¿˜å´ = ç¬¬ä¸€åŸç†åˆ†è§£) ã® CLI ãƒ„ãƒ¼ãƒ«",
        epilog="HGK å†…éƒ¨å¯¾è±¡ã‹ã‚‰æ§‹é€ ã‚’å‰¥ã„ã§æœ€å°ãƒãƒ£ãƒ³ã‚¯ã«åˆ†è§£ã™ã‚‹",
    )
    subparsers = parser.add_subparsers(dest="command", help="ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰")

    # decompose command
    p_decompose = subparsers.add_parser(
        "g",
        help="G(Y): å¯¾è±¡ã‚’ç¬¬ä¸€åŸç†ã«åˆ†è§£",
        description="HGK å†…éƒ¨å¯¾è±¡ã‹ã‚‰æ§‹é€ ã‚’å‰¥ã„ã§åŸå­çš„ãƒãƒ£ãƒ³ã‚¯ã‚’æŠ½å‡º",
    )
    p_decompose.add_argument("target", help="å¯¾è±¡å (e.g., eat, syn, ousia)")
    p_decompose.add_argument("--json", action="store_true", help="JSONå‡ºåŠ›")
    p_decompose.set_defaults(func=cmd_decompose)

    # compare command
    p_compare = subparsers.add_parser(
        "compare",
        help="G(A) vs G(B): äºŒã¤ã®å¯¾è±¡ã®åŸå­ãƒãƒ£ãƒ³ã‚¯ã‚’æ¯”è¼ƒ",
        description="äºŒã¤ã® HGK å†…éƒ¨å¯¾è±¡ã‚’åˆ†è§£ã—ã€å…±é€šãƒãƒ£ãƒ³ã‚¯ã‚’ç™ºè¦‹",
    )
    p_compare.add_argument("target_a", help="å¯¾è±¡A (e.g., eat)")
    p_compare.add_argument("target_b", help="å¯¾è±¡B (e.g., syn)")
    p_compare.add_argument("--json", action="store_true", help="JSONå‡ºåŠ›")
    p_compare.set_defaults(func=cmd_compare)

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return 0

    return args.func(args)


if __name__ == "__main__":
    sys.exit(main())
