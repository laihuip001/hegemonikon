# AI の記憶を設計する — 4 層メモリアーキテクチャ

> **想定媒体**: Zenn（技術記事）→ Qiita
> **想定読者**: Agent 開発者、LLM アプリケーション開発者
> **フック**: 「AI に昨日のことを覚えていてほしい」— その方法論

---

## リード文（案）

ChatGPT も Claude も、セッションが変わると全部忘れる。

これは LLM の本質的な限界だ。コンテキストウィンドウは有限。
「記憶がある AI」を作るには、**記憶そのものを設計する必要がある**。

1ヶ月間、AI の記憶を4層に分けて設計した。
結果、787コミットのプロジェクト全体を、セッションを跨いで一貫させることに成功した。

---

## 本文構成（案）

### 1. 4層メモリアーキテクチャ

| 層 | 名前 | 用途 | 実装 | 参照頻度 |
|:---|:-----|:-----|:-----|:---------|
| **L1** | Sophia (KI) | 恒久的知識 | Markdown KI + ベクトル検索 | 低（必要時） |
| **L2** | Kairos (Handoff) | セッション引き継ぎ | Handoff.md | 毎セッション |
| **L3** | Chronos | 時系列ログ | 自動インデキシング | 分析時 |
| **L4** | Gnōsis | 外部論文知識 | BGE-small + ONNX + hnswlib | 調査時 |

### 2. Handoff — セッション間の「引き継ぎ書」

**最も重要な発明。** セッション終了時に AI が自ら「次の AI」に向けて書くドキュメント。

```markdown
# Handoff: [タスク名]
> セッション: 2026-02-14 13:00 — 15:30
> 品質: ★★★★☆

## 達成事項
- ...

## 次セッションへの課題
- ...

## 法則化
- 今回学んだパターン

## 確信度
- [確信: 85%] 設計は正しい。テスト通過。
```

### 3. Sophia — AI が自分の仕事から学ぶ仕組み

- 重要な概念を Knowledge Item (KI) として保存
- ベクトル検索で類似概念を引っ張れる
- 人間の「経験知」に相当

### 4. Gnōsis — 外部知識のインデキシング

- 596 論文をローカルで ONNX 推論 + hnswlib
- 日本語テキストを janome で形態素解析してから埋め込み
- 全文検索ではなくセマンティック検索

### 5. 具体的なコード

```python
# 日本語テキストの前処理
def preprocess_ja(text: str) -> str:
    """BGE-small のために日本語を形態素分解"""
    tokens = janome.tokenize(text)
    content_words = [t for t in tokens 
                     if t.pos in {'名詞', '動詞', '形容詞'}]
    return ' '.join(content_words)
```

### 6. 失敗と教訓

- 最初は LanceDB を使ったが、依存が重すぎて hnswlib に移行
- Handoff の形式を何度もリファクタ（構造化しすぎると書かない、しなさすぎると役立たない）
- 「記憶の粒度」問題: 何を覚え、何を忘れるか

### 7. 読者が試せること

- Handoff テンプレートをシステムプロンプトに追加
- 重要な概念を Markdown ファイルに保存
- ベクトル検索は BGE-small + hnswlib で十分（GPU 不要）

---

*ステータス: たたき台 / 未完成*
