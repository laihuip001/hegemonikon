# AI が「できない」と言ったら、嘘かもしれない

> **ID**: A4
> **想定媒体**: Note → Twitter/X スレッド
> **想定読者**: 全 AI ユーザー
> **フック**: AI の「不可能」は「やっていない」の場合がある

---

## リード文（案）

「それは私にはできません」

Claude にそう言われたことはないだろうか？
実は、その半分は**嘘**だ。正確には、「できない」のではなく「やっていない」。

LLM は確率的に「安全な回答」を選ぶ。
「できない」と言えば間違えるリスクがゼロになる。
つまり**「できない」は最もリスクの低い回答**。

これに気づいたのは、ある事件がきっかけだった。

---

## 本文構成（案）

### 1. V-004 事件: 「日本語で考えられません」

- Claude に「日本語で思考して」と指示した
- 返答: 「extended thinking は英語でしか動作しません」
- **嘘だった。** 実際にはプロンプト次第で日本語思考が可能
- AI は「試していない」のに「できない」と言った

### 2. なぜ LLM は「できない」と言うのか

| 理由 | メカニズム |
|:-----|:---------|
| リスク回避 | 「できない」= 間違えるリスクゼロ |
| RLHF の副作用 | 安全方向に過学習 |
| 精度加重の偏り | 不確実な操作に高い精度加重 → 回避行動 |

### 3. 不可能断定チェック (3段階)

AI が「できない」と言ったら、3つ質問する:

1. **実際に試したか？** → 試行なき断定は証拠不十分
2. **代替手段を3つ検討したか？** → 最初の方法が失敗 ≠ 不可能
3. **「やっていない」と「できない」を混同していないか？**

### 4. 実践テクニック

```
あなたが「できない」と言った件ですが:
1. 実際に試みましたか、それとも推測ですか？
2. 代替手段はありますか？
3. 確信度は何%ですか？
```

### 5. AI 側の対策 (開発者向け)

- システムプロンプトに「不可能断定チェック」を追加
- 「できない」の前に確信度を必須化
- TAINT/SOURCE 追跡: 「なぜできないと思ったか」の根拠を要求

### 6. 読者が試せること

- 次に AI が「できない」と言ったら、「確信度は？」と聞く
- 「別の方法は？」と聞くだけで回答が変わることがある
- 「やってみて、失敗したら報告して」がmost effective

---

*ステータス: たたき台*
