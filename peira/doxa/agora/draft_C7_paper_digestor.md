# 596 本の論文を自動消化するパイプライン

> **ID**: C7
> **想定媒体**: Zenn（技術記事）
> **想定読者**: 研究者、Agent 開発者
> **フック**: 論文を「読む」のではなく「消化する」

---

## リード文（案）

596 本の論文が知識ベースにある。
全部読んだのか？ 読んでいない。**消化した**。

Digestor は、論文を自動収集→フィルタ→要約→インデキシングするパイプライン。
人間がやるのは「何に興味があるか」をトピックとして指定するだけ。

---

## 本文構成（案）

### 1. Digestor のアーキテクチャ

```
トピック定義 → Gnōsis スケジュール収集 → incoming/ に保存
→ /eat ワークフロー発動 → LLM で要約 → インデキシング
→ 統合検索で利用可能に
```

### 2. トピック定義

```yaml
topics:
  - "active inference"
  - "free energy principle"
  - "cognitive control"
  - "Markov categories"
```

### 3. 消化のワークフロー (/eat)

| Step | 処理 | ツール |
|:-----|:-----|:-------|
| 1 | incoming/ の候補を確認 | `check_incoming` |
| 2 | 候補リスト作成 | `list_candidates` |
| 3 | 選定 → 消化実行 | `run_digestor` |
| 4 | 3回以上読んで批判的に分析 | /noe + /dia |
| 5 | 既知の理論との摩擦を特定 | ~(/h*/k) |
| 6 | 消化完了 → processed/ に移動 | `mark_processed` |

### 4. 品質管理: 3回読み (F:[×3])

1回目: 概要把握
2回目: 批判的読解
3回目: 既知理論との統合

### 5. インデキシング

- BGE-small + ONNX でベクトル化
- hnswlib でセマンティック検索可能に
- タイトル、著者、要約、引用情報をメタデータとして保存

### 6. 実績

| 指標 | 値 |
|:-----|:---|
| 総論文数 | 596 |
| インデックスサイズ | 34,085 docs |
| 検索速度 | ~20ms |
| トピック数 | 8 |

### 7. 読者が試せること

- Semantic Scholar API で論文自動収集
- LLM で要約生成 → Markdown に保存
- BGE-small で埋め込み → hnswlib で検索

---

*ステータス: たたき台*
*関連: C1 (4層メモリ), C6 (ローカル検索), C9 (統合CLI)*
