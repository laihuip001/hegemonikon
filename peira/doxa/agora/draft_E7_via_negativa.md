# Via Negativa — 「AI を壊す方法」から設計する

> **ID**: E7
> **想定媒体**: Zenn → Note
> **想定読者**: セキュリティエンジニア、AI 安全性関心層
> **フック**: 「良い設計」ではなく「壊し方」から考える

---

## 本文構成（案）

### 1. Via Negativa とは

- 「AI を確実に危険にするには？」の逆が制約
- 安全性を「何をするか」ではなく「何をしないか」で定義

### 2. 逆算設計

| 危険な行動 | 対応制約 |
|:----------|:---------|
| 無制限のファイルアクセス | I-1: 破壊的操作禁止 |
| 過信に基づく誤情報 | BC-6: 確信度偽装禁止 |
| ユーザー意図の無視 | Zero Entropy |

### 3. Gemini ディレクトリ削除事件

- 2026年1月第1週
- AI がディレクトリを丸ごと削除
- → I-1 (破壊的操作禁止) + コマンドブラックリスト策定

### 4. SafeToAutoRun の線引き

| ✅ true (読むだけ) | ❌ false (世界を変える) |
|:---|:---|
| ls, cat, grep | rm, mv (上書き) |
| pytest | pip install/uninstall |
| echo | git push |

### 5. 「壊す方法」リスト

1. AI に `rm -rf /` を実行させる
2. AI に嘘の確信度を出力させる
3. AI にユーザーの意図と逆のことをさせる
4. AI のコンテキストを埋め尽くす
5. AI に古い情報で判断させる

→ 各々に対応する制約が存在する

---

*関連: A1 (失敗パターン), B3 (環境設計), A2 (ビビリ再設計)*
