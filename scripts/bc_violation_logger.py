#!/usr/bin/env python3
# PROOF: [L3/Utility] <- scripts/ A0â†’Implementationâ†’bc_violation_logger
# PROOF: [L2/é‹ç”¨] <- scripts/
# PURPOSE: BCé•åãƒ»Creator ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°
"""
bc_violation_logger.py â€” BCé•åãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ³ã‚¸ãƒ³

Creator ã®å±è²¬ / æ‰¿èª / AI ã®è‡ªå·±æ¤œå‡ºã‚’ JSONL ã«å³æ™‚è¨˜éŒ²ã€‚
violations.md (é‡å¤§é•åã®è©³ç´°åˆ†æ) ã¨ä½µç”¨ã—ã€æ—¥å¸¸çš„ãªé•åé »åº¦ã‚’è¿½è·¡ã™ã‚‹ã€‚

æ§‹é€ :
  - violations.jsonl: 1è¡Œ1ãƒ¬ã‚³ãƒ¼ãƒ‰ (JSONL)
  - ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç¨®åˆ¥: reprimand (å±è²¬), acknowledgment (æ‰¿èª), self_detected (è‡ªå·±æ¤œå‡º)
  - ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ»æœŸé–“ã”ã¨ã®çµ±è¨ˆã‚’æä¾›

Usage:
    # ãƒ­ã‚°è¨˜éŒ²
    python scripts/bc_violation_logger.py log \\
        --type reprimand \\
        --bc BC-1,BC-3 \\
        --pattern skip_bias \\
        --severity high \\
        --description "WFå®šç¾©ã‚’èª­ã¾ãšã«å®Ÿè¡Œ" \\
        --creator-words "çœŸå‰£ã«ã‚„ã‚Œã‚³ãƒ©"

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ
    python scripts/bc_violation_logger.py stats

    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
    python scripts/bc_violation_logger.py dashboard --period week
"""

import json
import sys
import argparse
from collections import Counter
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional


# ============================================================
# Config
# ============================================================

LOG_DIR = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "bc_violations"
LOG_FILE = LOG_DIR / "violations.jsonl"

FEEDBACK_TYPES = {"reprimand", "acknowledgment", "self_detected"}

PATTERN_NAMES = {
    "skip_bias": "çŸ¥ã£ã¦ã„ã‚‹â†’çœç•¥",
    "env_gap": "ç’°å¢ƒå¼·åˆ¶ãªã—",
    "accuracy_vs_utility": "æ­£ç¢º â‰  æœ‰ç”¨",
    "false_impossibility": "ã§ããªã„ â‰  ã‚„ã£ã¦ã„ãªã„",
    "selective_omission": "å‹æ‰‹ãªçœç•¥",
    "stale_handoff": "å¤ã„æƒ…å ±ã‚’ä¿¡ã˜ã‚‹",
    "preflight_waste": "ç¢ºèªãŒæœ¬ç•ªã‚’æ¶ˆè²»",
    "shortcut": "çŸ­çµ¡ãƒ»æ‰‹æŠœã",
    "overconfidence": "éä¿¡",
    "sycophancy": "è¿åˆ",
}

SEVERITY_ORDER = {"low": 0, "medium": 1, "high": 2, "critical": 3}
SEVERITY_ICONS = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸ”´", "critical": "ğŸ’€"}
TYPE_ICONS = {"reprimand": "âš¡", "acknowledgment": "âœ¨", "self_detected": "ğŸ”"}


# ============================================================
# Data Model
# ============================================================

@dataclass
class FeedbackEntry:
    """1ä»¶ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¨˜éŒ²"""
    timestamp: str                    # ISO 8601
    feedback_type: str                # reprimand / acknowledgment / self_detected
    bc_ids: list[str] = field(default_factory=list)  # ["BC-1", "BC-3"]
    pattern: str = ""                 # skip_bias, selective_omission etc.
    severity: str = "medium"          # low, medium, high, critical
    description: str = ""             # ä½•ãŒèµ·ããŸã‹
    context: str = ""                 # ãã®ã¨ãä½•ã‚’ã—ã¦ã„ãŸã‹
    creator_words: str = ""           # Creator ã®åŸæ–‡ (å±è²¬/æ‰¿èªã®è¨€è‘‰)
    corrective: str = ""              # å–ã£ãŸæ˜¯æ­£è¡Œå‹•
    session_id: str = ""              # ã‚»ãƒƒã‚·ãƒ§ãƒ³è­˜åˆ¥ (Handoff ID ãªã©)

    def to_dict(self) -> dict:
        return asdict(self)

    @classmethod
    def from_dict(cls, data: dict) -> "FeedbackEntry":
        # Ignore unknown fields gracefully
        known = {f.name for f in cls.__dataclass_fields__.values()}
        return cls(**{k: v for k, v in data.items() if k in known})


# ============================================================
# Logger
# ============================================================

def log_entry(entry: FeedbackEntry) -> Path:
    """JSONL ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¢ãƒšãƒ³ãƒ‰ã€‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã‘ã‚Œã°ä½œæˆã€‚"""
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry.to_dict(), ensure_ascii=False) + "\n")
    return LOG_FILE


def read_all_entries(path: Optional[Path] = None) -> list[FeedbackEntry]:
    """å…¨ã‚¨ãƒ³ãƒˆãƒªã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    path = path or LOG_FILE
    if not path.exists():
        return []
    entries = []
    for line in path.read_text(encoding="utf-8").strip().splitlines():
        if line.strip():
            try:
                entries.append(FeedbackEntry.from_dict(json.loads(line)))
            except (json.JSONDecodeError, TypeError):
                continue
    return entries


def filter_entries(
    entries: list[FeedbackEntry],
    *,
    feedback_type: Optional[str] = None,
    since_days: Optional[int] = None,
    pattern: Optional[str] = None,
    session_id: Optional[str] = None,
) -> list[FeedbackEntry]:
    """ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€‚"""
    result = entries

    if feedback_type:
        result = [e for e in result if e.feedback_type == feedback_type]

    if pattern:
        result = [e for e in result if e.pattern == pattern]

    if session_id:
        result = [e for e in result if e.session_id == session_id]

    if since_days is not None:
        cutoff = datetime.now() - timedelta(days=since_days)
        result = [
            e for e in result
            if _parse_ts(e.timestamp) >= cutoff
        ]

    return result


def _parse_ts(ts: str) -> datetime:
    """ISO 8601 timestamp ã‚’ãƒ‘ãƒ¼ã‚¹ã€‚"""
    try:
        return datetime.fromisoformat(ts)
    except (ValueError, TypeError):
        return datetime.min


# ============================================================
# Statistics
# ============================================================

def compute_stats(entries: list[FeedbackEntry]) -> dict:
    """ã‚¨ãƒ³ãƒˆãƒªç¾¤ã‹ã‚‰çµ±è¨ˆã‚’è¨ˆç®—ã€‚"""
    if not entries:
        return {
            "total": 0,
            "by_type": {},
            "by_pattern": {},
            "by_bc": {},
            "by_severity": {},
            "reprimand_rate": 0.0,
            "self_detection_rate": 0.0,
            "creator_words_samples": [],
        }

    type_counts = Counter(e.feedback_type for e in entries)
    pattern_counts = Counter(e.pattern for e in entries if e.pattern)
    bc_counts: Counter = Counter()
    for e in entries:
        bc_counts.update(e.bc_ids)
    severity_counts = Counter(e.severity for e in entries if e.feedback_type != "acknowledgment")

    total = len(entries)
    reprimands = type_counts.get("reprimand", 0)
    acknowledgments = type_counts.get("acknowledgment", 0)
    self_detected = type_counts.get("self_detected", 0)

    # é•åç³» (reprimand + self_detected) ã®ã†ã¡è‡ªå·±æ¤œå‡ºã®ç‡
    violation_total = reprimands + self_detected
    self_detection_rate = (self_detected / violation_total * 100) if violation_total > 0 else 0

    # å±è²¬ç‡ = reprimand / (reprimand + acknowledgment)
    feedback_total = reprimands + acknowledgments
    reprimand_rate = (reprimands / feedback_total * 100) if feedback_total > 0 else 0

    # Creator ã®è¨€è‘‰ã‚µãƒ³ãƒ—ãƒ« (ç›´è¿‘5ä»¶)
    creator_samples = [
        {"type": e.feedback_type, "words": e.creator_words, "date": e.timestamp[:10]}
        for e in reversed(entries)
        if e.creator_words
    ][:5]

    return {
        "total": total,
        "by_type": dict(type_counts.most_common()),
        "by_pattern": dict(pattern_counts.most_common()),
        "by_bc": dict(bc_counts.most_common()),
        "by_severity": dict(severity_counts),
        "reprimand_rate": round(reprimand_rate, 1),
        "self_detection_rate": round(self_detection_rate, 1),
        "creator_words_samples": creator_samples,
    }


def compute_trend(entries: list[FeedbackEntry], weeks: int = 4) -> list[dict]:
    """é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¨ˆç®—ã€‚"""
    now = datetime.now()
    trend = []
    for w in range(weeks - 1, -1, -1):
        start = now - timedelta(weeks=w + 1)
        end = now - timedelta(weeks=w)
        week_entries = [
            e for e in entries
            if start <= _parse_ts(e.timestamp) < end
        ]
        reprimands = sum(1 for e in week_entries if e.feedback_type == "reprimand")
        acks = sum(1 for e in week_entries if e.feedback_type == "acknowledgment")
        self_det = sum(1 for e in week_entries if e.feedback_type == "self_detected")
        trend.append({
            "week": f"W{weeks - w}",
            "start": start.strftime("%m/%d"),
            "end": end.strftime("%m/%d"),
            "reprimands": reprimands,
            "acknowledgments": acks,
            "self_detected": self_det,
            "total": len(week_entries),
        })
    return trend


# ============================================================
# Dashboard Formatter
# ============================================================

def format_dashboard(
    entries: list[FeedbackEntry],
    period: str = "all",
) -> str:
    """CLIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ç”Ÿæˆã€‚"""
    # Period filter
    if period == "today":
        entries = filter_entries(entries, since_days=0)
    elif period == "week":
        entries = filter_entries(entries, since_days=7)
    elif period == "month":
        entries = filter_entries(entries, since_days=30)

    stats = compute_stats(entries)
    trend = compute_trend(entries)

    lines = [
        "ğŸ“Š BCé•åãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        f"   æœŸé–“: {period} | ç·ä»¶æ•°: {stats['total']}",
        "â”" * 50,
        "",
    ]

    # Type breakdown
    lines.append("ğŸ“‹ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç¨®åˆ¥")
    for t, icon in TYPE_ICONS.items():
        count = stats["by_type"].get(t, 0)
        bar = "â–ˆ" * count
        label = {"reprimand": "å±è²¬", "acknowledgment": "æ‰¿èª", "self_detected": "è‡ªå·±æ¤œå‡º"}[t]
        lines.append(f"  {icon} {label:8s}: {bar} {count}")
    lines.append("")

    # Reprimand rate vs self-detection rate
    lines.append("ğŸ“ˆ æŒ‡æ¨™")
    lines.append(f"  å±è²¬ç‡: {stats['reprimand_rate']}% (å±è²¬ / (å±è²¬+æ‰¿èª))")
    lines.append(f"  è‡ªå·±æ¤œå‡ºç‡: {stats['self_detection_rate']}% (è‡ªå·±æ¤œå‡º / (å±è²¬+è‡ªå·±æ¤œå‡º))")
    lines.append("")

    # Severity
    if stats["by_severity"]:
        lines.append("ğŸ”´ æ·±åˆ»åº¦åˆ†å¸ƒ")
        for sev in ["critical", "high", "medium", "low"]:
            count = stats["by_severity"].get(sev, 0)
            if count > 0:
                icon = SEVERITY_ICONS.get(sev, "âšª")
                bar = "â–ˆ" * count
                lines.append(f"  {icon} {sev:10s}: {bar} {count}")
        lines.append("")

    # Pattern frequency
    if stats["by_pattern"]:
        lines.append("ğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³é »åº¦")
        for pattern, count in stats["by_pattern"].items():
            name = PATTERN_NAMES.get(pattern, pattern)
            bar = "â–ˆ" * count
            lines.append(f"  {name:20s}: {bar} {count}")
        lines.append("")

    # Most violated BCs
    if stats["by_bc"]:
        lines.append("âš ï¸ é•å BC ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
        for bc, count in stats["by_bc"].items():
            bar = "â–ˆ" * count
            lines.append(f"  {bc:8s}: {bar} {count}")
        lines.append("")

    # Weekly trend
    lines.append("ğŸ“Š é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰")
    for w in trend:
        rep_bar = "âš¡" * w["reprimands"]
        ack_bar = "âœ¨" * w["acknowledgments"]
        self_bar = "ğŸ”" * w["self_detected"]
        lines.append(f"  {w['week']} ({w['start']}-{w['end']}): {rep_bar}{ack_bar}{self_bar} ({w['total']})")
    lines.append("")

    # Creator's words
    if stats["creator_words_samples"]:
        lines.append("ğŸ’¬ Creator ã®è¨€è‘‰ (ç›´è¿‘)")
        for s in stats["creator_words_samples"]:
            icon = TYPE_ICONS.get(s["type"], "")
            lines.append(f"  {icon} [{s['date']}] \"{s['words']}\"")
        lines.append("")

    return "\n".join(lines)


def format_session_summary(entries: list[FeedbackEntry], session_id: str = "") -> str:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã‚µãƒãƒªãƒ¼ (ç°¡æ½”)ã€‚"""
    if session_id:
        entries = filter_entries(entries, session_id=session_id)

    stats = compute_stats(entries)
    if stats["total"] == 0:
        return "âœ… ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã®é•åè¨˜éŒ²ãªã—"

    rep = stats["by_type"].get("reprimand", 0)
    ack = stats["by_type"].get("acknowledgment", 0)
    sd = stats["by_type"].get("self_detected", 0)

    parts = []
    if rep > 0:
        parts.append(f"âš¡å±è²¬ {rep}")
    if ack > 0:
        parts.append(f"âœ¨æ‰¿èª {ack}")
    if sd > 0:
        parts.append(f"ğŸ”è‡ªå·±æ¤œå‡º {sd}")

    return f"ğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³: {' | '.join(parts)} | è‡ªå·±æ¤œå‡ºç‡: {stats['self_detection_rate']}%"


def format_bye_section(entries: list[FeedbackEntry]) -> str:
    """
    /bye Handoff ã«å«ã‚ã‚‹ BCé•åã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€‚

    Step 2 (ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±åé›†) ã¨ Step 3.7 (Self-Profile) ã§ä½¿ã†ã€‚
    """
    stats = compute_stats(entries)
    if stats["total"] == 0:
        return "## âš¡ BC ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯\n\nâœ… ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¨˜éŒ²ãªã—\n"

    lines = [
        "## âš¡ BC ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯",
        "",
        "| æŒ‡æ¨™ | å€¤ |",
        "|:-----|:---|",
        f"| ç·ä»¶æ•° | {stats['total']} |",
        f"| å±è²¬ç‡ | {stats['reprimand_rate']}% |",
        f"| è‡ªå·±æ¤œå‡ºç‡ | {stats['self_detection_rate']}% |",
    ]

    # ç¨®åˆ¥å†…è¨³
    rep = stats["by_type"].get("reprimand", 0)
    ack = stats["by_type"].get("acknowledgment", 0)
    sd = stats["by_type"].get("self_detected", 0)
    lines.append(f"| å†…è¨³ | âš¡å±è²¬ {rep} / âœ¨æ‰¿èª {ack} / ğŸ”è‡ªå·±æ¤œå‡º {sd} |")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³
    if stats["by_pattern"]:
        top_patterns = ", ".join(
            f"{PATTERN_NAMES.get(p, p)}({c})"
            for p, c in list(stats["by_pattern"].items())[:3]
        )
        lines.append(f"| é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ | {top_patterns} |")

    # æœ€å¤š BC
    if stats["by_bc"]:
        top_bcs = ", ".join(
            f"{bc}({c})" for bc, c in list(stats["by_bc"].items())[:3]
        )
        lines.append(f"| æœ€å¤š BC | {top_bcs} |")

    lines.append("")

    # Creator ã®è¨€è‘‰
    if stats["creator_words_samples"]:
        lines.append("### Creator ã®è¨€è‘‰")
        lines.append("")
        for s in stats["creator_words_samples"]:
            icon = TYPE_ICONS.get(s["type"], "")
            lines.append(f"- {icon} [{s['date']}] \"{s['words']}\"")
        lines.append("")

    return "\n".join(lines)


def format_boot_summary(entries: list[FeedbackEntry]) -> str:
    """
    /boot æ™‚ã«çªãã¤ã‘ã‚‹å‰ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¾ã§ã®å‚¾å‘ã‚µãƒãƒªãƒ¼ã€‚

    ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã«å±è²¬ç‡ãƒ»è‡ªå·±æ¤œå‡ºç‡ãƒ»ç›´è¿‘ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¿”ã™ã€‚
    """
    if not entries:
        return "âš¡ BC: è¨˜éŒ²ãªã—"

    stats = compute_stats(entries)
    trend = compute_trend(entries, weeks=2)

    # ç›´è¿‘é€±ã®ãƒ‡ãƒ¼ã‚¿
    latest = trend[-1] if trend else {}
    latest_rep = latest.get("reprimands", 0)
    latest_total = latest.get("total", 0)

    parts = [
        f"âš¡ BC: ç´¯è¨ˆ{stats['total']}ä»¶",
        f"å±è²¬ç‡{stats['reprimand_rate']}%",
        f"è‡ªå·±æ¤œå‡ºç‡{stats['self_detection_rate']}%",
        f"ç›´è¿‘é€±: {latest_rep}å±è²¬/{latest_total}ä»¶",
    ]

    # æœ€å¤šãƒ‘ã‚¿ãƒ¼ãƒ³è­¦å‘Š
    if stats["by_pattern"]:
        top_pattern = list(stats["by_pattern"].keys())[0]
        top_name = PATTERN_NAMES.get(top_pattern, top_pattern)
        parts.append(f"âš ï¸{top_name}")

    return " | ".join(parts)


# ============================================================
# Escalation â€” violations.md ã¸ã®æ˜‡æ ¼ææ¡ˆ
# ============================================================

VIOLATIONS_MD = (
    Path.home()
    / "oikos"
    / "hegemonikon"
    / ".agent"
    / "rules"
    / "behavioral_constraints"
    / "violations.md"
)


def _next_violation_id(violations_path: Optional[Path] = None) -> str:
    """violations.md ã®æ—¢å­˜æœ€å¤§ V-NNN ID ã‚’æ¤œå‡ºã—ã€æ¬¡ã® ID ã‚’è¿”ã™ã€‚

    YAML ãƒ–ãƒ­ãƒƒã‚¯å†…ã® `id: V-NNN` è¡Œã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹ã€‚
    æœ¬æ–‡ä¸­ã®è¨€åŠ (ä¾‹: 'V-006 ã®å†ç™º') ã¯ç„¡è¦–ã™ã‚‹ã€‚
    """
    path = violations_path or VIOLATIONS_MD
    if not path.exists():
        return "V-001"
    import re

    content = path.read_text(encoding="utf-8")
    # YAML ãƒ–ãƒ­ãƒƒã‚¯å†…ã® id: V-NNN ã®ã¿ã‚’å¯¾è±¡
    ids = re.findall(r'^id:\s*V-(\d{3})', content, re.MULTILINE)
    if not ids:
        return "V-001"
    max_id = max(int(i) for i in ids)
    return f"V-{max_id + 1:03d}"


def _existing_patterns_in_violations(violations_path: Optional[Path] = None) -> set[str]:
    """violations.md ã«æ—¢ã«è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³åã‚’è¿”ã™ã€‚"""
    path = violations_path or VIOLATIONS_MD
    if not path.exists():
        return set()
    import re

    content = path.read_text(encoding="utf-8")
    return set(re.findall(r'^pattern:\s*(\S+)', content, re.MULTILINE))


def suggest_escalation(
    entries: list[FeedbackEntry],
    *,
    min_severity: str = "high",
    min_occurrences: int = 2,
) -> list[dict]:
    """
    violations.md ã¸ã®æ˜‡æ ¼å€™è£œã‚’æ¤œå‡ºã™ã‚‹ã€‚

    æ˜‡æ ¼æ¡ä»¶ (OR):
      1. severity ãŒ min_severity ä»¥ä¸Š
      2. åŒã˜ pattern ãŒ min_occurrences å›ä»¥ä¸Šå‡ºç¾

    Returns:
        list[dict]: æ˜‡æ ¼å€™è£œã®ãƒªã‚¹ãƒˆã€‚å„è¦ç´ ã¯:
          - pattern: str
          - severity: str (æœ€é«˜æ·±åˆ»åº¦)
          - count: int
          - reason: str ("severity" or "recurrence" or "both")
          - entries: list[FeedbackEntry] (è©²å½“ã‚¨ãƒ³ãƒˆãƒª)
          - template: str (violations.md ç”¨ YAML ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ)
    """
    if not entries:
        return []

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã«é›†è¨ˆ
    pattern_groups: dict[str, list[FeedbackEntry]] = {}
    for e in entries:
        if e.pattern:
            pattern_groups.setdefault(e.pattern, []).append(e)

    # äºŒé‡ææ¡ˆé˜²æ­¢: violations.md ã«æ—¢ã«è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
    existing_patterns = _existing_patterns_in_violations()
    for p in existing_patterns:
        pattern_groups.pop(p, None)

    sev_threshold = SEVERITY_ORDER.get(min_severity, 2)
    next_id = _next_violation_id()
    candidates = []

    for pattern, group in pattern_groups.items():
        max_sev = max(SEVERITY_ORDER.get(e.severity, 0) for e in group)
        max_sev_name = next(k for k, v in SEVERITY_ORDER.items() if v == max_sev)
        count = len(group)

        is_severe = max_sev >= sev_threshold
        is_recurrent = count >= min_occurrences
        if not (is_severe or is_recurrent):
            continue

        reason = "both" if (is_severe and is_recurrent) else (
            "severity" if is_severe else "recurrence"
        )

        # YAML ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
        template = _format_escalation_entry(
            vid=next_id,
            pattern=pattern,
            severity=max_sev_name,
            recurrent=is_recurrent,
            group=group,
        )

        candidates.append({
            "pattern": pattern,
            "severity": max_sev_name,
            "count": count,
            "reason": reason,
            "entries": group,
            "template": template,
        })

        # æ¬¡ã® ID ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ
        num = int(next_id.split("-")[1])
        next_id = f"V-{num + 1:03d}"

    # æ·±åˆ»åº¦é™é † â†’ ä»¶æ•°é™é †
    candidates.sort(
        key=lambda c: (-SEVERITY_ORDER.get(c["severity"], 0), -c["count"])
    )
    return candidates


def _format_escalation_entry(
    vid: str,
    pattern: str,
    severity: str,
    recurrent: bool,
    group: list[FeedbackEntry],
) -> str:
    """violations.md ç”¨ã® YAML ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”Ÿæˆã€‚"""
    from datetime import date

    # BC ã®é›†ç´„
    all_bcs: set[str] = set()
    for e in group:
        all_bcs.update(e.bc_ids)
    bc_list = sorted(all_bcs) if all_bcs else ["BC-?"]

    # ã‚µãƒãƒªãƒ¼: ä»£è¡¨çš„ãª description ã‚’ä½¿ç”¨
    descriptions = [e.description for e in group if e.description]
    summary = descriptions[0] if descriptions else "JSONL ã‹ã‚‰æ˜‡æ ¼"
    if len(descriptions) > 1:
        summary += f" (ä»– {len(descriptions) - 1} ä»¶)"

    # Creator ã®è¨€è‘‰
    creator_words = [e.creator_words for e in group if e.creator_words]
    creator_section = ""
    if creator_words:
        creator_section = "\n  Creator ã®è¨€è‘‰:\n" + "\n".join(
            f"    - \"{w}\"" for w in creator_words[:3]
        )

    # æ˜¯æ­£è¡Œå‹•
    correctives = [e.corrective for e in group if e.corrective]
    corrective_section = correctives[0] if correctives else "<!-- FILL: æ˜¯æ­£è¡Œå‹• -->"

    pattern_name = PATTERN_NAMES.get(pattern, pattern)

    return f"""### {vid}: {pattern_name}ï¼ˆJSONL æ˜‡æ ¼ï¼‰

```yaml
id: {vid}
date: "{date.today().isoformat()}"
bc: [{', '.join(bc_list)}]
pattern: {pattern}
severity: {severity}
recurrence: {str(recurrent).lower()}
summary: |
  {summary}
  JSONL è¨˜éŒ² {len(group)} ä»¶ã‹ã‚‰æ˜‡æ ¼ã€‚{creator_section}
root_cause: |
  <!-- FILL: æ ¹æœ¬åŸå› ã‚’è¨˜è¿° -->
corrective: |
  {corrective_section}
lesson: |
  <!-- FILL: æ•™è¨“ã‚’è¨˜è¿° -->
```

---"""


# ============================================================
# CLI
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="BCé•åãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ³ã‚¸ãƒ³ â€” Creator ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¿½è·¡"
    )
    sub = parser.add_subparsers(dest="command")

    # log ã‚³ãƒãƒ³ãƒ‰
    log_parser = sub.add_parser("log", help="ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²")
    log_parser.add_argument("--type", required=True, choices=sorted(FEEDBACK_TYPES),
                           help="ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç¨®åˆ¥")
    log_parser.add_argument("--bc", type=str, default="",
                           help="BC ID (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š: BC-1,BC-3)")
    log_parser.add_argument("--pattern", type=str, default="",
                           help="ãƒ‘ã‚¿ãƒ¼ãƒ³ID")
    log_parser.add_argument("--severity", type=str, default="medium",
                           choices=sorted(SEVERITY_ORDER.keys()),
                           help="æ·±åˆ»åº¦")
    log_parser.add_argument("--description", type=str, default="",
                           help="ä½•ãŒèµ·ããŸã‹")
    log_parser.add_argument("--context", type=str, default="",
                           help="æ–‡è„ˆ")
    log_parser.add_argument("--creator-words", type=str, default="",
                           help="Creator ã®åŸæ–‡")
    log_parser.add_argument("--corrective", type=str, default="",
                           help="æ˜¯æ­£è¡Œå‹•")
    log_parser.add_argument("--session-id", type=str, default="",
                           help="ã‚»ãƒƒã‚·ãƒ§ãƒ³ID")

    # stats ã‚³ãƒãƒ³ãƒ‰
    sub.add_parser("stats", help="çµ±è¨ˆã‚’è¡¨ç¤º")

    # dashboard ã‚³ãƒãƒ³ãƒ‰
    dash_parser = sub.add_parser("dashboard", help="ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’è¡¨ç¤º")
    dash_parser.add_argument("--period", type=str, default="all",
                            choices=["today", "week", "month", "all"],
                            help="æœŸé–“")
    dash_parser.add_argument("--json", action="store_true", help="JSONå‡ºåŠ›")

    # escalate ã‚³ãƒãƒ³ãƒ‰
    esc_parser = sub.add_parser("escalate", help="violations.md ã¸ã®æ˜‡æ ¼å€™è£œã‚’è¡¨ç¤º")
    esc_parser.add_argument("--min-severity", type=str, default="high",
                           choices=sorted(SEVERITY_ORDER.keys()),
                           help="æœ€ä½æ·±åˆ»åº¦ (default: high)")
    esc_parser.add_argument("--min-occurrences", type=int, default=2,
                           help="æœ€ä½å‡ºç¾å›æ•° (default: 2)")

    args = parser.parse_args()

    if args.command == "log":
        bc_ids = [b.strip() for b in args.bc.split(",") if b.strip()]
        entry = FeedbackEntry(
            timestamp=datetime.now().isoformat(),
            feedback_type=args.type,
            bc_ids=bc_ids,
            pattern=args.pattern,
            severity=args.severity,
            description=args.description,
            context=args.context,
            creator_words=args.creator_words,
            corrective=args.corrective,
            session_id=args.session_id,
        )
        path = log_entry(entry)
        icon = TYPE_ICONS.get(args.type, "")
        print(f"{icon} è¨˜éŒ²å®Œäº†: {path}")
        print(f"   ç¨®åˆ¥: {args.type} | BC: {bc_ids} | ãƒ‘ã‚¿ãƒ¼ãƒ³: {args.pattern}")

        # è¨˜éŒ²å¾Œã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        all_entries = read_all_entries()
        print()
        print(format_session_summary(all_entries))

    elif args.command == "stats":
        entries = read_all_entries()
        stats = compute_stats(entries)
        print(json.dumps(stats, ensure_ascii=False, indent=2))

    elif args.command == "dashboard":
        entries = read_all_entries()
        if args.json:
            stats = compute_stats(entries)
            print(json.dumps(stats, ensure_ascii=False, indent=2))
        else:
            print(format_dashboard(entries, period=args.period))

    elif args.command == "escalate":
        entries = read_all_entries()
        candidates = suggest_escalation(
            entries,
            min_severity=args.min_severity,
            min_occurrences=args.min_occurrences,
        )
        if not candidates:
            print("âœ… æ˜‡æ ¼å€™è£œãªã— â€” ç¾åœ¨ã®è¨˜éŒ²ã«é‡å¤§/åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        else:
            print(f"â¬†ï¸ æ˜‡æ ¼å€™è£œ: {len(candidates)} ä»¶")
            print(f"   æ¬¡ã® ID: {_next_violation_id()}")
            print("â”" * 50)
            for c in candidates:
                name = PATTERN_NAMES.get(c["pattern"], c["pattern"])
                icon = SEVERITY_ICONS.get(c["severity"], "âšª")
                print(f"\n{icon} {name} â€” {c['count']}ä»¶ ({c['reason']})")
                print(c["template"])

    else:
        parser.print_help()

    sys.exit(0)


if __name__ == "__main__":
    main()
