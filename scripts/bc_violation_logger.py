#!/usr/bin/env python3
# PROOF: [L2/é‹ç”¨] <- scripts/
# PURPOSE: BCé•åãƒ»Creator ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°
"""
bc_violation_logger.py â€” BCé•åãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ³ã‚¸ãƒ³

Creator ã®å±è²¬ / æ‰¿èª / AI ã®è‡ªå·±æ¤œå‡ºã‚’ JSONL ã«å³æ™‚è¨˜éŒ²ã€‚
violations.md (é‡å¤§é•åã®è©³ç´°åˆ†æ) ã¨ä½µç”¨ã—ã€æ—¥å¸¸çš„ãªé•åé »åº¦ã‚’è¿½è·¡ã™ã‚‹ã€‚

æ§‹é€ :
  - violations.jsonl: 1è¡Œ1ãƒ¬ã‚³ãƒ¼ãƒ‰ (JSONL)
  - ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç¨®åˆ¥: reprimand (å±è²¬), acknowledgment (æ‰¿èª), self_detected (è‡ªå·±æ¤œå‡º)
  - ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ»æœŸé–“ã”ã¨ã®çµ±è¨ˆã‚’æä¾›

Usage:
    # ãƒ­ã‚°è¨˜éŒ²
    python scripts/bc_violation_logger.py log \\
        --type reprimand \\
        --bc BC-1,BC-3 \\
        --pattern skip_bias \\
        --severity high \\
        --description "WFå®šç¾©ã‚’èª­ã¾ãšã«å®Ÿè¡Œ" \\
        --creator-words "çœŸå‰£ã«ã‚„ã‚Œã‚³ãƒ©"

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ
    python scripts/bc_violation_logger.py stats

    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
    python scripts/bc_violation_logger.py dashboard --period week
"""

import json
import sys
import argparse
from collections import Counter
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional


# ============================================================
# Config
# ============================================================

LOG_DIR = Path.home() / "oikos" / "mneme" / ".hegemonikon" / "bc_violations"
LOG_FILE = LOG_DIR / "violations.jsonl"

FEEDBACK_TYPES = {"reprimand", "acknowledgment", "self_detected"}

PATTERN_NAMES = {
    "skip_bias": "çŸ¥ã£ã¦ã„ã‚‹â†’çœç•¥",
    "env_gap": "ç’°å¢ƒå¼·åˆ¶ãªã—",
    "accuracy_vs_utility": "æ­£ç¢º â‰  æœ‰ç”¨",
    "false_impossibility": "ã§ããªã„ â‰  ã‚„ã£ã¦ã„ãªã„",
    "selective_omission": "å‹æ‰‹ãªçœç•¥",
    "stale_handoff": "å¤ã„æƒ…å ±ã‚’ä¿¡ã˜ã‚‹",
    "preflight_waste": "ç¢ºèªãŒæœ¬ç•ªã‚’æ¶ˆè²»",
    "shortcut": "çŸ­çµ¡ãƒ»æ‰‹æŠœã",
    "overconfidence": "éä¿¡",
    "sycophancy": "è¿åˆ",
}

SEVERITY_ORDER = {"low": 0, "medium": 1, "high": 2, "critical": 3}
SEVERITY_ICONS = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸ”´", "critical": "ğŸ’€"}
TYPE_ICONS = {"reprimand": "âš¡", "acknowledgment": "âœ¨", "self_detected": "ğŸ”"}


# ============================================================
# Data Model
# ============================================================

@dataclass
class FeedbackEntry:
    """1ä»¶ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¨˜éŒ²"""
    timestamp: str                    # ISO 8601
    feedback_type: str                # reprimand / acknowledgment / self_detected
    bc_ids: list[str] = field(default_factory=list)  # ["BC-1", "BC-3"]
    pattern: str = ""                 # skip_bias, selective_omission etc.
    severity: str = "medium"          # low, medium, high, critical
    description: str = ""             # ä½•ãŒèµ·ããŸã‹
    context: str = ""                 # ãã®ã¨ãä½•ã‚’ã—ã¦ã„ãŸã‹
    creator_words: str = ""           # Creator ã®åŸæ–‡ (å±è²¬/æ‰¿èªã®è¨€è‘‰)
    corrective: str = ""              # å–ã£ãŸæ˜¯æ­£è¡Œå‹•
    session_id: str = ""              # ã‚»ãƒƒã‚·ãƒ§ãƒ³è­˜åˆ¥ (Handoff ID ãªã©)

    def to_dict(self) -> dict:
        return asdict(self)

    @classmethod
    def from_dict(cls, data: dict) -> "FeedbackEntry":
        # Ignore unknown fields gracefully
        known = {f.name for f in cls.__dataclass_fields__.values()}
        return cls(**{k: v for k, v in data.items() if k in known})


# ============================================================
# Logger
# ============================================================

def log_entry(entry: FeedbackEntry) -> Path:
    """JSONL ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¢ãƒšãƒ³ãƒ‰ã€‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã‘ã‚Œã°ä½œæˆã€‚"""
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry.to_dict(), ensure_ascii=False) + "\n")
    return LOG_FILE


def read_all_entries(path: Optional[Path] = None) -> list[FeedbackEntry]:
    """å…¨ã‚¨ãƒ³ãƒˆãƒªã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    path = path or LOG_FILE
    if not path.exists():
        return []
    entries = []
    for line in path.read_text(encoding="utf-8").strip().splitlines():
        if line.strip():
            try:
                entries.append(FeedbackEntry.from_dict(json.loads(line)))
            except (json.JSONDecodeError, TypeError):
                continue
    return entries


def filter_entries(
    entries: list[FeedbackEntry],
    *,
    feedback_type: Optional[str] = None,
    since_days: Optional[int] = None,
    pattern: Optional[str] = None,
    session_id: Optional[str] = None,
) -> list[FeedbackEntry]:
    """ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€‚"""
    result = entries

    if feedback_type:
        result = [e for e in result if e.feedback_type == feedback_type]

    if pattern:
        result = [e for e in result if e.pattern == pattern]

    if session_id:
        result = [e for e in result if e.session_id == session_id]

    if since_days is not None:
        cutoff = datetime.now() - timedelta(days=since_days)
        result = [
            e for e in result
            if _parse_ts(e.timestamp) >= cutoff
        ]

    return result


def _parse_ts(ts: str) -> datetime:
    """ISO 8601 timestamp ã‚’ãƒ‘ãƒ¼ã‚¹ã€‚"""
    try:
        return datetime.fromisoformat(ts)
    except (ValueError, TypeError):
        return datetime.min


# ============================================================
# Statistics
# ============================================================

def compute_stats(entries: list[FeedbackEntry]) -> dict:
    """ã‚¨ãƒ³ãƒˆãƒªç¾¤ã‹ã‚‰çµ±è¨ˆã‚’è¨ˆç®—ã€‚"""
    if not entries:
        return {
            "total": 0,
            "by_type": {},
            "by_pattern": {},
            "by_bc": {},
            "by_severity": {},
            "reprimand_rate": 0.0,
            "self_detection_rate": 0.0,
            "creator_words_samples": [],
        }

    type_counts = Counter(e.feedback_type for e in entries)
    pattern_counts = Counter(e.pattern for e in entries if e.pattern)
    bc_counts: Counter = Counter()
    for e in entries:
        bc_counts.update(e.bc_ids)
    severity_counts = Counter(e.severity for e in entries if e.feedback_type != "acknowledgment")

    total = len(entries)
    reprimands = type_counts.get("reprimand", 0)
    acknowledgments = type_counts.get("acknowledgment", 0)
    self_detected = type_counts.get("self_detected", 0)

    # é•åç³» (reprimand + self_detected) ã®ã†ã¡è‡ªå·±æ¤œå‡ºã®ç‡
    violation_total = reprimands + self_detected
    self_detection_rate = (self_detected / violation_total * 100) if violation_total > 0 else 0

    # å±è²¬ç‡ = reprimand / (reprimand + acknowledgment)
    feedback_total = reprimands + acknowledgments
    reprimand_rate = (reprimands / feedback_total * 100) if feedback_total > 0 else 0

    # Creator ã®è¨€è‘‰ã‚µãƒ³ãƒ—ãƒ« (ç›´è¿‘5ä»¶)
    creator_samples = [
        {"type": e.feedback_type, "words": e.creator_words, "date": e.timestamp[:10]}
        for e in reversed(entries)
        if e.creator_words
    ][:5]

    return {
        "total": total,
        "by_type": dict(type_counts.most_common()),
        "by_pattern": dict(pattern_counts.most_common()),
        "by_bc": dict(bc_counts.most_common()),
        "by_severity": dict(severity_counts),
        "reprimand_rate": round(reprimand_rate, 1),
        "self_detection_rate": round(self_detection_rate, 1),
        "creator_words_samples": creator_samples,
    }


def compute_trend(entries: list[FeedbackEntry], weeks: int = 4) -> list[dict]:
    """é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¨ˆç®—ã€‚"""
    now = datetime.now()
    trend = []
    for w in range(weeks - 1, -1, -1):
        start = now - timedelta(weeks=w + 1)
        end = now - timedelta(weeks=w)
        week_entries = [
            e for e in entries
            if start <= _parse_ts(e.timestamp) < end
        ]
        reprimands = sum(1 for e in week_entries if e.feedback_type == "reprimand")
        acks = sum(1 for e in week_entries if e.feedback_type == "acknowledgment")
        self_det = sum(1 for e in week_entries if e.feedback_type == "self_detected")
        trend.append({
            "week": f"W{weeks - w}",
            "start": start.strftime("%m/%d"),
            "end": end.strftime("%m/%d"),
            "reprimands": reprimands,
            "acknowledgments": acks,
            "self_detected": self_det,
            "total": len(week_entries),
        })
    return trend


# ============================================================
# Dashboard Formatter
# ============================================================

def format_dashboard(
    entries: list[FeedbackEntry],
    period: str = "all",
) -> str:
    """CLIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ç”Ÿæˆã€‚"""
    # Period filter
    if period == "today":
        entries = filter_entries(entries, since_days=0)
    elif period == "week":
        entries = filter_entries(entries, since_days=7)
    elif period == "month":
        entries = filter_entries(entries, since_days=30)

    stats = compute_stats(entries)
    trend = compute_trend(entries)

    lines = [
        "ğŸ“Š BCé•åãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        f"   æœŸé–“: {period} | ç·ä»¶æ•°: {stats['total']}",
        "â”" * 50,
        "",
    ]

    # Type breakdown
    lines.append("ğŸ“‹ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç¨®åˆ¥")
    for t, icon in TYPE_ICONS.items():
        count = stats["by_type"].get(t, 0)
        bar = "â–ˆ" * count
        label = {"reprimand": "å±è²¬", "acknowledgment": "æ‰¿èª", "self_detected": "è‡ªå·±æ¤œå‡º"}[t]
        lines.append(f"  {icon} {label:8s}: {bar} {count}")
    lines.append("")

    # Reprimand rate vs self-detection rate
    lines.append("ğŸ“ˆ æŒ‡æ¨™")
    lines.append(f"  å±è²¬ç‡: {stats['reprimand_rate']}% (å±è²¬ / (å±è²¬+æ‰¿èª))")
    lines.append(f"  è‡ªå·±æ¤œå‡ºç‡: {stats['self_detection_rate']}% (è‡ªå·±æ¤œå‡º / (å±è²¬+è‡ªå·±æ¤œå‡º))")
    lines.append("")

    # Severity
    if stats["by_severity"]:
        lines.append("ğŸ”´ æ·±åˆ»åº¦åˆ†å¸ƒ")
        for sev in ["critical", "high", "medium", "low"]:
            count = stats["by_severity"].get(sev, 0)
            if count > 0:
                icon = SEVERITY_ICONS.get(sev, "âšª")
                bar = "â–ˆ" * count
                lines.append(f"  {icon} {sev:10s}: {bar} {count}")
        lines.append("")

    # Pattern frequency
    if stats["by_pattern"]:
        lines.append("ğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³é »åº¦")
        for pattern, count in stats["by_pattern"].items():
            name = PATTERN_NAMES.get(pattern, pattern)
            bar = "â–ˆ" * count
            lines.append(f"  {name:20s}: {bar} {count}")
        lines.append("")

    # Most violated BCs
    if stats["by_bc"]:
        lines.append("âš ï¸ é•å BC ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
        for bc, count in stats["by_bc"].items():
            bar = "â–ˆ" * count
            lines.append(f"  {bc:8s}: {bar} {count}")
        lines.append("")

    # Weekly trend
    lines.append("ğŸ“Š é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰")
    for w in trend:
        rep_bar = "âš¡" * w["reprimands"]
        ack_bar = "âœ¨" * w["acknowledgments"]
        self_bar = "ğŸ”" * w["self_detected"]
        lines.append(f"  {w['week']} ({w['start']}-{w['end']}): {rep_bar}{ack_bar}{self_bar} ({w['total']})")
    lines.append("")

    # Creator's words
    if stats["creator_words_samples"]:
        lines.append("ğŸ’¬ Creator ã®è¨€è‘‰ (ç›´è¿‘)")
        for s in stats["creator_words_samples"]:
            icon = TYPE_ICONS.get(s["type"], "")
            lines.append(f"  {icon} [{s['date']}] \"{s['words']}\"")
        lines.append("")

    return "\n".join(lines)


def format_session_summary(entries: list[FeedbackEntry], session_id: str = "") -> str:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã‚µãƒãƒªãƒ¼ (ç°¡æ½”)ã€‚"""
    if session_id:
        entries = filter_entries(entries, session_id=session_id)

    stats = compute_stats(entries)
    if stats["total"] == 0:
        return "âœ… ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã®é•åè¨˜éŒ²ãªã—"

    rep = stats["by_type"].get("reprimand", 0)
    ack = stats["by_type"].get("acknowledgment", 0)
    sd = stats["by_type"].get("self_detected", 0)

    parts = []
    if rep > 0:
        parts.append(f"âš¡å±è²¬ {rep}")
    if ack > 0:
        parts.append(f"âœ¨æ‰¿èª {ack}")
    if sd > 0:
        parts.append(f"ğŸ”è‡ªå·±æ¤œå‡º {sd}")

    return f"ğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³: {' | '.join(parts)} | è‡ªå·±æ¤œå‡ºç‡: {stats['self_detection_rate']}%"


# ============================================================
# CLI
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="BCé•åãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ³ã‚¸ãƒ³ â€” Creator ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¿½è·¡"
    )
    sub = parser.add_subparsers(dest="command")

    # log ã‚³ãƒãƒ³ãƒ‰
    log_parser = sub.add_parser("log", help="ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²")
    log_parser.add_argument("--type", required=True, choices=sorted(FEEDBACK_TYPES),
                           help="ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç¨®åˆ¥")
    log_parser.add_argument("--bc", type=str, default="",
                           help="BC ID (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š: BC-1,BC-3)")
    log_parser.add_argument("--pattern", type=str, default="",
                           help="ãƒ‘ã‚¿ãƒ¼ãƒ³ID")
    log_parser.add_argument("--severity", type=str, default="medium",
                           choices=sorted(SEVERITY_ORDER.keys()),
                           help="æ·±åˆ»åº¦")
    log_parser.add_argument("--description", type=str, default="",
                           help="ä½•ãŒèµ·ããŸã‹")
    log_parser.add_argument("--context", type=str, default="",
                           help="æ–‡è„ˆ")
    log_parser.add_argument("--creator-words", type=str, default="",
                           help="Creator ã®åŸæ–‡")
    log_parser.add_argument("--corrective", type=str, default="",
                           help="æ˜¯æ­£è¡Œå‹•")
    log_parser.add_argument("--session-id", type=str, default="",
                           help="ã‚»ãƒƒã‚·ãƒ§ãƒ³ID")

    # stats ã‚³ãƒãƒ³ãƒ‰
    sub.add_parser("stats", help="çµ±è¨ˆã‚’è¡¨ç¤º")

    # dashboard ã‚³ãƒãƒ³ãƒ‰
    dash_parser = sub.add_parser("dashboard", help="ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’è¡¨ç¤º")
    dash_parser.add_argument("--period", type=str, default="all",
                            choices=["today", "week", "month", "all"],
                            help="æœŸé–“")
    dash_parser.add_argument("--json", action="store_true", help="JSONå‡ºåŠ›")

    args = parser.parse_args()

    if args.command == "log":
        bc_ids = [b.strip() for b in args.bc.split(",") if b.strip()]
        entry = FeedbackEntry(
            timestamp=datetime.now().isoformat(),
            feedback_type=args.type,
            bc_ids=bc_ids,
            pattern=args.pattern,
            severity=args.severity,
            description=args.description,
            context=args.context,
            creator_words=args.creator_words,
            corrective=args.corrective,
            session_id=args.session_id,
        )
        path = log_entry(entry)
        icon = TYPE_ICONS.get(args.type, "")
        print(f"{icon} è¨˜éŒ²å®Œäº†: {path}")
        print(f"   ç¨®åˆ¥: {args.type} | BC: {bc_ids} | ãƒ‘ã‚¿ãƒ¼ãƒ³: {args.pattern}")

        # è¨˜éŒ²å¾Œã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        all_entries = read_all_entries()
        print()
        print(format_session_summary(all_entries))

    elif args.command == "stats":
        entries = read_all_entries()
        stats = compute_stats(entries)
        print(json.dumps(stats, ensure_ascii=False, indent=2))

    elif args.command == "dashboard":
        entries = read_all_entries()
        if args.json:
            stats = compute_stats(entries)
            print(json.dumps(stats, ensure_ascii=False, indent=2))
        else:
            print(format_dashboard(entries, period=args.period))

    else:
        parser.print_help()

    sys.exit(0)


if __name__ == "__main__":
    main()
