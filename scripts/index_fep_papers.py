#!/usr/bin/env python3
# PROOF: [L3/ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£] <- scripts/ O4â†’åé›†ã—ãŸè«–æ–‡ã‚’GnÅsisã«æŠ•å…¥
# PURPOSE: collected_papers.json â†’ GnÅsis LanceDB ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¸ã®æŠ•å…¥
"""
FEP Papers â†’ GnÅsis Indexer
=============================

collect_fep_papers.py ã§åé›†ã—ãŸè«–æ–‡ã‚’ GnÅsis (LanceDB) ã«æŠ•å…¥ã™ã‚‹ã€‚

æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒ:
    primary_key, title, source, abstract, content, authors,
    doi, arxiv_id, url, citations, vector (dim=1024)

Usage:
    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆå¤‰æ›ã®ã¿ã€æŠ•å…¥ã—ãªã„ï¼‰
    python scripts/index_fep_papers.py --dry-run

    # æœ¬ç•ªå®Ÿè¡Œ
    python scripts/index_fep_papers.py

    # æŠ•å…¥å¾Œã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆã‚’è¡¨ç¤º
    python scripts/index_fep_papers.py --stats
"""

import sys
import json
from pathlib import Path

# Project root
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from mekhane.anamnesis.index import GnosisIndex, Embedder

# Input / Output
COLLECTED_FILE = PROJECT_ROOT / "data" / "fep_papers" / "collected_papers.json"
GNOSIS_DIR = PROJECT_ROOT / "gnosis_data"
LANCE_DIR = GNOSIS_DIR / "lancedb"
TABLE_NAME = "knowledge"


def build_records(collected: list[dict]) -> list[dict]:
    """collected_papers.json â†’ æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ¼ãƒæº–æ‹ ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›"""
    records: list[dict] = []
    skipped = 0

    for entry in collected:
        abstract = entry.get("abstract") or ""
        if len(abstract) < 20:
            skipped += 1
            continue

        paper_id = entry.get("paper_id", "")
        doi = entry.get("doi") or ""
        arxiv_id = entry.get("arxiv_id") or ""
        year = entry.get("year")
        authors = entry.get("authors", [])

        # primary_key: DOI > arXiv > source:id
        if doi:
            pk = f"doi:{doi}"
        elif arxiv_id:
            pk = f"arxiv:{arxiv_id}"
        else:
            pk = f"semantic_scholar:{paper_id}"

        title = entry.get("title", "")

        record = {
            "primary_key": pk,
            "title": title,
            "source": "semantic_scholar",
            "abstract": abstract[:2000],
            "content": f"{title} {abstract[:1000]}",  # embedding_text ç›¸å½“
            "authors": ", ".join(authors[:10]) if authors else "",
            "doi": doi,
            "arxiv_id": arxiv_id,
            "url": entry.get("url", ""),
            "citations": entry.get("citation_count", 0) or 0,
        }
        records.append(record)

    print(f"  å¤‰æ›: {len(records)} records (abstract ä¸è¶³ã§ {skipped} skip)")
    return records


def dedupe_against_index(records: list[dict], db) -> list[dict]:
    """æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã®é‡è¤‡é™¤å»"""
    import lancedb
    from mekhane.anamnesis.lancedb_compat import get_table_names

    if TABLE_NAME not in get_table_names(db):
        return records

    table = db.open_table(TABLE_NAME)

    # æ—¢å­˜ã® primary_key ã¨æ­£è¦åŒ– title ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    existing_pks = set()
    existing_titles = set()
    try:
        all_rows = table.to_pandas()
        existing_pks = set(all_rows["primary_key"].tolist())
        existing_titles = {
            t.lower().replace(" ", "").replace("-", "")
            for t in all_rows["title"].tolist()
            if t
        }
    except Exception as e:
        print(f"  âš ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}")

    new_records = []
    for r in records:
        if r["primary_key"] in existing_pks:
            continue
        norm_title = r["title"].lower().replace(" ", "").replace("-", "")
        if norm_title and norm_title in existing_titles:
            continue
        new_records.append(r)
        existing_pks.add(r["primary_key"])
        if norm_title:
            existing_titles.add(norm_title)

    print(f"  é‡è¤‡é™¤å»: {len(records)} â†’ {len(new_records)} (æ—¢å­˜ {len(records) - len(new_records)} ä»¶)")
    return new_records


def main():
    import argparse
    import lancedb

    parser = argparse.ArgumentParser(description="FEP Papers â†’ GnÅsis Indexer")
    parser.add_argument("--dry-run", action="store_true", help="å¤‰æ›ã®ã¿ã€æŠ•å…¥ã—ãªã„")
    parser.add_argument("--stats", action="store_true", help="æŠ•å…¥å¾Œã«çµ±è¨ˆè¡¨ç¤º")
    args = parser.parse_args()

    if not COLLECTED_FILE.exists():
        print(f"âŒ {COLLECTED_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« collect_fep_papers.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    # èª­ã¿è¾¼ã¿
    collected = json.loads(COLLECTED_FILE.read_text())
    print(f"ğŸ“‚ èª­è¾¼: {len(collected)} papers from {COLLECTED_FILE.name}")

    # ãƒ¬ã‚³ãƒ¼ãƒ‰æ§‹ç¯‰
    records = build_records(collected)

    if args.dry_run:
        print(f"\nğŸ” DRY RUN â€” æŠ•å…¥ã—ã¾ã›ã‚“")
        print(f"\n  ã‚µãƒ³ãƒ—ãƒ« (top 5):")
        for r in records[:5]:
            year = "?"
            print(f"    {r['citations']:>5}c | {r['title'][:60]}")
            print(f"      Key: {r['primary_key']}")
            print(f"      Abstract: {r['abstract'][:80]}...")
        return

    # DB æ¥ç¶š & é‡è¤‡é™¤å»
    LANCE_DIR.mkdir(parents=True, exist_ok=True)
    db = lancedb.connect(str(LANCE_DIR))
    records = dedupe_against_index(records, db)

    if not records:
        print("  âœ… å…¨ã¦æ—¢å­˜ã€‚æŠ•å…¥ä¸è¦ã€‚")
        return

    # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
    print(f"\nğŸš€ Embeddingç”Ÿæˆä¸­... ({len(records)} records)")
    embedder = Embedder()

    BATCH_SIZE = 32
    for i in range(0, len(records), BATCH_SIZE):
        batch = records[i : i + BATCH_SIZE]
        texts = [r["content"] for r in batch]
        vectors = embedder.embed_batch(texts)
        for r, v in zip(batch, vectors):
            r["vector"] = v
        print(f"  Processed {min(i + BATCH_SIZE, len(records))}/{len(records)}...")

    # LanceDB ã«è¿½åŠ 
    from mekhane.anamnesis.lancedb_compat import get_table_names

    if TABLE_NAME in get_table_names(db):
        table = db.open_table(TABLE_NAME)
        table.add(records)
    else:
        db.create_table(TABLE_NAME, data=records)

    print(f"\nâœ… æŠ•å…¥å®Œäº†: {len(records)} papers added to GnÅsis")

    # çµ±è¨ˆ
    index = GnosisIndex()
    stats = index.stats()
    print(f"\nğŸ“Š GnÅsis Index Stats:")
    for k, v in stats.items():
        print(f"    {k}: {v}")


if __name__ == "__main__":
    main()
